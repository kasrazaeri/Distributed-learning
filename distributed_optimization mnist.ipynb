{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DOL_Pr2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "j9X_i0BhPxCI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os, sys \n",
        "import random \n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "from torchvision.transforms import ToTensor\n",
        "import torch \n",
        "from torch import linalg as LA\n",
        "from torch import Tensor\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import random_split\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import SubsetRandomSampler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "cLiGYAROQIRq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4163e3bf-4eed-4b96-ab78-96c5adffe185"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mnist_trainset = datasets.MNIST(root='/content/drive/My Drive', train=True, download=True, transform=ToTensor())\n",
        "mnist_testset = datasets.MNIST(root='/content/drive/My Drive', train=False, download=True, transform=ToTensor())"
      ],
      "metadata": {
        "id": "3W2pOy84QITx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "uwgxMHW_QIWK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3995562-b4f8-4ece-b0c4-000abeea1ece"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        # defining convolution layers\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=5)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=2)                             \n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5)         \n",
        "        # defining fully connected layers\n",
        "        self.fc1 = nn.Linear(1024, 512)\n",
        "        # self.dropout = nn.Dropout2d()\n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "        # self.softmax nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.conv2(x)\n",
        "        # x = self.dropout(x)\n",
        "        x = self.maxpool(x)\n",
        "        # flatten the output of conv2 to (batch_size, 32 * 7 * 7)\n",
        "        x = x.view(x.size(0), -1)       \n",
        "        x = F.relu(self.fc1(x))\n",
        "        # x = F.dropout(x)\n",
        "        output = self.fc2(x)\n",
        "        return output    "
      ],
      "metadata": {
        "id": "0LYVPv-gQIYN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_messages(queue_i, weights_i, alpha_i, with_noise, alpha_update):\n",
        "    while len(queue_i) != 0:\n",
        "        weights_j, alpha_j = queue_i.pop()\n",
        "        for layer in weights_i.keys():\n",
        "            if  with_noise == True:\n",
        "                noise = torch.randn_like(weights_j[layer])\n",
        "            else:\n",
        "                noise = 0    \n",
        "            \n",
        "            weights_i[layer] = (alpha_j/(alpha_i+alpha_j)) * (weights_j[layer] + 0.1*noise) + (alpha_i/(alpha_i+alpha_j)) * weights_i[layer]\n",
        "            if alpha_update == True:\n",
        "                alpha_i += alpha_j\n",
        "\n",
        "    return weights_i, alpha_i        "
      ],
      "metadata": {
        "id": "-dPbh3OxjP_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def push_message(queue_j, weights_i, alpha_i):\n",
        "    queue_j.append([weights_i, alpha_i/2])"
      ],
      "metadata": {
        "id": "ywbfSAWxjQC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(iter_no, worker_num, with_noise=False, alpha_update=True):\n",
        "    net = nets_list[worker_num]\n",
        "    queue = queues_list[worker_num]\n",
        "    alpha = alpha_list[worker_num]\n",
        "\n",
        "    new_weights, new_alpha = process_messages(queue, net.state_dict(), alpha, with_noise, alpha_update)\n",
        "    net.load_state_dict(new_weights)\n",
        "    nets_list[worker_num] = net\n",
        "    alpha_list[worker_num] = new_alpha\n",
        "\n",
        "    net = nets_list[worker_num]\n",
        "    queue = queues_list[worker_num]\n",
        "    alpha = alpha_list[worker_num]\n",
        "\n",
        "    # dataset_size = len(mnist_trainset)\n",
        "    # data_indice = [np.random.choice(list(range(dataset_size)))]\n",
        "    # train_sampler = SubsetRandomSampler(data_indice)\n",
        "    # train_loader = torch.utils.data.DataLoader(dataset=mnist_trainset, shuffle=False, batch_size=batch_size, sampler=train_sampler)\n",
        "    try :\n",
        "        batch_data, batch_labels = data_iters[worker_num].next()\n",
        "    except:\n",
        "        train_loader = torch.utils.data.DataLoader(\n",
        "                 dataset=mnist_trainset,\n",
        "                 batch_size=batch_size,\n",
        "                 shuffle=True\n",
        "                 )\n",
        "    \n",
        "        data_iters[worker_num] = iter(train_loader)\n",
        "\n",
        "        batch_data, batch_labels = data_iters[worker_num].next()\n",
        "\n",
        "\n",
        "    # Transfer to GPU\n",
        "    batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
        "    # zero the parameter gradients\n",
        "    optimizer = optimizers[worker_num]\n",
        "    optimizer.zero_grad()\n",
        "    # Model computations\n",
        "    batch_outputs = net(batch_data)        \n",
        "    loss = criterion(batch_outputs, batch_labels)\n",
        "    loss.backward()\n",
        "    # print statistics\n",
        "    print('Training: ', 'Iteration No: ', iter_no+1, 'Worker Num: ', worker_num, '\\n',\n",
        "    'Loss: ', loss.item())\n",
        "    clipping_value = 1 # arbitrary value of your choosing\n",
        "    torch.nn.utils.clip_grad_norm(net.parameters(), clipping_value)\n",
        "    optimizer.step()\n",
        "    train_losses[worker_num].append(loss.item())\n",
        "\n",
        "    S = np.random.binomial(1, p, 1)\n",
        "    if S == 1:\n",
        "        j = np.random.choice(comm_matrix[worker_num])\n",
        "        push_message(queues_list[j], net.state_dict(), alpha)\n",
        "\n",
        "    nets_list[worker_num] = net   \n",
        "    optimizers[worker_num] = optimizer"
      ],
      "metadata": {
        "id": "PuxWLX7YjQH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(iter_no, networks):\n",
        "    layers = networks[0].state_dict().keys()\n",
        "    test_weights = {}\n",
        "    for layer in layers:\n",
        "        test_weights[layer] = 0\n",
        "        for network in networks:\n",
        "            test_weights[layer] += network.state_dict()[layer]\n",
        "\n",
        "        test_weights[layer] /= num_workers\n",
        "\n",
        "    test_network.load_state_dict(test_weights)    \n",
        "\n",
        "    # Validation\n",
        "    with torch.set_grad_enabled(False):\n",
        "        batch_losses = []\n",
        "        counter = 0\n",
        "        total = 0\n",
        "        for batch_data, batch_labels in test_loader:\n",
        "            total += len(batch_data)\n",
        "            # Transfer to GPU\n",
        "            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
        "            # Model computations\n",
        "            batch_outputs = test_network(batch_data)            \n",
        "            loss = criterion(batch_outputs, batch_labels)\n",
        "            batch_losses.append(loss.item())\n",
        "            \n",
        "            # compute accuracy\n",
        "            for i in range(len(batch_data)):\n",
        "                true = int(batch_labels[i])\n",
        "                pred = int(torch.argmax(batch_outputs[i,:]))\n",
        "                if pred == true:\n",
        "                    counter += 1\n",
        "\n",
        "        test_losses.append(sum(batch_losses) / len(batch_losses))\n",
        "        print('Test: ', 'Iteration No: ', iter_no+1,'\\n',\n",
        "            'Loss: ', test_losses[-1])\n",
        "        acc = (counter*100) / total   \n",
        "        test_accuracy.append(acc)    \n",
        "        print('Test accuracy: ', acc) \n",
        "    if acc >= 96:\n",
        "        print('96 % accuracy reached!')\n",
        "        \n",
        "    return acc"
      ],
      "metadata": {
        "id": "49GBs5MenPve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GOSGD"
      ],
      "metadata": {
        "id": "3HpCSWvEoFSc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_workers = 8\n",
        "all_workers = [i for i in range (num_workers)]"
      ],
      "metadata": {
        "id": "4e6toNrdQIbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nets_list = [CNN().to(device)]\n",
        "optimizers = [optim.SGD(nets_list[0].parameters(), lr = 0.1)]\n",
        "# schedulers = [optim.lr_scheduler.MultiStepLR(optimizers[0], milestones=[210], gamma=0.1)]\n",
        "for i in range(1,num_workers):\n",
        "    new_net = CNN().to(device)\n",
        "    new_net.load_state_dict(nets_list[0].state_dict())\n",
        "    nets_list.append(new_net)\n",
        "    optimizer = optim.SGD(new_net.parameters(), lr = 0.1) \n",
        "    optimizers.append(optimizer)\n",
        "    # scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[210], gamma=0.1)\n",
        "    # schedulers.append(scheduler)"
      ],
      "metadata": {
        "id": "YzjoDbgDAaXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_network = CNN().to(device)"
      ],
      "metadata": {
        "id": "nzokBBzmpPir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "queues_list = [[] for i in range (num_workers)]\n",
        "alpha_list = [1 / num_workers for i in range (num_workers)]"
      ],
      "metadata": {
        "id": "HvBxOLGsAc3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_iters = 1000\n",
        "p = 0.5\n",
        "batch_size = 128"
      ],
      "metadata": {
        "id": "V2u8h5l0EUk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_iters = []\n",
        "for i in range(num_workers):\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "                 dataset=mnist_trainset,\n",
        "                 batch_size=batch_size,\n",
        "                 shuffle=True\n",
        "                 )\n",
        "    \n",
        "    data_iters.append(iter(train_loader))"
      ],
      "metadata": {
        "id": "JnxxHKOP_GtW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader = torch.utils.data.DataLoader(\n",
        "                dataset=mnist_testset,\n",
        "                batch_size=batch_size,\n",
        "                shuffle=False\n",
        "                )"
      ],
      "metadata": {
        "id": "9QadYDElrfpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "train_losses = [[] for i in range (num_workers)]\n",
        "test_losses = []\n",
        "test_accuracy = []"
      ],
      "metadata": {
        "id": "u-w1r0l9A_2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comm_matrix = []\n",
        "for i in range (num_workers):\n",
        "    adj_nodes = list(range(num_workers))\n",
        "    del adj_nodes[i]\n",
        "    comm_matrix.append(adj_nodes)"
      ],
      "metadata": {
        "id": "tE8w0y2pFq09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for iter_no in range(max_iters):\n",
        "    for worker_num in range(num_workers):\n",
        "        train(iter_no, worker_num)\n",
        "    acc = test(iter_no, nets_list)\n",
        "    if acc >= 96 :\n",
        "        break\n",
        "\n",
        "    # for scheduler in schedulers:\n",
        "    #     scheduler.step()    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHkqSvSz8Ary",
        "outputId": "8a7640c1-c091-4b7f-f0f8-70a749cada60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training:  Iteration No:  1 Worker Num:  0 \n",
            " Loss:  2.301302671432495\n",
            "Training:  Iteration No:  1 Worker Num:  1 \n",
            " Loss:  2.3067712783813477\n",
            "Training:  Iteration No:  1 Worker Num:  2 \n",
            " Loss:  2.306518077850342\n",
            "Training:  Iteration No:  1 Worker Num:  3 \n",
            " Loss:  2.3070125579833984\n",
            "Training:  Iteration No:  1 Worker Num:  4 \n",
            " Loss:  2.2927677631378174\n",
            "Training:  Iteration No:  1 Worker Num:  5 \n",
            " Loss:  2.3039631843566895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            " Loss:  0.39585353368068044\n",
            "Test accuracy:  88.76\n",
            "Training:  Iteration No:  90 Worker Num:  0 \n",
            " Loss:  0.47639667987823486\n",
            "Training:  Iteration No:  90 Worker Num:  1 \n",
            " Loss:  0.3697565793991089\n",
            "Training:  Iteration No:  90 Worker Num:  2 \n",
            " Loss:  0.5603494644165039\n",
            "Training:  Iteration No:  90 Worker Num:  3 \n",
            " Loss:  0.3840983211994171\n",
            "Training:  Iteration No:  90 Worker Num:  4 \n",
            " Loss:  0.48018163442611694\n",
            "Training:  Iteration No:  90 Worker Num:  5 \n",
            " Loss:  0.42834582924842834\n",
            "Training:  Iteration No:  90 Worker Num:  6 \n",
            " Loss:  0.48394283652305603\n",
            "Training:  Iteration No:  90 Worker Num:  7 \n",
            " Loss:  0.532922089099884\n",
            "Test:  Iteration No:  90 \n",
            " Loss:  0.3857062511806247\n",
            "Test accuracy:  89.26\n",
            "Training:  Iteration No:  91 Worker Num:  0 \n",
            " Loss:  0.3828847408294678\n",
            "Training:  Iteration No:  91 Worker Num:  1 \n",
            " Loss:  0.336464524269104\n",
            "Training:  Iteration No:  91 Worker Num:  2 \n",
            " Loss:  0.43014800548553467\n",
            "Training:  Iteration No:  91 Worker Num:  3 \n",
            " Loss:  0.4156133532524109\n",
            "Training:  Iteration No:  91 Worker Num:  4 \n",
            " Loss:  0.5260251760482788\n",
            "Training:  Iteration No:  91 Worker Num:  5 \n",
            " Loss:  0.48188158869743347\n",
            "Training:  Iteration No:  91 Worker Num:  6 \n",
            " Loss:  0.40821003913879395\n",
            "Training:  Iteration No:  91 Worker Num:  7 \n",
            " Loss:  0.48383304476737976\n",
            "Test:  Iteration No:  91 \n",
            " Loss:  0.3918232838564281\n",
            "Test accuracy:  88.84\n",
            "Training:  Iteration No:  92 Worker Num:  0 \n",
            " Loss:  0.44880038499832153\n",
            "Training:  Iteration No:  92 Worker Num:  1 \n",
            " Loss:  0.5271837115287781\n",
            "Training:  Iteration No:  92 Worker Num:  2 \n",
            " Loss:  0.5744603276252747\n",
            "Training:  Iteration No:  92 Worker Num:  3 \n",
            " Loss:  0.41981399059295654\n",
            "Training:  Iteration No:  92 Worker Num:  4 \n",
            " Loss:  0.5251442193984985\n",
            "Training:  Iteration No:  92 Worker Num:  5 \n",
            " Loss:  0.4442959129810333\n",
            "Training:  Iteration No:  92 Worker Num:  6 \n",
            " Loss:  0.3512439429759979\n",
            "Training:  Iteration No:  92 Worker Num:  7 \n",
            " Loss:  0.48658037185668945\n",
            "Test:  Iteration No:  92 \n",
            " Loss:  0.38332704699869397\n",
            "Test accuracy:  89.37\n",
            "Training:  Iteration No:  93 Worker Num:  0 \n",
            " Loss:  0.4800431728363037\n",
            "Training:  Iteration No:  93 Worker Num:  1 \n",
            " Loss:  0.42561954259872437\n",
            "Training:  Iteration No:  93 Worker Num:  2 \n",
            " Loss:  0.3931783437728882\n",
            "Training:  Iteration No:  93 Worker Num:  3 \n",
            " Loss:  0.41292604804039\n",
            "Training:  Iteration No:  93 Worker Num:  4 \n",
            " Loss:  0.4743492305278778\n",
            "Training:  Iteration No:  93 Worker Num:  5 \n",
            " Loss:  0.4395003020763397\n",
            "Training:  Iteration No:  93 Worker Num:  6 \n",
            " Loss:  0.3853876292705536\n",
            "Training:  Iteration No:  93 Worker Num:  7 \n",
            " Loss:  0.34387460350990295\n",
            "Test:  Iteration No:  93 \n",
            " Loss:  0.38727340979289404\n",
            "Test accuracy:  88.78\n",
            "Training:  Iteration No:  94 Worker Num:  0 \n",
            " Loss:  0.4398719370365143\n",
            "Training:  Iteration No:  94 Worker Num:  1 \n",
            " Loss:  0.38079535961151123\n",
            "Training:  Iteration No:  94 Worker Num:  2 \n",
            " Loss:  0.4300767779350281\n",
            "Training:  Iteration No:  94 Worker Num:  3 \n",
            " Loss:  0.4123958945274353\n",
            "Training:  Iteration No:  94 Worker Num:  4 \n",
            " Loss:  0.6638883352279663\n",
            "Training:  Iteration No:  94 Worker Num:  5 \n",
            " Loss:  0.5223419070243835\n",
            "Training:  Iteration No:  94 Worker Num:  6 \n",
            " Loss:  0.4547400176525116\n",
            "Training:  Iteration No:  94 Worker Num:  7 \n",
            " Loss:  0.4591127932071686\n",
            "Test:  Iteration No:  94 \n",
            " Loss:  0.38518144077138056\n",
            "Test accuracy:  89.43\n",
            "Training:  Iteration No:  95 Worker Num:  0 \n",
            " Loss:  0.5665168762207031\n",
            "Training:  Iteration No:  95 Worker Num:  1 \n",
            " Loss:  0.4654563069343567\n",
            "Training:  Iteration No:  95 Worker Num:  2 \n",
            " Loss:  0.5355136394500732\n",
            "Training:  Iteration No:  95 Worker Num:  3 \n",
            " Loss:  0.38800397515296936\n",
            "Training:  Iteration No:  95 Worker Num:  4 \n",
            " Loss:  0.5993983149528503\n",
            "Training:  Iteration No:  95 Worker Num:  5 \n",
            " Loss:  0.394004225730896\n",
            "Training:  Iteration No:  95 Worker Num:  6 \n",
            " Loss:  0.39431509375572205\n",
            "Training:  Iteration No:  95 Worker Num:  7 \n",
            " Loss:  0.4725113809108734\n",
            "Test:  Iteration No:  95 \n",
            " Loss:  0.38555357454321054\n",
            "Test accuracy:  89.41\n",
            "Training:  Iteration No:  96 Worker Num:  0 \n",
            " Loss:  0.4748154878616333\n",
            "Training:  Iteration No:  96 Worker Num:  1 \n",
            " Loss:  0.46762141585350037\n",
            "Training:  Iteration No:  96 Worker Num:  2 \n",
            " Loss:  0.3575434982776642\n",
            "Training:  Iteration No:  96 Worker Num:  3 \n",
            " Loss:  0.4837563633918762\n",
            "Training:  Iteration No:  96 Worker Num:  4 \n",
            " Loss:  0.5164558291435242\n",
            "Training:  Iteration No:  96 Worker Num:  5 \n",
            " Loss:  0.3716278672218323\n",
            "Training:  Iteration No:  96 Worker Num:  6 \n",
            " Loss:  0.494489848613739\n",
            "Training:  Iteration No:  96 Worker Num:  7 \n",
            " Loss:  0.4647010564804077\n",
            "Test:  Iteration No:  96 \n",
            " Loss:  0.3743412769104861\n",
            "Test accuracy:  89.38\n",
            "Training:  Iteration No:  97 Worker Num:  0 \n",
            " Loss:  0.5302608013153076\n",
            "Training:  Iteration No:  97 Worker Num:  1 \n",
            " Loss:  0.4170680344104767\n",
            "Training:  Iteration No:  97 Worker Num:  2 \n",
            " Loss:  0.5388572812080383\n",
            "Training:  Iteration No:  97 Worker Num:  3 \n",
            " Loss:  0.5610137581825256\n",
            "Training:  Iteration No:  97 Worker Num:  4 \n",
            " Loss:  0.4804445505142212\n",
            "Training:  Iteration No:  97 Worker Num:  5 \n",
            " Loss:  0.33695805072784424\n",
            "Training:  Iteration No:  97 Worker Num:  6 \n",
            " Loss:  0.46859288215637207\n",
            "Training:  Iteration No:  97 Worker Num:  7 \n",
            " Loss:  0.5470842719078064\n",
            "Test:  Iteration No:  97 \n",
            " Loss:  0.3716416038289855\n",
            "Test accuracy:  89.8\n",
            "Training:  Iteration No:  98 Worker Num:  0 \n",
            " Loss:  0.4055861532688141\n",
            "Training:  Iteration No:  98 Worker Num:  1 \n",
            " Loss:  0.5634830594062805\n",
            "Training:  Iteration No:  98 Worker Num:  2 \n",
            " Loss:  0.35056212544441223\n",
            "Training:  Iteration No:  98 Worker Num:  3 \n",
            " Loss:  0.3586118817329407\n",
            "Training:  Iteration No:  98 Worker Num:  4 \n",
            " Loss:  0.3660656213760376\n",
            "Training:  Iteration No:  98 Worker Num:  5 \n",
            " Loss:  0.5144445896148682\n",
            "Training:  Iteration No:  98 Worker Num:  6 \n",
            " Loss:  0.44612663984298706\n",
            "Training:  Iteration No:  98 Worker Num:  7 \n",
            " Loss:  0.41722431778907776\n",
            "Test:  Iteration No:  98 \n",
            " Loss:  0.37455031030540226\n",
            "Test accuracy:  89.34\n",
            "Training:  Iteration No:  99 Worker Num:  0 \n",
            " Loss:  0.5054453015327454\n",
            "Training:  Iteration No:  99 Worker Num:  1 \n",
            " Loss:  0.41095662117004395\n",
            "Training:  Iteration No:  99 Worker Num:  2 \n",
            " Loss:  0.5737151503562927\n",
            "Training:  Iteration No:  99 Worker Num:  3 \n",
            " Loss:  0.3830950856208801\n",
            "Training:  Iteration No:  99 Worker Num:  4 \n",
            " Loss:  0.537984311580658\n",
            "Training:  Iteration No:  99 Worker Num:  5 \n",
            " Loss:  0.36404281854629517\n",
            "Training:  Iteration No:  99 Worker Num:  6 \n",
            " Loss:  0.46961456537246704\n",
            "Training:  Iteration No:  99 Worker Num:  7 \n",
            " Loss:  0.5472016930580139\n",
            "Test:  Iteration No:  99 \n",
            " Loss:  0.36743960491841354\n",
            "Test accuracy:  89.74\n",
            "Training:  Iteration No:  100 Worker Num:  0 \n",
            " Loss:  0.3223479390144348\n",
            "Training:  Iteration No:  100 Worker Num:  1 \n",
            " Loss:  0.44466331601142883\n",
            "Training:  Iteration No:  100 Worker Num:  2 \n",
            " Loss:  0.50545734167099\n",
            "Training:  Iteration No:  100 Worker Num:  3 \n",
            " Loss:  0.5196055173873901\n",
            "Training:  Iteration No:  100 Worker Num:  4 \n",
            " Loss:  0.42064669728279114\n",
            "Training:  Iteration No:  100 Worker Num:  5 \n",
            " Loss:  0.4314865171909332\n",
            "Training:  Iteration No:  100 Worker Num:  6 \n",
            " Loss:  0.45522817969322205\n",
            "Training:  Iteration No:  100 Worker Num:  7 \n",
            " Loss:  0.40136608481407166\n",
            "Test:  Iteration No:  100 \n",
            " Loss:  0.36858350901475434\n",
            "Test accuracy:  89.88\n",
            "Training:  Iteration No:  101 Worker Num:  0 \n",
            " Loss:  0.5692968964576721\n",
            "Training:  Iteration No:  101 Worker Num:  1 \n",
            " Loss:  0.48647746443748474\n",
            "Training:  Iteration No:  101 Worker Num:  2 \n",
            " Loss:  0.3680154085159302\n",
            "Training:  Iteration No:  101 Worker Num:  3 \n",
            " Loss:  0.4103240966796875\n",
            "Training:  Iteration No:  101 Worker Num:  4 \n",
            " Loss:  0.6256536841392517\n",
            "Training:  Iteration No:  101 Worker Num:  5 \n",
            " Loss:  0.41946548223495483\n",
            "Training:  Iteration No:  101 Worker Num:  6 \n",
            " Loss:  0.44303539395332336\n",
            "Training:  Iteration No:  101 Worker Num:  7 \n",
            " Loss:  0.46322715282440186\n",
            "Test:  Iteration No:  101 \n",
            " Loss:  0.36433934844747373\n",
            "Test accuracy:  89.76\n",
            "Training:  Iteration No:  102 Worker Num:  0 \n",
            " Loss:  0.5640023946762085\n",
            "Training:  Iteration No:  102 Worker Num:  1 \n",
            " Loss:  0.5379091501235962\n",
            "Training:  Iteration No:  102 Worker Num:  2 \n",
            " Loss:  0.47135061025619507\n",
            "Training:  Iteration No:  102 Worker Num:  3 \n",
            " Loss:  0.47425922751426697\n",
            "Training:  Iteration No:  102 Worker Num:  4 \n",
            " Loss:  0.4366864562034607\n",
            "Training:  Iteration No:  102 Worker Num:  5 \n",
            " Loss:  0.34117981791496277\n",
            "Training:  Iteration No:  102 Worker Num:  6 \n",
            " Loss:  0.46665000915527344\n",
            "Training:  Iteration No:  102 Worker Num:  7 \n",
            " Loss:  0.37450239062309265\n",
            "Test:  Iteration No:  102 \n",
            " Loss:  0.3578547384071199\n",
            "Test accuracy:  90.0\n",
            "Training:  Iteration No:  103 Worker Num:  0 \n",
            " Loss:  0.42016321420669556\n",
            "Training:  Iteration No:  103 Worker Num:  1 \n",
            " Loss:  0.4205816984176636\n",
            "Training:  Iteration No:  103 Worker Num:  2 \n",
            " Loss:  0.49555572867393494\n",
            "Training:  Iteration No:  103 Worker Num:  3 \n",
            " Loss:  0.5208258628845215\n",
            "Training:  Iteration No:  103 Worker Num:  4 \n",
            " Loss:  0.42248475551605225\n",
            "Training:  Iteration No:  103 Worker Num:  5 \n",
            " Loss:  0.4623842239379883\n",
            "Training:  Iteration No:  103 Worker Num:  6 \n",
            " Loss:  0.4026505947113037\n",
            "Training:  Iteration No:  103 Worker Num:  7 \n",
            " Loss:  0.4421132802963257\n",
            "Test:  Iteration No:  103 \n",
            " Loss:  0.36632553087193753\n",
            "Test accuracy:  89.81\n",
            "Training:  Iteration No:  104 Worker Num:  0 \n",
            " Loss:  0.594424843788147\n",
            "Training:  Iteration No:  104 Worker Num:  1 \n",
            " Loss:  0.4501482844352722\n",
            "Training:  Iteration No:  104 Worker Num:  2 \n",
            " Loss:  0.35494276881217957\n",
            "Training:  Iteration No:  104 Worker Num:  3 \n",
            " Loss:  0.5521174669265747\n",
            "Training:  Iteration No:  104 Worker Num:  4 \n",
            " Loss:  0.41799941658973694\n",
            "Training:  Iteration No:  104 Worker Num:  5 \n",
            " Loss:  0.3065670430660248\n",
            "Training:  Iteration No:  104 Worker Num:  6 \n",
            " Loss:  0.6081458926200867\n",
            "Training:  Iteration No:  104 Worker Num:  7 \n",
            " Loss:  0.5498627424240112\n",
            "Test:  Iteration No:  104 \n",
            " Loss:  0.3641242380195026\n",
            "Test accuracy:  89.36\n",
            "Training:  Iteration No:  105 Worker Num:  0 \n",
            " Loss:  0.3739728629589081\n",
            "Training:  Iteration No:  105 Worker Num:  1 \n",
            " Loss:  0.54884934425354\n",
            "Training:  Iteration No:  105 Worker Num:  2 \n",
            " Loss:  0.43423742055892944\n",
            "Training:  Iteration No:  105 Worker Num:  3 \n",
            " Loss:  0.4640326499938965\n",
            "Training:  Iteration No:  105 Worker Num:  4 \n",
            " Loss:  0.4077078104019165\n",
            "Training:  Iteration No:  105 Worker Num:  5 \n",
            " Loss:  0.47108352184295654\n",
            "Training:  Iteration No:  105 Worker Num:  6 \n",
            " Loss:  0.29313623905181885\n",
            "Training:  Iteration No:  105 Worker Num:  7 \n",
            " Loss:  0.4160796105861664\n",
            "Test:  Iteration No:  105 \n",
            " Loss:  0.3559830753203434\n",
            "Test accuracy:  90.08\n",
            "Training:  Iteration No:  106 Worker Num:  0 \n",
            " Loss:  0.35872799158096313\n",
            "Training:  Iteration No:  106 Worker Num:  1 \n",
            " Loss:  0.36753320693969727\n",
            "Training:  Iteration No:  106 Worker Num:  2 \n",
            " Loss:  0.4946480691432953\n",
            "Training:  Iteration No:  106 Worker Num:  3 \n",
            " Loss:  0.3756723403930664\n",
            "Training:  Iteration No:  106 Worker Num:  4 \n",
            " Loss:  0.37500709295272827\n",
            "Training:  Iteration No:  106 Worker Num:  5 \n",
            " Loss:  0.46842291951179504\n",
            "Training:  Iteration No:  106 Worker Num:  6 \n",
            " Loss:  0.4140875041484833\n",
            "Training:  Iteration No:  106 Worker Num:  7 \n",
            " Loss:  0.4682576060295105\n",
            "Test:  Iteration No:  106 \n",
            " Loss:  0.3564157241696044\n",
            "Test accuracy:  89.6\n",
            "Training:  Iteration No:  107 Worker Num:  0 \n",
            " Loss:  0.29008814692497253\n",
            "Training:  Iteration No:  107 Worker Num:  1 \n",
            " Loss:  0.3958595395088196\n",
            "Training:  Iteration No:  107 Worker Num:  2 \n",
            " Loss:  0.5607477426528931\n",
            "Training:  Iteration No:  107 Worker Num:  3 \n",
            " Loss:  0.4946075975894928\n",
            "Training:  Iteration No:  107 Worker Num:  4 \n",
            " Loss:  0.5453449487686157\n",
            "Training:  Iteration No:  107 Worker Num:  5 \n",
            " Loss:  0.4378368854522705\n",
            "Training:  Iteration No:  107 Worker Num:  6 \n",
            " Loss:  0.4755016565322876\n",
            "Training:  Iteration No:  107 Worker Num:  7 \n",
            " Loss:  0.42851439118385315\n",
            "Test:  Iteration No:  107 \n",
            " Loss:  0.35024878599598436\n",
            "Test accuracy:  90.14\n",
            "Training:  Iteration No:  108 Worker Num:  0 \n",
            " Loss:  0.35016337037086487\n",
            "Training:  Iteration No:  108 Worker Num:  1 \n",
            " Loss:  0.3998960852622986\n",
            "Training:  Iteration No:  108 Worker Num:  2 \n",
            " Loss:  0.4395662546157837\n",
            "Training:  Iteration No:  108 Worker Num:  3 \n",
            " Loss:  0.39968982338905334\n",
            "Training:  Iteration No:  108 Worker Num:  4 \n",
            " Loss:  0.43393972516059875\n",
            "Training:  Iteration No:  108 Worker Num:  5 \n",
            " Loss:  0.2990421652793884\n",
            "Training:  Iteration No:  108 Worker Num:  6 \n",
            " Loss:  0.46358999609947205\n",
            "Training:  Iteration No:  108 Worker Num:  7 \n",
            " Loss:  0.5967121124267578\n",
            "Test:  Iteration No:  108 \n",
            " Loss:  0.3501090256945242\n",
            "Test accuracy:  90.18\n",
            "Training:  Iteration No:  109 Worker Num:  0 \n",
            " Loss:  0.39980536699295044\n",
            "Training:  Iteration No:  109 Worker Num:  1 \n",
            " Loss:  0.37159842252731323\n",
            "Training:  Iteration No:  109 Worker Num:  2 \n",
            " Loss:  0.46584782004356384\n",
            "Training:  Iteration No:  109 Worker Num:  3 \n",
            " Loss:  0.5743817687034607\n",
            "Training:  Iteration No:  109 Worker Num:  4 \n",
            " Loss:  0.3719254732131958\n",
            "Training:  Iteration No:  109 Worker Num:  5 \n",
            " Loss:  0.40488532185554504\n",
            "Training:  Iteration No:  109 Worker Num:  6 \n",
            " Loss:  0.3665485680103302\n",
            "Training:  Iteration No:  109 Worker Num:  7 \n",
            " Loss:  0.461246132850647\n",
            "Test:  Iteration No:  109 \n",
            " Loss:  0.3517947623718388\n",
            "Test accuracy:  90.05\n",
            "Training:  Iteration No:  110 Worker Num:  0 \n",
            " Loss:  0.4720506966114044\n",
            "Training:  Iteration No:  110 Worker Num:  1 \n",
            " Loss:  0.42901158332824707\n",
            "Training:  Iteration No:  110 Worker Num:  2 \n",
            " Loss:  0.4165484607219696\n",
            "Training:  Iteration No:  110 Worker Num:  3 \n",
            " Loss:  0.4967520833015442\n",
            "Training:  Iteration No:  110 Worker Num:  4 \n",
            " Loss:  0.41128650307655334\n",
            "Training:  Iteration No:  110 Worker Num:  5 \n",
            " Loss:  0.3814624845981598\n",
            "Training:  Iteration No:  110 Worker Num:  6 \n",
            " Loss:  0.42881521582603455\n",
            "Training:  Iteration No:  110 Worker Num:  7 \n",
            " Loss:  0.30227869749069214\n",
            "Test:  Iteration No:  110 \n",
            " Loss:  0.34740935129266753\n",
            "Test accuracy:  90.11\n",
            "Training:  Iteration No:  111 Worker Num:  0 \n",
            " Loss:  0.44190025329589844\n",
            "Training:  Iteration No:  111 Worker Num:  1 \n",
            " Loss:  0.6828120946884155\n",
            "Training:  Iteration No:  111 Worker Num:  2 \n",
            " Loss:  0.3522574305534363\n",
            "Training:  Iteration No:  111 Worker Num:  3 \n",
            " Loss:  0.28779539465904236\n",
            "Training:  Iteration No:  111 Worker Num:  4 \n",
            " Loss:  0.3940957486629486\n",
            "Training:  Iteration No:  111 Worker Num:  5 \n",
            " Loss:  0.39203837513923645\n",
            "Training:  Iteration No:  111 Worker Num:  6 \n",
            " Loss:  0.36384671926498413\n",
            "Training:  Iteration No:  111 Worker Num:  7 \n",
            " Loss:  0.3699830174446106\n",
            "Test:  Iteration No:  111 \n",
            " Loss:  0.34277281792292114\n",
            "Test accuracy:  90.31\n",
            "Training:  Iteration No:  112 Worker Num:  0 \n",
            " Loss:  0.44158902764320374\n",
            "Training:  Iteration No:  112 Worker Num:  1 \n",
            " Loss:  0.4877035915851593\n",
            "Training:  Iteration No:  112 Worker Num:  2 \n",
            " Loss:  0.2578667104244232\n",
            "Training:  Iteration No:  112 Worker Num:  3 \n",
            " Loss:  0.6061719059944153\n",
            "Training:  Iteration No:  112 Worker Num:  4 \n",
            " Loss:  0.40367844700813293\n",
            "Training:  Iteration No:  112 Worker Num:  5 \n",
            " Loss:  0.48853108286857605\n",
            "Training:  Iteration No:  112 Worker Num:  6 \n",
            " Loss:  0.43344616889953613\n",
            "Training:  Iteration No:  112 Worker Num:  7 \n",
            " Loss:  0.3344545364379883\n",
            "Test:  Iteration No:  112 \n",
            " Loss:  0.3436793290738818\n",
            "Test accuracy:  90.35\n",
            "Training:  Iteration No:  113 Worker Num:  0 \n",
            " Loss:  0.42897045612335205\n",
            "Training:  Iteration No:  113 Worker Num:  1 \n",
            " Loss:  0.385672926902771\n",
            "Training:  Iteration No:  113 Worker Num:  2 \n",
            " Loss:  0.35090371966362\n",
            "Training:  Iteration No:  113 Worker Num:  3 \n",
            " Loss:  0.4354536831378937\n",
            "Training:  Iteration No:  113 Worker Num:  4 \n",
            " Loss:  0.37487712502479553\n",
            "Training:  Iteration No:  113 Worker Num:  5 \n",
            " Loss:  0.40894845128059387\n",
            "Training:  Iteration No:  113 Worker Num:  6 \n",
            " Loss:  0.38462162017822266\n",
            "Training:  Iteration No:  113 Worker Num:  7 \n",
            " Loss:  0.5216931104660034\n",
            "Test:  Iteration No:  113 \n",
            " Loss:  0.362541062073617\n",
            "Test accuracy:  88.81\n",
            "Training:  Iteration No:  114 Worker Num:  0 \n",
            " Loss:  0.35288479924201965\n",
            "Training:  Iteration No:  114 Worker Num:  1 \n",
            " Loss:  0.36632734537124634\n",
            "Training:  Iteration No:  114 Worker Num:  2 \n",
            " Loss:  0.38216158747673035\n",
            "Training:  Iteration No:  114 Worker Num:  3 \n",
            " Loss:  0.44821247458457947\n",
            "Training:  Iteration No:  114 Worker Num:  4 \n",
            " Loss:  0.3186061680316925\n",
            "Training:  Iteration No:  114 Worker Num:  5 \n",
            " Loss:  0.5130412578582764\n",
            "Training:  Iteration No:  114 Worker Num:  6 \n",
            " Loss:  0.3535655438899994\n",
            "Training:  Iteration No:  114 Worker Num:  7 \n",
            " Loss:  0.4018298089504242\n",
            "Test:  Iteration No:  114 \n",
            " Loss:  0.3421221705554407\n",
            "Test accuracy:  90.13\n",
            "Training:  Iteration No:  115 Worker Num:  0 \n",
            " Loss:  0.5802507400512695\n",
            "Training:  Iteration No:  115 Worker Num:  1 \n",
            " Loss:  0.252804696559906\n",
            "Training:  Iteration No:  115 Worker Num:  2 \n",
            " Loss:  0.4478946924209595\n",
            "Training:  Iteration No:  115 Worker Num:  3 \n",
            " Loss:  0.3230668306350708\n",
            "Training:  Iteration No:  115 Worker Num:  4 \n",
            " Loss:  0.4040655791759491\n",
            "Training:  Iteration No:  115 Worker Num:  5 \n",
            " Loss:  0.29255473613739014\n",
            "Training:  Iteration No:  115 Worker Num:  6 \n",
            " Loss:  0.3789924383163452\n",
            "Training:  Iteration No:  115 Worker Num:  7 \n",
            " Loss:  0.37430474162101746\n",
            "Test:  Iteration No:  115 \n",
            " Loss:  0.34523447287044945\n",
            "Test accuracy:  89.78\n",
            "Training:  Iteration No:  116 Worker Num:  0 \n",
            " Loss:  0.38291028141975403\n",
            "Training:  Iteration No:  116 Worker Num:  1 \n",
            " Loss:  0.40272173285484314\n",
            "Training:  Iteration No:  116 Worker Num:  2 \n",
            " Loss:  0.40886861085891724\n",
            "Training:  Iteration No:  116 Worker Num:  3 \n",
            " Loss:  0.4818665385246277\n",
            "Training:  Iteration No:  116 Worker Num:  4 \n",
            " Loss:  0.40062183141708374\n",
            "Training:  Iteration No:  116 Worker Num:  5 \n",
            " Loss:  0.36408355832099915\n",
            "Training:  Iteration No:  116 Worker Num:  6 \n",
            " Loss:  0.4073839485645294\n",
            "Training:  Iteration No:  116 Worker Num:  7 \n",
            " Loss:  0.4075644612312317\n",
            "Test:  Iteration No:  116 \n",
            " Loss:  0.33541083166116403\n",
            "Test accuracy:  90.49\n",
            "Training:  Iteration No:  117 Worker Num:  0 \n",
            " Loss:  0.2692689597606659\n",
            "Training:  Iteration No:  117 Worker Num:  1 \n",
            " Loss:  0.41214990615844727\n",
            "Training:  Iteration No:  117 Worker Num:  2 \n",
            " Loss:  0.2780609726905823\n",
            "Training:  Iteration No:  117 Worker Num:  3 \n",
            " Loss:  0.563261866569519\n",
            "Training:  Iteration No:  117 Worker Num:  4 \n",
            " Loss:  0.43639248609542847\n",
            "Training:  Iteration No:  117 Worker Num:  5 \n",
            " Loss:  0.40555623173713684\n",
            "Training:  Iteration No:  117 Worker Num:  6 \n",
            " Loss:  0.4070691168308258\n",
            "Training:  Iteration No:  117 Worker Num:  7 \n",
            " Loss:  0.27919459342956543\n",
            "Test:  Iteration No:  117 \n",
            " Loss:  0.33322104651339446\n",
            "Test accuracy:  90.41\n",
            "Training:  Iteration No:  118 Worker Num:  0 \n",
            " Loss:  0.38060978055000305\n",
            "Training:  Iteration No:  118 Worker Num:  1 \n",
            " Loss:  0.5791083574295044\n",
            "Training:  Iteration No:  118 Worker Num:  2 \n",
            " Loss:  0.3774295747280121\n",
            "Training:  Iteration No:  118 Worker Num:  3 \n",
            " Loss:  0.2787543535232544\n",
            "Training:  Iteration No:  118 Worker Num:  4 \n",
            " Loss:  0.44580352306365967\n",
            "Training:  Iteration No:  118 Worker Num:  5 \n",
            " Loss:  0.32738813757896423\n",
            "Training:  Iteration No:  118 Worker Num:  6 \n",
            " Loss:  0.38574105501174927\n",
            "Training:  Iteration No:  118 Worker Num:  7 \n",
            " Loss:  0.4599900543689728\n",
            "Test:  Iteration No:  118 \n",
            " Loss:  0.3307106051169619\n",
            "Test accuracy:  90.47\n",
            "Training:  Iteration No:  119 Worker Num:  0 \n",
            " Loss:  0.42905953526496887\n",
            "Training:  Iteration No:  119 Worker Num:  1 \n",
            " Loss:  0.6293050646781921\n",
            "Training:  Iteration No:  119 Worker Num:  2 \n",
            " Loss:  0.37399664521217346\n",
            "Training:  Iteration No:  119 Worker Num:  3 \n",
            " Loss:  0.47427311539649963\n",
            "Training:  Iteration No:  119 Worker Num:  4 \n",
            " Loss:  0.3218532204627991\n",
            "Training:  Iteration No:  119 Worker Num:  5 \n",
            " Loss:  0.3559684753417969\n",
            "Training:  Iteration No:  119 Worker Num:  6 \n",
            " Loss:  0.5032996535301208\n",
            "Training:  Iteration No:  119 Worker Num:  7 \n",
            " Loss:  0.5157895684242249\n",
            "Test:  Iteration No:  119 \n",
            " Loss:  0.3282032956050921\n",
            "Test accuracy:  90.78\n",
            "Training:  Iteration No:  120 Worker Num:  0 \n",
            " Loss:  0.3461020886898041\n",
            "Training:  Iteration No:  120 Worker Num:  1 \n",
            " Loss:  0.45403429865837097\n",
            "Training:  Iteration No:  120 Worker Num:  2 \n",
            " Loss:  0.4633241891860962\n",
            "Training:  Iteration No:  120 Worker Num:  3 \n",
            " Loss:  0.33754390478134155\n",
            "Training:  Iteration No:  120 Worker Num:  4 \n",
            " Loss:  0.5421315431594849\n",
            "Training:  Iteration No:  120 Worker Num:  5 \n",
            " Loss:  0.46113067865371704\n",
            "Training:  Iteration No:  120 Worker Num:  6 \n",
            " Loss:  0.37321722507476807\n",
            "Training:  Iteration No:  120 Worker Num:  7 \n",
            " Loss:  0.41673192381858826\n",
            "Test:  Iteration No:  120 \n",
            " Loss:  0.32539946864111513\n",
            "Test accuracy:  90.78\n",
            "Training:  Iteration No:  121 Worker Num:  0 \n",
            " Loss:  0.2863653004169464\n",
            "Training:  Iteration No:  121 Worker Num:  1 \n",
            " Loss:  0.3860321044921875\n",
            "Training:  Iteration No:  121 Worker Num:  2 \n",
            " Loss:  0.28642162680625916\n",
            "Training:  Iteration No:  121 Worker Num:  3 \n",
            " Loss:  0.6725526452064514\n",
            "Training:  Iteration No:  121 Worker Num:  4 \n",
            " Loss:  0.30176445841789246\n",
            "Training:  Iteration No:  121 Worker Num:  5 \n",
            " Loss:  0.3117018938064575\n",
            "Training:  Iteration No:  121 Worker Num:  6 \n",
            " Loss:  0.32279840111732483\n",
            "Training:  Iteration No:  121 Worker Num:  7 \n",
            " Loss:  0.4444575309753418\n",
            "Test:  Iteration No:  121 \n",
            " Loss:  0.3279496025341221\n",
            "Test accuracy:  90.48\n",
            "Training:  Iteration No:  122 Worker Num:  0 \n",
            " Loss:  0.3275269567966461\n",
            "Training:  Iteration No:  122 Worker Num:  1 \n",
            " Loss:  0.3806937634944916\n",
            "Training:  Iteration No:  122 Worker Num:  2 \n",
            " Loss:  0.29675281047821045\n",
            "Training:  Iteration No:  122 Worker Num:  3 \n",
            " Loss:  0.36617597937583923\n",
            "Training:  Iteration No:  122 Worker Num:  4 \n",
            " Loss:  0.5271269083023071\n",
            "Training:  Iteration No:  122 Worker Num:  5 \n",
            " Loss:  0.3290577530860901\n",
            "Training:  Iteration No:  122 Worker Num:  6 \n",
            " Loss:  0.3889199495315552\n",
            "Training:  Iteration No:  122 Worker Num:  7 \n",
            " Loss:  0.41835081577301025\n",
            "Test:  Iteration No:  122 \n",
            " Loss:  0.32769008565552626\n",
            "Test accuracy:  90.67\n",
            "Training:  Iteration No:  123 Worker Num:  0 \n",
            " Loss:  0.2683798372745514\n",
            "Training:  Iteration No:  123 Worker Num:  1 \n",
            " Loss:  0.47108393907546997\n",
            "Training:  Iteration No:  123 Worker Num:  2 \n",
            " Loss:  0.366639107465744\n",
            "Training:  Iteration No:  123 Worker Num:  3 \n",
            " Loss:  0.46872222423553467\n",
            "Training:  Iteration No:  123 Worker Num:  4 \n",
            " Loss:  0.38664528727531433\n",
            "Training:  Iteration No:  123 Worker Num:  5 \n",
            " Loss:  0.3184055685997009\n",
            "Training:  Iteration No:  123 Worker Num:  6 \n",
            " Loss:  0.3553777039051056\n",
            "Training:  Iteration No:  123 Worker Num:  7 \n",
            " Loss:  0.48732757568359375\n",
            "Test:  Iteration No:  123 \n",
            " Loss:  0.3243004599515396\n",
            "Test accuracy:  90.41\n",
            "Training:  Iteration No:  124 Worker Num:  0 \n",
            " Loss:  0.39945611357688904\n",
            "Training:  Iteration No:  124 Worker Num:  1 \n",
            " Loss:  0.35621798038482666\n",
            "Training:  Iteration No:  124 Worker Num:  2 \n",
            " Loss:  0.5166311860084534\n",
            "Training:  Iteration No:  124 Worker Num:  3 \n",
            " Loss:  0.39685696363449097\n",
            "Training:  Iteration No:  124 Worker Num:  4 \n",
            " Loss:  0.3942314386367798\n",
            "Training:  Iteration No:  124 Worker Num:  5 \n",
            " Loss:  0.27965739369392395\n",
            "Training:  Iteration No:  124 Worker Num:  6 \n",
            " Loss:  0.4038608968257904\n",
            "Training:  Iteration No:  124 Worker Num:  7 \n",
            " Loss:  0.3308697044849396\n",
            "Test:  Iteration No:  124 \n",
            " Loss:  0.3372298386183721\n",
            "Test accuracy:  90.49\n",
            "Training:  Iteration No:  125 Worker Num:  0 \n",
            " Loss:  0.5140047073364258\n",
            "Training:  Iteration No:  125 Worker Num:  1 \n",
            " Loss:  0.5659635066986084\n",
            "Training:  Iteration No:  125 Worker Num:  2 \n",
            " Loss:  0.3918221890926361\n",
            "Training:  Iteration No:  125 Worker Num:  3 \n",
            " Loss:  0.4922647476196289\n",
            "Training:  Iteration No:  125 Worker Num:  4 \n",
            " Loss:  0.3702421188354492\n",
            "Training:  Iteration No:  125 Worker Num:  5 \n",
            " Loss:  0.5235715508460999\n",
            "Training:  Iteration No:  125 Worker Num:  6 \n",
            " Loss:  0.40822315216064453\n",
            "Training:  Iteration No:  125 Worker Num:  7 \n",
            " Loss:  0.39342203736305237\n",
            "Test:  Iteration No:  125 \n",
            " Loss:  0.3280178984697861\n",
            "Test accuracy:  90.53\n",
            "Training:  Iteration No:  126 Worker Num:  0 \n",
            " Loss:  0.44542208313941956\n",
            "Training:  Iteration No:  126 Worker Num:  1 \n",
            " Loss:  0.4130789041519165\n",
            "Training:  Iteration No:  126 Worker Num:  2 \n",
            " Loss:  0.3514467179775238\n",
            "Training:  Iteration No:  126 Worker Num:  3 \n",
            " Loss:  0.39178457856178284\n",
            "Training:  Iteration No:  126 Worker Num:  4 \n",
            " Loss:  0.3786299228668213\n",
            "Training:  Iteration No:  126 Worker Num:  5 \n",
            " Loss:  0.41829541325569153\n",
            "Training:  Iteration No:  126 Worker Num:  6 \n",
            " Loss:  0.4589763879776001\n",
            "Training:  Iteration No:  126 Worker Num:  7 \n",
            " Loss:  0.3141900599002838\n",
            "Test:  Iteration No:  126 \n",
            " Loss:  0.33036385863264905\n",
            "Test accuracy:  90.47\n",
            "Training:  Iteration No:  127 Worker Num:  0 \n",
            " Loss:  0.33402350544929504\n",
            "Training:  Iteration No:  127 Worker Num:  1 \n",
            " Loss:  0.4670194387435913\n",
            "Training:  Iteration No:  127 Worker Num:  2 \n",
            " Loss:  0.31602078676223755\n",
            "Training:  Iteration No:  127 Worker Num:  3 \n",
            " Loss:  0.3330475687980652\n",
            "Training:  Iteration No:  127 Worker Num:  4 \n",
            " Loss:  0.3090631365776062\n",
            "Training:  Iteration No:  127 Worker Num:  5 \n",
            " Loss:  0.3577713072299957\n",
            "Training:  Iteration No:  127 Worker Num:  6 \n",
            " Loss:  0.3902837634086609\n",
            "Training:  Iteration No:  127 Worker Num:  7 \n",
            " Loss:  0.35502690076828003\n",
            "Test:  Iteration No:  127 \n",
            " Loss:  0.3245872277530688\n",
            "Test accuracy:  90.56\n",
            "Training:  Iteration No:  128 Worker Num:  0 \n",
            " Loss:  0.28148818016052246\n",
            "Training:  Iteration No:  128 Worker Num:  1 \n",
            " Loss:  0.5562620162963867\n",
            "Training:  Iteration No:  128 Worker Num:  2 \n",
            " Loss:  0.5006324052810669\n",
            "Training:  Iteration No:  128 Worker Num:  3 \n",
            " Loss:  0.29732561111450195\n",
            "Training:  Iteration No:  128 Worker Num:  4 \n",
            " Loss:  0.37643131613731384\n",
            "Training:  Iteration No:  128 Worker Num:  5 \n",
            " Loss:  0.28016990423202515\n",
            "Training:  Iteration No:  128 Worker Num:  6 \n",
            " Loss:  0.41363394260406494\n",
            "Training:  Iteration No:  128 Worker Num:  7 \n",
            " Loss:  0.3247319757938385\n",
            "Test:  Iteration No:  128 \n",
            " Loss:  0.31727791568146474\n",
            "Test accuracy:  90.78\n",
            "Training:  Iteration No:  129 Worker Num:  0 \n",
            " Loss:  0.366398423910141\n",
            "Training:  Iteration No:  129 Worker Num:  1 \n",
            " Loss:  0.3200627863407135\n",
            "Training:  Iteration No:  129 Worker Num:  2 \n",
            " Loss:  0.25227224826812744\n",
            "Training:  Iteration No:  129 Worker Num:  3 \n",
            " Loss:  0.4207877516746521\n",
            "Training:  Iteration No:  129 Worker Num:  4 \n",
            " Loss:  0.32193437218666077\n",
            "Training:  Iteration No:  129 Worker Num:  5 \n",
            " Loss:  0.3473385274410248\n",
            "Training:  Iteration No:  129 Worker Num:  6 \n",
            " Loss:  0.3467693626880646\n",
            "Training:  Iteration No:  129 Worker Num:  7 \n",
            " Loss:  0.3587232232093811\n",
            "Test:  Iteration No:  129 \n",
            " Loss:  0.3174206594688983\n",
            "Test accuracy:  90.95\n",
            "Training:  Iteration No:  130 Worker Num:  0 \n",
            " Loss:  0.3788864314556122\n",
            "Training:  Iteration No:  130 Worker Num:  1 \n",
            " Loss:  0.4913550913333893\n",
            "Training:  Iteration No:  130 Worker Num:  2 \n",
            " Loss:  0.4175194501876831\n",
            "Training:  Iteration No:  130 Worker Num:  3 \n",
            " Loss:  0.3263947367668152\n",
            "Training:  Iteration No:  130 Worker Num:  4 \n",
            " Loss:  0.44224777817726135\n",
            "Training:  Iteration No:  130 Worker Num:  5 \n",
            " Loss:  0.3364235460758209\n",
            "Training:  Iteration No:  130 Worker Num:  6 \n",
            " Loss:  0.38073262572288513\n",
            "Training:  Iteration No:  130 Worker Num:  7 \n",
            " Loss:  0.43322843313217163\n",
            "Test:  Iteration No:  130 \n",
            " Loss:  0.31686566831379\n",
            "Test accuracy:  90.72\n",
            "Training:  Iteration No:  131 Worker Num:  0 \n",
            " Loss:  0.3250010013580322\n",
            "Training:  Iteration No:  131 Worker Num:  1 \n",
            " Loss:  0.571900486946106\n",
            "Training:  Iteration No:  131 Worker Num:  2 \n",
            " Loss:  0.4141184687614441\n",
            "Training:  Iteration No:  131 Worker Num:  3 \n",
            " Loss:  0.3695824444293976\n",
            "Training:  Iteration No:  131 Worker Num:  4 \n",
            " Loss:  0.39722487330436707\n",
            "Training:  Iteration No:  131 Worker Num:  5 \n",
            " Loss:  0.331977903842926\n",
            "Training:  Iteration No:  131 Worker Num:  6 \n",
            " Loss:  0.37159088253974915\n",
            "Training:  Iteration No:  131 Worker Num:  7 \n",
            " Loss:  0.3360092341899872\n",
            "Test:  Iteration No:  131 \n",
            " Loss:  0.3174269246526911\n",
            "Test accuracy:  90.81\n",
            "Training:  Iteration No:  132 Worker Num:  0 \n",
            " Loss:  0.5188627243041992\n",
            "Training:  Iteration No:  132 Worker Num:  1 \n",
            " Loss:  0.3654247224330902\n",
            "Training:  Iteration No:  132 Worker Num:  2 \n",
            " Loss:  0.40702325105667114\n",
            "Training:  Iteration No:  132 Worker Num:  3 \n",
            " Loss:  0.38801252841949463\n",
            "Training:  Iteration No:  132 Worker Num:  4 \n",
            " Loss:  0.41433343291282654\n",
            "Training:  Iteration No:  132 Worker Num:  5 \n",
            " Loss:  0.24696972966194153\n",
            "Training:  Iteration No:  132 Worker Num:  6 \n",
            " Loss:  0.4352143406867981\n",
            "Training:  Iteration No:  132 Worker Num:  7 \n",
            " Loss:  0.4172556400299072\n",
            "Test:  Iteration No:  132 \n",
            " Loss:  0.3149827231617668\n",
            "Test accuracy:  91.01\n",
            "Training:  Iteration No:  133 Worker Num:  0 \n",
            " Loss:  0.2614588439464569\n",
            "Training:  Iteration No:  133 Worker Num:  1 \n",
            " Loss:  0.47135406732559204\n",
            "Training:  Iteration No:  133 Worker Num:  2 \n",
            " Loss:  0.4296054244041443\n",
            "Training:  Iteration No:  133 Worker Num:  3 \n",
            " Loss:  0.4921652674674988\n",
            "Training:  Iteration No:  133 Worker Num:  4 \n",
            " Loss:  0.4370611906051636\n",
            "Training:  Iteration No:  133 Worker Num:  5 \n",
            " Loss:  0.3717116713523865\n",
            "Training:  Iteration No:  133 Worker Num:  6 \n",
            " Loss:  0.43821004033088684\n",
            "Training:  Iteration No:  133 Worker Num:  7 \n",
            " Loss:  0.36561569571495056\n",
            "Test:  Iteration No:  133 \n",
            " Loss:  0.3134779421777665\n",
            "Test accuracy:  90.91\n",
            "Training:  Iteration No:  134 Worker Num:  0 \n",
            " Loss:  0.3448776304721832\n",
            "Training:  Iteration No:  134 Worker Num:  1 \n",
            " Loss:  0.3183758854866028\n",
            "Training:  Iteration No:  134 Worker Num:  2 \n",
            " Loss:  0.30234214663505554\n",
            "Training:  Iteration No:  134 Worker Num:  3 \n",
            " Loss:  0.3434799313545227\n",
            "Training:  Iteration No:  134 Worker Num:  4 \n",
            " Loss:  0.43628761172294617\n",
            "Training:  Iteration No:  134 Worker Num:  5 \n",
            " Loss:  0.4046967327594757\n",
            "Training:  Iteration No:  134 Worker Num:  6 \n",
            " Loss:  0.3454221785068512\n",
            "Training:  Iteration No:  134 Worker Num:  7 \n",
            " Loss:  0.4283900558948517\n",
            "Test:  Iteration No:  134 \n",
            " Loss:  0.3114178239167491\n",
            "Test accuracy:  90.79\n",
            "Training:  Iteration No:  135 Worker Num:  0 \n",
            " Loss:  0.2905920147895813\n",
            "Training:  Iteration No:  135 Worker Num:  1 \n",
            " Loss:  0.43645235896110535\n",
            "Training:  Iteration No:  135 Worker Num:  2 \n",
            " Loss:  0.27108582854270935\n",
            "Training:  Iteration No:  135 Worker Num:  3 \n",
            " Loss:  0.43811991810798645\n",
            "Training:  Iteration No:  135 Worker Num:  4 \n",
            " Loss:  0.27684950828552246\n",
            "Training:  Iteration No:  135 Worker Num:  5 \n",
            " Loss:  0.3373362720012665\n",
            "Training:  Iteration No:  135 Worker Num:  6 \n",
            " Loss:  0.3332405686378479\n",
            "Training:  Iteration No:  135 Worker Num:  7 \n",
            " Loss:  0.40884119272232056\n",
            "Test:  Iteration No:  135 \n",
            " Loss:  0.31030842770315425\n",
            "Test accuracy:  90.92\n",
            "Training:  Iteration No:  136 Worker Num:  0 \n",
            " Loss:  0.2600877285003662\n",
            "Training:  Iteration No:  136 Worker Num:  1 \n",
            " Loss:  0.40610530972480774\n",
            "Training:  Iteration No:  136 Worker Num:  2 \n",
            " Loss:  0.2904345691204071\n",
            "Training:  Iteration No:  136 Worker Num:  3 \n",
            " Loss:  0.32735344767570496\n",
            "Training:  Iteration No:  136 Worker Num:  4 \n",
            " Loss:  0.38389110565185547\n",
            "Training:  Iteration No:  136 Worker Num:  5 \n",
            " Loss:  0.49458909034729004\n",
            "Training:  Iteration No:  136 Worker Num:  6 \n",
            " Loss:  0.4027060270309448\n",
            "Training:  Iteration No:  136 Worker Num:  7 \n",
            " Loss:  0.29903435707092285\n",
            "Test:  Iteration No:  136 \n",
            " Loss:  0.30443252151525474\n",
            "Test accuracy:  91.19\n",
            "Training:  Iteration No:  137 Worker Num:  0 \n",
            " Loss:  0.5029851198196411\n",
            "Training:  Iteration No:  137 Worker Num:  1 \n",
            " Loss:  0.3581710457801819\n",
            "Training:  Iteration No:  137 Worker Num:  2 \n",
            " Loss:  0.3644963204860687\n",
            "Training:  Iteration No:  137 Worker Num:  3 \n",
            " Loss:  0.4662877917289734\n",
            "Training:  Iteration No:  137 Worker Num:  4 \n",
            " Loss:  0.42435628175735474\n",
            "Training:  Iteration No:  137 Worker Num:  5 \n",
            " Loss:  0.39697739481925964\n",
            "Training:  Iteration No:  137 Worker Num:  6 \n",
            " Loss:  0.3663092851638794\n",
            "Training:  Iteration No:  137 Worker Num:  7 \n",
            " Loss:  0.37280404567718506\n",
            "Test:  Iteration No:  137 \n",
            " Loss:  0.3084698234555088\n",
            "Test accuracy:  90.91\n",
            "Training:  Iteration No:  138 Worker Num:  0 \n",
            " Loss:  0.5479834675788879\n",
            "Training:  Iteration No:  138 Worker Num:  1 \n",
            " Loss:  0.32975491881370544\n",
            "Training:  Iteration No:  138 Worker Num:  2 \n",
            " Loss:  0.37993353605270386\n",
            "Training:  Iteration No:  138 Worker Num:  3 \n",
            " Loss:  0.35293397307395935\n",
            "Training:  Iteration No:  138 Worker Num:  4 \n",
            " Loss:  0.3341963589191437\n",
            "Training:  Iteration No:  138 Worker Num:  5 \n",
            " Loss:  0.4212825894355774\n",
            "Training:  Iteration No:  138 Worker Num:  6 \n",
            " Loss:  0.38543108105659485\n",
            "Training:  Iteration No:  138 Worker Num:  7 \n",
            " Loss:  0.40159064531326294\n",
            "Test:  Iteration No:  138 \n",
            " Loss:  0.3031147036559974\n",
            "Test accuracy:  91.38\n",
            "Training:  Iteration No:  139 Worker Num:  0 \n",
            " Loss:  0.46759694814682007\n",
            "Training:  Iteration No:  139 Worker Num:  1 \n",
            " Loss:  0.4482080042362213\n",
            "Training:  Iteration No:  139 Worker Num:  2 \n",
            " Loss:  0.3129776120185852\n",
            "Training:  Iteration No:  139 Worker Num:  3 \n",
            " Loss:  0.4163059890270233\n",
            "Training:  Iteration No:  139 Worker Num:  4 \n",
            " Loss:  0.4702463150024414\n",
            "Training:  Iteration No:  139 Worker Num:  5 \n",
            " Loss:  0.37980183959007263\n",
            "Training:  Iteration No:  139 Worker Num:  6 \n",
            " Loss:  0.361478328704834\n",
            "Training:  Iteration No:  139 Worker Num:  7 \n",
            " Loss:  0.3919450044631958\n",
            "Test:  Iteration No:  139 \n",
            " Loss:  0.3063936795143387\n",
            "Test accuracy:  91.17\n",
            "Training:  Iteration No:  140 Worker Num:  0 \n",
            " Loss:  0.37513554096221924\n",
            "Training:  Iteration No:  140 Worker Num:  1 \n",
            " Loss:  0.41128116846084595\n",
            "Training:  Iteration No:  140 Worker Num:  2 \n",
            " Loss:  0.39081814885139465\n",
            "Training:  Iteration No:  140 Worker Num:  3 \n",
            " Loss:  0.6613748669624329\n",
            "Training:  Iteration No:  140 Worker Num:  4 \n",
            " Loss:  0.3292531967163086\n",
            "Training:  Iteration No:  140 Worker Num:  5 \n",
            " Loss:  0.36430248618125916\n",
            "Training:  Iteration No:  140 Worker Num:  6 \n",
            " Loss:  0.3964990973472595\n",
            "Training:  Iteration No:  140 Worker Num:  7 \n",
            " Loss:  0.3084671199321747\n",
            "Test:  Iteration No:  140 \n",
            " Loss:  0.2995720148746726\n",
            "Test accuracy:  91.25\n",
            "Training:  Iteration No:  141 Worker Num:  0 \n",
            " Loss:  0.46321192383766174\n",
            "Training:  Iteration No:  141 Worker Num:  1 \n",
            " Loss:  0.32104411721229553\n",
            "Training:  Iteration No:  141 Worker Num:  2 \n",
            " Loss:  0.44486764073371887\n",
            "Training:  Iteration No:  141 Worker Num:  3 \n",
            " Loss:  0.3996482789516449\n",
            "Training:  Iteration No:  141 Worker Num:  4 \n",
            " Loss:  0.40325456857681274\n",
            "Training:  Iteration No:  141 Worker Num:  5 \n",
            " Loss:  0.23648619651794434\n",
            "Training:  Iteration No:  141 Worker Num:  6 \n",
            " Loss:  0.34690335392951965\n",
            "Training:  Iteration No:  141 Worker Num:  7 \n",
            " Loss:  0.3014909029006958\n",
            "Test:  Iteration No:  141 \n",
            " Loss:  0.29699612828561023\n",
            "Test accuracy:  91.54\n",
            "Training:  Iteration No:  142 Worker Num:  0 \n",
            " Loss:  0.3488692343235016\n",
            "Training:  Iteration No:  142 Worker Num:  1 \n",
            " Loss:  0.25346294045448303\n",
            "Training:  Iteration No:  142 Worker Num:  2 \n",
            " Loss:  0.19925202429294586\n",
            "Training:  Iteration No:  142 Worker Num:  3 \n",
            " Loss:  0.3120717704296112\n",
            "Training:  Iteration No:  142 Worker Num:  4 \n",
            " Loss:  0.42706215381622314\n",
            "Training:  Iteration No:  142 Worker Num:  5 \n",
            " Loss:  0.286344051361084\n",
            "Training:  Iteration No:  142 Worker Num:  6 \n",
            " Loss:  0.2143869400024414\n",
            "Training:  Iteration No:  142 Worker Num:  7 \n",
            " Loss:  0.35864898562431335\n",
            "Test:  Iteration No:  142 \n",
            " Loss:  0.30405239742013473\n",
            "Test accuracy:  91.26\n",
            "Training:  Iteration No:  143 Worker Num:  0 \n",
            " Loss:  0.3451312482357025\n",
            "Training:  Iteration No:  143 Worker Num:  1 \n",
            " Loss:  0.3525388538837433\n",
            "Training:  Iteration No:  143 Worker Num:  2 \n",
            " Loss:  0.4399654269218445\n",
            "Training:  Iteration No:  143 Worker Num:  3 \n",
            " Loss:  0.48186030983924866\n",
            "Training:  Iteration No:  143 Worker Num:  4 \n",
            " Loss:  0.4531034231185913\n",
            "Training:  Iteration No:  143 Worker Num:  5 \n",
            " Loss:  0.22734388709068298\n",
            "Training:  Iteration No:  143 Worker Num:  6 \n",
            " Loss:  0.3718493580818176\n",
            "Training:  Iteration No:  143 Worker Num:  7 \n",
            " Loss:  0.3641164302825928\n",
            "Test:  Iteration No:  143 \n",
            " Loss:  0.2968970851241788\n",
            "Test accuracy:  91.28\n",
            "Training:  Iteration No:  144 Worker Num:  0 \n",
            " Loss:  0.3452695906162262\n",
            "Training:  Iteration No:  144 Worker Num:  1 \n",
            " Loss:  0.4757261276245117\n",
            "Training:  Iteration No:  144 Worker Num:  2 \n",
            " Loss:  0.3458077907562256\n",
            "Training:  Iteration No:  144 Worker Num:  3 \n",
            " Loss:  0.30411428213119507\n",
            "Training:  Iteration No:  144 Worker Num:  4 \n",
            " Loss:  0.5831766128540039\n",
            "Training:  Iteration No:  144 Worker Num:  5 \n",
            " Loss:  0.42803964018821716\n",
            "Training:  Iteration No:  144 Worker Num:  6 \n",
            " Loss:  0.3281431794166565\n",
            "Training:  Iteration No:  144 Worker Num:  7 \n",
            " Loss:  0.3605055809020996\n",
            "Test:  Iteration No:  144 \n",
            " Loss:  0.2983353377048728\n",
            "Test accuracy:  91.43\n",
            "Training:  Iteration No:  145 Worker Num:  0 \n",
            " Loss:  0.34170404076576233\n",
            "Training:  Iteration No:  145 Worker Num:  1 \n",
            " Loss:  0.37593162059783936\n",
            "Training:  Iteration No:  145 Worker Num:  2 \n",
            " Loss:  0.4560699164867401\n",
            "Training:  Iteration No:  145 Worker Num:  3 \n",
            " Loss:  0.3729079067707062\n",
            "Training:  Iteration No:  145 Worker Num:  4 \n",
            " Loss:  0.23518502712249756\n",
            "Training:  Iteration No:  145 Worker Num:  5 \n",
            " Loss:  0.38916444778442383\n",
            "Training:  Iteration No:  145 Worker Num:  6 \n",
            " Loss:  0.27444684505462646\n",
            "Training:  Iteration No:  145 Worker Num:  7 \n",
            " Loss:  0.4369806945323944\n",
            "Test:  Iteration No:  145 \n",
            " Loss:  0.29803853880472575\n",
            "Test accuracy:  91.52\n",
            "Training:  Iteration No:  146 Worker Num:  0 \n",
            " Loss:  0.3821127712726593\n",
            "Training:  Iteration No:  146 Worker Num:  1 \n",
            " Loss:  0.2878583073616028\n",
            "Training:  Iteration No:  146 Worker Num:  2 \n",
            " Loss:  0.4513537585735321\n",
            "Training:  Iteration No:  146 Worker Num:  3 \n",
            " Loss:  0.3131316304206848\n",
            "Training:  Iteration No:  146 Worker Num:  4 \n",
            " Loss:  0.31911325454711914\n",
            "Training:  Iteration No:  146 Worker Num:  5 \n",
            " Loss:  0.2151212841272354\n",
            "Training:  Iteration No:  146 Worker Num:  6 \n",
            " Loss:  0.3297453224658966\n",
            "Training:  Iteration No:  146 Worker Num:  7 \n",
            " Loss:  0.300098717212677\n",
            "Test:  Iteration No:  146 \n",
            " Loss:  0.3013359223362766\n",
            "Test accuracy:  91.12\n",
            "Training:  Iteration No:  147 Worker Num:  0 \n",
            " Loss:  0.35210537910461426\n",
            "Training:  Iteration No:  147 Worker Num:  1 \n",
            " Loss:  0.35518062114715576\n",
            "Training:  Iteration No:  147 Worker Num:  2 \n",
            " Loss:  0.3290579617023468\n",
            "Training:  Iteration No:  147 Worker Num:  3 \n",
            " Loss:  0.3772185146808624\n",
            "Training:  Iteration No:  147 Worker Num:  4 \n",
            " Loss:  0.32388773560523987\n",
            "Training:  Iteration No:  147 Worker Num:  5 \n",
            " Loss:  0.38122087717056274\n",
            "Training:  Iteration No:  147 Worker Num:  6 \n",
            " Loss:  0.3182031512260437\n",
            "Training:  Iteration No:  147 Worker Num:  7 \n",
            " Loss:  0.25868403911590576\n",
            "Test:  Iteration No:  147 \n",
            " Loss:  0.29348460202918775\n",
            "Test accuracy:  91.3\n",
            "Training:  Iteration No:  148 Worker Num:  0 \n",
            " Loss:  0.33225640654563904\n",
            "Training:  Iteration No:  148 Worker Num:  1 \n",
            " Loss:  0.4243726134300232\n",
            "Training:  Iteration No:  148 Worker Num:  2 \n",
            " Loss:  0.3202784061431885\n",
            "Training:  Iteration No:  148 Worker Num:  3 \n",
            " Loss:  0.4226558804512024\n",
            "Training:  Iteration No:  148 Worker Num:  4 \n",
            " Loss:  0.418301522731781\n",
            "Training:  Iteration No:  148 Worker Num:  5 \n",
            " Loss:  0.3361739218235016\n",
            "Training:  Iteration No:  148 Worker Num:  6 \n",
            " Loss:  0.33796000480651855\n",
            "Training:  Iteration No:  148 Worker Num:  7 \n",
            " Loss:  0.5403937101364136\n",
            "Test:  Iteration No:  148 \n",
            " Loss:  0.2901743451911437\n",
            "Test accuracy:  91.47\n",
            "Training:  Iteration No:  149 Worker Num:  0 \n",
            " Loss:  0.3684161305427551\n",
            "Training:  Iteration No:  149 Worker Num:  1 \n",
            " Loss:  0.37169280648231506\n",
            "Training:  Iteration No:  149 Worker Num:  2 \n",
            " Loss:  0.36775535345077515\n",
            "Training:  Iteration No:  149 Worker Num:  3 \n",
            " Loss:  0.3698308765888214\n",
            "Training:  Iteration No:  149 Worker Num:  4 \n",
            " Loss:  0.32597583532333374\n",
            "Training:  Iteration No:  149 Worker Num:  5 \n",
            " Loss:  0.3323315382003784\n",
            "Training:  Iteration No:  149 Worker Num:  6 \n",
            " Loss:  0.3144088387489319\n",
            "Training:  Iteration No:  149 Worker Num:  7 \n",
            " Loss:  0.289591908454895\n",
            "Test:  Iteration No:  149 \n",
            " Loss:  0.28904317786233336\n",
            "Test accuracy:  91.72\n",
            "Training:  Iteration No:  150 Worker Num:  0 \n",
            " Loss:  0.33785274624824524\n",
            "Training:  Iteration No:  150 Worker Num:  1 \n",
            " Loss:  0.27614593505859375\n",
            "Training:  Iteration No:  150 Worker Num:  2 \n",
            " Loss:  0.3221786320209503\n",
            "Training:  Iteration No:  150 Worker Num:  3 \n",
            " Loss:  0.36433854699134827\n",
            "Training:  Iteration No:  150 Worker Num:  4 \n",
            " Loss:  0.5116337537765503\n",
            "Training:  Iteration No:  150 Worker Num:  5 \n",
            " Loss:  0.21249167621135712\n",
            "Training:  Iteration No:  150 Worker Num:  6 \n",
            " Loss:  0.3234216272830963\n",
            "Training:  Iteration No:  150 Worker Num:  7 \n",
            " Loss:  0.34406813979148865\n",
            "Test:  Iteration No:  150 \n",
            " Loss:  0.28913753781514834\n",
            "Test accuracy:  91.49\n",
            "Training:  Iteration No:  151 Worker Num:  0 \n",
            " Loss:  0.27023205161094666\n",
            "Training:  Iteration No:  151 Worker Num:  1 \n",
            " Loss:  0.3127305507659912\n",
            "Training:  Iteration No:  151 Worker Num:  2 \n",
            " Loss:  0.4138360023498535\n",
            "Training:  Iteration No:  151 Worker Num:  3 \n",
            " Loss:  0.45478400588035583\n",
            "Training:  Iteration No:  151 Worker Num:  4 \n",
            " Loss:  0.19975878298282623\n",
            "Training:  Iteration No:  151 Worker Num:  5 \n",
            " Loss:  0.3503850996494293\n",
            "Training:  Iteration No:  151 Worker Num:  6 \n",
            " Loss:  0.3894402086734772\n",
            "Training:  Iteration No:  151 Worker Num:  7 \n",
            " Loss:  0.31954750418663025\n",
            "Test:  Iteration No:  151 \n",
            " Loss:  0.28608065461622006\n",
            "Test accuracy:  91.64\n",
            "Training:  Iteration No:  152 Worker Num:  0 \n",
            " Loss:  0.34552592039108276\n",
            "Training:  Iteration No:  152 Worker Num:  1 \n",
            " Loss:  0.2724035084247589\n",
            "Training:  Iteration No:  152 Worker Num:  2 \n",
            " Loss:  0.34974730014801025\n",
            "Training:  Iteration No:  152 Worker Num:  3 \n",
            " Loss:  0.19005578756332397\n",
            "Training:  Iteration No:  152 Worker Num:  4 \n",
            " Loss:  0.24373306334018707\n",
            "Training:  Iteration No:  152 Worker Num:  5 \n",
            " Loss:  0.2611787021160126\n",
            "Training:  Iteration No:  152 Worker Num:  6 \n",
            " Loss:  0.5408371090888977\n",
            "Training:  Iteration No:  152 Worker Num:  7 \n",
            " Loss:  0.282912015914917\n",
            "Test:  Iteration No:  152 \n",
            " Loss:  0.28674451007118706\n",
            "Test accuracy:  91.58\n",
            "Training:  Iteration No:  153 Worker Num:  0 \n",
            " Loss:  0.29839789867401123\n",
            "Training:  Iteration No:  153 Worker Num:  1 \n",
            " Loss:  0.25375956296920776\n",
            "Training:  Iteration No:  153 Worker Num:  2 \n",
            " Loss:  0.3604869544506073\n",
            "Training:  Iteration No:  153 Worker Num:  3 \n",
            " Loss:  0.4660797119140625\n",
            "Training:  Iteration No:  153 Worker Num:  4 \n",
            " Loss:  0.252881795167923\n",
            "Training:  Iteration No:  153 Worker Num:  5 \n",
            " Loss:  0.26953744888305664\n",
            "Training:  Iteration No:  153 Worker Num:  6 \n",
            " Loss:  0.3200864791870117\n",
            "Training:  Iteration No:  153 Worker Num:  7 \n",
            " Loss:  0.3014217019081116\n",
            "Test:  Iteration No:  153 \n",
            " Loss:  0.28503681799467606\n",
            "Test accuracy:  91.45\n",
            "Training:  Iteration No:  154 Worker Num:  0 \n",
            " Loss:  0.3143230080604553\n",
            "Training:  Iteration No:  154 Worker Num:  1 \n",
            " Loss:  0.2533480226993561\n",
            "Training:  Iteration No:  154 Worker Num:  2 \n",
            " Loss:  0.38138678669929504\n",
            "Training:  Iteration No:  154 Worker Num:  3 \n",
            " Loss:  0.29614710807800293\n",
            "Training:  Iteration No:  154 Worker Num:  4 \n",
            " Loss:  0.33400923013687134\n",
            "Training:  Iteration No:  154 Worker Num:  5 \n",
            " Loss:  0.41399407386779785\n",
            "Training:  Iteration No:  154 Worker Num:  6 \n",
            " Loss:  0.39472153782844543\n",
            "Training:  Iteration No:  154 Worker Num:  7 \n",
            " Loss:  0.28925272822380066\n",
            "Test:  Iteration No:  154 \n",
            " Loss:  0.28280464095311075\n",
            "Test accuracy:  91.73\n",
            "Training:  Iteration No:  155 Worker Num:  0 \n",
            " Loss:  0.4029889702796936\n",
            "Training:  Iteration No:  155 Worker Num:  1 \n",
            " Loss:  0.29546815156936646\n",
            "Training:  Iteration No:  155 Worker Num:  2 \n",
            " Loss:  0.3462875783443451\n",
            "Training:  Iteration No:  155 Worker Num:  3 \n",
            " Loss:  0.30329155921936035\n",
            "Training:  Iteration No:  155 Worker Num:  4 \n",
            " Loss:  0.3320973515510559\n",
            "Training:  Iteration No:  155 Worker Num:  5 \n",
            " Loss:  0.28494471311569214\n",
            "Training:  Iteration No:  155 Worker Num:  6 \n",
            " Loss:  0.27579519152641296\n",
            "Training:  Iteration No:  155 Worker Num:  7 \n",
            " Loss:  0.2327721267938614\n",
            "Test:  Iteration No:  155 \n",
            " Loss:  0.2859654614961223\n",
            "Test accuracy:  91.59\n",
            "Training:  Iteration No:  156 Worker Num:  0 \n",
            " Loss:  0.4278169870376587\n",
            "Training:  Iteration No:  156 Worker Num:  1 \n",
            " Loss:  0.3859558701515198\n",
            "Training:  Iteration No:  156 Worker Num:  2 \n",
            " Loss:  0.32314881682395935\n",
            "Training:  Iteration No:  156 Worker Num:  3 \n",
            " Loss:  0.30505797266960144\n",
            "Training:  Iteration No:  156 Worker Num:  4 \n",
            " Loss:  0.37095826864242554\n",
            "Training:  Iteration No:  156 Worker Num:  5 \n",
            " Loss:  0.2307577282190323\n",
            "Training:  Iteration No:  156 Worker Num:  6 \n",
            " Loss:  0.23864279687404633\n",
            "Training:  Iteration No:  156 Worker Num:  7 \n",
            " Loss:  0.35057881474494934\n",
            "Test:  Iteration No:  156 \n",
            " Loss:  0.2924879254608215\n",
            "Test accuracy:  91.49\n",
            "Training:  Iteration No:  157 Worker Num:  0 \n",
            " Loss:  0.2841871976852417\n",
            "Training:  Iteration No:  157 Worker Num:  1 \n",
            " Loss:  0.3545732796192169\n",
            "Training:  Iteration No:  157 Worker Num:  2 \n",
            " Loss:  0.25285452604293823\n",
            "Training:  Iteration No:  157 Worker Num:  3 \n",
            " Loss:  0.4347497224807739\n",
            "Training:  Iteration No:  157 Worker Num:  4 \n",
            " Loss:  0.32705315947532654\n",
            "Training:  Iteration No:  157 Worker Num:  5 \n",
            " Loss:  0.2772575616836548\n",
            "Training:  Iteration No:  157 Worker Num:  6 \n",
            " Loss:  0.3916356861591339\n",
            "Training:  Iteration No:  157 Worker Num:  7 \n",
            " Loss:  0.3095347583293915\n",
            "Test:  Iteration No:  157 \n",
            " Loss:  0.28405673184160946\n",
            "Test accuracy:  91.7\n",
            "Training:  Iteration No:  158 Worker Num:  0 \n",
            " Loss:  0.284504771232605\n",
            "Training:  Iteration No:  158 Worker Num:  1 \n",
            " Loss:  0.3168063461780548\n",
            "Training:  Iteration No:  158 Worker Num:  2 \n",
            " Loss:  0.4261648654937744\n",
            "Training:  Iteration No:  158 Worker Num:  3 \n",
            " Loss:  0.2544647455215454\n",
            "Training:  Iteration No:  158 Worker Num:  4 \n",
            " Loss:  0.3840904235839844\n",
            "Training:  Iteration No:  158 Worker Num:  5 \n",
            " Loss:  0.2866758108139038\n",
            "Training:  Iteration No:  158 Worker Num:  6 \n",
            " Loss:  0.2893216609954834\n",
            "Training:  Iteration No:  158 Worker Num:  7 \n",
            " Loss:  0.30313706398010254\n",
            "Test:  Iteration No:  158 \n",
            " Loss:  0.27782883542247966\n",
            "Test accuracy:  91.78\n",
            "Training:  Iteration No:  159 Worker Num:  0 \n",
            " Loss:  0.3760756254196167\n",
            "Training:  Iteration No:  159 Worker Num:  1 \n",
            " Loss:  0.4041106402873993\n",
            "Training:  Iteration No:  159 Worker Num:  2 \n",
            " Loss:  0.31927505135536194\n",
            "Training:  Iteration No:  159 Worker Num:  3 \n",
            " Loss:  0.262893944978714\n",
            "Training:  Iteration No:  159 Worker Num:  4 \n",
            " Loss:  0.42032596468925476\n",
            "Training:  Iteration No:  159 Worker Num:  5 \n",
            " Loss:  0.42043405771255493\n",
            "Training:  Iteration No:  159 Worker Num:  6 \n",
            " Loss:  0.42914944887161255\n",
            "Training:  Iteration No:  159 Worker Num:  7 \n",
            " Loss:  0.34830570220947266\n",
            "Test:  Iteration No:  159 \n",
            " Loss:  0.2746798079225081\n",
            "Test accuracy:  91.98\n",
            "Training:  Iteration No:  160 Worker Num:  0 \n",
            " Loss:  0.29644566774368286\n",
            "Training:  Iteration No:  160 Worker Num:  1 \n",
            " Loss:  0.4125065505504608\n",
            "Training:  Iteration No:  160 Worker Num:  2 \n",
            " Loss:  0.43846842646598816\n",
            "Training:  Iteration No:  160 Worker Num:  3 \n",
            " Loss:  0.4461524188518524\n",
            "Training:  Iteration No:  160 Worker Num:  4 \n",
            " Loss:  0.3800238072872162\n",
            "Training:  Iteration No:  160 Worker Num:  5 \n",
            " Loss:  0.25435030460357666\n",
            "Training:  Iteration No:  160 Worker Num:  6 \n",
            " Loss:  0.2967161536216736\n",
            "Training:  Iteration No:  160 Worker Num:  7 \n",
            " Loss:  0.24660904705524445\n",
            "Test:  Iteration No:  160 \n",
            " Loss:  0.2740241578674015\n",
            "Test accuracy:  91.68\n",
            "Training:  Iteration No:  161 Worker Num:  0 \n",
            " Loss:  0.40155264735221863\n",
            "Training:  Iteration No:  161 Worker Num:  1 \n",
            " Loss:  0.22065453231334686\n",
            "Training:  Iteration No:  161 Worker Num:  2 \n",
            " Loss:  0.4417475163936615\n",
            "Training:  Iteration No:  161 Worker Num:  3 \n",
            " Loss:  0.4277585744857788\n",
            "Training:  Iteration No:  161 Worker Num:  4 \n",
            " Loss:  0.37559884786605835\n",
            "Training:  Iteration No:  161 Worker Num:  5 \n",
            " Loss:  0.4265066087245941\n",
            "Training:  Iteration No:  161 Worker Num:  6 \n",
            " Loss:  0.47860419750213623\n",
            "Training:  Iteration No:  161 Worker Num:  7 \n",
            " Loss:  0.2430722564458847\n",
            "Test:  Iteration No:  161 \n",
            " Loss:  0.27409883866770357\n",
            "Test accuracy:  91.73\n",
            "Training:  Iteration No:  162 Worker Num:  0 \n",
            " Loss:  0.2477836310863495\n",
            "Training:  Iteration No:  162 Worker Num:  1 \n",
            " Loss:  0.46060681343078613\n",
            "Training:  Iteration No:  162 Worker Num:  2 \n",
            " Loss:  0.2712704539299011\n",
            "Training:  Iteration No:  162 Worker Num:  3 \n",
            " Loss:  0.3067924380302429\n",
            "Training:  Iteration No:  162 Worker Num:  4 \n",
            " Loss:  0.29173222184181213\n",
            "Training:  Iteration No:  162 Worker Num:  5 \n",
            " Loss:  0.4116435945034027\n",
            "Training:  Iteration No:  162 Worker Num:  6 \n",
            " Loss:  0.3685559332370758\n",
            "Training:  Iteration No:  162 Worker Num:  7 \n",
            " Loss:  0.42556461691856384\n",
            "Test:  Iteration No:  162 \n",
            " Loss:  0.2744452517856903\n",
            "Test accuracy:  92.06\n",
            "Training:  Iteration No:  163 Worker Num:  0 \n",
            " Loss:  0.352270245552063\n",
            "Training:  Iteration No:  163 Worker Num:  1 \n",
            " Loss:  0.2128704935312271\n",
            "Training:  Iteration No:  163 Worker Num:  2 \n",
            " Loss:  0.3333168029785156\n",
            "Training:  Iteration No:  163 Worker Num:  3 \n",
            " Loss:  0.28195351362228394\n",
            "Training:  Iteration No:  163 Worker Num:  4 \n",
            " Loss:  0.43973249197006226\n",
            "Training:  Iteration No:  163 Worker Num:  5 \n",
            " Loss:  0.32799091935157776\n",
            "Training:  Iteration No:  163 Worker Num:  6 \n",
            " Loss:  0.29201361536979675\n",
            "Training:  Iteration No:  163 Worker Num:  7 \n",
            " Loss:  0.2734656035900116\n",
            "Test:  Iteration No:  163 \n",
            " Loss:  0.27786622253022614\n",
            "Test accuracy:  92.04\n",
            "Training:  Iteration No:  164 Worker Num:  0 \n",
            " Loss:  0.5086947083473206\n",
            "Training:  Iteration No:  164 Worker Num:  1 \n",
            " Loss:  0.3843430280685425\n",
            "Training:  Iteration No:  164 Worker Num:  2 \n",
            " Loss:  0.24744753539562225\n",
            "Training:  Iteration No:  164 Worker Num:  3 \n",
            " Loss:  0.3126187324523926\n",
            "Training:  Iteration No:  164 Worker Num:  4 \n",
            " Loss:  0.29828348755836487\n",
            "Training:  Iteration No:  164 Worker Num:  5 \n",
            " Loss:  0.3149067461490631\n",
            "Training:  Iteration No:  164 Worker Num:  6 \n",
            " Loss:  0.30493810772895813\n",
            "Training:  Iteration No:  164 Worker Num:  7 \n",
            " Loss:  0.35790425539016724\n",
            "Test:  Iteration No:  164 \n",
            " Loss:  0.2747893275905259\n",
            "Test accuracy:  91.83\n",
            "Training:  Iteration No:  165 Worker Num:  0 \n",
            " Loss:  0.3438430726528168\n",
            "Training:  Iteration No:  165 Worker Num:  1 \n",
            " Loss:  0.27685561776161194\n",
            "Training:  Iteration No:  165 Worker Num:  2 \n",
            " Loss:  0.28981441259384155\n",
            "Training:  Iteration No:  165 Worker Num:  3 \n",
            " Loss:  0.1702730357646942\n",
            "Training:  Iteration No:  165 Worker Num:  4 \n",
            " Loss:  0.2681525647640228\n",
            "Training:  Iteration No:  165 Worker Num:  5 \n",
            " Loss:  0.3616337776184082\n",
            "Training:  Iteration No:  165 Worker Num:  6 \n",
            " Loss:  0.3888566493988037\n",
            "Training:  Iteration No:  165 Worker Num:  7 \n",
            " Loss:  0.3127725124359131\n",
            "Test:  Iteration No:  165 \n",
            " Loss:  0.2721298487409006\n",
            "Test accuracy:  92.14\n",
            "Training:  Iteration No:  166 Worker Num:  0 \n",
            " Loss:  0.41529422998428345\n",
            "Training:  Iteration No:  166 Worker Num:  1 \n",
            " Loss:  0.3940645158290863\n",
            "Training:  Iteration No:  166 Worker Num:  2 \n",
            " Loss:  0.31062158942222595\n",
            "Training:  Iteration No:  166 Worker Num:  3 \n",
            " Loss:  0.40033453702926636\n",
            "Training:  Iteration No:  166 Worker Num:  4 \n",
            " Loss:  0.29178738594055176\n",
            "Training:  Iteration No:  166 Worker Num:  5 \n",
            " Loss:  0.2709919810295105\n",
            "Training:  Iteration No:  166 Worker Num:  6 \n",
            " Loss:  0.3091498017311096\n",
            "Training:  Iteration No:  166 Worker Num:  7 \n",
            " Loss:  0.30677324533462524\n",
            "Test:  Iteration No:  166 \n",
            " Loss:  0.267692333205214\n",
            "Test accuracy:  92.13\n",
            "Training:  Iteration No:  167 Worker Num:  0 \n",
            " Loss:  0.21396876871585846\n",
            "Training:  Iteration No:  167 Worker Num:  1 \n",
            " Loss:  0.22976699471473694\n",
            "Training:  Iteration No:  167 Worker Num:  2 \n",
            " Loss:  0.37965819239616394\n",
            "Training:  Iteration No:  167 Worker Num:  3 \n",
            " Loss:  0.2163134068250656\n",
            "Training:  Iteration No:  167 Worker Num:  4 \n",
            " Loss:  0.3614913821220398\n",
            "Training:  Iteration No:  167 Worker Num:  5 \n",
            " Loss:  0.3436186611652374\n",
            "Training:  Iteration No:  167 Worker Num:  6 \n",
            " Loss:  0.35412880778312683\n",
            "Training:  Iteration No:  167 Worker Num:  7 \n",
            " Loss:  0.34007570147514343\n",
            "Test:  Iteration No:  167 \n",
            " Loss:  0.2700200008487777\n",
            "Test accuracy:  91.96\n",
            "Training:  Iteration No:  168 Worker Num:  0 \n",
            " Loss:  0.3008282482624054\n",
            "Training:  Iteration No:  168 Worker Num:  1 \n",
            " Loss:  0.5235310792922974\n",
            "Training:  Iteration No:  168 Worker Num:  2 \n",
            " Loss:  0.24800334870815277\n",
            "Training:  Iteration No:  168 Worker Num:  3 \n",
            " Loss:  0.443128377199173\n",
            "Training:  Iteration No:  168 Worker Num:  4 \n",
            " Loss:  0.4523397982120514\n",
            "Training:  Iteration No:  168 Worker Num:  5 \n",
            " Loss:  0.4102901518344879\n",
            "Training:  Iteration No:  168 Worker Num:  6 \n",
            " Loss:  0.32907727360725403\n",
            "Training:  Iteration No:  168 Worker Num:  7 \n",
            " Loss:  0.3009820282459259\n",
            "Test:  Iteration No:  168 \n",
            " Loss:  0.26658465932535974\n",
            "Test accuracy:  92.19\n",
            "Training:  Iteration No:  169 Worker Num:  0 \n",
            " Loss:  0.2903187870979309\n",
            "Training:  Iteration No:  169 Worker Num:  1 \n",
            " Loss:  0.4236797094345093\n",
            "Training:  Iteration No:  169 Worker Num:  2 \n",
            " Loss:  0.3704400360584259\n",
            "Training:  Iteration No:  169 Worker Num:  3 \n",
            " Loss:  0.2923184931278229\n",
            "Training:  Iteration No:  169 Worker Num:  4 \n",
            " Loss:  0.238926500082016\n",
            "Training:  Iteration No:  169 Worker Num:  5 \n",
            " Loss:  0.20844766497612\n",
            "Training:  Iteration No:  169 Worker Num:  6 \n",
            " Loss:  0.23509769141674042\n",
            "Training:  Iteration No:  169 Worker Num:  7 \n",
            " Loss:  0.343006432056427\n",
            "Test:  Iteration No:  169 \n",
            " Loss:  0.26259079523667506\n",
            "Test accuracy:  92.12\n",
            "Training:  Iteration No:  170 Worker Num:  0 \n",
            " Loss:  0.2841082811355591\n",
            "Training:  Iteration No:  170 Worker Num:  1 \n",
            " Loss:  0.3977048993110657\n",
            "Training:  Iteration No:  170 Worker Num:  2 \n",
            " Loss:  0.36309537291526794\n",
            "Training:  Iteration No:  170 Worker Num:  3 \n",
            " Loss:  0.3031024932861328\n",
            "Training:  Iteration No:  170 Worker Num:  4 \n",
            " Loss:  0.262342631816864\n",
            "Training:  Iteration No:  170 Worker Num:  5 \n",
            " Loss:  0.33375704288482666\n",
            "Training:  Iteration No:  170 Worker Num:  6 \n",
            " Loss:  0.391533762216568\n",
            "Training:  Iteration No:  170 Worker Num:  7 \n",
            " Loss:  0.29150497913360596\n",
            "Test:  Iteration No:  170 \n",
            " Loss:  0.2658877664163143\n",
            "Test accuracy:  92.31\n",
            "Training:  Iteration No:  171 Worker Num:  0 \n",
            " Loss:  0.29483115673065186\n",
            "Training:  Iteration No:  171 Worker Num:  1 \n",
            " Loss:  0.40608906745910645\n",
            "Training:  Iteration No:  171 Worker Num:  2 \n",
            " Loss:  0.35298627614974976\n",
            "Training:  Iteration No:  171 Worker Num:  3 \n",
            " Loss:  0.30643630027770996\n",
            "Training:  Iteration No:  171 Worker Num:  4 \n",
            " Loss:  0.3257523477077484\n",
            "Training:  Iteration No:  171 Worker Num:  5 \n",
            " Loss:  0.39004260301589966\n",
            "Training:  Iteration No:  171 Worker Num:  6 \n",
            " Loss:  0.25587785243988037\n",
            "Training:  Iteration No:  171 Worker Num:  7 \n",
            " Loss:  0.28271105885505676\n",
            "Test:  Iteration No:  171 \n",
            " Loss:  0.26399887412126305\n",
            "Test accuracy:  92.25\n",
            "Training:  Iteration No:  172 Worker Num:  0 \n",
            " Loss:  0.2105223834514618\n",
            "Training:  Iteration No:  172 Worker Num:  1 \n",
            " Loss:  0.2892071306705475\n",
            "Training:  Iteration No:  172 Worker Num:  2 \n",
            " Loss:  0.2873181700706482\n",
            "Training:  Iteration No:  172 Worker Num:  3 \n",
            " Loss:  0.30850550532341003\n",
            "Training:  Iteration No:  172 Worker Num:  4 \n",
            " Loss:  0.3150630593299866\n",
            "Training:  Iteration No:  172 Worker Num:  5 \n",
            " Loss:  0.2179705649614334\n",
            "Training:  Iteration No:  172 Worker Num:  6 \n",
            " Loss:  0.24887317419052124\n",
            "Training:  Iteration No:  172 Worker Num:  7 \n",
            " Loss:  0.34013089537620544\n",
            "Test:  Iteration No:  172 \n",
            " Loss:  0.27032877497752256\n",
            "Test accuracy:  91.98\n",
            "Training:  Iteration No:  173 Worker Num:  0 \n",
            " Loss:  0.4663790166378021\n",
            "Training:  Iteration No:  173 Worker Num:  1 \n",
            " Loss:  0.2486974000930786\n",
            "Training:  Iteration No:  173 Worker Num:  2 \n",
            " Loss:  0.27918121218681335\n",
            "Training:  Iteration No:  173 Worker Num:  3 \n",
            " Loss:  0.20665548741817474\n",
            "Training:  Iteration No:  173 Worker Num:  4 \n",
            " Loss:  0.19779260456562042\n",
            "Training:  Iteration No:  173 Worker Num:  5 \n",
            " Loss:  0.2457791417837143\n",
            "Training:  Iteration No:  173 Worker Num:  6 \n",
            " Loss:  0.30300846695899963\n",
            "Training:  Iteration No:  173 Worker Num:  7 \n",
            " Loss:  0.19683195650577545\n",
            "Test:  Iteration No:  173 \n",
            " Loss:  0.26575097257766545\n",
            "Test accuracy:  91.88\n",
            "Training:  Iteration No:  174 Worker Num:  0 \n",
            " Loss:  0.3431897759437561\n",
            "Training:  Iteration No:  174 Worker Num:  1 \n",
            " Loss:  0.3160800039768219\n",
            "Training:  Iteration No:  174 Worker Num:  2 \n",
            " Loss:  0.4108700156211853\n",
            "Training:  Iteration No:  174 Worker Num:  3 \n",
            " Loss:  0.45116326212882996\n",
            "Training:  Iteration No:  174 Worker Num:  4 \n",
            " Loss:  0.34817227721214294\n",
            "Training:  Iteration No:  174 Worker Num:  5 \n",
            " Loss:  0.2711913585662842\n",
            "Training:  Iteration No:  174 Worker Num:  6 \n",
            " Loss:  0.3378678560256958\n",
            "Training:  Iteration No:  174 Worker Num:  7 \n",
            " Loss:  0.2901047170162201\n",
            "Test:  Iteration No:  174 \n",
            " Loss:  0.26501274721909174\n",
            "Test accuracy:  92.18\n",
            "Training:  Iteration No:  175 Worker Num:  0 \n",
            " Loss:  0.36943382024765015\n",
            "Training:  Iteration No:  175 Worker Num:  1 \n",
            " Loss:  0.3377362787723541\n",
            "Training:  Iteration No:  175 Worker Num:  2 \n",
            " Loss:  0.25709596276283264\n",
            "Training:  Iteration No:  175 Worker Num:  3 \n",
            " Loss:  0.31376126408576965\n",
            "Training:  Iteration No:  175 Worker Num:  4 \n",
            " Loss:  0.2233591079711914\n",
            "Training:  Iteration No:  175 Worker Num:  5 \n",
            " Loss:  0.2697933614253998\n",
            "Training:  Iteration No:  175 Worker Num:  6 \n",
            " Loss:  0.24860210716724396\n",
            "Training:  Iteration No:  175 Worker Num:  7 \n",
            " Loss:  0.19848783314228058\n",
            "Test:  Iteration No:  175 \n",
            " Loss:  0.26055479898482936\n",
            "Test accuracy:  92.36\n",
            "Training:  Iteration No:  176 Worker Num:  0 \n",
            " Loss:  0.3147284686565399\n",
            "Training:  Iteration No:  176 Worker Num:  1 \n",
            " Loss:  0.29483672976493835\n",
            "Training:  Iteration No:  176 Worker Num:  2 \n",
            " Loss:  0.3068719804286957\n",
            "Training:  Iteration No:  176 Worker Num:  3 \n",
            " Loss:  0.29095524549484253\n",
            "Training:  Iteration No:  176 Worker Num:  4 \n",
            " Loss:  0.399450421333313\n",
            "Training:  Iteration No:  176 Worker Num:  5 \n",
            " Loss:  0.36899346113204956\n",
            "Training:  Iteration No:  176 Worker Num:  6 \n",
            " Loss:  0.2686411142349243\n",
            "Training:  Iteration No:  176 Worker Num:  7 \n",
            " Loss:  0.23059599101543427\n",
            "Test:  Iteration No:  176 \n",
            " Loss:  0.2597536448436447\n",
            "Test accuracy:  92.28\n",
            "Training:  Iteration No:  177 Worker Num:  0 \n",
            " Loss:  0.2741827666759491\n",
            "Training:  Iteration No:  177 Worker Num:  1 \n",
            " Loss:  0.2526235282421112\n",
            "Training:  Iteration No:  177 Worker Num:  2 \n",
            " Loss:  0.40301260352134705\n",
            "Training:  Iteration No:  177 Worker Num:  3 \n",
            " Loss:  0.3588011860847473\n",
            "Training:  Iteration No:  177 Worker Num:  4 \n",
            " Loss:  0.4482896327972412\n",
            "Training:  Iteration No:  177 Worker Num:  5 \n",
            " Loss:  0.2579919993877411\n",
            "Training:  Iteration No:  177 Worker Num:  6 \n",
            " Loss:  0.3924926519393921\n",
            "Training:  Iteration No:  177 Worker Num:  7 \n",
            " Loss:  0.38521021604537964\n",
            "Test:  Iteration No:  177 \n",
            " Loss:  0.2632839509957953\n",
            "Test accuracy:  92.42\n",
            "Training:  Iteration No:  178 Worker Num:  0 \n",
            " Loss:  0.21762479841709137\n",
            "Training:  Iteration No:  178 Worker Num:  1 \n",
            " Loss:  0.2607142925262451\n",
            "Training:  Iteration No:  178 Worker Num:  2 \n",
            " Loss:  0.23620520532131195\n",
            "Training:  Iteration No:  178 Worker Num:  3 \n",
            " Loss:  0.3213319480419159\n",
            "Training:  Iteration No:  178 Worker Num:  4 \n",
            " Loss:  0.37177181243896484\n",
            "Training:  Iteration No:  178 Worker Num:  5 \n",
            " Loss:  0.38342851400375366\n",
            "Training:  Iteration No:  178 Worker Num:  6 \n",
            " Loss:  0.26334822177886963\n",
            "Training:  Iteration No:  178 Worker Num:  7 \n",
            " Loss:  0.191197007894516\n",
            "Test:  Iteration No:  178 \n",
            " Loss:  0.2571342169672628\n",
            "Test accuracy:  92.72\n",
            "Training:  Iteration No:  179 Worker Num:  0 \n",
            " Loss:  0.41663333773612976\n",
            "Training:  Iteration No:  179 Worker Num:  1 \n",
            " Loss:  0.3262917995452881\n",
            "Training:  Iteration No:  179 Worker Num:  2 \n",
            " Loss:  0.3107415437698364\n",
            "Training:  Iteration No:  179 Worker Num:  3 \n",
            " Loss:  0.34853774309158325\n",
            "Training:  Iteration No:  179 Worker Num:  4 \n",
            " Loss:  0.19969908893108368\n",
            "Training:  Iteration No:  179 Worker Num:  5 \n",
            " Loss:  0.26444193720817566\n",
            "Training:  Iteration No:  179 Worker Num:  6 \n",
            " Loss:  0.34247395396232605\n",
            "Training:  Iteration No:  179 Worker Num:  7 \n",
            " Loss:  0.2593647241592407\n",
            "Test:  Iteration No:  179 \n",
            " Loss:  0.2564245916525774\n",
            "Test accuracy:  92.39\n",
            "Training:  Iteration No:  180 Worker Num:  0 \n",
            " Loss:  0.28310832381248474\n",
            "Training:  Iteration No:  180 Worker Num:  1 \n",
            " Loss:  0.2917841076850891\n",
            "Training:  Iteration No:  180 Worker Num:  2 \n",
            " Loss:  0.3627684414386749\n",
            "Training:  Iteration No:  180 Worker Num:  3 \n",
            " Loss:  0.28396642208099365\n",
            "Training:  Iteration No:  180 Worker Num:  4 \n",
            " Loss:  0.4119721055030823\n",
            "Training:  Iteration No:  180 Worker Num:  5 \n",
            " Loss:  0.47171998023986816\n",
            "Training:  Iteration No:  180 Worker Num:  6 \n",
            " Loss:  0.2887462079524994\n",
            "Training:  Iteration No:  180 Worker Num:  7 \n",
            " Loss:  0.2607667148113251\n",
            "Test:  Iteration No:  180 \n",
            " Loss:  0.2612536044767763\n",
            "Test accuracy:  92.35\n",
            "Training:  Iteration No:  181 Worker Num:  0 \n",
            " Loss:  0.40552493929862976\n",
            "Training:  Iteration No:  181 Worker Num:  1 \n",
            " Loss:  0.307714581489563\n",
            "Training:  Iteration No:  181 Worker Num:  2 \n",
            " Loss:  0.31521210074424744\n",
            "Training:  Iteration No:  181 Worker Num:  3 \n",
            " Loss:  0.279297798871994\n",
            "Training:  Iteration No:  181 Worker Num:  4 \n",
            " Loss:  0.2134866565465927\n",
            "Training:  Iteration No:  181 Worker Num:  5 \n",
            " Loss:  0.23937691748142242\n",
            "Training:  Iteration No:  181 Worker Num:  6 \n",
            " Loss:  0.25037330389022827\n",
            "Training:  Iteration No:  181 Worker Num:  7 \n",
            " Loss:  0.24342691898345947\n",
            "Test:  Iteration No:  181 \n",
            " Loss:  0.2528615750466721\n",
            "Test accuracy:  92.53\n",
            "Training:  Iteration No:  182 Worker Num:  0 \n",
            " Loss:  0.2859470546245575\n",
            "Training:  Iteration No:  182 Worker Num:  1 \n",
            " Loss:  0.38178595900535583\n",
            "Training:  Iteration No:  182 Worker Num:  2 \n",
            " Loss:  0.3752143979072571\n",
            "Training:  Iteration No:  182 Worker Num:  3 \n",
            " Loss:  0.3284930884838104\n",
            "Training:  Iteration No:  182 Worker Num:  4 \n",
            " Loss:  0.4474922716617584\n",
            "Training:  Iteration No:  182 Worker Num:  5 \n",
            " Loss:  0.255401611328125\n",
            "Training:  Iteration No:  182 Worker Num:  6 \n",
            " Loss:  0.4584156274795532\n",
            "Training:  Iteration No:  182 Worker Num:  7 \n",
            " Loss:  0.298020601272583\n",
            "Test:  Iteration No:  182 \n",
            " Loss:  0.258386957541674\n",
            "Test accuracy:  92.25\n",
            "Training:  Iteration No:  183 Worker Num:  0 \n",
            " Loss:  0.22867169976234436\n",
            "Training:  Iteration No:  183 Worker Num:  1 \n",
            " Loss:  0.2677558362483978\n",
            "Training:  Iteration No:  183 Worker Num:  2 \n",
            " Loss:  0.28234678506851196\n",
            "Training:  Iteration No:  183 Worker Num:  3 \n",
            " Loss:  0.4706878066062927\n",
            "Training:  Iteration No:  183 Worker Num:  4 \n",
            " Loss:  0.20790919661521912\n",
            "Training:  Iteration No:  183 Worker Num:  5 \n",
            " Loss:  0.35572826862335205\n",
            "Training:  Iteration No:  183 Worker Num:  6 \n",
            " Loss:  0.2647913694381714\n",
            "Training:  Iteration No:  183 Worker Num:  7 \n",
            " Loss:  0.19969139993190765\n",
            "Test:  Iteration No:  183 \n",
            " Loss:  0.2532984736269411\n",
            "Test accuracy:  92.44\n",
            "Training:  Iteration No:  184 Worker Num:  0 \n",
            " Loss:  0.35275375843048096\n",
            "Training:  Iteration No:  184 Worker Num:  1 \n",
            " Loss:  0.2303251326084137\n",
            "Training:  Iteration No:  184 Worker Num:  2 \n",
            " Loss:  0.2548312246799469\n",
            "Training:  Iteration No:  184 Worker Num:  3 \n",
            " Loss:  0.3244939148426056\n",
            "Training:  Iteration No:  184 Worker Num:  4 \n",
            " Loss:  0.3094918429851532\n",
            "Training:  Iteration No:  184 Worker Num:  5 \n",
            " Loss:  0.31149822473526\n",
            "Training:  Iteration No:  184 Worker Num:  6 \n",
            " Loss:  0.3036784529685974\n",
            "Training:  Iteration No:  184 Worker Num:  7 \n",
            " Loss:  0.25015491247177124\n",
            "Test:  Iteration No:  184 \n",
            " Loss:  0.25084026025820383\n",
            "Test accuracy:  92.49\n",
            "Training:  Iteration No:  185 Worker Num:  0 \n",
            " Loss:  0.30305367708206177\n",
            "Training:  Iteration No:  185 Worker Num:  1 \n",
            " Loss:  0.2559564411640167\n",
            "Training:  Iteration No:  185 Worker Num:  2 \n",
            " Loss:  0.4103848934173584\n",
            "Training:  Iteration No:  185 Worker Num:  3 \n",
            " Loss:  0.32281336188316345\n",
            "Training:  Iteration No:  185 Worker Num:  4 \n",
            " Loss:  0.13977892696857452\n",
            "Training:  Iteration No:  185 Worker Num:  5 \n",
            " Loss:  0.3382209539413452\n",
            "Training:  Iteration No:  185 Worker Num:  6 \n",
            " Loss:  0.3822900652885437\n",
            "Training:  Iteration No:  185 Worker Num:  7 \n",
            " Loss:  0.21127191185951233\n",
            "Test:  Iteration No:  185 \n",
            " Loss:  0.2513650948582571\n",
            "Test accuracy:  92.52\n",
            "Training:  Iteration No:  186 Worker Num:  0 \n",
            " Loss:  0.2377810925245285\n",
            "Training:  Iteration No:  186 Worker Num:  1 \n",
            " Loss:  0.213938906788826\n",
            "Training:  Iteration No:  186 Worker Num:  2 \n",
            " Loss:  0.34073925018310547\n",
            "Training:  Iteration No:  186 Worker Num:  3 \n",
            " Loss:  0.5278381109237671\n",
            "Training:  Iteration No:  186 Worker Num:  4 \n",
            " Loss:  0.27843937277793884\n",
            "Training:  Iteration No:  186 Worker Num:  5 \n",
            " Loss:  0.36835166811943054\n",
            "Training:  Iteration No:  186 Worker Num:  6 \n",
            " Loss:  0.3208910822868347\n",
            "Training:  Iteration No:  186 Worker Num:  7 \n",
            " Loss:  0.4992687404155731\n",
            "Test:  Iteration No:  186 \n",
            " Loss:  0.2501118411840517\n",
            "Test accuracy:  92.67\n",
            "Training:  Iteration No:  187 Worker Num:  0 \n",
            " Loss:  0.3174455165863037\n",
            "Training:  Iteration No:  187 Worker Num:  1 \n",
            " Loss:  0.4100651144981384\n",
            "Training:  Iteration No:  187 Worker Num:  2 \n",
            " Loss:  0.25623491406440735\n",
            "Training:  Iteration No:  187 Worker Num:  3 \n",
            " Loss:  0.23452553153038025\n",
            "Training:  Iteration No:  187 Worker Num:  4 \n",
            " Loss:  0.27303871512413025\n",
            "Training:  Iteration No:  187 Worker Num:  5 \n",
            " Loss:  0.19969035685062408\n",
            "Training:  Iteration No:  187 Worker Num:  6 \n",
            " Loss:  0.4442206025123596\n",
            "Training:  Iteration No:  187 Worker Num:  7 \n",
            " Loss:  0.20939543843269348\n",
            "Test:  Iteration No:  187 \n",
            " Loss:  0.2471857919487395\n",
            "Test accuracy:  92.53\n",
            "Training:  Iteration No:  188 Worker Num:  0 \n",
            " Loss:  0.2667371332645416\n",
            "Training:  Iteration No:  188 Worker Num:  1 \n",
            " Loss:  0.263108491897583\n",
            "Training:  Iteration No:  188 Worker Num:  2 \n",
            " Loss:  0.2954314053058624\n",
            "Training:  Iteration No:  188 Worker Num:  3 \n",
            " Loss:  0.31678506731987\n",
            "Training:  Iteration No:  188 Worker Num:  4 \n",
            " Loss:  0.27330392599105835\n",
            "Training:  Iteration No:  188 Worker Num:  5 \n",
            " Loss:  0.2856096029281616\n",
            "Training:  Iteration No:  188 Worker Num:  6 \n",
            " Loss:  0.2689873278141022\n",
            "Training:  Iteration No:  188 Worker Num:  7 \n",
            " Loss:  0.21499386429786682\n",
            "Test:  Iteration No:  188 \n",
            " Loss:  0.24412495249151428\n",
            "Test accuracy:  92.74\n",
            "Training:  Iteration No:  189 Worker Num:  0 \n",
            " Loss:  0.3761070966720581\n",
            "Training:  Iteration No:  189 Worker Num:  1 \n",
            " Loss:  0.29060599207878113\n",
            "Training:  Iteration No:  189 Worker Num:  2 \n",
            " Loss:  0.24650563299655914\n",
            "Training:  Iteration No:  189 Worker Num:  3 \n",
            " Loss:  0.2951330244541168\n",
            "Training:  Iteration No:  189 Worker Num:  4 \n",
            " Loss:  0.2976440489292145\n",
            "Training:  Iteration No:  189 Worker Num:  5 \n",
            " Loss:  0.31690073013305664\n",
            "Training:  Iteration No:  189 Worker Num:  6 \n",
            " Loss:  0.2393294721841812\n",
            "Training:  Iteration No:  189 Worker Num:  7 \n",
            " Loss:  0.2049066722393036\n",
            "Test:  Iteration No:  189 \n",
            " Loss:  0.24582600890647008\n",
            "Test accuracy:  92.71\n",
            "Training:  Iteration No:  190 Worker Num:  0 \n",
            " Loss:  0.371917188167572\n",
            "Training:  Iteration No:  190 Worker Num:  1 \n",
            " Loss:  0.32703733444213867\n",
            "Training:  Iteration No:  190 Worker Num:  2 \n",
            " Loss:  0.32765036821365356\n",
            "Training:  Iteration No:  190 Worker Num:  3 \n",
            " Loss:  0.29494965076446533\n",
            "Training:  Iteration No:  190 Worker Num:  4 \n",
            " Loss:  0.2347925752401352\n",
            "Training:  Iteration No:  190 Worker Num:  5 \n",
            " Loss:  0.3008147180080414\n",
            "Training:  Iteration No:  190 Worker Num:  6 \n",
            " Loss:  0.3743019700050354\n",
            "Training:  Iteration No:  190 Worker Num:  7 \n",
            " Loss:  0.3579546809196472\n",
            "Test:  Iteration No:  190 \n",
            " Loss:  0.24923697984011112\n",
            "Test accuracy:  92.46\n",
            "Training:  Iteration No:  191 Worker Num:  0 \n",
            " Loss:  0.28051528334617615\n",
            "Training:  Iteration No:  191 Worker Num:  1 \n",
            " Loss:  0.25235316157341003\n",
            "Training:  Iteration No:  191 Worker Num:  2 \n",
            " Loss:  0.40346598625183105\n",
            "Training:  Iteration No:  191 Worker Num:  3 \n",
            " Loss:  0.30491796135902405\n",
            "Training:  Iteration No:  191 Worker Num:  4 \n",
            " Loss:  0.232924684882164\n",
            "Training:  Iteration No:  191 Worker Num:  5 \n",
            " Loss:  0.15781836211681366\n",
            "Training:  Iteration No:  191 Worker Num:  6 \n",
            " Loss:  0.3569399416446686\n",
            "Training:  Iteration No:  191 Worker Num:  7 \n",
            " Loss:  0.4004240036010742\n",
            "Test:  Iteration No:  191 \n",
            " Loss:  0.2471042698885821\n",
            "Test accuracy:  92.84\n",
            "Training:  Iteration No:  192 Worker Num:  0 \n",
            " Loss:  0.4139616787433624\n",
            "Training:  Iteration No:  192 Worker Num:  1 \n",
            " Loss:  0.23518456518650055\n",
            "Training:  Iteration No:  192 Worker Num:  2 \n",
            " Loss:  0.2795467674732208\n",
            "Training:  Iteration No:  192 Worker Num:  3 \n",
            " Loss:  0.2878492772579193\n",
            "Training:  Iteration No:  192 Worker Num:  4 \n",
            " Loss:  0.33212292194366455\n",
            "Training:  Iteration No:  192 Worker Num:  5 \n",
            " Loss:  0.3849775195121765\n",
            "Training:  Iteration No:  192 Worker Num:  6 \n",
            " Loss:  0.23475782573223114\n",
            "Training:  Iteration No:  192 Worker Num:  7 \n",
            " Loss:  0.2649809718132019\n",
            "Test:  Iteration No:  192 \n",
            " Loss:  0.24633729757293116\n",
            "Test accuracy:  92.76\n",
            "Training:  Iteration No:  193 Worker Num:  0 \n",
            " Loss:  0.28248658776283264\n",
            "Training:  Iteration No:  193 Worker Num:  1 \n",
            " Loss:  0.24832391738891602\n",
            "Training:  Iteration No:  193 Worker Num:  2 \n",
            " Loss:  0.25303325057029724\n",
            "Training:  Iteration No:  193 Worker Num:  3 \n",
            " Loss:  0.360198974609375\n",
            "Training:  Iteration No:  193 Worker Num:  4 \n",
            " Loss:  0.2453112155199051\n",
            "Training:  Iteration No:  193 Worker Num:  5 \n",
            " Loss:  0.3478037118911743\n",
            "Training:  Iteration No:  193 Worker Num:  6 \n",
            " Loss:  0.3042069971561432\n",
            "Training:  Iteration No:  193 Worker Num:  7 \n",
            " Loss:  0.335566908121109\n",
            "Test:  Iteration No:  193 \n",
            " Loss:  0.24121484905481339\n",
            "Test accuracy:  93.01\n",
            "Training:  Iteration No:  194 Worker Num:  0 \n",
            " Loss:  0.2849985659122467\n",
            "Training:  Iteration No:  194 Worker Num:  1 \n",
            " Loss:  0.37515461444854736\n",
            "Training:  Iteration No:  194 Worker Num:  2 \n",
            " Loss:  0.296475350856781\n",
            "Training:  Iteration No:  194 Worker Num:  3 \n",
            " Loss:  0.23630578815937042\n",
            "Training:  Iteration No:  194 Worker Num:  4 \n",
            " Loss:  0.3238902688026428\n",
            "Training:  Iteration No:  194 Worker Num:  5 \n",
            " Loss:  0.33765339851379395\n",
            "Training:  Iteration No:  194 Worker Num:  6 \n",
            " Loss:  0.3291967213153839\n",
            "Training:  Iteration No:  194 Worker Num:  7 \n",
            " Loss:  0.3933727443218231\n",
            "Test:  Iteration No:  194 \n",
            " Loss:  0.23862413262736193\n",
            "Test accuracy:  92.99\n",
            "Training:  Iteration No:  195 Worker Num:  0 \n",
            " Loss:  0.1833360642194748\n",
            "Training:  Iteration No:  195 Worker Num:  1 \n",
            " Loss:  0.37602177262306213\n",
            "Training:  Iteration No:  195 Worker Num:  2 \n",
            " Loss:  0.34651198983192444\n",
            "Training:  Iteration No:  195 Worker Num:  3 \n",
            " Loss:  0.2867775857448578\n",
            "Training:  Iteration No:  195 Worker Num:  4 \n",
            " Loss:  0.26554664969444275\n",
            "Training:  Iteration No:  195 Worker Num:  5 \n",
            " Loss:  0.279488742351532\n",
            "Training:  Iteration No:  195 Worker Num:  6 \n",
            " Loss:  0.1955655962228775\n",
            "Training:  Iteration No:  195 Worker Num:  7 \n",
            " Loss:  0.33435380458831787\n",
            "Test:  Iteration No:  195 \n",
            " Loss:  0.239446733689195\n",
            "Test accuracy:  93.01\n",
            "Training:  Iteration No:  196 Worker Num:  0 \n",
            " Loss:  0.25691550970077515\n",
            "Training:  Iteration No:  196 Worker Num:  1 \n",
            " Loss:  0.32939672470092773\n",
            "Training:  Iteration No:  196 Worker Num:  2 \n",
            " Loss:  0.35935255885124207\n",
            "Training:  Iteration No:  196 Worker Num:  3 \n",
            " Loss:  0.20022311806678772\n",
            "Training:  Iteration No:  196 Worker Num:  4 \n",
            " Loss:  0.225246861577034\n",
            "Training:  Iteration No:  196 Worker Num:  5 \n",
            " Loss:  0.2227647304534912\n",
            "Training:  Iteration No:  196 Worker Num:  6 \n",
            " Loss:  0.29818588495254517\n",
            "Training:  Iteration No:  196 Worker Num:  7 \n",
            " Loss:  0.26697438955307007\n",
            "Test:  Iteration No:  196 \n",
            " Loss:  0.24171975323388092\n",
            "Test accuracy:  92.9\n",
            "Training:  Iteration No:  197 Worker Num:  0 \n",
            " Loss:  0.2899320721626282\n",
            "Training:  Iteration No:  197 Worker Num:  1 \n",
            " Loss:  0.22490613162517548\n",
            "Training:  Iteration No:  197 Worker Num:  2 \n",
            " Loss:  0.32112917304039\n",
            "Training:  Iteration No:  197 Worker Num:  3 \n",
            " Loss:  0.3650088608264923\n",
            "Training:  Iteration No:  197 Worker Num:  4 \n",
            " Loss:  0.2630082964897156\n",
            "Training:  Iteration No:  197 Worker Num:  5 \n",
            " Loss:  0.26582905650138855\n",
            "Training:  Iteration No:  197 Worker Num:  6 \n",
            " Loss:  0.22362887859344482\n",
            "Training:  Iteration No:  197 Worker Num:  7 \n",
            " Loss:  0.37117040157318115\n",
            "Test:  Iteration No:  197 \n",
            " Loss:  0.23496569247468363\n",
            "Test accuracy:  93.07\n",
            "Training:  Iteration No:  198 Worker Num:  0 \n",
            " Loss:  0.3821614384651184\n",
            "Training:  Iteration No:  198 Worker Num:  1 \n",
            " Loss:  0.29233092069625854\n",
            "Training:  Iteration No:  198 Worker Num:  2 \n",
            " Loss:  0.31058329343795776\n",
            "Training:  Iteration No:  198 Worker Num:  3 \n",
            " Loss:  0.29490649700164795\n",
            "Training:  Iteration No:  198 Worker Num:  4 \n",
            " Loss:  0.3209471106529236\n",
            "Training:  Iteration No:  198 Worker Num:  5 \n",
            " Loss:  0.3075706958770752\n",
            "Training:  Iteration No:  198 Worker Num:  6 \n",
            " Loss:  0.21974420547485352\n",
            "Training:  Iteration No:  198 Worker Num:  7 \n",
            " Loss:  0.3184935450553894\n",
            "Test:  Iteration No:  198 \n",
            " Loss:  0.23955058396051201\n",
            "Test accuracy:  92.96\n",
            "Training:  Iteration No:  199 Worker Num:  0 \n",
            " Loss:  0.29766350984573364\n",
            "Training:  Iteration No:  199 Worker Num:  1 \n",
            " Loss:  0.3240199089050293\n",
            "Training:  Iteration No:  199 Worker Num:  2 \n",
            " Loss:  0.29782596230506897\n",
            "Training:  Iteration No:  199 Worker Num:  3 \n",
            " Loss:  0.2920861840248108\n",
            "Training:  Iteration No:  199 Worker Num:  4 \n",
            " Loss:  0.18269488215446472\n",
            "Training:  Iteration No:  199 Worker Num:  5 \n",
            " Loss:  0.4153987467288971\n",
            "Training:  Iteration No:  199 Worker Num:  6 \n",
            " Loss:  0.25621041655540466\n",
            "Training:  Iteration No:  199 Worker Num:  7 \n",
            " Loss:  0.19310416281223297\n",
            "Test:  Iteration No:  199 \n",
            " Loss:  0.2355177636387982\n",
            "Test accuracy:  93.24\n",
            "Training:  Iteration No:  200 Worker Num:  0 \n",
            " Loss:  0.2713737189769745\n",
            "Training:  Iteration No:  200 Worker Num:  1 \n",
            " Loss:  0.3873298168182373\n",
            "Training:  Iteration No:  200 Worker Num:  2 \n",
            " Loss:  0.2812685966491699\n",
            "Training:  Iteration No:  200 Worker Num:  3 \n",
            " Loss:  0.279731810092926\n",
            "Training:  Iteration No:  200 Worker Num:  4 \n",
            " Loss:  0.2639312446117401\n",
            "Training:  Iteration No:  200 Worker Num:  5 \n",
            " Loss:  0.327751100063324\n",
            "Training:  Iteration No:  200 Worker Num:  6 \n",
            " Loss:  0.33261826634407043\n",
            "Training:  Iteration No:  200 Worker Num:  7 \n",
            " Loss:  0.22435471415519714\n",
            "Test:  Iteration No:  200 \n",
            " Loss:  0.2372476442045049\n",
            "Test accuracy:  92.85\n",
            "Training:  Iteration No:  201 Worker Num:  0 \n",
            " Loss:  0.2486427277326584\n",
            "Training:  Iteration No:  201 Worker Num:  1 \n",
            " Loss:  0.15552271902561188\n",
            "Training:  Iteration No:  201 Worker Num:  2 \n",
            " Loss:  0.1341724842786789\n",
            "Training:  Iteration No:  201 Worker Num:  3 \n",
            " Loss:  0.32092025876045227\n",
            "Training:  Iteration No:  201 Worker Num:  4 \n",
            " Loss:  0.24501267075538635\n",
            "Training:  Iteration No:  201 Worker Num:  5 \n",
            " Loss:  0.24367934465408325\n",
            "Training:  Iteration No:  201 Worker Num:  6 \n",
            " Loss:  0.29689499735832214\n",
            "Training:  Iteration No:  201 Worker Num:  7 \n",
            " Loss:  0.5195361375808716\n",
            "Test:  Iteration No:  201 \n",
            " Loss:  0.2346332067788779\n",
            "Test accuracy:  93.15\n",
            "Training:  Iteration No:  202 Worker Num:  0 \n",
            " Loss:  0.3201349079608917\n",
            "Training:  Iteration No:  202 Worker Num:  1 \n",
            " Loss:  0.19740505516529083\n",
            "Training:  Iteration No:  202 Worker Num:  2 \n",
            " Loss:  0.3808882534503937\n",
            "Training:  Iteration No:  202 Worker Num:  3 \n",
            " Loss:  0.45876652002334595\n",
            "Training:  Iteration No:  202 Worker Num:  4 \n",
            " Loss:  0.24936887621879578\n",
            "Training:  Iteration No:  202 Worker Num:  5 \n",
            " Loss:  0.3301143944263458\n",
            "Training:  Iteration No:  202 Worker Num:  6 \n",
            " Loss:  0.35093456506729126\n",
            "Training:  Iteration No:  202 Worker Num:  7 \n",
            " Loss:  0.24639937281608582\n",
            "Test:  Iteration No:  202 \n",
            " Loss:  0.23735169402641965\n",
            "Test accuracy:  93.07\n",
            "Training:  Iteration No:  203 Worker Num:  0 \n",
            " Loss:  0.19355261325836182\n",
            "Training:  Iteration No:  203 Worker Num:  1 \n",
            " Loss:  0.28934353590011597\n",
            "Training:  Iteration No:  203 Worker Num:  2 \n",
            " Loss:  0.275634765625\n",
            "Training:  Iteration No:  203 Worker Num:  3 \n",
            " Loss:  0.2764303386211395\n",
            "Training:  Iteration No:  203 Worker Num:  4 \n",
            " Loss:  0.40039998292922974\n",
            "Training:  Iteration No:  203 Worker Num:  5 \n",
            " Loss:  0.32058507204055786\n",
            "Training:  Iteration No:  203 Worker Num:  6 \n",
            " Loss:  0.4036552906036377\n",
            "Training:  Iteration No:  203 Worker Num:  7 \n",
            " Loss:  0.2385135143995285\n",
            "Test:  Iteration No:  203 \n",
            " Loss:  0.2330163574624288\n",
            "Test accuracy:  93.26\n",
            "Training:  Iteration No:  204 Worker Num:  0 \n",
            " Loss:  0.23489099740982056\n",
            "Training:  Iteration No:  204 Worker Num:  1 \n",
            " Loss:  0.4008719027042389\n",
            "Training:  Iteration No:  204 Worker Num:  2 \n",
            " Loss:  0.3519126772880554\n",
            "Training:  Iteration No:  204 Worker Num:  3 \n",
            " Loss:  0.2202659696340561\n",
            "Training:  Iteration No:  204 Worker Num:  4 \n",
            " Loss:  0.33343759179115295\n",
            "Training:  Iteration No:  204 Worker Num:  5 \n",
            " Loss:  0.3570546507835388\n",
            "Training:  Iteration No:  204 Worker Num:  6 \n",
            " Loss:  0.38777321577072144\n",
            "Training:  Iteration No:  204 Worker Num:  7 \n",
            " Loss:  0.28311797976493835\n",
            "Test:  Iteration No:  204 \n",
            " Loss:  0.2311792158392034\n",
            "Test accuracy:  93.07\n",
            "Training:  Iteration No:  205 Worker Num:  0 \n",
            " Loss:  0.15103645622730255\n",
            "Training:  Iteration No:  205 Worker Num:  1 \n",
            " Loss:  0.3135856091976166\n",
            "Training:  Iteration No:  205 Worker Num:  2 \n",
            " Loss:  0.1763608306646347\n",
            "Training:  Iteration No:  205 Worker Num:  3 \n",
            " Loss:  0.2976672649383545\n",
            "Training:  Iteration No:  205 Worker Num:  4 \n",
            " Loss:  0.2402459979057312\n",
            "Training:  Iteration No:  205 Worker Num:  5 \n",
            " Loss:  0.23780423402786255\n",
            "Training:  Iteration No:  205 Worker Num:  6 \n",
            " Loss:  0.3982822895050049\n",
            "Training:  Iteration No:  205 Worker Num:  7 \n",
            " Loss:  0.22010497748851776\n",
            "Test:  Iteration No:  205 \n",
            " Loss:  0.2324709570436161\n",
            "Test accuracy:  93.11\n",
            "Training:  Iteration No:  206 Worker Num:  0 \n",
            " Loss:  0.3467330038547516\n",
            "Training:  Iteration No:  206 Worker Num:  1 \n",
            " Loss:  0.3245500326156616\n",
            "Training:  Iteration No:  206 Worker Num:  2 \n",
            " Loss:  0.2429761290550232\n",
            "Training:  Iteration No:  206 Worker Num:  3 \n",
            " Loss:  0.3322001099586487\n",
            "Training:  Iteration No:  206 Worker Num:  4 \n",
            " Loss:  0.20712818205356598\n",
            "Training:  Iteration No:  206 Worker Num:  5 \n",
            " Loss:  0.30422577261924744\n",
            "Training:  Iteration No:  206 Worker Num:  6 \n",
            " Loss:  0.24215050041675568\n",
            "Training:  Iteration No:  206 Worker Num:  7 \n",
            " Loss:  0.299861878156662\n",
            "Test:  Iteration No:  206 \n",
            " Loss:  0.23070335376394702\n",
            "Test accuracy:  93.19\n",
            "Training:  Iteration No:  207 Worker Num:  0 \n",
            " Loss:  0.22478532791137695\n",
            "Training:  Iteration No:  207 Worker Num:  1 \n",
            " Loss:  0.3876020908355713\n",
            "Training:  Iteration No:  207 Worker Num:  2 \n",
            " Loss:  0.304047554731369\n",
            "Training:  Iteration No:  207 Worker Num:  3 \n",
            " Loss:  0.29863283038139343\n",
            "Training:  Iteration No:  207 Worker Num:  4 \n",
            " Loss:  0.18108250200748444\n",
            "Training:  Iteration No:  207 Worker Num:  5 \n",
            " Loss:  0.29572632908821106\n",
            "Training:  Iteration No:  207 Worker Num:  6 \n",
            " Loss:  0.26687800884246826\n",
            "Training:  Iteration No:  207 Worker Num:  7 \n",
            " Loss:  0.3243361711502075\n",
            "Test:  Iteration No:  207 \n",
            " Loss:  0.2260083167966974\n",
            "Test accuracy:  93.25\n",
            "Training:  Iteration No:  208 Worker Num:  0 \n",
            " Loss:  0.2647557854652405\n",
            "Training:  Iteration No:  208 Worker Num:  1 \n",
            " Loss:  0.31348907947540283\n",
            "Training:  Iteration No:  208 Worker Num:  2 \n",
            " Loss:  0.324270099401474\n",
            "Training:  Iteration No:  208 Worker Num:  3 \n",
            " Loss:  0.2518801689147949\n",
            "Training:  Iteration No:  208 Worker Num:  4 \n",
            " Loss:  0.20304815471172333\n",
            "Training:  Iteration No:  208 Worker Num:  5 \n",
            " Loss:  0.2461717575788498\n",
            "Training:  Iteration No:  208 Worker Num:  6 \n",
            " Loss:  0.2454516887664795\n",
            "Training:  Iteration No:  208 Worker Num:  7 \n",
            " Loss:  0.23108750581741333\n",
            "Test:  Iteration No:  208 \n",
            " Loss:  0.23560533669034514\n",
            "Test accuracy:  92.86\n",
            "Training:  Iteration No:  209 Worker Num:  0 \n",
            " Loss:  0.18025918304920197\n",
            "Training:  Iteration No:  209 Worker Num:  1 \n",
            " Loss:  0.4367828071117401\n",
            "Training:  Iteration No:  209 Worker Num:  2 \n",
            " Loss:  0.1955661028623581\n",
            "Training:  Iteration No:  209 Worker Num:  3 \n",
            " Loss:  0.1736540049314499\n",
            "Training:  Iteration No:  209 Worker Num:  4 \n",
            " Loss:  0.19539332389831543\n",
            "Training:  Iteration No:  209 Worker Num:  5 \n",
            " Loss:  0.3447570204734802\n",
            "Training:  Iteration No:  209 Worker Num:  6 \n",
            " Loss:  0.22048652172088623\n",
            "Training:  Iteration No:  209 Worker Num:  7 \n",
            " Loss:  0.3302438259124756\n",
            "Test:  Iteration No:  209 \n",
            " Loss:  0.2234435660080819\n",
            "Test accuracy:  93.39\n",
            "Training:  Iteration No:  210 Worker Num:  0 \n",
            " Loss:  0.3435716927051544\n",
            "Training:  Iteration No:  210 Worker Num:  1 \n",
            " Loss:  0.22323042154312134\n",
            "Training:  Iteration No:  210 Worker Num:  2 \n",
            " Loss:  0.24661384522914886\n",
            "Training:  Iteration No:  210 Worker Num:  3 \n",
            " Loss:  0.37866994738578796\n",
            "Training:  Iteration No:  210 Worker Num:  4 \n",
            " Loss:  0.3266275227069855\n",
            "Training:  Iteration No:  210 Worker Num:  5 \n",
            " Loss:  0.2338310331106186\n",
            "Training:  Iteration No:  210 Worker Num:  6 \n",
            " Loss:  0.4061993658542633\n",
            "Training:  Iteration No:  210 Worker Num:  7 \n",
            " Loss:  0.35099342465400696\n",
            "Test:  Iteration No:  210 \n",
            " Loss:  0.22379625815002224\n",
            "Test accuracy:  93.53\n",
            "Training:  Iteration No:  211 Worker Num:  0 \n",
            " Loss:  0.15607132017612457\n",
            "Training:  Iteration No:  211 Worker Num:  1 \n",
            " Loss:  0.2197665572166443\n",
            "Training:  Iteration No:  211 Worker Num:  2 \n",
            " Loss:  0.26513412594795227\n",
            "Training:  Iteration No:  211 Worker Num:  3 \n",
            " Loss:  0.24952127039432526\n",
            "Training:  Iteration No:  211 Worker Num:  4 \n",
            " Loss:  0.33256492018699646\n",
            "Training:  Iteration No:  211 Worker Num:  5 \n",
            " Loss:  0.33708611130714417\n",
            "Training:  Iteration No:  211 Worker Num:  6 \n",
            " Loss:  0.2255927473306656\n",
            "Training:  Iteration No:  211 Worker Num:  7 \n",
            " Loss:  0.29448366165161133\n",
            "Test:  Iteration No:  211 \n",
            " Loss:  0.22570177811327613\n",
            "Test accuracy:  93.19\n",
            "Training:  Iteration No:  212 Worker Num:  0 \n",
            " Loss:  0.22155001759529114\n",
            "Training:  Iteration No:  212 Worker Num:  1 \n",
            " Loss:  0.2868025600910187\n",
            "Training:  Iteration No:  212 Worker Num:  2 \n",
            " Loss:  0.25339993834495544\n",
            "Training:  Iteration No:  212 Worker Num:  3 \n",
            " Loss:  0.24327051639556885\n",
            "Training:  Iteration No:  212 Worker Num:  4 \n",
            " Loss:  0.2947508990764618\n",
            "Training:  Iteration No:  212 Worker Num:  5 \n",
            " Loss:  0.2025323510169983\n",
            "Training:  Iteration No:  212 Worker Num:  6 \n",
            " Loss:  0.31244635581970215\n",
            "Training:  Iteration No:  212 Worker Num:  7 \n",
            " Loss:  0.41698315739631653\n",
            "Test:  Iteration No:  212 \n",
            " Loss:  0.2251533365608016\n",
            "Test accuracy:  93.26\n",
            "Training:  Iteration No:  213 Worker Num:  0 \n",
            " Loss:  0.30395177006721497\n",
            "Training:  Iteration No:  213 Worker Num:  1 \n",
            " Loss:  0.16981975734233856\n",
            "Training:  Iteration No:  213 Worker Num:  2 \n",
            " Loss:  0.29037144780158997\n",
            "Training:  Iteration No:  213 Worker Num:  3 \n",
            " Loss:  0.2871302664279938\n",
            "Training:  Iteration No:  213 Worker Num:  4 \n",
            " Loss:  0.25449663400650024\n",
            "Training:  Iteration No:  213 Worker Num:  5 \n",
            " Loss:  0.29415807127952576\n",
            "Training:  Iteration No:  213 Worker Num:  6 \n",
            " Loss:  0.19471144676208496\n",
            "Training:  Iteration No:  213 Worker Num:  7 \n",
            " Loss:  0.2770635783672333\n",
            "Test:  Iteration No:  213 \n",
            " Loss:  0.22605749566249456\n",
            "Test accuracy:  93.21\n",
            "Training:  Iteration No:  214 Worker Num:  0 \n",
            " Loss:  0.32424071431159973\n",
            "Training:  Iteration No:  214 Worker Num:  1 \n",
            " Loss:  0.2942808270454407\n",
            "Training:  Iteration No:  214 Worker Num:  2 \n",
            " Loss:  0.2701861560344696\n",
            "Training:  Iteration No:  214 Worker Num:  3 \n",
            " Loss:  0.29991021752357483\n",
            "Training:  Iteration No:  214 Worker Num:  4 \n",
            " Loss:  0.20509888231754303\n",
            "Training:  Iteration No:  214 Worker Num:  5 \n",
            " Loss:  0.34876763820648193\n",
            "Training:  Iteration No:  214 Worker Num:  6 \n",
            " Loss:  0.34524133801460266\n",
            "Training:  Iteration No:  214 Worker Num:  7 \n",
            " Loss:  0.2862160801887512\n",
            "Test:  Iteration No:  214 \n",
            " Loss:  0.22305868474082857\n",
            "Test accuracy:  93.4\n",
            "Training:  Iteration No:  215 Worker Num:  0 \n",
            " Loss:  0.24543283879756927\n",
            "Training:  Iteration No:  215 Worker Num:  1 \n",
            " Loss:  0.23437465727329254\n",
            "Training:  Iteration No:  215 Worker Num:  2 \n",
            " Loss:  0.24227389693260193\n",
            "Training:  Iteration No:  215 Worker Num:  3 \n",
            " Loss:  0.36203113198280334\n",
            "Training:  Iteration No:  215 Worker Num:  4 \n",
            " Loss:  0.20383526384830475\n",
            "Training:  Iteration No:  215 Worker Num:  5 \n",
            " Loss:  0.33277636766433716\n",
            "Training:  Iteration No:  215 Worker Num:  6 \n",
            " Loss:  0.31574562191963196\n",
            "Training:  Iteration No:  215 Worker Num:  7 \n",
            " Loss:  0.3159725069999695\n",
            "Test:  Iteration No:  215 \n",
            " Loss:  0.22332707383445924\n",
            "Test accuracy:  93.24\n",
            "Training:  Iteration No:  216 Worker Num:  0 \n",
            " Loss:  0.3276682496070862\n",
            "Training:  Iteration No:  216 Worker Num:  1 \n",
            " Loss:  0.16980455815792084\n",
            "Training:  Iteration No:  216 Worker Num:  2 \n",
            " Loss:  0.3313707411289215\n",
            "Training:  Iteration No:  216 Worker Num:  3 \n",
            " Loss:  0.22007107734680176\n",
            "Training:  Iteration No:  216 Worker Num:  4 \n",
            " Loss:  0.24267467856407166\n",
            "Training:  Iteration No:  216 Worker Num:  5 \n",
            " Loss:  0.24454908072948456\n",
            "Training:  Iteration No:  216 Worker Num:  6 \n",
            " Loss:  0.28093600273132324\n",
            "Training:  Iteration No:  216 Worker Num:  7 \n",
            " Loss:  0.32289430499076843\n",
            "Test:  Iteration No:  216 \n",
            " Loss:  0.21844209285004984\n",
            "Test accuracy:  93.43\n",
            "Training:  Iteration No:  217 Worker Num:  0 \n",
            " Loss:  0.32587730884552\n",
            "Training:  Iteration No:  217 Worker Num:  1 \n",
            " Loss:  0.22579129040241241\n",
            "Training:  Iteration No:  217 Worker Num:  2 \n",
            " Loss:  0.19994258880615234\n",
            "Training:  Iteration No:  217 Worker Num:  3 \n",
            " Loss:  0.21135179698467255\n",
            "Training:  Iteration No:  217 Worker Num:  4 \n",
            " Loss:  0.29057905077934265\n",
            "Training:  Iteration No:  217 Worker Num:  5 \n",
            " Loss:  0.3195270001888275\n",
            "Training:  Iteration No:  217 Worker Num:  6 \n",
            " Loss:  0.3380703926086426\n",
            "Training:  Iteration No:  217 Worker Num:  7 \n",
            " Loss:  0.1781216412782669\n",
            "Test:  Iteration No:  217 \n",
            " Loss:  0.2211894036824756\n",
            "Test accuracy:  93.4\n",
            "Training:  Iteration No:  218 Worker Num:  0 \n",
            " Loss:  0.10391926765441895\n",
            "Training:  Iteration No:  218 Worker Num:  1 \n",
            " Loss:  0.22214549779891968\n",
            "Training:  Iteration No:  218 Worker Num:  2 \n",
            " Loss:  0.2785249650478363\n",
            "Training:  Iteration No:  218 Worker Num:  3 \n",
            " Loss:  0.25555673241615295\n",
            "Training:  Iteration No:  218 Worker Num:  4 \n",
            " Loss:  0.2006901353597641\n",
            "Training:  Iteration No:  218 Worker Num:  5 \n",
            " Loss:  0.26190242171287537\n",
            "Training:  Iteration No:  218 Worker Num:  6 \n",
            " Loss:  0.2765999436378479\n",
            "Training:  Iteration No:  218 Worker Num:  7 \n",
            " Loss:  0.18062074482440948\n",
            "Test:  Iteration No:  218 \n",
            " Loss:  0.22186830162388993\n",
            "Test accuracy:  93.26\n",
            "Training:  Iteration No:  219 Worker Num:  0 \n",
            " Loss:  0.14983732998371124\n",
            "Training:  Iteration No:  219 Worker Num:  1 \n",
            " Loss:  0.32379618287086487\n",
            "Training:  Iteration No:  219 Worker Num:  2 \n",
            " Loss:  0.3229324221611023\n",
            "Training:  Iteration No:  219 Worker Num:  3 \n",
            " Loss:  0.2793086767196655\n",
            "Training:  Iteration No:  219 Worker Num:  4 \n",
            " Loss:  0.30565279722213745\n",
            "Training:  Iteration No:  219 Worker Num:  5 \n",
            " Loss:  0.2518880069255829\n",
            "Training:  Iteration No:  219 Worker Num:  6 \n",
            " Loss:  0.33730271458625793\n",
            "Training:  Iteration No:  219 Worker Num:  7 \n",
            " Loss:  0.16770194470882416\n",
            "Test:  Iteration No:  219 \n",
            " Loss:  0.2212259647306762\n",
            "Test accuracy:  93.33\n",
            "Training:  Iteration No:  220 Worker Num:  0 \n",
            " Loss:  0.3528742492198944\n",
            "Training:  Iteration No:  220 Worker Num:  1 \n",
            " Loss:  0.25089943408966064\n",
            "Training:  Iteration No:  220 Worker Num:  2 \n",
            " Loss:  0.24432294070720673\n",
            "Training:  Iteration No:  220 Worker Num:  3 \n",
            " Loss:  0.3165137767791748\n",
            "Training:  Iteration No:  220 Worker Num:  4 \n",
            " Loss:  0.2722442150115967\n",
            "Training:  Iteration No:  220 Worker Num:  5 \n",
            " Loss:  0.40041565895080566\n",
            "Training:  Iteration No:  220 Worker Num:  6 \n",
            " Loss:  0.21227534115314484\n",
            "Training:  Iteration No:  220 Worker Num:  7 \n",
            " Loss:  0.19219841063022614\n",
            "Test:  Iteration No:  220 \n",
            " Loss:  0.21820221141075033\n",
            "Test accuracy:  93.43\n",
            "Training:  Iteration No:  221 Worker Num:  0 \n",
            " Loss:  0.22287963330745697\n",
            "Training:  Iteration No:  221 Worker Num:  1 \n",
            " Loss:  0.16474388539791107\n",
            "Training:  Iteration No:  221 Worker Num:  2 \n",
            " Loss:  0.26035550236701965\n",
            "Training:  Iteration No:  221 Worker Num:  3 \n",
            " Loss:  0.28082412481307983\n",
            "Training:  Iteration No:  221 Worker Num:  4 \n",
            " Loss:  0.1924372762441635\n",
            "Training:  Iteration No:  221 Worker Num:  5 \n",
            " Loss:  0.27236753702163696\n",
            "Training:  Iteration No:  221 Worker Num:  6 \n",
            " Loss:  0.22262045741081238\n",
            "Training:  Iteration No:  221 Worker Num:  7 \n",
            " Loss:  0.12881897389888763\n",
            "Test:  Iteration No:  221 \n",
            " Loss:  0.22159853176791458\n",
            "Test accuracy:  93.21\n",
            "Training:  Iteration No:  222 Worker Num:  0 \n",
            " Loss:  0.286969393491745\n",
            "Training:  Iteration No:  222 Worker Num:  1 \n",
            " Loss:  0.26345789432525635\n",
            "Training:  Iteration No:  222 Worker Num:  2 \n",
            " Loss:  0.1955978125333786\n",
            "Training:  Iteration No:  222 Worker Num:  3 \n",
            " Loss:  0.23001009225845337\n",
            "Training:  Iteration No:  222 Worker Num:  4 \n",
            " Loss:  0.28365978598594666\n",
            "Training:  Iteration No:  222 Worker Num:  5 \n",
            " Loss:  0.20709596574306488\n",
            "Training:  Iteration No:  222 Worker Num:  6 \n",
            " Loss:  0.2928467392921448\n",
            "Training:  Iteration No:  222 Worker Num:  7 \n",
            " Loss:  0.25163277983665466\n",
            "Test:  Iteration No:  222 \n",
            " Loss:  0.2207396915654027\n",
            "Test accuracy:  93.33\n",
            "Training:  Iteration No:  223 Worker Num:  0 \n",
            " Loss:  0.2540339231491089\n",
            "Training:  Iteration No:  223 Worker Num:  1 \n",
            " Loss:  0.21788857877254486\n",
            "Training:  Iteration No:  223 Worker Num:  2 \n",
            " Loss:  0.20884035527706146\n",
            "Training:  Iteration No:  223 Worker Num:  3 \n",
            " Loss:  0.26366546750068665\n",
            "Training:  Iteration No:  223 Worker Num:  4 \n",
            " Loss:  0.17502732574939728\n",
            "Training:  Iteration No:  223 Worker Num:  5 \n",
            " Loss:  0.24161475896835327\n",
            "Training:  Iteration No:  223 Worker Num:  6 \n",
            " Loss:  0.22895264625549316\n",
            "Training:  Iteration No:  223 Worker Num:  7 \n",
            " Loss:  0.4417558014392853\n",
            "Test:  Iteration No:  223 \n",
            " Loss:  0.21764589923940883\n",
            "Test accuracy:  93.39\n",
            "Training:  Iteration No:  224 Worker Num:  0 \n",
            " Loss:  0.1891164928674698\n",
            "Training:  Iteration No:  224 Worker Num:  1 \n",
            " Loss:  0.3812393546104431\n",
            "Training:  Iteration No:  224 Worker Num:  2 \n",
            " Loss:  0.2859031558036804\n",
            "Training:  Iteration No:  224 Worker Num:  3 \n",
            " Loss:  0.2508128583431244\n",
            "Training:  Iteration No:  224 Worker Num:  4 \n",
            " Loss:  0.2702205181121826\n",
            "Training:  Iteration No:  224 Worker Num:  5 \n",
            " Loss:  0.3465043604373932\n",
            "Training:  Iteration No:  224 Worker Num:  6 \n",
            " Loss:  0.2429545670747757\n",
            "Training:  Iteration No:  224 Worker Num:  7 \n",
            " Loss:  0.2844238579273224\n",
            "Test:  Iteration No:  224 \n",
            " Loss:  0.2145414431048911\n",
            "Test accuracy:  93.56\n",
            "Training:  Iteration No:  225 Worker Num:  0 \n",
            " Loss:  0.2613881230354309\n",
            "Training:  Iteration No:  225 Worker Num:  1 \n",
            " Loss:  0.2362787425518036\n",
            "Training:  Iteration No:  225 Worker Num:  2 \n",
            " Loss:  0.33834972977638245\n",
            "Training:  Iteration No:  225 Worker Num:  3 \n",
            " Loss:  0.2948057949542999\n",
            "Training:  Iteration No:  225 Worker Num:  4 \n",
            " Loss:  0.27747973799705505\n",
            "Training:  Iteration No:  225 Worker Num:  5 \n",
            " Loss:  0.2995465099811554\n",
            "Training:  Iteration No:  225 Worker Num:  6 \n",
            " Loss:  0.2202894240617752\n",
            "Training:  Iteration No:  225 Worker Num:  7 \n",
            " Loss:  0.2867412269115448\n",
            "Test:  Iteration No:  225 \n",
            " Loss:  0.21937482285348675\n",
            "Test accuracy:  93.33\n",
            "Training:  Iteration No:  226 Worker Num:  0 \n",
            " Loss:  0.2958329916000366\n",
            "Training:  Iteration No:  226 Worker Num:  1 \n",
            " Loss:  0.249022975564003\n",
            "Training:  Iteration No:  226 Worker Num:  2 \n",
            " Loss:  0.2891460657119751\n",
            "Training:  Iteration No:  226 Worker Num:  3 \n",
            " Loss:  0.257588654756546\n",
            "Training:  Iteration No:  226 Worker Num:  4 \n",
            " Loss:  0.30266401171684265\n",
            "Training:  Iteration No:  226 Worker Num:  5 \n",
            " Loss:  0.2856849730014801\n",
            "Training:  Iteration No:  226 Worker Num:  6 \n",
            " Loss:  0.23755310475826263\n",
            "Training:  Iteration No:  226 Worker Num:  7 \n",
            " Loss:  0.25622111558914185\n",
            "Test:  Iteration No:  226 \n",
            " Loss:  0.217666229975846\n",
            "Test accuracy:  93.49\n",
            "Training:  Iteration No:  227 Worker Num:  0 \n",
            " Loss:  0.3492642045021057\n",
            "Training:  Iteration No:  227 Worker Num:  1 \n",
            " Loss:  0.33271080255508423\n",
            "Training:  Iteration No:  227 Worker Num:  2 \n",
            " Loss:  0.21895399689674377\n",
            "Training:  Iteration No:  227 Worker Num:  3 \n",
            " Loss:  0.3448496162891388\n",
            "Training:  Iteration No:  227 Worker Num:  4 \n",
            " Loss:  0.2274194210767746\n",
            "Training:  Iteration No:  227 Worker Num:  5 \n",
            " Loss:  0.23324203491210938\n",
            "Training:  Iteration No:  227 Worker Num:  6 \n",
            " Loss:  0.18666450679302216\n",
            "Training:  Iteration No:  227 Worker Num:  7 \n",
            " Loss:  0.28222936391830444\n",
            "Test:  Iteration No:  227 \n",
            " Loss:  0.212110899294479\n",
            "Test accuracy:  93.73\n",
            "Training:  Iteration No:  228 Worker Num:  0 \n",
            " Loss:  0.2903124988079071\n",
            "Training:  Iteration No:  228 Worker Num:  1 \n",
            " Loss:  0.21487821638584137\n",
            "Training:  Iteration No:  228 Worker Num:  2 \n",
            " Loss:  0.22931791841983795\n",
            "Training:  Iteration No:  228 Worker Num:  3 \n",
            " Loss:  0.32607704401016235\n",
            "Training:  Iteration No:  228 Worker Num:  4 \n",
            " Loss:  0.26288163661956787\n",
            "Training:  Iteration No:  228 Worker Num:  5 \n",
            " Loss:  0.35955727100372314\n",
            "Training:  Iteration No:  228 Worker Num:  6 \n",
            " Loss:  0.15878576040267944\n",
            "Training:  Iteration No:  228 Worker Num:  7 \n",
            " Loss:  0.29290398955345154\n",
            "Test:  Iteration No:  228 \n",
            " Loss:  0.21550679617101634\n",
            "Test accuracy:  93.61\n",
            "Training:  Iteration No:  229 Worker Num:  0 \n",
            " Loss:  0.23966775834560394\n",
            "Training:  Iteration No:  229 Worker Num:  1 \n",
            " Loss:  0.31699639558792114\n",
            "Training:  Iteration No:  229 Worker Num:  2 \n",
            " Loss:  0.4805346429347992\n",
            "Training:  Iteration No:  229 Worker Num:  3 \n",
            " Loss:  0.16795101761817932\n",
            "Training:  Iteration No:  229 Worker Num:  4 \n",
            " Loss:  0.2261127233505249\n",
            "Training:  Iteration No:  229 Worker Num:  5 \n",
            " Loss:  0.1979297548532486\n",
            "Training:  Iteration No:  229 Worker Num:  6 \n",
            " Loss:  0.2578374147415161\n",
            "Training:  Iteration No:  229 Worker Num:  7 \n",
            " Loss:  0.220831498503685\n",
            "Test:  Iteration No:  229 \n",
            " Loss:  0.21013005492807943\n",
            "Test accuracy:  93.78\n",
            "Training:  Iteration No:  230 Worker Num:  0 \n",
            " Loss:  0.39399033784866333\n",
            "Training:  Iteration No:  230 Worker Num:  1 \n",
            " Loss:  0.22498677670955658\n",
            "Training:  Iteration No:  230 Worker Num:  2 \n",
            " Loss:  0.30668699741363525\n",
            "Training:  Iteration No:  230 Worker Num:  3 \n",
            " Loss:  0.2277267724275589\n",
            "Training:  Iteration No:  230 Worker Num:  4 \n",
            " Loss:  0.3465695381164551\n",
            "Training:  Iteration No:  230 Worker Num:  5 \n",
            " Loss:  0.21266396343708038\n",
            "Training:  Iteration No:  230 Worker Num:  6 \n",
            " Loss:  0.2625611424446106\n",
            "Training:  Iteration No:  230 Worker Num:  7 \n",
            " Loss:  0.20453712344169617\n",
            "Test:  Iteration No:  230 \n",
            " Loss:  0.2107856598081468\n",
            "Test accuracy:  93.76\n",
            "Training:  Iteration No:  231 Worker Num:  0 \n",
            " Loss:  0.21067292988300323\n",
            "Training:  Iteration No:  231 Worker Num:  1 \n",
            " Loss:  0.2975863814353943\n",
            "Training:  Iteration No:  231 Worker Num:  2 \n",
            " Loss:  0.3222547769546509\n",
            "Training:  Iteration No:  231 Worker Num:  3 \n",
            " Loss:  0.16792117059230804\n",
            "Training:  Iteration No:  231 Worker Num:  4 \n",
            " Loss:  0.31090110540390015\n",
            "Training:  Iteration No:  231 Worker Num:  5 \n",
            " Loss:  0.3213704526424408\n",
            "Training:  Iteration No:  231 Worker Num:  6 \n",
            " Loss:  0.2215125411748886\n",
            "Training:  Iteration No:  231 Worker Num:  7 \n",
            " Loss:  0.3189619779586792\n",
            "Test:  Iteration No:  231 \n",
            " Loss:  0.21095353955568014\n",
            "Test accuracy:  93.64\n",
            "Training:  Iteration No:  232 Worker Num:  0 \n",
            " Loss:  0.19317996501922607\n",
            "Training:  Iteration No:  232 Worker Num:  1 \n",
            " Loss:  0.22095996141433716\n",
            "Training:  Iteration No:  232 Worker Num:  2 \n",
            " Loss:  0.1615356057882309\n",
            "Training:  Iteration No:  232 Worker Num:  3 \n",
            " Loss:  0.3012857139110565\n",
            "Training:  Iteration No:  232 Worker Num:  4 \n",
            " Loss:  0.1512242704629898\n",
            "Training:  Iteration No:  232 Worker Num:  5 \n",
            " Loss:  0.1541421115398407\n",
            "Training:  Iteration No:  232 Worker Num:  6 \n",
            " Loss:  0.37426266074180603\n",
            "Training:  Iteration No:  232 Worker Num:  7 \n",
            " Loss:  0.2525336742401123\n",
            "Test:  Iteration No:  232 \n",
            " Loss:  0.20978085117862572\n",
            "Test accuracy:  93.71\n",
            "Training:  Iteration No:  233 Worker Num:  0 \n",
            " Loss:  0.3155941367149353\n",
            "Training:  Iteration No:  233 Worker Num:  1 \n",
            " Loss:  0.26075971126556396\n",
            "Training:  Iteration No:  233 Worker Num:  2 \n",
            " Loss:  0.21357959508895874\n",
            "Training:  Iteration No:  233 Worker Num:  3 \n",
            " Loss:  0.18334947526454926\n",
            "Training:  Iteration No:  233 Worker Num:  4 \n",
            " Loss:  0.33331912755966187\n",
            "Training:  Iteration No:  233 Worker Num:  5 \n",
            " Loss:  0.2348393350839615\n",
            "Training:  Iteration No:  233 Worker Num:  6 \n",
            " Loss:  0.12991780042648315\n",
            "Training:  Iteration No:  233 Worker Num:  7 \n",
            " Loss:  0.32273271679878235\n",
            "Test:  Iteration No:  233 \n",
            " Loss:  0.21036282154625352\n",
            "Test accuracy:  93.73\n",
            "Training:  Iteration No:  234 Worker Num:  0 \n",
            " Loss:  0.28786298632621765\n",
            "Training:  Iteration No:  234 Worker Num:  1 \n",
            " Loss:  0.3510269820690155\n",
            "Training:  Iteration No:  234 Worker Num:  2 \n",
            " Loss:  0.24980808794498444\n",
            "Training:  Iteration No:  234 Worker Num:  3 \n",
            " Loss:  0.26013901829719543\n",
            "Training:  Iteration No:  234 Worker Num:  4 \n",
            " Loss:  0.2699766159057617\n",
            "Training:  Iteration No:  234 Worker Num:  5 \n",
            " Loss:  0.16870065033435822\n",
            "Training:  Iteration No:  234 Worker Num:  6 \n",
            " Loss:  0.3391086757183075\n",
            "Training:  Iteration No:  234 Worker Num:  7 \n",
            " Loss:  0.3056209683418274\n",
            "Test:  Iteration No:  234 \n",
            " Loss:  0.20895004983331206\n",
            "Test accuracy:  93.77\n",
            "Training:  Iteration No:  235 Worker Num:  0 \n",
            " Loss:  0.26921403408050537\n",
            "Training:  Iteration No:  235 Worker Num:  1 \n",
            " Loss:  0.1740223616361618\n",
            "Training:  Iteration No:  235 Worker Num:  2 \n",
            " Loss:  0.2408965528011322\n",
            "Training:  Iteration No:  235 Worker Num:  3 \n",
            " Loss:  0.27394118905067444\n",
            "Training:  Iteration No:  235 Worker Num:  4 \n",
            " Loss:  0.29224658012390137\n",
            "Training:  Iteration No:  235 Worker Num:  5 \n",
            " Loss:  0.3122946321964264\n",
            "Training:  Iteration No:  235 Worker Num:  6 \n",
            " Loss:  0.2317153513431549\n",
            "Training:  Iteration No:  235 Worker Num:  7 \n",
            " Loss:  0.3277893662452698\n",
            "Test:  Iteration No:  235 \n",
            " Loss:  0.20814598285699193\n",
            "Test accuracy:  93.86\n",
            "Training:  Iteration No:  236 Worker Num:  0 \n",
            " Loss:  0.20332837104797363\n",
            "Training:  Iteration No:  236 Worker Num:  1 \n",
            " Loss:  0.20250745117664337\n",
            "Training:  Iteration No:  236 Worker Num:  2 \n",
            " Loss:  0.2736360430717468\n",
            "Training:  Iteration No:  236 Worker Num:  3 \n",
            " Loss:  0.25447672605514526\n",
            "Training:  Iteration No:  236 Worker Num:  4 \n",
            " Loss:  0.21862606704235077\n",
            "Training:  Iteration No:  236 Worker Num:  5 \n",
            " Loss:  0.3329755961894989\n",
            "Training:  Iteration No:  236 Worker Num:  6 \n",
            " Loss:  0.2100018709897995\n",
            "Training:  Iteration No:  236 Worker Num:  7 \n",
            " Loss:  0.3499206006526947\n",
            "Test:  Iteration No:  236 \n",
            " Loss:  0.21305036031982943\n",
            "Test accuracy:  93.61\n",
            "Training:  Iteration No:  237 Worker Num:  0 \n",
            " Loss:  0.19807469844818115\n",
            "Training:  Iteration No:  237 Worker Num:  1 \n",
            " Loss:  0.2767656445503235\n",
            "Training:  Iteration No:  237 Worker Num:  2 \n",
            " Loss:  0.2241266518831253\n",
            "Training:  Iteration No:  237 Worker Num:  3 \n",
            " Loss:  0.3759113848209381\n",
            "Training:  Iteration No:  237 Worker Num:  4 \n",
            " Loss:  0.21489983797073364\n",
            "Training:  Iteration No:  237 Worker Num:  5 \n",
            " Loss:  0.23356936872005463\n",
            "Training:  Iteration No:  237 Worker Num:  6 \n",
            " Loss:  0.24113327264785767\n",
            "Training:  Iteration No:  237 Worker Num:  7 \n",
            " Loss:  0.2692370116710663\n",
            "Test:  Iteration No:  237 \n",
            " Loss:  0.2067385742417242\n",
            "Test accuracy:  93.88\n",
            "Training:  Iteration No:  238 Worker Num:  0 \n",
            " Loss:  0.37209123373031616\n",
            "Training:  Iteration No:  238 Worker Num:  1 \n",
            " Loss:  0.44339942932128906\n",
            "Training:  Iteration No:  238 Worker Num:  2 \n",
            " Loss:  0.22978034615516663\n",
            "Training:  Iteration No:  238 Worker Num:  3 \n",
            " Loss:  0.2951892018318176\n",
            "Training:  Iteration No:  238 Worker Num:  4 \n",
            " Loss:  0.3337642252445221\n",
            "Training:  Iteration No:  238 Worker Num:  5 \n",
            " Loss:  0.3518363833427429\n",
            "Training:  Iteration No:  238 Worker Num:  6 \n",
            " Loss:  0.2877580523490906\n",
            "Training:  Iteration No:  238 Worker Num:  7 \n",
            " Loss:  0.2188957929611206\n",
            "Test:  Iteration No:  238 \n",
            " Loss:  0.209898809018203\n",
            "Test accuracy:  93.75\n",
            "Training:  Iteration No:  239 Worker Num:  0 \n",
            " Loss:  0.1988062858581543\n",
            "Training:  Iteration No:  239 Worker Num:  1 \n",
            " Loss:  0.2061425894498825\n",
            "Training:  Iteration No:  239 Worker Num:  2 \n",
            " Loss:  0.2792418897151947\n",
            "Training:  Iteration No:  239 Worker Num:  3 \n",
            " Loss:  0.35128697752952576\n",
            "Training:  Iteration No:  239 Worker Num:  4 \n",
            " Loss:  0.21421419084072113\n",
            "Training:  Iteration No:  239 Worker Num:  5 \n",
            " Loss:  0.2741566300392151\n",
            "Training:  Iteration No:  239 Worker Num:  6 \n",
            " Loss:  0.2205817699432373\n",
            "Training:  Iteration No:  239 Worker Num:  7 \n",
            " Loss:  0.27762556076049805\n",
            "Test:  Iteration No:  239 \n",
            " Loss:  0.20575570140646982\n",
            "Test accuracy:  93.89\n",
            "Training:  Iteration No:  240 Worker Num:  0 \n",
            " Loss:  0.22882330417633057\n",
            "Training:  Iteration No:  240 Worker Num:  1 \n",
            " Loss:  0.29882392287254333\n",
            "Training:  Iteration No:  240 Worker Num:  2 \n",
            " Loss:  0.2703601121902466\n",
            "Training:  Iteration No:  240 Worker Num:  3 \n",
            " Loss:  0.2523980736732483\n",
            "Training:  Iteration No:  240 Worker Num:  4 \n",
            " Loss:  0.264935165643692\n",
            "Training:  Iteration No:  240 Worker Num:  5 \n",
            " Loss:  0.2565596401691437\n",
            "Training:  Iteration No:  240 Worker Num:  6 \n",
            " Loss:  0.27086716890335083\n",
            "Training:  Iteration No:  240 Worker Num:  7 \n",
            " Loss:  0.35515114665031433\n",
            "Test:  Iteration No:  240 \n",
            " Loss:  0.2086034658117385\n",
            "Test accuracy:  93.77\n",
            "Training:  Iteration No:  241 Worker Num:  0 \n",
            " Loss:  0.22315460443496704\n",
            "Training:  Iteration No:  241 Worker Num:  1 \n",
            " Loss:  0.28087541460990906\n",
            "Training:  Iteration No:  241 Worker Num:  2 \n",
            " Loss:  0.2053997665643692\n",
            "Training:  Iteration No:  241 Worker Num:  3 \n",
            " Loss:  0.24217547476291656\n",
            "Training:  Iteration No:  241 Worker Num:  4 \n",
            " Loss:  0.2455664575099945\n",
            "Training:  Iteration No:  241 Worker Num:  5 \n",
            " Loss:  0.3289474546909332\n",
            "Training:  Iteration No:  241 Worker Num:  6 \n",
            " Loss:  0.2292197346687317\n",
            "Training:  Iteration No:  241 Worker Num:  7 \n",
            " Loss:  0.22063550353050232\n",
            "Test:  Iteration No:  241 \n",
            " Loss:  0.20615796830882377\n",
            "Test accuracy:  93.91\n",
            "Training:  Iteration No:  242 Worker Num:  0 \n",
            " Loss:  0.21328221261501312\n",
            "Training:  Iteration No:  242 Worker Num:  1 \n",
            " Loss:  0.16501086950302124\n",
            "Training:  Iteration No:  242 Worker Num:  2 \n",
            " Loss:  0.23118647933006287\n",
            "Training:  Iteration No:  242 Worker Num:  3 \n",
            " Loss:  0.24057507514953613\n",
            "Training:  Iteration No:  242 Worker Num:  4 \n",
            " Loss:  0.4050230383872986\n",
            "Training:  Iteration No:  242 Worker Num:  5 \n",
            " Loss:  0.1974831372499466\n",
            "Training:  Iteration No:  242 Worker Num:  6 \n",
            " Loss:  0.20929914712905884\n",
            "Training:  Iteration No:  242 Worker Num:  7 \n",
            " Loss:  0.467954158782959\n",
            "Test:  Iteration No:  242 \n",
            " Loss:  0.20109996772548067\n",
            "Test accuracy:  94.0\n",
            "Training:  Iteration No:  243 Worker Num:  0 \n",
            " Loss:  0.25506094098091125\n",
            "Training:  Iteration No:  243 Worker Num:  1 \n",
            " Loss:  0.3356577754020691\n",
            "Training:  Iteration No:  243 Worker Num:  2 \n",
            " Loss:  0.19549575448036194\n",
            "Training:  Iteration No:  243 Worker Num:  3 \n",
            " Loss:  0.28254804015159607\n",
            "Training:  Iteration No:  243 Worker Num:  4 \n",
            " Loss:  0.2646651268005371\n",
            "Training:  Iteration No:  243 Worker Num:  5 \n",
            " Loss:  0.37351563572883606\n",
            "Training:  Iteration No:  243 Worker Num:  6 \n",
            " Loss:  0.16842612624168396\n",
            "Training:  Iteration No:  243 Worker Num:  7 \n",
            " Loss:  0.21399708092212677\n",
            "Test:  Iteration No:  243 \n",
            " Loss:  0.20088718737228006\n",
            "Test accuracy:  93.98\n",
            "Training:  Iteration No:  244 Worker Num:  0 \n",
            " Loss:  0.20648957788944244\n",
            "Training:  Iteration No:  244 Worker Num:  1 \n",
            " Loss:  0.21017712354660034\n",
            "Training:  Iteration No:  244 Worker Num:  2 \n",
            " Loss:  0.27789759635925293\n",
            "Training:  Iteration No:  244 Worker Num:  3 \n",
            " Loss:  0.2603381872177124\n",
            "Training:  Iteration No:  244 Worker Num:  4 \n",
            " Loss:  0.21827107667922974\n",
            "Training:  Iteration No:  244 Worker Num:  5 \n",
            " Loss:  0.35067635774612427\n",
            "Training:  Iteration No:  244 Worker Num:  6 \n",
            " Loss:  0.2921689748764038\n",
            "Training:  Iteration No:  244 Worker Num:  7 \n",
            " Loss:  0.2660132050514221\n",
            "Test:  Iteration No:  244 \n",
            " Loss:  0.19897863640202373\n",
            "Test accuracy:  94.13\n",
            "Training:  Iteration No:  245 Worker Num:  0 \n",
            " Loss:  0.2159091681241989\n",
            "Training:  Iteration No:  245 Worker Num:  1 \n",
            " Loss:  0.3614100217819214\n",
            "Training:  Iteration No:  245 Worker Num:  2 \n",
            " Loss:  0.17792969942092896\n",
            "Training:  Iteration No:  245 Worker Num:  3 \n",
            " Loss:  0.3296213150024414\n",
            "Training:  Iteration No:  245 Worker Num:  4 \n",
            " Loss:  0.3300164043903351\n",
            "Training:  Iteration No:  245 Worker Num:  5 \n",
            " Loss:  0.3037300109863281\n",
            "Training:  Iteration No:  245 Worker Num:  6 \n",
            " Loss:  0.2532346546649933\n",
            "Training:  Iteration No:  245 Worker Num:  7 \n",
            " Loss:  0.23532328009605408\n",
            "Test:  Iteration No:  245 \n",
            " Loss:  0.19762033472851484\n",
            "Test accuracy:  94.18\n",
            "Training:  Iteration No:  246 Worker Num:  0 \n",
            " Loss:  0.29436132311820984\n",
            "Training:  Iteration No:  246 Worker Num:  1 \n",
            " Loss:  0.4419945180416107\n",
            "Training:  Iteration No:  246 Worker Num:  2 \n",
            " Loss:  0.17533883452415466\n",
            "Training:  Iteration No:  246 Worker Num:  3 \n",
            " Loss:  0.23438704013824463\n",
            "Training:  Iteration No:  246 Worker Num:  4 \n",
            " Loss:  0.2935173213481903\n",
            "Training:  Iteration No:  246 Worker Num:  5 \n",
            " Loss:  0.2942315936088562\n",
            "Training:  Iteration No:  246 Worker Num:  6 \n",
            " Loss:  0.19186316430568695\n",
            "Training:  Iteration No:  246 Worker Num:  7 \n",
            " Loss:  0.2029673457145691\n",
            "Test:  Iteration No:  246 \n",
            " Loss:  0.1968175541798148\n",
            "Test accuracy:  94.13\n",
            "Training:  Iteration No:  247 Worker Num:  0 \n",
            " Loss:  0.25422316789627075\n",
            "Training:  Iteration No:  247 Worker Num:  1 \n",
            " Loss:  0.18209399282932281\n",
            "Training:  Iteration No:  247 Worker Num:  2 \n",
            " Loss:  0.267072856426239\n",
            "Training:  Iteration No:  247 Worker Num:  3 \n",
            " Loss:  0.2502659559249878\n",
            "Training:  Iteration No:  247 Worker Num:  4 \n",
            " Loss:  0.13900522887706757\n",
            "Training:  Iteration No:  247 Worker Num:  5 \n",
            " Loss:  0.21378353238105774\n",
            "Training:  Iteration No:  247 Worker Num:  6 \n",
            " Loss:  0.23206807672977448\n",
            "Training:  Iteration No:  247 Worker Num:  7 \n",
            " Loss:  0.3669666349887848\n",
            "Test:  Iteration No:  247 \n",
            " Loss:  0.1997488166523885\n",
            "Test accuracy:  94.09\n",
            "Training:  Iteration No:  248 Worker Num:  0 \n",
            " Loss:  0.25241589546203613\n",
            "Training:  Iteration No:  248 Worker Num:  1 \n",
            " Loss:  0.18546201288700104\n",
            "Training:  Iteration No:  248 Worker Num:  2 \n",
            " Loss:  0.19840610027313232\n",
            "Training:  Iteration No:  248 Worker Num:  3 \n",
            " Loss:  0.2803102731704712\n",
            "Training:  Iteration No:  248 Worker Num:  4 \n",
            " Loss:  0.16305410861968994\n",
            "Training:  Iteration No:  248 Worker Num:  5 \n",
            " Loss:  0.2710452079772949\n",
            "Training:  Iteration No:  248 Worker Num:  6 \n",
            " Loss:  0.19029852747917175\n",
            "Training:  Iteration No:  248 Worker Num:  7 \n",
            " Loss:  0.24141448736190796\n",
            "Test:  Iteration No:  248 \n",
            " Loss:  0.200531400480791\n",
            "Test accuracy:  94.15\n",
            "Training:  Iteration No:  249 Worker Num:  0 \n",
            " Loss:  0.2653960883617401\n",
            "Training:  Iteration No:  249 Worker Num:  1 \n",
            " Loss:  0.29478719830513\n",
            "Training:  Iteration No:  249 Worker Num:  2 \n",
            " Loss:  0.29276514053344727\n",
            "Training:  Iteration No:  249 Worker Num:  3 \n",
            " Loss:  0.3570612370967865\n",
            "Training:  Iteration No:  249 Worker Num:  4 \n",
            " Loss:  0.2738971710205078\n",
            "Training:  Iteration No:  249 Worker Num:  5 \n",
            " Loss:  0.26032203435897827\n",
            "Training:  Iteration No:  249 Worker Num:  6 \n",
            " Loss:  0.29131314158439636\n",
            "Training:  Iteration No:  249 Worker Num:  7 \n",
            " Loss:  0.3651706874370575\n",
            "Test:  Iteration No:  249 \n",
            " Loss:  0.19416131194728084\n",
            "Test accuracy:  94.18\n",
            "Training:  Iteration No:  250 Worker Num:  0 \n",
            " Loss:  0.19204740226268768\n",
            "Training:  Iteration No:  250 Worker Num:  1 \n",
            " Loss:  0.2940559685230255\n",
            "Training:  Iteration No:  250 Worker Num:  2 \n",
            " Loss:  0.17979200184345245\n",
            "Training:  Iteration No:  250 Worker Num:  3 \n",
            " Loss:  0.18473868072032928\n",
            "Training:  Iteration No:  250 Worker Num:  4 \n",
            " Loss:  0.311920166015625\n",
            "Training:  Iteration No:  250 Worker Num:  5 \n",
            " Loss:  0.17902468144893646\n",
            "Training:  Iteration No:  250 Worker Num:  6 \n",
            " Loss:  0.2036740481853485\n",
            "Training:  Iteration No:  250 Worker Num:  7 \n",
            " Loss:  0.2699279487133026\n",
            "Test:  Iteration No:  250 \n",
            " Loss:  0.196247530393774\n",
            "Test accuracy:  94.03\n",
            "Training:  Iteration No:  251 Worker Num:  0 \n",
            " Loss:  0.165367990732193\n",
            "Training:  Iteration No:  251 Worker Num:  1 \n",
            " Loss:  0.1913258582353592\n",
            "Training:  Iteration No:  251 Worker Num:  2 \n",
            " Loss:  0.3073377311229706\n",
            "Training:  Iteration No:  251 Worker Num:  3 \n",
            " Loss:  0.2021988183259964\n",
            "Training:  Iteration No:  251 Worker Num:  4 \n",
            " Loss:  0.15230710804462433\n",
            "Training:  Iteration No:  251 Worker Num:  5 \n",
            " Loss:  0.14736704528331757\n",
            "Training:  Iteration No:  251 Worker Num:  6 \n",
            " Loss:  0.3166883885860443\n",
            "Training:  Iteration No:  251 Worker Num:  7 \n",
            " Loss:  0.12562137842178345\n",
            "Test:  Iteration No:  251 \n",
            " Loss:  0.19540594801118102\n",
            "Test accuracy:  94.05\n",
            "Training:  Iteration No:  252 Worker Num:  0 \n",
            " Loss:  0.26043757796287537\n",
            "Training:  Iteration No:  252 Worker Num:  1 \n",
            " Loss:  0.29810065031051636\n",
            "Training:  Iteration No:  252 Worker Num:  2 \n",
            " Loss:  0.18463431298732758\n",
            "Training:  Iteration No:  252 Worker Num:  3 \n",
            " Loss:  0.36019638180732727\n",
            "Training:  Iteration No:  252 Worker Num:  4 \n",
            " Loss:  0.27871644496917725\n",
            "Training:  Iteration No:  252 Worker Num:  5 \n",
            " Loss:  0.26442980766296387\n",
            "Training:  Iteration No:  252 Worker Num:  6 \n",
            " Loss:  0.2046864926815033\n",
            "Training:  Iteration No:  252 Worker Num:  7 \n",
            " Loss:  0.21468889713287354\n",
            "Test:  Iteration No:  252 \n",
            " Loss:  0.19887476263544226\n",
            "Test accuracy:  94.18\n",
            "Training:  Iteration No:  253 Worker Num:  0 \n",
            " Loss:  0.27344828844070435\n",
            "Training:  Iteration No:  253 Worker Num:  1 \n",
            " Loss:  0.29052919149398804\n",
            "Training:  Iteration No:  253 Worker Num:  2 \n",
            " Loss:  0.2934376895427704\n",
            "Training:  Iteration No:  253 Worker Num:  3 \n",
            " Loss:  0.20832587778568268\n",
            "Training:  Iteration No:  253 Worker Num:  4 \n",
            " Loss:  0.3178510069847107\n",
            "Training:  Iteration No:  253 Worker Num:  5 \n",
            " Loss:  0.25319501757621765\n",
            "Training:  Iteration No:  253 Worker Num:  6 \n",
            " Loss:  0.17011526226997375\n",
            "Training:  Iteration No:  253 Worker Num:  7 \n",
            " Loss:  0.21114619076251984\n",
            "Test:  Iteration No:  253 \n",
            " Loss:  0.1930649000090323\n",
            "Test accuracy:  94.46\n",
            "Training:  Iteration No:  254 Worker Num:  0 \n",
            " Loss:  0.3373151123523712\n",
            "Training:  Iteration No:  254 Worker Num:  1 \n",
            " Loss:  0.24848672747612\n",
            "Training:  Iteration No:  254 Worker Num:  2 \n",
            " Loss:  0.185971200466156\n",
            "Training:  Iteration No:  254 Worker Num:  3 \n",
            " Loss:  0.20035460591316223\n",
            "Training:  Iteration No:  254 Worker Num:  4 \n",
            " Loss:  0.256473571062088\n",
            "Training:  Iteration No:  254 Worker Num:  5 \n",
            " Loss:  0.14688169956207275\n",
            "Training:  Iteration No:  254 Worker Num:  6 \n",
            " Loss:  0.20623402297496796\n",
            "Training:  Iteration No:  254 Worker Num:  7 \n",
            " Loss:  0.31749701499938965\n",
            "Test:  Iteration No:  254 \n",
            " Loss:  0.20188853433473578\n",
            "Test accuracy:  94.0\n",
            "Training:  Iteration No:  255 Worker Num:  0 \n",
            " Loss:  0.1839280128479004\n",
            "Training:  Iteration No:  255 Worker Num:  1 \n",
            " Loss:  0.19539767503738403\n",
            "Training:  Iteration No:  255 Worker Num:  2 \n",
            " Loss:  0.20269624888896942\n",
            "Training:  Iteration No:  255 Worker Num:  3 \n",
            " Loss:  0.22084550559520721\n",
            "Training:  Iteration No:  255 Worker Num:  4 \n",
            " Loss:  0.27001953125\n",
            "Training:  Iteration No:  255 Worker Num:  5 \n",
            " Loss:  0.23692551255226135\n",
            "Training:  Iteration No:  255 Worker Num:  6 \n",
            " Loss:  0.27472707629203796\n",
            "Training:  Iteration No:  255 Worker Num:  7 \n",
            " Loss:  0.22158272564411163\n",
            "Test:  Iteration No:  255 \n",
            " Loss:  0.19583962860081014\n",
            "Test accuracy:  94.18\n",
            "Training:  Iteration No:  256 Worker Num:  0 \n",
            " Loss:  0.2126787006855011\n",
            "Training:  Iteration No:  256 Worker Num:  1 \n",
            " Loss:  0.1442544311285019\n",
            "Training:  Iteration No:  256 Worker Num:  2 \n",
            " Loss:  0.22589966654777527\n",
            "Training:  Iteration No:  256 Worker Num:  3 \n",
            " Loss:  0.22753016650676727\n",
            "Training:  Iteration No:  256 Worker Num:  4 \n",
            " Loss:  0.17608733475208282\n",
            "Training:  Iteration No:  256 Worker Num:  5 \n",
            " Loss:  0.15123283863067627\n",
            "Training:  Iteration No:  256 Worker Num:  6 \n",
            " Loss:  0.3045946955680847\n",
            "Training:  Iteration No:  256 Worker Num:  7 \n",
            " Loss:  0.3195798695087433\n",
            "Test:  Iteration No:  256 \n",
            " Loss:  0.19056380054430117\n",
            "Test accuracy:  94.34\n",
            "Training:  Iteration No:  257 Worker Num:  0 \n",
            " Loss:  0.25762462615966797\n",
            "Training:  Iteration No:  257 Worker Num:  1 \n",
            " Loss:  0.3388803005218506\n",
            "Training:  Iteration No:  257 Worker Num:  2 \n",
            " Loss:  0.18945983052253723\n",
            "Training:  Iteration No:  257 Worker Num:  3 \n",
            " Loss:  0.32875218987464905\n",
            "Training:  Iteration No:  257 Worker Num:  4 \n",
            " Loss:  0.137979656457901\n",
            "Training:  Iteration No:  257 Worker Num:  5 \n",
            " Loss:  0.23992381989955902\n",
            "Training:  Iteration No:  257 Worker Num:  6 \n",
            " Loss:  0.14335012435913086\n",
            "Training:  Iteration No:  257 Worker Num:  7 \n",
            " Loss:  0.18695230782032013\n",
            "Test:  Iteration No:  257 \n",
            " Loss:  0.1970361525522946\n",
            "Test accuracy:  94.03\n",
            "Training:  Iteration No:  258 Worker Num:  0 \n",
            " Loss:  0.19118551909923553\n",
            "Training:  Iteration No:  258 Worker Num:  1 \n",
            " Loss:  0.25569334626197815\n",
            "Training:  Iteration No:  258 Worker Num:  2 \n",
            " Loss:  0.349754273891449\n",
            "Training:  Iteration No:  258 Worker Num:  3 \n",
            " Loss:  0.326739102602005\n",
            "Training:  Iteration No:  258 Worker Num:  4 \n",
            " Loss:  0.1371762752532959\n",
            "Training:  Iteration No:  258 Worker Num:  5 \n",
            " Loss:  0.3382425606250763\n",
            "Training:  Iteration No:  258 Worker Num:  6 \n",
            " Loss:  0.18845129013061523\n",
            "Training:  Iteration No:  258 Worker Num:  7 \n",
            " Loss:  0.1972680538892746\n",
            "Test:  Iteration No:  258 \n",
            " Loss:  0.19155067339822462\n",
            "Test accuracy:  94.22\n",
            "Training:  Iteration No:  259 Worker Num:  0 \n",
            " Loss:  0.21685385704040527\n",
            "Training:  Iteration No:  259 Worker Num:  1 \n",
            " Loss:  0.1836797147989273\n",
            "Training:  Iteration No:  259 Worker Num:  2 \n",
            " Loss:  0.23231321573257446\n",
            "Training:  Iteration No:  259 Worker Num:  3 \n",
            " Loss:  0.24557259678840637\n",
            "Training:  Iteration No:  259 Worker Num:  4 \n",
            " Loss:  0.22674496471881866\n",
            "Training:  Iteration No:  259 Worker Num:  5 \n",
            " Loss:  0.13773123919963837\n",
            "Training:  Iteration No:  259 Worker Num:  6 \n",
            " Loss:  0.2154526561498642\n",
            "Training:  Iteration No:  259 Worker Num:  7 \n",
            " Loss:  0.1964825540781021\n",
            "Test:  Iteration No:  259 \n",
            " Loss:  0.1926825045714084\n",
            "Test accuracy:  94.03\n",
            "Training:  Iteration No:  260 Worker Num:  0 \n",
            " Loss:  0.1910848617553711\n",
            "Training:  Iteration No:  260 Worker Num:  1 \n",
            " Loss:  0.15467093884944916\n",
            "Training:  Iteration No:  260 Worker Num:  2 \n",
            " Loss:  0.25611063838005066\n",
            "Training:  Iteration No:  260 Worker Num:  3 \n",
            " Loss:  0.21065178513526917\n",
            "Training:  Iteration No:  260 Worker Num:  4 \n",
            " Loss:  0.12721015512943268\n",
            "Training:  Iteration No:  260 Worker Num:  5 \n",
            " Loss:  0.14954324066638947\n",
            "Training:  Iteration No:  260 Worker Num:  6 \n",
            " Loss:  0.17406757175922394\n",
            "Training:  Iteration No:  260 Worker Num:  7 \n",
            " Loss:  0.23776501417160034\n",
            "Test:  Iteration No:  260 \n",
            " Loss:  0.188921766438156\n",
            "Test accuracy:  94.34\n",
            "Training:  Iteration No:  261 Worker Num:  0 \n",
            " Loss:  0.21575072407722473\n",
            "Training:  Iteration No:  261 Worker Num:  1 \n",
            " Loss:  0.38132691383361816\n",
            "Training:  Iteration No:  261 Worker Num:  2 \n",
            " Loss:  0.3246547281742096\n",
            "Training:  Iteration No:  261 Worker Num:  3 \n",
            " Loss:  0.23390039801597595\n",
            "Training:  Iteration No:  261 Worker Num:  4 \n",
            " Loss:  0.20581884682178497\n",
            "Training:  Iteration No:  261 Worker Num:  5 \n",
            " Loss:  0.21939410269260406\n",
            "Training:  Iteration No:  261 Worker Num:  6 \n",
            " Loss:  0.19255760312080383\n",
            "Training:  Iteration No:  261 Worker Num:  7 \n",
            " Loss:  0.22299210727214813\n",
            "Test:  Iteration No:  261 \n",
            " Loss:  0.18825729935182423\n",
            "Test accuracy:  94.25\n",
            "Training:  Iteration No:  262 Worker Num:  0 \n",
            " Loss:  0.27458301186561584\n",
            "Training:  Iteration No:  262 Worker Num:  1 \n",
            " Loss:  0.21676720678806305\n",
            "Training:  Iteration No:  262 Worker Num:  2 \n",
            " Loss:  0.274164617061615\n",
            "Training:  Iteration No:  262 Worker Num:  3 \n",
            " Loss:  0.22330084443092346\n",
            "Training:  Iteration No:  262 Worker Num:  4 \n",
            " Loss:  0.1947251260280609\n",
            "Training:  Iteration No:  262 Worker Num:  5 \n",
            " Loss:  0.21221807599067688\n",
            "Training:  Iteration No:  262 Worker Num:  6 \n",
            " Loss:  0.2820385694503784\n",
            "Training:  Iteration No:  262 Worker Num:  7 \n",
            " Loss:  0.1492932289838791\n",
            "Test:  Iteration No:  262 \n",
            " Loss:  0.18985218412089575\n",
            "Test accuracy:  94.26\n",
            "Training:  Iteration No:  263 Worker Num:  0 \n",
            " Loss:  0.11555567383766174\n",
            "Training:  Iteration No:  263 Worker Num:  1 \n",
            " Loss:  0.2038670927286148\n",
            "Training:  Iteration No:  263 Worker Num:  2 \n",
            " Loss:  0.22097904980182648\n",
            "Training:  Iteration No:  263 Worker Num:  3 \n",
            " Loss:  0.14576561748981476\n",
            "Training:  Iteration No:  263 Worker Num:  4 \n",
            " Loss:  0.14552704989910126\n",
            "Training:  Iteration No:  263 Worker Num:  5 \n",
            " Loss:  0.2514996826648712\n",
            "Training:  Iteration No:  263 Worker Num:  6 \n",
            " Loss:  0.3004491627216339\n",
            "Training:  Iteration No:  263 Worker Num:  7 \n",
            " Loss:  0.21020562946796417\n",
            "Test:  Iteration No:  263 \n",
            " Loss:  0.1921150983087247\n",
            "Test accuracy:  94.36\n",
            "Training:  Iteration No:  264 Worker Num:  0 \n",
            " Loss:  0.1854063868522644\n",
            "Training:  Iteration No:  264 Worker Num:  1 \n",
            " Loss:  0.1988471895456314\n",
            "Training:  Iteration No:  264 Worker Num:  2 \n",
            " Loss:  0.18333831429481506\n",
            "Training:  Iteration No:  264 Worker Num:  3 \n",
            " Loss:  0.28812113404273987\n",
            "Training:  Iteration No:  264 Worker Num:  4 \n",
            " Loss:  0.18601524829864502\n",
            "Training:  Iteration No:  264 Worker Num:  5 \n",
            " Loss:  0.1304313987493515\n",
            "Training:  Iteration No:  264 Worker Num:  6 \n",
            " Loss:  0.2032730132341385\n",
            "Training:  Iteration No:  264 Worker Num:  7 \n",
            " Loss:  0.27991941571235657\n",
            "Test:  Iteration No:  264 \n",
            " Loss:  0.1865635376447175\n",
            "Test accuracy:  94.43\n",
            "Training:  Iteration No:  265 Worker Num:  0 \n",
            " Loss:  0.2561008036136627\n",
            "Training:  Iteration No:  265 Worker Num:  1 \n",
            " Loss:  0.16380125284194946\n",
            "Training:  Iteration No:  265 Worker Num:  2 \n",
            " Loss:  0.23696239292621613\n",
            "Training:  Iteration No:  265 Worker Num:  3 \n",
            " Loss:  0.1645304560661316\n",
            "Training:  Iteration No:  265 Worker Num:  4 \n",
            " Loss:  0.10414935648441315\n",
            "Training:  Iteration No:  265 Worker Num:  5 \n",
            " Loss:  0.15092116594314575\n",
            "Training:  Iteration No:  265 Worker Num:  6 \n",
            " Loss:  0.19984669983386993\n",
            "Training:  Iteration No:  265 Worker Num:  7 \n",
            " Loss:  0.22400124371051788\n",
            "Test:  Iteration No:  265 \n",
            " Loss:  0.18525258837197023\n",
            "Test accuracy:  94.45\n",
            "Training:  Iteration No:  266 Worker Num:  0 \n",
            " Loss:  0.2309781014919281\n",
            "Training:  Iteration No:  266 Worker Num:  1 \n",
            " Loss:  0.18441392481327057\n",
            "Training:  Iteration No:  266 Worker Num:  2 \n",
            " Loss:  0.195862278342247\n",
            "Training:  Iteration No:  266 Worker Num:  3 \n",
            " Loss:  0.16931374371051788\n",
            "Training:  Iteration No:  266 Worker Num:  4 \n",
            " Loss:  0.19913333654403687\n",
            "Training:  Iteration No:  266 Worker Num:  5 \n",
            " Loss:  0.17606455087661743\n",
            "Training:  Iteration No:  266 Worker Num:  6 \n",
            " Loss:  0.2347589135169983\n",
            "Training:  Iteration No:  266 Worker Num:  7 \n",
            " Loss:  0.1712496131658554\n",
            "Test:  Iteration No:  266 \n",
            " Loss:  0.1830385125631207\n",
            "Test accuracy:  94.42\n",
            "Training:  Iteration No:  267 Worker Num:  0 \n",
            " Loss:  0.30338481068611145\n",
            "Training:  Iteration No:  267 Worker Num:  1 \n",
            " Loss:  0.1850503385066986\n",
            "Training:  Iteration No:  267 Worker Num:  2 \n",
            " Loss:  0.41073131561279297\n",
            "Training:  Iteration No:  267 Worker Num:  3 \n",
            " Loss:  0.18203575909137726\n",
            "Training:  Iteration No:  267 Worker Num:  4 \n",
            " Loss:  0.34906095266342163\n",
            "Training:  Iteration No:  267 Worker Num:  5 \n",
            " Loss:  0.36031678318977356\n",
            "Training:  Iteration No:  267 Worker Num:  6 \n",
            " Loss:  0.21702724695205688\n",
            "Training:  Iteration No:  267 Worker Num:  7 \n",
            " Loss:  0.22818006575107574\n",
            "Test:  Iteration No:  267 \n",
            " Loss:  0.18350960024267057\n",
            "Test accuracy:  94.3\n",
            "Training:  Iteration No:  268 Worker Num:  0 \n",
            " Loss:  0.11667274683713913\n",
            "Training:  Iteration No:  268 Worker Num:  1 \n",
            " Loss:  0.16461749374866486\n",
            "Training:  Iteration No:  268 Worker Num:  2 \n",
            " Loss:  0.18960216641426086\n",
            "Training:  Iteration No:  268 Worker Num:  3 \n",
            " Loss:  0.18643108010292053\n",
            "Training:  Iteration No:  268 Worker Num:  4 \n",
            " Loss:  0.2214588075876236\n",
            "Training:  Iteration No:  268 Worker Num:  5 \n",
            " Loss:  0.18936291337013245\n",
            "Training:  Iteration No:  268 Worker Num:  6 \n",
            " Loss:  0.19899001717567444\n",
            "Training:  Iteration No:  268 Worker Num:  7 \n",
            " Loss:  0.20576603710651398\n",
            "Test:  Iteration No:  268 \n",
            " Loss:  0.18468956089331\n",
            "Test accuracy:  94.33\n",
            "Training:  Iteration No:  269 Worker Num:  0 \n",
            " Loss:  0.15112820267677307\n",
            "Training:  Iteration No:  269 Worker Num:  1 \n",
            " Loss:  0.2265634834766388\n",
            "Training:  Iteration No:  269 Worker Num:  2 \n",
            " Loss:  0.12008126825094223\n",
            "Training:  Iteration No:  269 Worker Num:  3 \n",
            " Loss:  0.17556548118591309\n",
            "Training:  Iteration No:  269 Worker Num:  4 \n",
            " Loss:  0.19566398859024048\n",
            "Training:  Iteration No:  269 Worker Num:  5 \n",
            " Loss:  0.2678038477897644\n",
            "Training:  Iteration No:  269 Worker Num:  6 \n",
            " Loss:  0.3473367989063263\n",
            "Training:  Iteration No:  269 Worker Num:  7 \n",
            " Loss:  0.121558777987957\n",
            "Test:  Iteration No:  269 \n",
            " Loss:  0.1840506195196811\n",
            "Test accuracy:  94.38\n",
            "Training:  Iteration No:  270 Worker Num:  0 \n",
            " Loss:  0.19568590819835663\n",
            "Training:  Iteration No:  270 Worker Num:  1 \n",
            " Loss:  0.22318239510059357\n",
            "Training:  Iteration No:  270 Worker Num:  2 \n",
            " Loss:  0.16856616735458374\n",
            "Training:  Iteration No:  270 Worker Num:  3 \n",
            " Loss:  0.23954267799854279\n",
            "Training:  Iteration No:  270 Worker Num:  4 \n",
            " Loss:  0.15262126922607422\n",
            "Training:  Iteration No:  270 Worker Num:  5 \n",
            " Loss:  0.16475676000118256\n",
            "Training:  Iteration No:  270 Worker Num:  6 \n",
            " Loss:  0.219315305352211\n",
            "Training:  Iteration No:  270 Worker Num:  7 \n",
            " Loss:  0.23080651462078094\n",
            "Test:  Iteration No:  270 \n",
            " Loss:  0.18112838328403386\n",
            "Test accuracy:  94.55\n",
            "Training:  Iteration No:  271 Worker Num:  0 \n",
            " Loss:  0.19530124962329865\n",
            "Training:  Iteration No:  271 Worker Num:  1 \n",
            " Loss:  0.3190239369869232\n",
            "Training:  Iteration No:  271 Worker Num:  2 \n",
            " Loss:  0.2181013524532318\n",
            "Training:  Iteration No:  271 Worker Num:  3 \n",
            " Loss:  0.22366119921207428\n",
            "Training:  Iteration No:  271 Worker Num:  4 \n",
            " Loss:  0.3252294957637787\n",
            "Training:  Iteration No:  271 Worker Num:  5 \n",
            " Loss:  0.29843729734420776\n",
            "Training:  Iteration No:  271 Worker Num:  6 \n",
            " Loss:  0.1529369205236435\n",
            "Training:  Iteration No:  271 Worker Num:  7 \n",
            " Loss:  0.15833425521850586\n",
            "Test:  Iteration No:  271 \n",
            " Loss:  0.1881387250945915\n",
            "Test accuracy:  94.35\n",
            "Training:  Iteration No:  272 Worker Num:  0 \n",
            " Loss:  0.28364235162734985\n",
            "Training:  Iteration No:  272 Worker Num:  1 \n",
            " Loss:  0.20146679878234863\n",
            "Training:  Iteration No:  272 Worker Num:  2 \n",
            " Loss:  0.25927048921585083\n",
            "Training:  Iteration No:  272 Worker Num:  3 \n",
            " Loss:  0.32980167865753174\n",
            "Training:  Iteration No:  272 Worker Num:  4 \n",
            " Loss:  0.21024854481220245\n",
            "Training:  Iteration No:  272 Worker Num:  5 \n",
            " Loss:  0.18959753215312958\n",
            "Training:  Iteration No:  272 Worker Num:  6 \n",
            " Loss:  0.27952656149864197\n",
            "Training:  Iteration No:  272 Worker Num:  7 \n",
            " Loss:  0.3548116087913513\n",
            "Test:  Iteration No:  272 \n",
            " Loss:  0.18154217643498244\n",
            "Test accuracy:  94.53\n",
            "Training:  Iteration No:  273 Worker Num:  0 \n",
            " Loss:  0.17654365301132202\n",
            "Training:  Iteration No:  273 Worker Num:  1 \n",
            " Loss:  0.22576043009757996\n",
            "Training:  Iteration No:  273 Worker Num:  2 \n",
            " Loss:  0.248015895485878\n",
            "Training:  Iteration No:  273 Worker Num:  3 \n",
            " Loss:  0.18750792741775513\n",
            "Training:  Iteration No:  273 Worker Num:  4 \n",
            " Loss:  0.1760958433151245\n",
            "Training:  Iteration No:  273 Worker Num:  5 \n",
            " Loss:  0.23180554807186127\n",
            "Training:  Iteration No:  273 Worker Num:  6 \n",
            " Loss:  0.2675585448741913\n",
            "Training:  Iteration No:  273 Worker Num:  7 \n",
            " Loss:  0.23186619579792023\n",
            "Test:  Iteration No:  273 \n",
            " Loss:  0.18481420772739604\n",
            "Test accuracy:  94.55\n",
            "Training:  Iteration No:  274 Worker Num:  0 \n",
            " Loss:  0.22329647839069366\n",
            "Training:  Iteration No:  274 Worker Num:  1 \n",
            " Loss:  0.19652433693408966\n",
            "Training:  Iteration No:  274 Worker Num:  2 \n",
            " Loss:  0.24786269664764404\n",
            "Training:  Iteration No:  274 Worker Num:  3 \n",
            " Loss:  0.18332414329051971\n",
            "Training:  Iteration No:  274 Worker Num:  4 \n",
            " Loss:  0.17721393704414368\n",
            "Training:  Iteration No:  274 Worker Num:  5 \n",
            " Loss:  0.15996310114860535\n",
            "Training:  Iteration No:  274 Worker Num:  6 \n",
            " Loss:  0.18785354495048523\n",
            "Training:  Iteration No:  274 Worker Num:  7 \n",
            " Loss:  0.33642107248306274\n",
            "Test:  Iteration No:  274 \n",
            " Loss:  0.1812684248861727\n",
            "Test accuracy:  94.67\n",
            "Training:  Iteration No:  275 Worker Num:  0 \n",
            " Loss:  0.23638693988323212\n",
            "Training:  Iteration No:  275 Worker Num:  1 \n",
            " Loss:  0.17110921442508698\n",
            "Training:  Iteration No:  275 Worker Num:  2 \n",
            " Loss:  0.3226781487464905\n",
            "Training:  Iteration No:  275 Worker Num:  3 \n",
            " Loss:  0.2007712721824646\n",
            "Training:  Iteration No:  275 Worker Num:  4 \n",
            " Loss:  0.4671838581562042\n",
            "Training:  Iteration No:  275 Worker Num:  5 \n",
            " Loss:  0.2666437029838562\n",
            "Training:  Iteration No:  275 Worker Num:  6 \n",
            " Loss:  0.30960291624069214\n",
            "Training:  Iteration No:  275 Worker Num:  7 \n",
            " Loss:  0.16554434597492218\n",
            "Test:  Iteration No:  275 \n",
            " Loss:  0.18071895649750966\n",
            "Test accuracy:  94.74\n",
            "Training:  Iteration No:  276 Worker Num:  0 \n",
            " Loss:  0.221434086561203\n",
            "Training:  Iteration No:  276 Worker Num:  1 \n",
            " Loss:  0.3110387325286865\n",
            "Training:  Iteration No:  276 Worker Num:  2 \n",
            " Loss:  0.16931256651878357\n",
            "Training:  Iteration No:  276 Worker Num:  3 \n",
            " Loss:  0.18019956350326538\n",
            "Training:  Iteration No:  276 Worker Num:  4 \n",
            " Loss:  0.17581339180469513\n",
            "Training:  Iteration No:  276 Worker Num:  5 \n",
            " Loss:  0.4202447533607483\n",
            "Training:  Iteration No:  276 Worker Num:  6 \n",
            " Loss:  0.15463456511497498\n",
            "Training:  Iteration No:  276 Worker Num:  7 \n",
            " Loss:  0.1638672649860382\n",
            "Test:  Iteration No:  276 \n",
            " Loss:  0.176802812615716\n",
            "Test accuracy:  94.64\n",
            "Training:  Iteration No:  277 Worker Num:  0 \n",
            " Loss:  0.30974021553993225\n",
            "Training:  Iteration No:  277 Worker Num:  1 \n",
            " Loss:  0.15399079024791718\n",
            "Training:  Iteration No:  277 Worker Num:  2 \n",
            " Loss:  0.1458866000175476\n",
            "Training:  Iteration No:  277 Worker Num:  3 \n",
            " Loss:  0.23087868094444275\n",
            "Training:  Iteration No:  277 Worker Num:  4 \n",
            " Loss:  0.13913436233997345\n",
            "Training:  Iteration No:  277 Worker Num:  5 \n",
            " Loss:  0.21829403936862946\n",
            "Training:  Iteration No:  277 Worker Num:  6 \n",
            " Loss:  0.23783765733242035\n",
            "Training:  Iteration No:  277 Worker Num:  7 \n",
            " Loss:  0.22558192908763885\n",
            "Test:  Iteration No:  277 \n",
            " Loss:  0.17796739768472652\n",
            "Test accuracy:  94.65\n",
            "Training:  Iteration No:  278 Worker Num:  0 \n",
            " Loss:  0.25556015968322754\n",
            "Training:  Iteration No:  278 Worker Num:  1 \n",
            " Loss:  0.19721156358718872\n",
            "Training:  Iteration No:  278 Worker Num:  2 \n",
            " Loss:  0.3411160111427307\n",
            "Training:  Iteration No:  278 Worker Num:  3 \n",
            " Loss:  0.14807429909706116\n",
            "Training:  Iteration No:  278 Worker Num:  4 \n",
            " Loss:  0.17831718921661377\n",
            "Training:  Iteration No:  278 Worker Num:  5 \n",
            " Loss:  0.2733044922351837\n",
            "Training:  Iteration No:  278 Worker Num:  6 \n",
            " Loss:  0.21143671870231628\n",
            "Training:  Iteration No:  278 Worker Num:  7 \n",
            " Loss:  0.2915771007537842\n",
            "Test:  Iteration No:  278 \n",
            " Loss:  0.17983595197548782\n",
            "Test accuracy:  94.57\n",
            "Training:  Iteration No:  279 Worker Num:  0 \n",
            " Loss:  0.18313568830490112\n",
            "Training:  Iteration No:  279 Worker Num:  1 \n",
            " Loss:  0.31236889958381653\n",
            "Training:  Iteration No:  279 Worker Num:  2 \n",
            " Loss:  0.23823757469654083\n",
            "Training:  Iteration No:  279 Worker Num:  3 \n",
            " Loss:  0.32193219661712646\n",
            "Training:  Iteration No:  279 Worker Num:  4 \n",
            " Loss:  0.11780528724193573\n",
            "Training:  Iteration No:  279 Worker Num:  5 \n",
            " Loss:  0.1935361623764038\n",
            "Training:  Iteration No:  279 Worker Num:  6 \n",
            " Loss:  0.3598231077194214\n",
            "Training:  Iteration No:  279 Worker Num:  7 \n",
            " Loss:  0.20445744693279266\n",
            "Test:  Iteration No:  279 \n",
            " Loss:  0.17683350611949647\n",
            "Test accuracy:  94.58\n",
            "Training:  Iteration No:  280 Worker Num:  0 \n",
            " Loss:  0.28784313797950745\n",
            "Training:  Iteration No:  280 Worker Num:  1 \n",
            " Loss:  0.3067966401576996\n",
            "Training:  Iteration No:  280 Worker Num:  2 \n",
            " Loss:  0.19172915816307068\n",
            "Training:  Iteration No:  280 Worker Num:  3 \n",
            " Loss:  0.250431627035141\n",
            "Training:  Iteration No:  280 Worker Num:  4 \n",
            " Loss:  0.17097187042236328\n",
            "Training:  Iteration No:  280 Worker Num:  5 \n",
            " Loss:  0.1784108281135559\n",
            "Training:  Iteration No:  280 Worker Num:  6 \n",
            " Loss:  0.26007241010665894\n",
            "Training:  Iteration No:  280 Worker Num:  7 \n",
            " Loss:  0.22006842494010925\n",
            "Test:  Iteration No:  280 \n",
            " Loss:  0.17715856131119065\n",
            "Test accuracy:  94.74\n",
            "Training:  Iteration No:  281 Worker Num:  0 \n",
            " Loss:  0.20077559351921082\n",
            "Training:  Iteration No:  281 Worker Num:  1 \n",
            " Loss:  0.18606655299663544\n",
            "Training:  Iteration No:  281 Worker Num:  2 \n",
            " Loss:  0.2782471776008606\n",
            "Training:  Iteration No:  281 Worker Num:  3 \n",
            " Loss:  0.12008589506149292\n",
            "Training:  Iteration No:  281 Worker Num:  4 \n",
            " Loss:  0.22812721133232117\n",
            "Training:  Iteration No:  281 Worker Num:  5 \n",
            " Loss:  0.16579914093017578\n",
            "Training:  Iteration No:  281 Worker Num:  6 \n",
            " Loss:  0.3096388578414917\n",
            "Training:  Iteration No:  281 Worker Num:  7 \n",
            " Loss:  0.1621970683336258\n",
            "Test:  Iteration No:  281 \n",
            " Loss:  0.17613398860196902\n",
            "Test accuracy:  94.58\n",
            "Training:  Iteration No:  282 Worker Num:  0 \n",
            " Loss:  0.17822816967964172\n",
            "Training:  Iteration No:  282 Worker Num:  1 \n",
            " Loss:  0.15130628645420074\n",
            "Training:  Iteration No:  282 Worker Num:  2 \n",
            " Loss:  0.20079872012138367\n",
            "Training:  Iteration No:  282 Worker Num:  3 \n",
            " Loss:  0.2145184576511383\n",
            "Training:  Iteration No:  282 Worker Num:  4 \n",
            " Loss:  0.38508930802345276\n",
            "Training:  Iteration No:  282 Worker Num:  5 \n",
            " Loss:  0.25705641508102417\n",
            "Training:  Iteration No:  282 Worker Num:  6 \n",
            " Loss:  0.1324729025363922\n",
            "Training:  Iteration No:  282 Worker Num:  7 \n",
            " Loss:  0.19705791771411896\n",
            "Test:  Iteration No:  282 \n",
            " Loss:  0.17391566191858884\n",
            "Test accuracy:  94.81\n",
            "Training:  Iteration No:  283 Worker Num:  0 \n",
            " Loss:  0.3222006559371948\n",
            "Training:  Iteration No:  283 Worker Num:  1 \n",
            " Loss:  0.3469451069831848\n",
            "Training:  Iteration No:  283 Worker Num:  2 \n",
            " Loss:  0.17461343109607697\n",
            "Training:  Iteration No:  283 Worker Num:  3 \n",
            " Loss:  0.2719112038612366\n",
            "Training:  Iteration No:  283 Worker Num:  4 \n",
            " Loss:  0.2470494955778122\n",
            "Training:  Iteration No:  283 Worker Num:  5 \n",
            " Loss:  0.18493607640266418\n",
            "Training:  Iteration No:  283 Worker Num:  6 \n",
            " Loss:  0.20233608782291412\n",
            "Training:  Iteration No:  283 Worker Num:  7 \n",
            " Loss:  0.23139485716819763\n",
            "Test:  Iteration No:  283 \n",
            " Loss:  0.1731344921384714\n",
            "Test accuracy:  94.89\n",
            "Training:  Iteration No:  284 Worker Num:  0 \n",
            " Loss:  0.23157012462615967\n",
            "Training:  Iteration No:  284 Worker Num:  1 \n",
            " Loss:  0.1439104527235031\n",
            "Training:  Iteration No:  284 Worker Num:  2 \n",
            " Loss:  0.2283720225095749\n",
            "Training:  Iteration No:  284 Worker Num:  3 \n",
            " Loss:  0.28510043025016785\n",
            "Training:  Iteration No:  284 Worker Num:  4 \n",
            " Loss:  0.22668197751045227\n",
            "Training:  Iteration No:  284 Worker Num:  5 \n",
            " Loss:  0.22501705586910248\n",
            "Training:  Iteration No:  284 Worker Num:  6 \n",
            " Loss:  0.2424144446849823\n",
            "Training:  Iteration No:  284 Worker Num:  7 \n",
            " Loss:  0.14748425781726837\n",
            "Test:  Iteration No:  284 \n",
            " Loss:  0.17441281360350078\n",
            "Test accuracy:  94.87\n",
            "Training:  Iteration No:  285 Worker Num:  0 \n",
            " Loss:  0.19322560727596283\n",
            "Training:  Iteration No:  285 Worker Num:  1 \n",
            " Loss:  0.2003588080406189\n",
            "Training:  Iteration No:  285 Worker Num:  2 \n",
            " Loss:  0.15627102553844452\n",
            "Training:  Iteration No:  285 Worker Num:  3 \n",
            " Loss:  0.27024930715560913\n",
            "Training:  Iteration No:  285 Worker Num:  4 \n",
            " Loss:  0.18367184698581696\n",
            "Training:  Iteration No:  285 Worker Num:  5 \n",
            " Loss:  0.25689923763275146\n",
            "Training:  Iteration No:  285 Worker Num:  6 \n",
            " Loss:  0.26636192202568054\n",
            "Training:  Iteration No:  285 Worker Num:  7 \n",
            " Loss:  0.2742950916290283\n",
            "Test:  Iteration No:  285 \n",
            " Loss:  0.1719734669556912\n",
            "Test accuracy:  94.94\n",
            "Training:  Iteration No:  286 Worker Num:  0 \n",
            " Loss:  0.19061172008514404\n",
            "Training:  Iteration No:  286 Worker Num:  1 \n",
            " Loss:  0.09969548881053925\n",
            "Training:  Iteration No:  286 Worker Num:  2 \n",
            " Loss:  0.2607851028442383\n",
            "Training:  Iteration No:  286 Worker Num:  3 \n",
            " Loss:  0.3200215995311737\n",
            "Training:  Iteration No:  286 Worker Num:  4 \n",
            " Loss:  0.2994966506958008\n",
            "Training:  Iteration No:  286 Worker Num:  5 \n",
            " Loss:  0.19759412109851837\n",
            "Training:  Iteration No:  286 Worker Num:  6 \n",
            " Loss:  0.1767674833536148\n",
            "Training:  Iteration No:  286 Worker Num:  7 \n",
            " Loss:  0.3086598515510559\n",
            "Test:  Iteration No:  286 \n",
            " Loss:  0.17454862868696264\n",
            "Test accuracy:  94.73\n",
            "Training:  Iteration No:  287 Worker Num:  0 \n",
            " Loss:  0.18779630959033966\n",
            "Training:  Iteration No:  287 Worker Num:  1 \n",
            " Loss:  0.22041963040828705\n",
            "Training:  Iteration No:  287 Worker Num:  2 \n",
            " Loss:  0.16620425879955292\n",
            "Training:  Iteration No:  287 Worker Num:  3 \n",
            " Loss:  0.13186556100845337\n",
            "Training:  Iteration No:  287 Worker Num:  4 \n",
            " Loss:  0.2336694598197937\n",
            "Training:  Iteration No:  287 Worker Num:  5 \n",
            " Loss:  0.12754596769809723\n",
            "Training:  Iteration No:  287 Worker Num:  6 \n",
            " Loss:  0.16930066049098969\n",
            "Training:  Iteration No:  287 Worker Num:  7 \n",
            " Loss:  0.2296046018600464\n",
            "Test:  Iteration No:  287 \n",
            " Loss:  0.17101889848709106\n",
            "Test accuracy:  94.86\n",
            "Training:  Iteration No:  288 Worker Num:  0 \n",
            " Loss:  0.2366359829902649\n",
            "Training:  Iteration No:  288 Worker Num:  1 \n",
            " Loss:  0.10129518806934357\n",
            "Training:  Iteration No:  288 Worker Num:  2 \n",
            " Loss:  0.2194298356771469\n",
            "Training:  Iteration No:  288 Worker Num:  3 \n",
            " Loss:  0.34910812973976135\n",
            "Training:  Iteration No:  288 Worker Num:  4 \n",
            " Loss:  0.30881375074386597\n",
            "Training:  Iteration No:  288 Worker Num:  5 \n",
            " Loss:  0.1655426323413849\n",
            "Training:  Iteration No:  288 Worker Num:  6 \n",
            " Loss:  0.23897992074489594\n",
            "Training:  Iteration No:  288 Worker Num:  7 \n",
            " Loss:  0.18452264368534088\n",
            "Test:  Iteration No:  288 \n",
            " Loss:  0.17532575685719523\n",
            "Test accuracy:  94.86\n",
            "Training:  Iteration No:  289 Worker Num:  0 \n",
            " Loss:  0.20471809804439545\n",
            "Training:  Iteration No:  289 Worker Num:  1 \n",
            " Loss:  0.23111264407634735\n",
            "Training:  Iteration No:  289 Worker Num:  2 \n",
            " Loss:  0.23808319866657257\n",
            "Training:  Iteration No:  289 Worker Num:  3 \n",
            " Loss:  0.2402777373790741\n",
            "Training:  Iteration No:  289 Worker Num:  4 \n",
            " Loss:  0.1812671273946762\n",
            "Training:  Iteration No:  289 Worker Num:  5 \n",
            " Loss:  0.33471915125846863\n",
            "Training:  Iteration No:  289 Worker Num:  6 \n",
            " Loss:  0.29481565952301025\n",
            "Training:  Iteration No:  289 Worker Num:  7 \n",
            " Loss:  0.26537802815437317\n",
            "Test:  Iteration No:  289 \n",
            " Loss:  0.17069743952089095\n",
            "Test accuracy:  94.88\n",
            "Training:  Iteration No:  290 Worker Num:  0 \n",
            " Loss:  0.2987208962440491\n",
            "Training:  Iteration No:  290 Worker Num:  1 \n",
            " Loss:  0.09992970526218414\n",
            "Training:  Iteration No:  290 Worker Num:  2 \n",
            " Loss:  0.21607720851898193\n",
            "Training:  Iteration No:  290 Worker Num:  3 \n",
            " Loss:  0.15214429795742035\n",
            "Training:  Iteration No:  290 Worker Num:  4 \n",
            " Loss:  0.23499755561351776\n",
            "Training:  Iteration No:  290 Worker Num:  5 \n",
            " Loss:  0.31075623631477356\n",
            "Training:  Iteration No:  290 Worker Num:  6 \n",
            " Loss:  0.19641605019569397\n",
            "Training:  Iteration No:  290 Worker Num:  7 \n",
            " Loss:  0.25131756067276\n",
            "Test:  Iteration No:  290 \n",
            " Loss:  0.1686042234407667\n",
            "Test accuracy:  95.05\n",
            "Training:  Iteration No:  291 Worker Num:  0 \n",
            " Loss:  0.2628692090511322\n",
            "Training:  Iteration No:  291 Worker Num:  1 \n",
            " Loss:  0.18626630306243896\n",
            "Training:  Iteration No:  291 Worker Num:  2 \n",
            " Loss:  0.23928919434547424\n",
            "Training:  Iteration No:  291 Worker Num:  3 \n",
            " Loss:  0.21205823123455048\n",
            "Training:  Iteration No:  291 Worker Num:  4 \n",
            " Loss:  0.16254131495952606\n",
            "Training:  Iteration No:  291 Worker Num:  5 \n",
            " Loss:  0.20243065059185028\n",
            "Training:  Iteration No:  291 Worker Num:  6 \n",
            " Loss:  0.2542974054813385\n",
            "Training:  Iteration No:  291 Worker Num:  7 \n",
            " Loss:  0.23266412317752838\n",
            "Test:  Iteration No:  291 \n",
            " Loss:  0.16950288066003896\n",
            "Test accuracy:  95.02\n",
            "Training:  Iteration No:  292 Worker Num:  0 \n",
            " Loss:  0.1858530342578888\n",
            "Training:  Iteration No:  292 Worker Num:  1 \n",
            " Loss:  0.21087999641895294\n",
            "Training:  Iteration No:  292 Worker Num:  2 \n",
            " Loss:  0.1589520275592804\n",
            "Training:  Iteration No:  292 Worker Num:  3 \n",
            " Loss:  0.12509077787399292\n",
            "Training:  Iteration No:  292 Worker Num:  4 \n",
            " Loss:  0.15840494632720947\n",
            "Training:  Iteration No:  292 Worker Num:  5 \n",
            " Loss:  0.30427709221839905\n",
            "Training:  Iteration No:  292 Worker Num:  6 \n",
            " Loss:  0.18279999494552612\n",
            "Training:  Iteration No:  292 Worker Num:  7 \n",
            " Loss:  0.15455353260040283\n",
            "Test:  Iteration No:  292 \n",
            " Loss:  0.1699796184319767\n",
            "Test accuracy:  94.99\n",
            "Training:  Iteration No:  293 Worker Num:  0 \n",
            " Loss:  0.1266302913427353\n",
            "Training:  Iteration No:  293 Worker Num:  1 \n",
            " Loss:  0.23371364176273346\n",
            "Training:  Iteration No:  293 Worker Num:  2 \n",
            " Loss:  0.23715022206306458\n",
            "Training:  Iteration No:  293 Worker Num:  3 \n",
            " Loss:  0.14154212176799774\n",
            "Training:  Iteration No:  293 Worker Num:  4 \n",
            " Loss:  0.21192987263202667\n",
            "Training:  Iteration No:  293 Worker Num:  5 \n",
            " Loss:  0.15373989939689636\n",
            "Training:  Iteration No:  293 Worker Num:  6 \n",
            " Loss:  0.25817468762397766\n",
            "Training:  Iteration No:  293 Worker Num:  7 \n",
            " Loss:  0.3243314325809479\n",
            "Test:  Iteration No:  293 \n",
            " Loss:  0.17027379996202227\n",
            "Test accuracy:  95.01\n",
            "Training:  Iteration No:  294 Worker Num:  0 \n",
            " Loss:  0.24698439240455627\n",
            "Training:  Iteration No:  294 Worker Num:  1 \n",
            " Loss:  0.14808274805545807\n",
            "Training:  Iteration No:  294 Worker Num:  2 \n",
            " Loss:  0.120506152510643\n",
            "Training:  Iteration No:  294 Worker Num:  3 \n",
            " Loss:  0.25367945432662964\n",
            "Training:  Iteration No:  294 Worker Num:  4 \n",
            " Loss:  0.25621718168258667\n",
            "Training:  Iteration No:  294 Worker Num:  5 \n",
            " Loss:  0.19707199931144714\n",
            "Training:  Iteration No:  294 Worker Num:  6 \n",
            " Loss:  0.3593767583370209\n",
            "Training:  Iteration No:  294 Worker Num:  7 \n",
            " Loss:  0.1523531973361969\n",
            "Test:  Iteration No:  294 \n",
            " Loss:  0.16923853981344006\n",
            "Test accuracy:  94.96\n",
            "Training:  Iteration No:  295 Worker Num:  0 \n",
            " Loss:  0.19629718363285065\n",
            "Training:  Iteration No:  295 Worker Num:  1 \n",
            " Loss:  0.22209225594997406\n",
            "Training:  Iteration No:  295 Worker Num:  2 \n",
            " Loss:  0.2484203726053238\n",
            "Training:  Iteration No:  295 Worker Num:  3 \n",
            " Loss:  0.22538121044635773\n",
            "Training:  Iteration No:  295 Worker Num:  4 \n",
            " Loss:  0.2047269344329834\n",
            "Training:  Iteration No:  295 Worker Num:  5 \n",
            " Loss:  0.16214214265346527\n",
            "Training:  Iteration No:  295 Worker Num:  6 \n",
            " Loss:  0.1885397881269455\n",
            "Training:  Iteration No:  295 Worker Num:  7 \n",
            " Loss:  0.20346486568450928\n",
            "Test:  Iteration No:  295 \n",
            " Loss:  0.16840339971918472\n",
            "Test accuracy:  95.02\n",
            "Training:  Iteration No:  296 Worker Num:  0 \n",
            " Loss:  0.303872287273407\n",
            "Training:  Iteration No:  296 Worker Num:  1 \n",
            " Loss:  0.17908845841884613\n",
            "Training:  Iteration No:  296 Worker Num:  2 \n",
            " Loss:  0.17537353932857513\n",
            "Training:  Iteration No:  296 Worker Num:  3 \n",
            " Loss:  0.22262883186340332\n",
            "Training:  Iteration No:  296 Worker Num:  4 \n",
            " Loss:  0.21677379310131073\n",
            "Training:  Iteration No:  296 Worker Num:  5 \n",
            " Loss:  0.16216765344142914\n",
            "Training:  Iteration No:  296 Worker Num:  6 \n",
            " Loss:  0.21441516280174255\n",
            "Training:  Iteration No:  296 Worker Num:  7 \n",
            " Loss:  0.18078410625457764\n",
            "Test:  Iteration No:  296 \n",
            " Loss:  0.16618959378168174\n",
            "Test accuracy:  94.99\n",
            "Training:  Iteration No:  297 Worker Num:  0 \n",
            " Loss:  0.20114217698574066\n",
            "Training:  Iteration No:  297 Worker Num:  1 \n",
            " Loss:  0.19789822399616241\n",
            "Training:  Iteration No:  297 Worker Num:  2 \n",
            " Loss:  0.31034722924232483\n",
            "Training:  Iteration No:  297 Worker Num:  3 \n",
            " Loss:  0.16692940890789032\n",
            "Training:  Iteration No:  297 Worker Num:  4 \n",
            " Loss:  0.09667516499757767\n",
            "Training:  Iteration No:  297 Worker Num:  5 \n",
            " Loss:  0.11785352230072021\n",
            "Training:  Iteration No:  297 Worker Num:  6 \n",
            " Loss:  0.2057439237833023\n",
            "Training:  Iteration No:  297 Worker Num:  7 \n",
            " Loss:  0.25243669748306274\n",
            "Test:  Iteration No:  297 \n",
            " Loss:  0.16723477047620505\n",
            "Test accuracy:  94.93\n",
            "Training:  Iteration No:  298 Worker Num:  0 \n",
            " Loss:  0.2444876879453659\n",
            "Training:  Iteration No:  298 Worker Num:  1 \n",
            " Loss:  0.13724283874034882\n",
            "Training:  Iteration No:  298 Worker Num:  2 \n",
            " Loss:  0.17104297876358032\n",
            "Training:  Iteration No:  298 Worker Num:  3 \n",
            " Loss:  0.18999558687210083\n",
            "Training:  Iteration No:  298 Worker Num:  4 \n",
            " Loss:  0.23802463710308075\n",
            "Training:  Iteration No:  298 Worker Num:  5 \n",
            " Loss:  0.2639915943145752\n",
            "Training:  Iteration No:  298 Worker Num:  6 \n",
            " Loss:  0.13417071104049683\n",
            "Training:  Iteration No:  298 Worker Num:  7 \n",
            " Loss:  0.2408539354801178\n",
            "Test:  Iteration No:  298 \n",
            " Loss:  0.16918717640676076\n",
            "Test accuracy:  94.94\n",
            "Training:  Iteration No:  299 Worker Num:  0 \n",
            " Loss:  0.38215959072113037\n",
            "Training:  Iteration No:  299 Worker Num:  1 \n",
            " Loss:  0.33291125297546387\n",
            "Training:  Iteration No:  299 Worker Num:  2 \n",
            " Loss:  0.16359524428844452\n",
            "Training:  Iteration No:  299 Worker Num:  3 \n",
            " Loss:  0.14043432474136353\n",
            "Training:  Iteration No:  299 Worker Num:  4 \n",
            " Loss:  0.2072741985321045\n",
            "Training:  Iteration No:  299 Worker Num:  5 \n",
            " Loss:  0.13056747615337372\n",
            "Training:  Iteration No:  299 Worker Num:  6 \n",
            " Loss:  0.20891450345516205\n",
            "Training:  Iteration No:  299 Worker Num:  7 \n",
            " Loss:  0.21741066873073578\n",
            "Test:  Iteration No:  299 \n",
            " Loss:  0.16850618913746143\n",
            "Test accuracy:  95.07\n",
            "Training:  Iteration No:  300 Worker Num:  0 \n",
            " Loss:  0.2376202493906021\n",
            "Training:  Iteration No:  300 Worker Num:  1 \n",
            " Loss:  0.23962301015853882\n",
            "Training:  Iteration No:  300 Worker Num:  2 \n",
            " Loss:  0.1659024953842163\n",
            "Training:  Iteration No:  300 Worker Num:  3 \n",
            " Loss:  0.13131371140480042\n",
            "Training:  Iteration No:  300 Worker Num:  4 \n",
            " Loss:  0.20644305646419525\n",
            "Training:  Iteration No:  300 Worker Num:  5 \n",
            " Loss:  0.3234410583972931\n",
            "Training:  Iteration No:  300 Worker Num:  6 \n",
            " Loss:  0.14639179408550262\n",
            "Training:  Iteration No:  300 Worker Num:  7 \n",
            " Loss:  0.17730356752872467\n",
            "Test:  Iteration No:  300 \n",
            " Loss:  0.16401054254509037\n",
            "Test accuracy:  95.14\n",
            "Training:  Iteration No:  301 Worker Num:  0 \n",
            " Loss:  0.26272106170654297\n",
            "Training:  Iteration No:  301 Worker Num:  1 \n",
            " Loss:  0.19833296537399292\n",
            "Training:  Iteration No:  301 Worker Num:  2 \n",
            " Loss:  0.19867870211601257\n",
            "Training:  Iteration No:  301 Worker Num:  3 \n",
            " Loss:  0.12490112334489822\n",
            "Training:  Iteration No:  301 Worker Num:  4 \n",
            " Loss:  0.19800755381584167\n",
            "Training:  Iteration No:  301 Worker Num:  5 \n",
            " Loss:  0.07698313146829605\n",
            "Training:  Iteration No:  301 Worker Num:  6 \n",
            " Loss:  0.23691286146640778\n",
            "Training:  Iteration No:  301 Worker Num:  7 \n",
            " Loss:  0.19893352687358856\n",
            "Test:  Iteration No:  301 \n",
            " Loss:  0.16536548384170555\n",
            "Test accuracy:  94.89\n",
            "Training:  Iteration No:  302 Worker Num:  0 \n",
            " Loss:  0.15420527756214142\n",
            "Training:  Iteration No:  302 Worker Num:  1 \n",
            " Loss:  0.26605936884880066\n",
            "Training:  Iteration No:  302 Worker Num:  2 \n",
            " Loss:  0.1591411978006363\n",
            "Training:  Iteration No:  302 Worker Num:  3 \n",
            " Loss:  0.23042970895767212\n",
            "Training:  Iteration No:  302 Worker Num:  4 \n",
            " Loss:  0.23049145936965942\n",
            "Training:  Iteration No:  302 Worker Num:  5 \n",
            " Loss:  0.09053090959787369\n",
            "Training:  Iteration No:  302 Worker Num:  6 \n",
            " Loss:  0.23208491504192352\n",
            "Training:  Iteration No:  302 Worker Num:  7 \n",
            " Loss:  0.20672711730003357\n",
            "Test:  Iteration No:  302 \n",
            " Loss:  0.16706522534118046\n",
            "Test accuracy:  94.96\n",
            "Training:  Iteration No:  303 Worker Num:  0 \n",
            " Loss:  0.2837579846382141\n",
            "Training:  Iteration No:  303 Worker Num:  1 \n",
            " Loss:  0.1427498608827591\n",
            "Training:  Iteration No:  303 Worker Num:  2 \n",
            " Loss:  0.1802399903535843\n",
            "Training:  Iteration No:  303 Worker Num:  3 \n",
            " Loss:  0.11634492129087448\n",
            "Training:  Iteration No:  303 Worker Num:  4 \n",
            " Loss:  0.18928156793117523\n",
            "Training:  Iteration No:  303 Worker Num:  5 \n",
            " Loss:  0.26499974727630615\n",
            "Training:  Iteration No:  303 Worker Num:  6 \n",
            " Loss:  0.21591195464134216\n",
            "Training:  Iteration No:  303 Worker Num:  7 \n",
            " Loss:  0.13755613565444946\n",
            "Test:  Iteration No:  303 \n",
            " Loss:  0.17247584101425695\n",
            "Test accuracy:  94.93\n",
            "Training:  Iteration No:  304 Worker Num:  0 \n",
            " Loss:  0.1664644032716751\n",
            "Training:  Iteration No:  304 Worker Num:  1 \n",
            " Loss:  0.1821417212486267\n",
            "Training:  Iteration No:  304 Worker Num:  2 \n",
            " Loss:  0.20026947557926178\n",
            "Training:  Iteration No:  304 Worker Num:  3 \n",
            " Loss:  0.1254679411649704\n",
            "Training:  Iteration No:  304 Worker Num:  4 \n",
            " Loss:  0.16535578668117523\n",
            "Training:  Iteration No:  304 Worker Num:  5 \n",
            " Loss:  0.19123801589012146\n",
            "Training:  Iteration No:  304 Worker Num:  6 \n",
            " Loss:  0.2294045090675354\n",
            "Training:  Iteration No:  304 Worker Num:  7 \n",
            " Loss:  0.1774262934923172\n",
            "Test:  Iteration No:  304 \n",
            " Loss:  0.1639610094171536\n",
            "Test accuracy:  95.03\n",
            "Training:  Iteration No:  305 Worker Num:  0 \n",
            " Loss:  0.15659089386463165\n",
            "Training:  Iteration No:  305 Worker Num:  1 \n",
            " Loss:  0.21968549489974976\n",
            "Training:  Iteration No:  305 Worker Num:  2 \n",
            " Loss:  0.22889825701713562\n",
            "Training:  Iteration No:  305 Worker Num:  3 \n",
            " Loss:  0.25837981700897217\n",
            "Training:  Iteration No:  305 Worker Num:  4 \n",
            " Loss:  0.08900537341833115\n",
            "Training:  Iteration No:  305 Worker Num:  5 \n",
            " Loss:  0.2894585430622101\n",
            "Training:  Iteration No:  305 Worker Num:  6 \n",
            " Loss:  0.20781926810741425\n",
            "Training:  Iteration No:  305 Worker Num:  7 \n",
            " Loss:  0.16631576418876648\n",
            "Test:  Iteration No:  305 \n",
            " Loss:  0.16353230819671968\n",
            "Test accuracy:  95.17\n",
            "Training:  Iteration No:  306 Worker Num:  0 \n",
            " Loss:  0.13155268132686615\n",
            "Training:  Iteration No:  306 Worker Num:  1 \n",
            " Loss:  0.30185389518737793\n",
            "Training:  Iteration No:  306 Worker Num:  2 \n",
            " Loss:  0.17602626979351044\n",
            "Training:  Iteration No:  306 Worker Num:  3 \n",
            " Loss:  0.19388462603092194\n",
            "Training:  Iteration No:  306 Worker Num:  4 \n",
            " Loss:  0.28474247455596924\n",
            "Training:  Iteration No:  306 Worker Num:  5 \n",
            " Loss:  0.3535435199737549\n",
            "Training:  Iteration No:  306 Worker Num:  6 \n",
            " Loss:  0.217288538813591\n",
            "Training:  Iteration No:  306 Worker Num:  7 \n",
            " Loss:  0.1813884675502777\n",
            "Test:  Iteration No:  306 \n",
            " Loss:  0.16093711497827987\n",
            "Test accuracy:  95.29\n",
            "Training:  Iteration No:  307 Worker Num:  0 \n",
            " Loss:  0.2568509876728058\n",
            "Training:  Iteration No:  307 Worker Num:  1 \n",
            " Loss:  0.07510151714086533\n",
            "Training:  Iteration No:  307 Worker Num:  2 \n",
            " Loss:  0.24874718487262726\n",
            "Training:  Iteration No:  307 Worker Num:  3 \n",
            " Loss:  0.20713527500629425\n",
            "Training:  Iteration No:  307 Worker Num:  4 \n",
            " Loss:  0.2301691174507141\n",
            "Training:  Iteration No:  307 Worker Num:  5 \n",
            " Loss:  0.18018250167369843\n",
            "Training:  Iteration No:  307 Worker Num:  6 \n",
            " Loss:  0.2341931313276291\n",
            "Training:  Iteration No:  307 Worker Num:  7 \n",
            " Loss:  0.20310050249099731\n",
            "Test:  Iteration No:  307 \n",
            " Loss:  0.16552463030041772\n",
            "Test accuracy:  95.19\n",
            "Training:  Iteration No:  308 Worker Num:  0 \n",
            " Loss:  0.16717691719532013\n",
            "Training:  Iteration No:  308 Worker Num:  1 \n",
            " Loss:  0.19274722039699554\n",
            "Training:  Iteration No:  308 Worker Num:  2 \n",
            " Loss:  0.17109647393226624\n",
            "Training:  Iteration No:  308 Worker Num:  3 \n",
            " Loss:  0.29520097374916077\n",
            "Training:  Iteration No:  308 Worker Num:  4 \n",
            " Loss:  0.10720878839492798\n",
            "Training:  Iteration No:  308 Worker Num:  5 \n",
            " Loss:  0.1173768937587738\n",
            "Training:  Iteration No:  308 Worker Num:  6 \n",
            " Loss:  0.18194350600242615\n",
            "Training:  Iteration No:  308 Worker Num:  7 \n",
            " Loss:  0.29031702876091003\n",
            "Test:  Iteration No:  308 \n",
            " Loss:  0.1597141346281182\n",
            "Test accuracy:  95.42\n",
            "Training:  Iteration No:  309 Worker Num:  0 \n",
            " Loss:  0.1925433874130249\n",
            "Training:  Iteration No:  309 Worker Num:  1 \n",
            " Loss:  0.1203480139374733\n",
            "Training:  Iteration No:  309 Worker Num:  2 \n",
            " Loss:  0.23957091569900513\n",
            "Training:  Iteration No:  309 Worker Num:  3 \n",
            " Loss:  0.2163197547197342\n",
            "Training:  Iteration No:  309 Worker Num:  4 \n",
            " Loss:  0.14785246551036835\n",
            "Training:  Iteration No:  309 Worker Num:  5 \n",
            " Loss:  0.25321897864341736\n",
            "Training:  Iteration No:  309 Worker Num:  6 \n",
            " Loss:  0.22156046330928802\n",
            "Training:  Iteration No:  309 Worker Num:  7 \n",
            " Loss:  0.14372828602790833\n",
            "Test:  Iteration No:  309 \n",
            " Loss:  0.158771744770104\n",
            "Test accuracy:  95.29\n",
            "Training:  Iteration No:  310 Worker Num:  0 \n",
            " Loss:  0.17800132930278778\n",
            "Training:  Iteration No:  310 Worker Num:  1 \n",
            " Loss:  0.1352023333311081\n",
            "Training:  Iteration No:  310 Worker Num:  2 \n",
            " Loss:  0.18690966069698334\n",
            "Training:  Iteration No:  310 Worker Num:  3 \n",
            " Loss:  0.2676149010658264\n",
            "Training:  Iteration No:  310 Worker Num:  4 \n",
            " Loss:  0.3238205015659332\n",
            "Training:  Iteration No:  310 Worker Num:  5 \n",
            " Loss:  0.1653619408607483\n",
            "Training:  Iteration No:  310 Worker Num:  6 \n",
            " Loss:  0.20215116441249847\n",
            "Training:  Iteration No:  310 Worker Num:  7 \n",
            " Loss:  0.13005131483078003\n",
            "Test:  Iteration No:  310 \n",
            " Loss:  0.1673640783734714\n",
            "Test accuracy:  94.89\n",
            "Training:  Iteration No:  311 Worker Num:  0 \n",
            " Loss:  0.13137470185756683\n",
            "Training:  Iteration No:  311 Worker Num:  1 \n",
            " Loss:  0.16966931521892548\n",
            "Training:  Iteration No:  311 Worker Num:  2 \n",
            " Loss:  0.16198720037937164\n",
            "Training:  Iteration No:  311 Worker Num:  3 \n",
            " Loss:  0.1931048333644867\n",
            "Training:  Iteration No:  311 Worker Num:  4 \n",
            " Loss:  0.1279061734676361\n",
            "Training:  Iteration No:  311 Worker Num:  5 \n",
            " Loss:  0.12549957633018494\n",
            "Training:  Iteration No:  311 Worker Num:  6 \n",
            " Loss:  0.09460603445768356\n",
            "Training:  Iteration No:  311 Worker Num:  7 \n",
            " Loss:  0.11576317995786667\n",
            "Test:  Iteration No:  311 \n",
            " Loss:  0.16277154576316288\n",
            "Test accuracy:  95.25\n",
            "Training:  Iteration No:  312 Worker Num:  0 \n",
            " Loss:  0.22742822766304016\n",
            "Training:  Iteration No:  312 Worker Num:  1 \n",
            " Loss:  0.28526994585990906\n",
            "Training:  Iteration No:  312 Worker Num:  2 \n",
            " Loss:  0.1654803603887558\n",
            "Training:  Iteration No:  312 Worker Num:  3 \n",
            " Loss:  0.19693996012210846\n",
            "Training:  Iteration No:  312 Worker Num:  4 \n",
            " Loss:  0.24035543203353882\n",
            "Training:  Iteration No:  312 Worker Num:  5 \n",
            " Loss:  0.1722733974456787\n",
            "Training:  Iteration No:  312 Worker Num:  6 \n",
            " Loss:  0.1362801343202591\n",
            "Training:  Iteration No:  312 Worker Num:  7 \n",
            " Loss:  0.2105935513973236\n",
            "Test:  Iteration No:  312 \n",
            " Loss:  0.16086125562462625\n",
            "Test accuracy:  95.03\n",
            "Training:  Iteration No:  313 Worker Num:  0 \n",
            " Loss:  0.1836063265800476\n",
            "Training:  Iteration No:  313 Worker Num:  1 \n",
            " Loss:  0.17829638719558716\n",
            "Training:  Iteration No:  313 Worker Num:  2 \n",
            " Loss:  0.21688714623451233\n",
            "Training:  Iteration No:  313 Worker Num:  3 \n",
            " Loss:  0.18856734037399292\n",
            "Training:  Iteration No:  313 Worker Num:  4 \n",
            " Loss:  0.23275010287761688\n",
            "Training:  Iteration No:  313 Worker Num:  5 \n",
            " Loss:  0.14122173190116882\n",
            "Training:  Iteration No:  313 Worker Num:  6 \n",
            " Loss:  0.2371300607919693\n",
            "Training:  Iteration No:  313 Worker Num:  7 \n",
            " Loss:  0.2147417664527893\n",
            "Test:  Iteration No:  313 \n",
            " Loss:  0.15755882590443274\n",
            "Test accuracy:  95.23\n",
            "Training:  Iteration No:  314 Worker Num:  0 \n",
            " Loss:  0.22785907983779907\n",
            "Training:  Iteration No:  314 Worker Num:  1 \n",
            " Loss:  0.2528940737247467\n",
            "Training:  Iteration No:  314 Worker Num:  2 \n",
            " Loss:  0.15717172622680664\n",
            "Training:  Iteration No:  314 Worker Num:  3 \n",
            " Loss:  0.2892129719257355\n",
            "Training:  Iteration No:  314 Worker Num:  4 \n",
            " Loss:  0.19410324096679688\n",
            "Training:  Iteration No:  314 Worker Num:  5 \n",
            " Loss:  0.2407962530851364\n",
            "Training:  Iteration No:  314 Worker Num:  6 \n",
            " Loss:  0.16164158284664154\n",
            "Training:  Iteration No:  314 Worker Num:  7 \n",
            " Loss:  0.13994520902633667\n",
            "Test:  Iteration No:  314 \n",
            " Loss:  0.15465432088940015\n",
            "Test accuracy:  95.36\n",
            "Training:  Iteration No:  315 Worker Num:  0 \n",
            " Loss:  0.2975270748138428\n",
            "Training:  Iteration No:  315 Worker Num:  1 \n",
            " Loss:  0.13671335577964783\n",
            "Training:  Iteration No:  315 Worker Num:  2 \n",
            " Loss:  0.3045819401741028\n",
            "Training:  Iteration No:  315 Worker Num:  3 \n",
            " Loss:  0.15380091965198517\n",
            "Training:  Iteration No:  315 Worker Num:  4 \n",
            " Loss:  0.20566396415233612\n",
            "Training:  Iteration No:  315 Worker Num:  5 \n",
            " Loss:  0.17035144567489624\n",
            "Training:  Iteration No:  315 Worker Num:  6 \n",
            " Loss:  0.12325054407119751\n",
            "Training:  Iteration No:  315 Worker Num:  7 \n",
            " Loss:  0.23418062925338745\n",
            "Test:  Iteration No:  315 \n",
            " Loss:  0.1602122479861107\n",
            "Test accuracy:  95.31\n",
            "Training:  Iteration No:  316 Worker Num:  0 \n",
            " Loss:  0.18462008237838745\n",
            "Training:  Iteration No:  316 Worker Num:  1 \n",
            " Loss:  0.17240072786808014\n",
            "Training:  Iteration No:  316 Worker Num:  2 \n",
            " Loss:  0.17628435790538788\n",
            "Training:  Iteration No:  316 Worker Num:  3 \n",
            " Loss:  0.24948665499687195\n",
            "Training:  Iteration No:  316 Worker Num:  4 \n",
            " Loss:  0.14195875823497772\n",
            "Training:  Iteration No:  316 Worker Num:  5 \n",
            " Loss:  0.13782721757888794\n",
            "Training:  Iteration No:  316 Worker Num:  6 \n",
            " Loss:  0.2180270105600357\n",
            "Training:  Iteration No:  316 Worker Num:  7 \n",
            " Loss:  0.19516976177692413\n",
            "Test:  Iteration No:  316 \n",
            " Loss:  0.15560107721204433\n",
            "Test accuracy:  95.43\n",
            "Training:  Iteration No:  317 Worker Num:  0 \n",
            " Loss:  0.2072538137435913\n",
            "Training:  Iteration No:  317 Worker Num:  1 \n",
            " Loss:  0.26024362444877625\n",
            "Training:  Iteration No:  317 Worker Num:  2 \n",
            " Loss:  0.3730897903442383\n",
            "Training:  Iteration No:  317 Worker Num:  3 \n",
            " Loss:  0.30896520614624023\n",
            "Training:  Iteration No:  317 Worker Num:  4 \n",
            " Loss:  0.13849994540214539\n",
            "Training:  Iteration No:  317 Worker Num:  5 \n",
            " Loss:  0.2500533163547516\n",
            "Training:  Iteration No:  317 Worker Num:  6 \n",
            " Loss:  0.28527599573135376\n",
            "Training:  Iteration No:  317 Worker Num:  7 \n",
            " Loss:  0.13421082496643066\n",
            "Test:  Iteration No:  317 \n",
            " Loss:  0.15468335859595408\n",
            "Test accuracy:  95.42\n",
            "Training:  Iteration No:  318 Worker Num:  0 \n",
            " Loss:  0.20731787383556366\n",
            "Training:  Iteration No:  318 Worker Num:  1 \n",
            " Loss:  0.14850902557373047\n",
            "Training:  Iteration No:  318 Worker Num:  2 \n",
            " Loss:  0.17585894465446472\n",
            "Training:  Iteration No:  318 Worker Num:  3 \n",
            " Loss:  0.12648263573646545\n",
            "Training:  Iteration No:  318 Worker Num:  4 \n",
            " Loss:  0.16584022343158722\n",
            "Training:  Iteration No:  318 Worker Num:  5 \n",
            " Loss:  0.1914730668067932\n",
            "Training:  Iteration No:  318 Worker Num:  6 \n",
            " Loss:  0.1545664370059967\n",
            "Training:  Iteration No:  318 Worker Num:  7 \n",
            " Loss:  0.25939351320266724\n",
            "Test:  Iteration No:  318 \n",
            " Loss:  0.15373907329608935\n",
            "Test accuracy:  95.4\n",
            "Training:  Iteration No:  319 Worker Num:  0 \n",
            " Loss:  0.26886919140815735\n",
            "Training:  Iteration No:  319 Worker Num:  1 \n",
            " Loss:  0.09326069802045822\n",
            "Training:  Iteration No:  319 Worker Num:  2 \n",
            " Loss:  0.15593524277210236\n",
            "Training:  Iteration No:  319 Worker Num:  3 \n",
            " Loss:  0.19246694445610046\n",
            "Training:  Iteration No:  319 Worker Num:  4 \n",
            " Loss:  0.15979154407978058\n",
            "Training:  Iteration No:  319 Worker Num:  5 \n",
            " Loss:  0.2975117862224579\n",
            "Training:  Iteration No:  319 Worker Num:  6 \n",
            " Loss:  0.10475161671638489\n",
            "Training:  Iteration No:  319 Worker Num:  7 \n",
            " Loss:  0.2345859557390213\n",
            "Test:  Iteration No:  319 \n",
            " Loss:  0.1545033446878572\n",
            "Test accuracy:  95.61\n",
            "Training:  Iteration No:  320 Worker Num:  0 \n",
            " Loss:  0.2767501175403595\n",
            "Training:  Iteration No:  320 Worker Num:  1 \n",
            " Loss:  0.20964816212654114\n",
            "Training:  Iteration No:  320 Worker Num:  2 \n",
            " Loss:  0.20313695073127747\n",
            "Training:  Iteration No:  320 Worker Num:  3 \n",
            " Loss:  0.2084892839193344\n",
            "Training:  Iteration No:  320 Worker Num:  4 \n",
            " Loss:  0.16075195372104645\n",
            "Training:  Iteration No:  320 Worker Num:  5 \n",
            " Loss:  0.14366444945335388\n",
            "Training:  Iteration No:  320 Worker Num:  6 \n",
            " Loss:  0.17543303966522217\n",
            "Training:  Iteration No:  320 Worker Num:  7 \n",
            " Loss:  0.14332827925682068\n",
            "Test:  Iteration No:  320 \n",
            " Loss:  0.1535957881744621\n",
            "Test accuracy:  95.48\n",
            "Training:  Iteration No:  321 Worker Num:  0 \n",
            " Loss:  0.14722542464733124\n",
            "Training:  Iteration No:  321 Worker Num:  1 \n",
            " Loss:  0.2104235291481018\n",
            "Training:  Iteration No:  321 Worker Num:  2 \n",
            " Loss:  0.24854519963264465\n",
            "Training:  Iteration No:  321 Worker Num:  3 \n",
            " Loss:  0.24113880097866058\n",
            "Training:  Iteration No:  321 Worker Num:  4 \n",
            " Loss:  0.28541386127471924\n",
            "Training:  Iteration No:  321 Worker Num:  5 \n",
            " Loss:  0.14289844036102295\n",
            "Training:  Iteration No:  321 Worker Num:  6 \n",
            " Loss:  0.16496799886226654\n",
            "Training:  Iteration No:  321 Worker Num:  7 \n",
            " Loss:  0.23975591361522675\n",
            "Test:  Iteration No:  321 \n",
            " Loss:  0.15335489548978548\n",
            "Test accuracy:  95.36\n",
            "Training:  Iteration No:  322 Worker Num:  0 \n",
            " Loss:  0.22461417317390442\n",
            "Training:  Iteration No:  322 Worker Num:  1 \n",
            " Loss:  0.22981880605220795\n",
            "Training:  Iteration No:  322 Worker Num:  2 \n",
            " Loss:  0.17228366434574127\n",
            "Training:  Iteration No:  322 Worker Num:  3 \n",
            " Loss:  0.24835598468780518\n",
            "Training:  Iteration No:  322 Worker Num:  4 \n",
            " Loss:  0.176686093211174\n",
            "Training:  Iteration No:  322 Worker Num:  5 \n",
            " Loss:  0.22564607858657837\n",
            "Training:  Iteration No:  322 Worker Num:  6 \n",
            " Loss:  0.25076842308044434\n",
            "Training:  Iteration No:  322 Worker Num:  7 \n",
            " Loss:  0.29040300846099854\n",
            "Test:  Iteration No:  322 \n",
            " Loss:  0.15251164438344444\n",
            "Test accuracy:  95.49\n",
            "Training:  Iteration No:  323 Worker Num:  0 \n",
            " Loss:  0.2444770187139511\n",
            "Training:  Iteration No:  323 Worker Num:  1 \n",
            " Loss:  0.20676791667938232\n",
            "Training:  Iteration No:  323 Worker Num:  2 \n",
            " Loss:  0.08940167725086212\n",
            "Training:  Iteration No:  323 Worker Num:  3 \n",
            " Loss:  0.2435297667980194\n",
            "Training:  Iteration No:  323 Worker Num:  4 \n",
            " Loss:  0.1537684202194214\n",
            "Training:  Iteration No:  323 Worker Num:  5 \n",
            " Loss:  0.19076845049858093\n",
            "Training:  Iteration No:  323 Worker Num:  6 \n",
            " Loss:  0.3274572491645813\n",
            "Training:  Iteration No:  323 Worker Num:  7 \n",
            " Loss:  0.23542368412017822\n",
            "Test:  Iteration No:  323 \n",
            " Loss:  0.15378742450494556\n",
            "Test accuracy:  95.39\n",
            "Training:  Iteration No:  324 Worker Num:  0 \n",
            " Loss:  0.13195492327213287\n",
            "Training:  Iteration No:  324 Worker Num:  1 \n",
            " Loss:  0.3166634142398834\n",
            "Training:  Iteration No:  324 Worker Num:  2 \n",
            " Loss:  0.28990960121154785\n",
            "Training:  Iteration No:  324 Worker Num:  3 \n",
            " Loss:  0.1958840787410736\n",
            "Training:  Iteration No:  324 Worker Num:  4 \n",
            " Loss:  0.22820691764354706\n",
            "Training:  Iteration No:  324 Worker Num:  5 \n",
            " Loss:  0.08711077272891998\n",
            "Training:  Iteration No:  324 Worker Num:  6 \n",
            " Loss:  0.2225511372089386\n",
            "Training:  Iteration No:  324 Worker Num:  7 \n",
            " Loss:  0.28201401233673096\n",
            "Test:  Iteration No:  324 \n",
            " Loss:  0.1500032880767897\n",
            "Test accuracy:  95.58\n",
            "Training:  Iteration No:  325 Worker Num:  0 \n",
            " Loss:  0.17839089035987854\n",
            "Training:  Iteration No:  325 Worker Num:  1 \n",
            " Loss:  0.24033787846565247\n",
            "Training:  Iteration No:  325 Worker Num:  2 \n",
            " Loss:  0.14657066762447357\n",
            "Training:  Iteration No:  325 Worker Num:  3 \n",
            " Loss:  0.17677822709083557\n",
            "Training:  Iteration No:  325 Worker Num:  4 \n",
            " Loss:  0.16993939876556396\n",
            "Training:  Iteration No:  325 Worker Num:  5 \n",
            " Loss:  0.1926325559616089\n",
            "Training:  Iteration No:  325 Worker Num:  6 \n",
            " Loss:  0.18307307362556458\n",
            "Training:  Iteration No:  325 Worker Num:  7 \n",
            " Loss:  0.14337067306041718\n",
            "Test:  Iteration No:  325 \n",
            " Loss:  0.15165960567095613\n",
            "Test accuracy:  95.42\n",
            "Training:  Iteration No:  326 Worker Num:  0 \n",
            " Loss:  0.17658527195453644\n",
            "Training:  Iteration No:  326 Worker Num:  1 \n",
            " Loss:  0.11833691596984863\n",
            "Training:  Iteration No:  326 Worker Num:  2 \n",
            " Loss:  0.1329738348722458\n",
            "Training:  Iteration No:  326 Worker Num:  3 \n",
            " Loss:  0.16347269713878632\n",
            "Training:  Iteration No:  326 Worker Num:  4 \n",
            " Loss:  0.18003208935260773\n",
            "Training:  Iteration No:  326 Worker Num:  5 \n",
            " Loss:  0.2801891565322876\n",
            "Training:  Iteration No:  326 Worker Num:  6 \n",
            " Loss:  0.21785639226436615\n",
            "Training:  Iteration No:  326 Worker Num:  7 \n",
            " Loss:  0.151021346449852\n",
            "Test:  Iteration No:  326 \n",
            " Loss:  0.15113024351693974\n",
            "Test accuracy:  95.53\n",
            "Training:  Iteration No:  327 Worker Num:  0 \n",
            " Loss:  0.17993950843811035\n",
            "Training:  Iteration No:  327 Worker Num:  1 \n",
            " Loss:  0.11977406591176987\n",
            "Training:  Iteration No:  327 Worker Num:  2 \n",
            " Loss:  0.17361627519130707\n",
            "Training:  Iteration No:  327 Worker Num:  3 \n",
            " Loss:  0.10704077780246735\n",
            "Training:  Iteration No:  327 Worker Num:  4 \n",
            " Loss:  0.27177730202674866\n",
            "Training:  Iteration No:  327 Worker Num:  5 \n",
            " Loss:  0.1174846738576889\n",
            "Training:  Iteration No:  327 Worker Num:  6 \n",
            " Loss:  0.21728365123271942\n",
            "Training:  Iteration No:  327 Worker Num:  7 \n",
            " Loss:  0.18213070929050446\n",
            "Test:  Iteration No:  327 \n",
            " Loss:  0.15096825214480109\n",
            "Test accuracy:  95.46\n",
            "Training:  Iteration No:  328 Worker Num:  0 \n",
            " Loss:  0.3542535603046417\n",
            "Training:  Iteration No:  328 Worker Num:  1 \n",
            " Loss:  0.22319121658802032\n",
            "Training:  Iteration No:  328 Worker Num:  2 \n",
            " Loss:  0.17417559027671814\n",
            "Training:  Iteration No:  328 Worker Num:  3 \n",
            " Loss:  0.16755461692810059\n",
            "Training:  Iteration No:  328 Worker Num:  4 \n",
            " Loss:  0.24945035576820374\n",
            "Training:  Iteration No:  328 Worker Num:  5 \n",
            " Loss:  0.23172152042388916\n",
            "Training:  Iteration No:  328 Worker Num:  6 \n",
            " Loss:  0.1517699509859085\n",
            "Training:  Iteration No:  328 Worker Num:  7 \n",
            " Loss:  0.1595369130373001\n",
            "Test:  Iteration No:  328 \n",
            " Loss:  0.14833499709309278\n",
            "Test accuracy:  95.76\n",
            "Training:  Iteration No:  329 Worker Num:  0 \n",
            " Loss:  0.13900908827781677\n",
            "Training:  Iteration No:  329 Worker Num:  1 \n",
            " Loss:  0.17054703831672668\n",
            "Training:  Iteration No:  329 Worker Num:  2 \n",
            " Loss:  0.15231779217720032\n",
            "Training:  Iteration No:  329 Worker Num:  3 \n",
            " Loss:  0.28960663080215454\n",
            "Training:  Iteration No:  329 Worker Num:  4 \n",
            " Loss:  0.11556845903396606\n",
            "Training:  Iteration No:  329 Worker Num:  5 \n",
            " Loss:  0.23713895678520203\n",
            "Training:  Iteration No:  329 Worker Num:  6 \n",
            " Loss:  0.18385319411754608\n",
            "Training:  Iteration No:  329 Worker Num:  7 \n",
            " Loss:  0.12102124094963074\n",
            "Test:  Iteration No:  329 \n",
            " Loss:  0.15281062989931884\n",
            "Test accuracy:  95.37\n",
            "Training:  Iteration No:  330 Worker Num:  0 \n",
            " Loss:  0.1713283658027649\n",
            "Training:  Iteration No:  330 Worker Num:  1 \n",
            " Loss:  0.275022029876709\n",
            "Training:  Iteration No:  330 Worker Num:  2 \n",
            " Loss:  0.20803967118263245\n",
            "Training:  Iteration No:  330 Worker Num:  3 \n",
            " Loss:  0.13413502275943756\n",
            "Training:  Iteration No:  330 Worker Num:  4 \n",
            " Loss:  0.23484855890274048\n",
            "Training:  Iteration No:  330 Worker Num:  5 \n",
            " Loss:  0.20412680506706238\n",
            "Training:  Iteration No:  330 Worker Num:  6 \n",
            " Loss:  0.15361452102661133\n",
            "Training:  Iteration No:  330 Worker Num:  7 \n",
            " Loss:  0.1756746768951416\n",
            "Test:  Iteration No:  330 \n",
            " Loss:  0.1480899077215338\n",
            "Test accuracy:  95.58\n",
            "Training:  Iteration No:  331 Worker Num:  0 \n",
            " Loss:  0.14722435176372528\n",
            "Training:  Iteration No:  331 Worker Num:  1 \n",
            " Loss:  0.1705658733844757\n",
            "Training:  Iteration No:  331 Worker Num:  2 \n",
            " Loss:  0.29957854747772217\n",
            "Training:  Iteration No:  331 Worker Num:  3 \n",
            " Loss:  0.0965694859623909\n",
            "Training:  Iteration No:  331 Worker Num:  4 \n",
            " Loss:  0.15254080295562744\n",
            "Training:  Iteration No:  331 Worker Num:  5 \n",
            " Loss:  0.29203903675079346\n",
            "Training:  Iteration No:  331 Worker Num:  6 \n",
            " Loss:  0.22054028511047363\n",
            "Training:  Iteration No:  331 Worker Num:  7 \n",
            " Loss:  0.1580171287059784\n",
            "Test:  Iteration No:  331 \n",
            " Loss:  0.1467172227847048\n",
            "Test accuracy:  95.57\n",
            "Training:  Iteration No:  332 Worker Num:  0 \n",
            " Loss:  0.17466209828853607\n",
            "Training:  Iteration No:  332 Worker Num:  1 \n",
            " Loss:  0.22682301700115204\n",
            "Training:  Iteration No:  332 Worker Num:  2 \n",
            " Loss:  0.15713870525360107\n",
            "Training:  Iteration No:  332 Worker Num:  3 \n",
            " Loss:  0.12415377050638199\n",
            "Training:  Iteration No:  332 Worker Num:  4 \n",
            " Loss:  0.1866758018732071\n",
            "Training:  Iteration No:  332 Worker Num:  5 \n",
            " Loss:  0.21083062887191772\n",
            "Training:  Iteration No:  332 Worker Num:  6 \n",
            " Loss:  0.1402454823255539\n",
            "Training:  Iteration No:  332 Worker Num:  7 \n",
            " Loss:  0.11285702884197235\n",
            "Test:  Iteration No:  332 \n",
            " Loss:  0.14461765327502654\n",
            "Test accuracy:  95.71\n",
            "Training:  Iteration No:  333 Worker Num:  0 \n",
            " Loss:  0.17263799905776978\n",
            "Training:  Iteration No:  333 Worker Num:  1 \n",
            " Loss:  0.16940784454345703\n",
            "Training:  Iteration No:  333 Worker Num:  2 \n",
            " Loss:  0.14291420578956604\n",
            "Training:  Iteration No:  333 Worker Num:  3 \n",
            " Loss:  0.1562396138906479\n",
            "Training:  Iteration No:  333 Worker Num:  4 \n",
            " Loss:  0.2284785807132721\n",
            "Training:  Iteration No:  333 Worker Num:  5 \n",
            " Loss:  0.21713052690029144\n",
            "Training:  Iteration No:  333 Worker Num:  6 \n",
            " Loss:  0.1845732033252716\n",
            "Training:  Iteration No:  333 Worker Num:  7 \n",
            " Loss:  0.21805614233016968\n",
            "Test:  Iteration No:  333 \n",
            " Loss:  0.14752606276564206\n",
            "Test accuracy:  95.69\n",
            "Training:  Iteration No:  334 Worker Num:  0 \n",
            " Loss:  0.1793782114982605\n",
            "Training:  Iteration No:  334 Worker Num:  1 \n",
            " Loss:  0.2034018486738205\n",
            "Training:  Iteration No:  334 Worker Num:  2 \n",
            " Loss:  0.21620313823223114\n",
            "Training:  Iteration No:  334 Worker Num:  3 \n",
            " Loss:  0.1108010783791542\n",
            "Training:  Iteration No:  334 Worker Num:  4 \n",
            " Loss:  0.11437291651964188\n",
            "Training:  Iteration No:  334 Worker Num:  5 \n",
            " Loss:  0.18550898134708405\n",
            "Training:  Iteration No:  334 Worker Num:  6 \n",
            " Loss:  0.20027796924114227\n",
            "Training:  Iteration No:  334 Worker Num:  7 \n",
            " Loss:  0.244858056306839\n",
            "Test:  Iteration No:  334 \n",
            " Loss:  0.14720503503716068\n",
            "Test accuracy:  95.66\n",
            "Training:  Iteration No:  335 Worker Num:  0 \n",
            " Loss:  0.15136539936065674\n",
            "Training:  Iteration No:  335 Worker Num:  1 \n",
            " Loss:  0.21765658259391785\n",
            "Training:  Iteration No:  335 Worker Num:  2 \n",
            " Loss:  0.20862995088100433\n",
            "Training:  Iteration No:  335 Worker Num:  3 \n",
            " Loss:  0.13102030754089355\n",
            "Training:  Iteration No:  335 Worker Num:  4 \n",
            " Loss:  0.21830612421035767\n",
            "Training:  Iteration No:  335 Worker Num:  5 \n",
            " Loss:  0.21266528964042664\n",
            "Training:  Iteration No:  335 Worker Num:  6 \n",
            " Loss:  0.12558147311210632\n",
            "Training:  Iteration No:  335 Worker Num:  7 \n",
            " Loss:  0.06388451159000397\n",
            "Test:  Iteration No:  335 \n",
            " Loss:  0.14436754141050048\n",
            "Test accuracy:  95.69\n",
            "Training:  Iteration No:  336 Worker Num:  0 \n",
            " Loss:  0.13909044861793518\n",
            "Training:  Iteration No:  336 Worker Num:  1 \n",
            " Loss:  0.17969651520252228\n",
            "Training:  Iteration No:  336 Worker Num:  2 \n",
            " Loss:  0.11839562654495239\n",
            "Training:  Iteration No:  336 Worker Num:  3 \n",
            " Loss:  0.2704550623893738\n",
            "Training:  Iteration No:  336 Worker Num:  4 \n",
            " Loss:  0.13807564973831177\n",
            "Training:  Iteration No:  336 Worker Num:  5 \n",
            " Loss:  0.12314821034669876\n",
            "Training:  Iteration No:  336 Worker Num:  6 \n",
            " Loss:  0.12043263763189316\n",
            "Training:  Iteration No:  336 Worker Num:  7 \n",
            " Loss:  0.20824997127056122\n",
            "Test:  Iteration No:  336 \n",
            " Loss:  0.14977447082040996\n",
            "Test accuracy:  95.58\n",
            "Training:  Iteration No:  337 Worker Num:  0 \n",
            " Loss:  0.20085062086582184\n",
            "Training:  Iteration No:  337 Worker Num:  1 \n",
            " Loss:  0.24089361727237701\n",
            "Training:  Iteration No:  337 Worker Num:  2 \n",
            " Loss:  0.200034961104393\n",
            "Training:  Iteration No:  337 Worker Num:  3 \n",
            " Loss:  0.15697278082370758\n",
            "Training:  Iteration No:  337 Worker Num:  4 \n",
            " Loss:  0.16691473126411438\n",
            "Training:  Iteration No:  337 Worker Num:  5 \n",
            " Loss:  0.23938435316085815\n",
            "Training:  Iteration No:  337 Worker Num:  6 \n",
            " Loss:  0.14168734848499298\n",
            "Training:  Iteration No:  337 Worker Num:  7 \n",
            " Loss:  0.2940449118614197\n",
            "Test:  Iteration No:  337 \n",
            " Loss:  0.1476325236579191\n",
            "Test accuracy:  95.69\n",
            "Training:  Iteration No:  338 Worker Num:  0 \n",
            " Loss:  0.21456967294216156\n",
            "Training:  Iteration No:  338 Worker Num:  1 \n",
            " Loss:  0.1437952220439911\n",
            "Training:  Iteration No:  338 Worker Num:  2 \n",
            " Loss:  0.1839343011379242\n",
            "Training:  Iteration No:  338 Worker Num:  3 \n",
            " Loss:  0.2507108449935913\n",
            "Training:  Iteration No:  338 Worker Num:  4 \n",
            " Loss:  0.2656608819961548\n",
            "Training:  Iteration No:  338 Worker Num:  5 \n",
            " Loss:  0.4041261672973633\n",
            "Training:  Iteration No:  338 Worker Num:  6 \n",
            " Loss:  0.372652530670166\n",
            "Training:  Iteration No:  338 Worker Num:  7 \n",
            " Loss:  0.17409369349479675\n",
            "Test:  Iteration No:  338 \n",
            " Loss:  0.1426451037685045\n",
            "Test accuracy:  95.74\n",
            "Training:  Iteration No:  339 Worker Num:  0 \n",
            " Loss:  0.18622653186321259\n",
            "Training:  Iteration No:  339 Worker Num:  1 \n",
            " Loss:  0.1448599100112915\n",
            "Training:  Iteration No:  339 Worker Num:  2 \n",
            " Loss:  0.17018945515155792\n",
            "Training:  Iteration No:  339 Worker Num:  3 \n",
            " Loss:  0.10978439450263977\n",
            "Training:  Iteration No:  339 Worker Num:  4 \n",
            " Loss:  0.1751193106174469\n",
            "Training:  Iteration No:  339 Worker Num:  5 \n",
            " Loss:  0.2344951182603836\n",
            "Training:  Iteration No:  339 Worker Num:  6 \n",
            " Loss:  0.16324858367443085\n",
            "Training:  Iteration No:  339 Worker Num:  7 \n",
            " Loss:  0.14691610634326935\n",
            "Test:  Iteration No:  339 \n",
            " Loss:  0.14467716657945626\n",
            "Test accuracy:  95.7\n",
            "Training:  Iteration No:  340 Worker Num:  0 \n",
            " Loss:  0.12912225723266602\n",
            "Training:  Iteration No:  340 Worker Num:  1 \n",
            " Loss:  0.24837808310985565\n",
            "Training:  Iteration No:  340 Worker Num:  2 \n",
            " Loss:  0.14126521348953247\n",
            "Training:  Iteration No:  340 Worker Num:  3 \n",
            " Loss:  0.2044827938079834\n",
            "Training:  Iteration No:  340 Worker Num:  4 \n",
            " Loss:  0.13732841610908508\n",
            "Training:  Iteration No:  340 Worker Num:  5 \n",
            " Loss:  0.1532822549343109\n",
            "Training:  Iteration No:  340 Worker Num:  6 \n",
            " Loss:  0.16247014701366425\n",
            "Training:  Iteration No:  340 Worker Num:  7 \n",
            " Loss:  0.15846435725688934\n",
            "Test:  Iteration No:  340 \n",
            " Loss:  0.1459636106450535\n",
            "Test accuracy:  95.77\n",
            "Training:  Iteration No:  341 Worker Num:  0 \n",
            " Loss:  0.2898949682712555\n",
            "Training:  Iteration No:  341 Worker Num:  1 \n",
            " Loss:  0.20231600105762482\n",
            "Training:  Iteration No:  341 Worker Num:  2 \n",
            " Loss:  0.12473033368587494\n",
            "Training:  Iteration No:  341 Worker Num:  3 \n",
            " Loss:  0.47720304131507874\n",
            "Training:  Iteration No:  341 Worker Num:  4 \n",
            " Loss:  0.13528963923454285\n",
            "Training:  Iteration No:  341 Worker Num:  5 \n",
            " Loss:  0.1349715292453766\n",
            "Training:  Iteration No:  341 Worker Num:  6 \n",
            " Loss:  0.19266065955162048\n",
            "Training:  Iteration No:  341 Worker Num:  7 \n",
            " Loss:  0.13657599687576294\n",
            "Test:  Iteration No:  341 \n",
            " Loss:  0.14382775820190488\n",
            "Test accuracy:  95.73\n",
            "Training:  Iteration No:  342 Worker Num:  0 \n",
            " Loss:  0.1908382773399353\n",
            "Training:  Iteration No:  342 Worker Num:  1 \n",
            " Loss:  0.243671253323555\n",
            "Training:  Iteration No:  342 Worker Num:  2 \n",
            " Loss:  0.1327456533908844\n",
            "Training:  Iteration No:  342 Worker Num:  3 \n",
            " Loss:  0.15409012138843536\n",
            "Training:  Iteration No:  342 Worker Num:  4 \n",
            " Loss:  0.29527685046195984\n",
            "Training:  Iteration No:  342 Worker Num:  5 \n",
            " Loss:  0.20457445085048676\n",
            "Training:  Iteration No:  342 Worker Num:  6 \n",
            " Loss:  0.14766371250152588\n",
            "Training:  Iteration No:  342 Worker Num:  7 \n",
            " Loss:  0.0812687873840332\n",
            "Test:  Iteration No:  342 \n",
            " Loss:  0.143222662773502\n",
            "Test accuracy:  95.92\n",
            "Training:  Iteration No:  343 Worker Num:  0 \n",
            " Loss:  0.14543484151363373\n",
            "Training:  Iteration No:  343 Worker Num:  1 \n",
            " Loss:  0.14828801155090332\n",
            "Training:  Iteration No:  343 Worker Num:  2 \n",
            " Loss:  0.20741021633148193\n",
            "Training:  Iteration No:  343 Worker Num:  3 \n",
            " Loss:  0.1700524091720581\n",
            "Training:  Iteration No:  343 Worker Num:  4 \n",
            " Loss:  0.11087818443775177\n",
            "Training:  Iteration No:  343 Worker Num:  5 \n",
            " Loss:  0.19005024433135986\n",
            "Training:  Iteration No:  343 Worker Num:  6 \n",
            " Loss:  0.1926216036081314\n",
            "Training:  Iteration No:  343 Worker Num:  7 \n",
            " Loss:  0.18903835117816925\n",
            "Test:  Iteration No:  343 \n",
            " Loss:  0.14112047058282584\n",
            "Test accuracy:  95.81\n",
            "Training:  Iteration No:  344 Worker Num:  0 \n",
            " Loss:  0.2922816574573517\n",
            "Training:  Iteration No:  344 Worker Num:  1 \n",
            " Loss:  0.12494458258152008\n",
            "Training:  Iteration No:  344 Worker Num:  2 \n",
            " Loss:  0.08088330179452896\n",
            "Training:  Iteration No:  344 Worker Num:  3 \n",
            " Loss:  0.1914205253124237\n",
            "Training:  Iteration No:  344 Worker Num:  4 \n",
            " Loss:  0.30099767446517944\n",
            "Training:  Iteration No:  344 Worker Num:  5 \n",
            " Loss:  0.19576822221279144\n",
            "Training:  Iteration No:  344 Worker Num:  6 \n",
            " Loss:  0.16066372394561768\n",
            "Training:  Iteration No:  344 Worker Num:  7 \n",
            " Loss:  0.1507687270641327\n",
            "Test:  Iteration No:  344 \n",
            " Loss:  0.14555805439388828\n",
            "Test accuracy:  95.59\n",
            "Training:  Iteration No:  345 Worker Num:  0 \n",
            " Loss:  0.15985268354415894\n",
            "Training:  Iteration No:  345 Worker Num:  1 \n",
            " Loss:  0.206219881772995\n",
            "Training:  Iteration No:  345 Worker Num:  2 \n",
            " Loss:  0.35066601634025574\n",
            "Training:  Iteration No:  345 Worker Num:  3 \n",
            " Loss:  0.14885017275810242\n",
            "Training:  Iteration No:  345 Worker Num:  4 \n",
            " Loss:  0.20799367129802704\n",
            "Training:  Iteration No:  345 Worker Num:  5 \n",
            " Loss:  0.12824973464012146\n",
            "Training:  Iteration No:  345 Worker Num:  6 \n",
            " Loss:  0.21007485687732697\n",
            "Training:  Iteration No:  345 Worker Num:  7 \n",
            " Loss:  0.3677191734313965\n",
            "Test:  Iteration No:  345 \n",
            " Loss:  0.1399116783662216\n",
            "Test accuracy:  95.84\n",
            "Training:  Iteration No:  346 Worker Num:  0 \n",
            " Loss:  0.12836624681949615\n",
            "Training:  Iteration No:  346 Worker Num:  1 \n",
            " Loss:  0.14971058070659637\n",
            "Training:  Iteration No:  346 Worker Num:  2 \n",
            " Loss:  0.12577924132347107\n",
            "Training:  Iteration No:  346 Worker Num:  3 \n",
            " Loss:  0.08036888390779495\n",
            "Training:  Iteration No:  346 Worker Num:  4 \n",
            " Loss:  0.16789625585079193\n",
            "Training:  Iteration No:  346 Worker Num:  5 \n",
            " Loss:  0.23000359535217285\n",
            "Training:  Iteration No:  346 Worker Num:  6 \n",
            " Loss:  0.23023322224617004\n",
            "Training:  Iteration No:  346 Worker Num:  7 \n",
            " Loss:  0.21217317879199982\n",
            "Test:  Iteration No:  346 \n",
            " Loss:  0.14044074069331342\n",
            "Test accuracy:  95.82\n",
            "Training:  Iteration No:  347 Worker Num:  0 \n",
            " Loss:  0.1827758550643921\n",
            "Training:  Iteration No:  347 Worker Num:  1 \n",
            " Loss:  0.1666809618473053\n",
            "Training:  Iteration No:  347 Worker Num:  2 \n",
            " Loss:  0.2886563837528229\n",
            "Training:  Iteration No:  347 Worker Num:  3 \n",
            " Loss:  0.20392820239067078\n",
            "Training:  Iteration No:  347 Worker Num:  4 \n",
            " Loss:  0.16048170626163483\n",
            "Training:  Iteration No:  347 Worker Num:  5 \n",
            " Loss:  0.11386311799287796\n",
            "Training:  Iteration No:  347 Worker Num:  6 \n",
            " Loss:  0.14455744624137878\n",
            "Training:  Iteration No:  347 Worker Num:  7 \n",
            " Loss:  0.13024617731571198\n",
            "Test:  Iteration No:  347 \n",
            " Loss:  0.1464262821452244\n",
            "Test accuracy:  95.52\n",
            "Training:  Iteration No:  348 Worker Num:  0 \n",
            " Loss:  0.16883032023906708\n",
            "Training:  Iteration No:  348 Worker Num:  1 \n",
            " Loss:  0.2080080658197403\n",
            "Training:  Iteration No:  348 Worker Num:  2 \n",
            " Loss:  0.12661752104759216\n",
            "Training:  Iteration No:  348 Worker Num:  3 \n",
            " Loss:  0.15260431170463562\n",
            "Training:  Iteration No:  348 Worker Num:  4 \n",
            " Loss:  0.23120595514774323\n",
            "Training:  Iteration No:  348 Worker Num:  5 \n",
            " Loss:  0.13212700188159943\n",
            "Training:  Iteration No:  348 Worker Num:  6 \n",
            " Loss:  0.18769434094429016\n",
            "Training:  Iteration No:  348 Worker Num:  7 \n",
            " Loss:  0.22939370572566986\n",
            "Test:  Iteration No:  348 \n",
            " Loss:  0.13829960911593672\n",
            "Test accuracy:  95.81\n",
            "Training:  Iteration No:  349 Worker Num:  0 \n",
            " Loss:  0.2255311757326126\n",
            "Training:  Iteration No:  349 Worker Num:  1 \n",
            " Loss:  0.08644402772188187\n",
            "Training:  Iteration No:  349 Worker Num:  2 \n",
            " Loss:  0.13821639120578766\n",
            "Training:  Iteration No:  349 Worker Num:  3 \n",
            " Loss:  0.13144207000732422\n",
            "Training:  Iteration No:  349 Worker Num:  4 \n",
            " Loss:  0.2267289161682129\n",
            "Training:  Iteration No:  349 Worker Num:  5 \n",
            " Loss:  0.15721586346626282\n",
            "Training:  Iteration No:  349 Worker Num:  6 \n",
            " Loss:  0.19533970952033997\n",
            "Training:  Iteration No:  349 Worker Num:  7 \n",
            " Loss:  0.19680015742778778\n",
            "Test:  Iteration No:  349 \n",
            " Loss:  0.13916000467430376\n",
            "Test accuracy:  95.91\n",
            "Training:  Iteration No:  350 Worker Num:  0 \n",
            " Loss:  0.17778322100639343\n",
            "Training:  Iteration No:  350 Worker Num:  1 \n",
            " Loss:  0.12528173625469208\n",
            "Training:  Iteration No:  350 Worker Num:  2 \n",
            " Loss:  0.192047119140625\n",
            "Training:  Iteration No:  350 Worker Num:  3 \n",
            " Loss:  0.16538311541080475\n",
            "Training:  Iteration No:  350 Worker Num:  4 \n",
            " Loss:  0.1070733442902565\n",
            "Training:  Iteration No:  350 Worker Num:  5 \n",
            " Loss:  0.18088233470916748\n",
            "Training:  Iteration No:  350 Worker Num:  6 \n",
            " Loss:  0.31218045949935913\n",
            "Training:  Iteration No:  350 Worker Num:  7 \n",
            " Loss:  0.1259799301624298\n",
            "Test:  Iteration No:  350 \n",
            " Loss:  0.14300227441082272\n",
            "Test accuracy:  95.75\n",
            "Training:  Iteration No:  351 Worker Num:  0 \n",
            " Loss:  0.1256350874900818\n",
            "Training:  Iteration No:  351 Worker Num:  1 \n",
            " Loss:  0.11354642361402512\n",
            "Training:  Iteration No:  351 Worker Num:  2 \n",
            " Loss:  0.2506885528564453\n",
            "Training:  Iteration No:  351 Worker Num:  3 \n",
            " Loss:  0.19616162776947021\n",
            "Training:  Iteration No:  351 Worker Num:  4 \n",
            " Loss:  0.2048603892326355\n",
            "Training:  Iteration No:  351 Worker Num:  5 \n",
            " Loss:  0.08681874722242355\n",
            "Training:  Iteration No:  351 Worker Num:  6 \n",
            " Loss:  0.28891533613204956\n",
            "Training:  Iteration No:  351 Worker Num:  7 \n",
            " Loss:  0.2687506675720215\n",
            "Test:  Iteration No:  351 \n",
            " Loss:  0.13773364958624484\n",
            "Test accuracy:  95.95\n",
            "Training:  Iteration No:  352 Worker Num:  0 \n",
            " Loss:  0.14524056017398834\n",
            "Training:  Iteration No:  352 Worker Num:  1 \n",
            " Loss:  0.07331264764070511\n",
            "Training:  Iteration No:  352 Worker Num:  2 \n",
            " Loss:  0.1939474493265152\n",
            "Training:  Iteration No:  352 Worker Num:  3 \n",
            " Loss:  0.10178389400243759\n",
            "Training:  Iteration No:  352 Worker Num:  4 \n",
            " Loss:  0.20324569940567017\n",
            "Training:  Iteration No:  352 Worker Num:  5 \n",
            " Loss:  0.1331237107515335\n",
            "Training:  Iteration No:  352 Worker Num:  6 \n",
            " Loss:  0.11827520281076431\n",
            "Training:  Iteration No:  352 Worker Num:  7 \n",
            " Loss:  0.12539896368980408\n",
            "Test:  Iteration No:  352 \n",
            " Loss:  0.1377437138411252\n",
            "Test accuracy:  96.07\n",
            "96 % accuracy reached!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max(test_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkfAkEllzg-F",
        "outputId": "dc49ae9b-02d4-4e65-845b-bbb2e90d381e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "96.07"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(test_accuracy)\n",
        "plt.xlabel('Iteration Number')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Test accuracy');"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "tu9r1u5WzhDF",
        "outputId": "43dcb33e-48b6-47ed-8570-55857c2eb0c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZwdVZn/8c/T+55OL+l09gQCCWEJMcMqigYRcQkyiMKMZhyUcUYdl/GnjDOvGfU3juiM4vrTwcExKsOqCIoiGIjsgRASCCSQhexLd3rf+y7P74+qvrnpdIdOd27f7r7f9+vVr1t16t6qpyudeu45p+occ3dEREQAstIdgIiIjB1KCiIikqCkICIiCUoKIiKSoKQgIiIJSgoiIpKgpCAiIglKCjLumFl70k/czLqS1v9iGPtbbWYfSUWsIuNNTroDEDle7l7St2xmO4CPuPsf0xdRaplZjrtH0x2HZAbVFGTCMLMsM7vBzLaZWYOZ3WlmFeG2AjP7RVjebGbPmlmNmX0VuAj4fljT+P4g+77LzA6YWYuZPWpmi5K2FZrZN81sZ7j9cTMrDLe90cyeDI+528z+Kiw/onZiZn9lZo8nrbuZfdzMtgBbwrLvhPtoNbPnzOyipPdnm9kXw9+9Ldw+08x+YGbf7Pe73Gdmnxn5GZeJSElBJpJPAlcAbwamAU3AD8JtK4BJwEygEvgY0OXu/wQ8BnzC3Uvc/ROD7Pv3wHxgCrAOuDVp238CbwAuACqAzwNxM5sdfu57QDWwGFh/HL/PFcC5wGnh+rPhPiqA/wXuMrOCcNtngWuAy4Ey4K+BTmAlcI2ZZQGYWRVwSfh5kaOo+Ugmko8RXNz3AJjZl4BdZvZBIEKQDE529xeA545nx+7+k77lcL9NZjYJaCO4AJ/n7nvDtzwZvu9a4I/ufltY3hD+DNXX3L0xKYZfJG37ppn9M3AqsAH4CPB5d38l3L6h75hm1gIsAx4CPgCsdveDxxGHZBDVFGQimQ3cEzbVNAObgBhQA/wc+ANwu5ntM7NvmFnuUHYaNs3cGDbNtAI7wk1V4U8BsG2Aj84cpHyodveL43NmtilsomomqPlUDeFYK4G/DJf/kuBciAxISUEmkt3AO9y9POmnwN33unvE3b/s7qcRNPO8C/hQ+LnXGyr4WmA5QbPLJGBOWG7AIaAbOGmQeAYqB+gAipLWpw7wnkRcYf/B54GrgcnuXg60hDG83rF+ASw3s7OAhcCvB3mfiJKCTCg/Ar4atuVjZtVmtjxcfouZnWFm2UArQXNSPPzcQWDeMfZbCvQQNP0UAf/et8Hd48BPgG+Z2bSwVnG+meUT9DtcYmZXm1mOmVWa2eLwo+uBK82syMxOBq57nd+tFIgC9UCOmf0LQd9Bn/8G/q+ZzbfAmWZWGca4h6A/4ufAL92963WOJRlMSUEmku8A9wEPmlkb8DRBRy0E38TvJkgIm4A/cbgZ5TvAVWbWZGbfHWC/PwN2AnuBl8P9Jvsc8CLBhbcR+DqQ5e67CDp+/yEsXw+cFX7mJqCXICGt5MiO64H8AXgAeDWMpZsjm5e+BdwJPBj+jrcAhUnbVwJnoKYjeR2mSXZEJj4zexNBM9Js1396OQbVFEQmuLBD/VPAfyshyOtJWVIws5+YWZ2ZbUwqqzCzh8xsS/g6OSw3M/uumW01sxfMbEmq4hLJJGa2EGgGaoFvpzkcGQdSWVP4KXBZv7IbgFXuPh9YFa4DvIPgwaD5wPXAD1MYl0jGcPdN7l7s7he4e2u645GxL2VJwd0fJehcS7acoMOL8PWKpPKfeeBpoNzMalMVm4iIDGy0n2iucff94fIBgoeKAKZz5J0Ue8Ky/RxDVVWVz5kz50THKCIyoT333HOH3L16oG1pG+bC3d3MjrvTy8yuJ2hiYtasWaxdu/aExyYiMpGZ2c7Bto323UcH+5qFwte6sHwvwWP6fWaEZUdx95vdfam7L62uHjDRiYjIMI12UriPYLRKwtd7k8o/FN6FdB7QktTMJCIioyRlzUdmdhtwMVBlZnuAfwVuBO40s+sInsq8Onz77wie/NxKMNzvh1MVl4iIDC5lScHdrxlk07IB3uvAx1MVi4iIDI2eaBYRkQQlBRERSVBSEBGRBCUFEZExIhqL094THbC8qzfGwdZufv70Tp7b2X+wiBNHczSLiAxTXWs3L+9v5dy5leRkGwa8tK+VOVXFtHRG+O2L+4hEnbcumMLOxg6e2tZALO7kZBu1kwrpjcbZWtdOdpZxqL2HnQ2d7G/pCt7f0Elnb4yLT63mNxv20RMN5oTqe/3Xd5/Ghy+ce8J/JyUFEclIsbizs6GDaeWFFORmA7CnqZPfv3iA9p4oj22pp7o0n4+9+SQ2H2hjdmURdz67m5auCK8ebCcSi9PaHaE7EqckP4dY3Im50xuNH3Wsm/74KgCl+TkU5mUTjTuNHb1kGcyuLKY3GmdKWT6zK4u4/Iyp/OLpXdSU5TOlLJ9b1+ziwpMrmV1ZjDusuGA2qzbVcemigWZwHblxPcnO0qVLXcNciEx8B1q6aeuOcFJ1CWbgDq81dJCfk8Wvn9/LwdYemjp7WVhbRiQWp6Urwkv7Wlm/u5mKojyyDEoKcmjqjFCcl83k4jx2N3ZyqL2XGZMLmTG5kI6eGC/ubUkcc3p5IS1dkSOac8qLcplaVsDsyiImF+WRl5PFhSdXsfqVOsyMLIP5U0o52NrNtPJCKorzeOVAG/m5Wbzx5CoWTC0jLydota9v6yEny5hcnHfU79vc2UtBbjZZZqzd0ci58yrJzrKj3jdcZvacuy8dcJuSgoikwvrdzWyvb+fKJTPYXt9O7aRCCvOy6YnG2NXQSW15Idvq2nluZxPNnb1UluRz5oxJNLT38uDLB5hUmMuepi621LWzta49sd+qkjxys7PY39KdKCsvyqUgJ5sDrd1kGRTl5VBVksdbFkyhvTtK3KGlK8Lkoly6o3GaO3spK8zl3LkV/PK5PWRnGTnZWbz5lGredWYt5YV5FOVn8/yuZr56/8u8b+lMssy44uxpFOWN/wYWJQURGZZY3MnOMnqiMdwhGneaO3tZs72RQ+09PLy5jqbOXqaXB9NB72/p5swZk4g7/Pr5vUTjzlkzy9mwu5lZFUWYgQE7GjqPedyivGw6e2NUleSxaNokLppfRWlBDnubu9nX3EV3JMaZMyZR39bDB8+bw6zKIgBeOdBGdWk+FQN8+5bDjpUUxn/KExH2NXeRk21MKS04otzdMTu62WF3Yyc90Ti52cZ3Vm0hy4zndzUxqTCXuVUl7G3upK4t6PicP6WEzQfaKMrLpjcaJxo//EVywdRSZlcWs6+5C3eYUlbAfRv2kZudxZ8vmcHupk42H2jjoxfN5Vfr9jKjoohdDR380+ULibkzvbyQP5tTQU1ZPnuaunhpXys1ZfnMrymlsydKWWFuor1/KE6dWjr8kyiAagoiY1ZHT5SOnij3rt/H5WfWsruxky117eTnZDGjvJBXD7bRFYnT0RPlB6u3Mrkoj/eePZ2uSIyntzVQVpjLxr0tzK8ppTQ/h5f3t3LevEoOtnbz0r4W4g45WUZudhaxuHPuvApauyLsbe5mTmURVSX5lBTkcP8L+3nXmbXkZGcxqTCXkvxsLl00lUmFudSUFRwVd3IiisUddycnOytRPliiktGj5iORUdYbjbOrsZOTp5QctS0Wdzp6ozS297KrsZMFtaXc+/w+ttS18ev1+4jHnZqyAurauonEgv+fWQbxY/xXvWRhDa8ebONAazfxuHPmjEk0d0a4+NQprN/dRCzuzKkq5uFNdSycVsa5cytwh+5IjOsumsvUsoJBL9TxuJN1Ajs5Jf3UfCSSApFYnJf3tTK7sojyojxiceexLfXE3fnh6m08u6OJj140l7q2HqaWFZCdZWyta2fj3hb2t3aTZZZos4+FV/wrl0xnalkBW+vaeUtZNQU5wZ0yv3txPx+9aB6zK4voicaJxpxTakpY/Uo9927Yy7fefxYleTk4QQI5kd/ElRAyi2oKIv24O6tfqaemrIDJxbn8v0e2UZQXtGs/8kod82tKeXFPC+09URo7einIzeKzbzuF376wnxf2BLc0FuVls2TWZB7feoiC3Cy6I3Fysoy5VcXMrizmpCnFAJwzp4If/Wkb71s6k8tOn0pZQW7afm/JHKopSMZp74myYXczF5xUSU80zobdzZw6tZSyglxWPrWDx7ccYsnsybR1Rzmpupj7NuzjpOoSyotyuef5vexs6CQ/J4vSghxau6O4O5GYc968Ch7ZXMfcqmLOmlnORfOruHf9Xv79d5spzsvmW1efRU1ZAadPn8SkwlwOtfdQmJvNhj3NzCgvStwlk2zZwpqjfwGRNFFNQcat5A7LJ7Ye4lB7Dz9cvY25VcW8erCNbfUdLJ5Zzvb6dlq7o5xSE7Tvv3qwnbycLHrDu28iMaeyOI+2nii90ThvPLmKy06fyjOvNdIVifGpZfMpLchhe30Hb1kw5ag2dnfnlYNtVBTlMWWAjleRsWbMdTSb2aeAjxLcsvxjd/+2mVUAdwBzgB3A1e7edKz9KClMLPuau7jn+b3MmFzI+fMqKcjLPqI55fldTfzHH16hKC+H6tJ8fvncHubXlDC9vJAHXz54xL7mVhXz1gVTeOa1xsS3+q/9bhNzqor5+2XzufjUalo6I0wpy2fdzmZOqy0jKwvq2no4qfrozmGRiWRMJQUzOx24HTgH6AUeAD4GXA80uvuNZnYDMNndv3CsfSkpjB993+o7e6P8cVMdxXnZLFtYQzQW5x/u2kAs7jwdPhAFQWdpVUk+C2rL2NPYSV5OFpsPtFFVkkdHT4xY3HnHGVPZ09TFrsZO3nv2dApzs1lYW8alp9UM2Dna0N5DeVHeCR0uQGQ8Gmt9CguBNe7eCWBmfwKuBJYTzOkMsBJYDRwzKcjYEI3FycnOYmtdMEjYgy8d5M2nVtPeHeWBl/bT1h3l/hf2M7eqmKbOXg619wLwl+fN4s5n99AbCwYQO29eBb/4yDk8vLmOveGDTA3tPZxSU0pHb5T3LJ7Gh86fgwFZZhTmDf2hJoDKkvwT/auLTDjpqCksBO4Fzge6gFXAWuCD7l4evseApr71fp+/nqBWwaxZs96wc+fO0QpdCL7xQzCYV28szq1rdnHbM7t4w6zJrNpcN+jnrlg8jfr2HsoL81hYW8p/PvhqYtuMyYU89vm36IEmkVEypmoK7r7JzL4OPAh0AOuBWL/3uJkNmK3c/WbgZgiaj1IcbkZKfvL050/v5OdP7eTqpTPZfqiD/S1dxB3WbG9IjOueZbBqcx1/d/FJ1E4q4LRpZXz2zg3sbOjkzadUM7kol29dvTjRpNMdifHdVVvpjcW55pyZfOzNJykhiIwRabkl1d1vAW4BMLN/B/YAB82s1t33m1ktMPjXTjlh3J0HNh5g8axyaicV8v2Ht/D7jQe46f2L+dW6vfzoT9sA+OrvNh3xuYLcLL763tMpzM1m/pRStta38d6zZyS2/+pvL+CJbQ28+8zaoy74BbnZnD69jHW7mvk/b1+gwctExpC0JAUzm+LudWY2i6A/4TxgLrACuDF8vTcdsWWKXQ2d/PvvNtHQ0cOzO5qYNqmAipI8Nu5tBeDSmx4F4PIzpnLl2TO48YHNvPOMWroiMbbXt3PR/Gr+4tzZif2dMWPSEfuvLMnnPWdNG/T41547m9OmlSkhiIwx6bol9TGgEogAn3X3VWZWCdwJzAJ2EtySesyJSHX30dC8vK+Vwrxs5lYFT9G290R59/ce51BbcKfP/JoS9jR1MaUsn5ysLCqL89jd1Mm3rl7MomllatoRmWDGVJ8CgLtfNEBZA7AsDeFMSN2RGP92/8s8ua2BXQ2dFOVl8+EL5/K+pTNY+eQOdjR0cNtHz+OsGeXkZgcTjPSJh+PwaMwbkcyjYS4mqBt/v5lfPL2Lc+ZWcOb0Sazb1cx3Vm3h1jW7ONTew58vmcF58yoH/KySgUjmUlKYAA60dCfmp31i6yG217fzs6d28KHzZ/OV5acDwbf/dbua+Pj/rmNKaT6fvmR+eoMWkTFJSWEc2t/Sxf0v7Ofti6ayv6Wbq//rKQBW/vU5rPjJMwCcNbOcz7391MRnsrKMpXMqWPPFS9ISs4iMD0oK40R3JMauxk6mlRfy7u89waH2Hm5ds4vCpKkKV/zkGfJysvjBtUu4aH7VcU1jKCICSgrjxk1/fJX/+tP2xPo/v3Mh/3Z/8OzA5y49hW31HTzzWiNfWb5IQzGLyLApKYxhkVicf7l3I3946SCNHcF4QTVl+fzZnAo+ctE8zpxRzsond3DtubN1v7+InBBKCmPQHc/uYvuhDnC47ZndifJvv38xV5w9PbF+ztwKzplbkY4QRWSCUlIYQ2Jx545nd/PFe15MlL1h9mS+snwRP1y9jUsXqVlIRFJLSSHNtta1c8vjr1FakMNPHn+NaNw5Z24F33zfWTy+9RAXnlTFrMoivn/tknSHKiIZQEkhjdydD//0GXY3dh1R/rlLT2VmRRHXnDMrTZGJSKZSUkiDHYc6+NXze7nwpEp2N3bxxcsXUFqQy5tOqeZgazdLZk1Od4gikqGUFEbR/pYu3vuDJykpyGFrXTvfXbUFgPPnVSVGGZ1eXpjOEEUkw2W9/lvkRHlyawMHWrvZWtfOKTWHJ4dfUFuaxqhERA5TUhhFmw+0Jpa/cdVZzK4s4uxZ5eRm659BRMYGNR+Ngnjc+a9Ht/M/T+zgjOmTuOn9izl5SgkPfuZNGBqRVETGDn1FTYF43Hl480EisWAO4y/95iW+/sBmonFnVkURJ08Jmo7yc7LJy9E/gYiMHemajvMzwEcAB14EPgzUArcTzMj2HPBBd+9NR3wj9fDmOj7ys7UsnT2ZuVXF3PXcHq56wwye2tbAexYPPkWliEi6jXpSMLPpwN8Dp7l7l5ndCXwAuBy4yd1vN7MfAdcBPxzt+E6EJ7c1ALB2ZxOb9rdy6Wk1fO3KM9R3ICJjXrr6FHKAQjOLAEXAfuCtwLXh9pXAlxinSeHp7Q1ccFIlP/7QUorysjXHsYiMG6P+1dXd9wL/CewiSAYtBM1Fze4eDd+2B5g+0OfN7HozW2tma+vr60cj5OPS0hlh04FWzptXSXF+jhKCiIwro54UzGwysByYC0wDioHLhvp5d7/Z3Ze6+9Lq6uoURTl8L+5twR09lSwi41I6GrkvAV5z93p3jwC/Ai4Eys2srzlrBrA3DbGN2It7WwBYNK0szZGIiBy/dCSFXcB5ZlZkQdvKMuBl4BHgqvA9K4B70xDbiG3c18KMyYVM1qQ3IjIOjXpHs7uvMbO7gXVAFHgeuBm4H7jdzP4tLLtltGMbrgc2HuBPr9Zx+vRJ3P/Cfi5bNDXdIYmIDEta7j5y938F/rVf8XbgnDSEM2J/d+tzxB0gmCXtstOVFERkfNIwFyMUizs5WVn0xuLk52Tx2OffwpSygnSHJSIyLEoKI/TqwTZ6Y3GuWDyNS06rUUIQkXFNSWGYWrsjfPaODUwpywfgs287lVmVRWmOSkRkZJQUhunBlw7yx00HgeD205kVmhxHRMY/DcYzTI++evhp6r84d7aeXBaRCUE1hWH4+9ue574N+7hkYQ1nTJ/ElUsGHJFDRGTcUVI4Tt2RGPdt2AfAZ942n0XTJqU5IhGRE0fNR8cQjztfvf9lXjvUkSjbXh8sf++as5UQRGTCUVI4hn0tXfz4sdf4w0sHEmVb6toAOKWmNF1hiYikjJLCMTR3RgBo7Dg8AdyrB9vIyTLmVhWnKywRkZRRUjiGvmTQ0J6cFNqZU1WsuZVFZELSle0YmjqDZNDY0ZMo29XQqVqCiExYSgrH0NTRlxQO1xT2NXcxvVwPqonIxKSkcAyNYZ9CQ5gUWroitPVElRREZMJSUjiG5s4jawr7mrsAmKakICITVDrmaD7VzNYn/bSa2afNrMLMHjKzLeFr2ic57ksGnb0xuiMx9jb1JQWNhCoiE9OoJwV3f8XdF7v7YuANQCdwD3ADsMrd5wOrwvW06utoBvjxo9v5yM/WAjB9smoKIjIxpbv5aBmwzd13AsuBlWH5SuCKtEUVauqIkJ0VDHT3zYdeTZRXFeenKyQRkZRKd1L4AHBbuFzj7vvD5QNAzUAfMLPrzWytma2tr68f6C0nTFNn71G3ny6aVkZWlkZEFZGJKW1JwczygPcAd/Xf5u4O+ECfc/eb3X2puy+trq5OWXyrX6ljf0s3l58+ldzsIAn8zZvm8ZtPvDFlxxQRSbd01hTeAaxz94Ph+kEzqwUIX+vSFhnw9Qde4eQpJfztxSdTE06xuaC2VLUEEZnQ0pkUruFw0xHAfcCKcHkFcO+oRxSKxuJsrWvjkoU1FOZlc8nCoCXr5GoNgiciE1ta5lMws2LgbcDfJBXfCNxpZtcBO4Gr0xEbwM7GTiIxZ/6UEgC+ePlCli2cwhkzNFS2iExsaUkK7t4BVPYrayC4Gynttta1A3BymBTycrK4aH7q+i9ERMaKdN99NCb1JYWTwqQgIpIplBQGsK2+ndpJBZTka7ZSEcksSgoDaOzoZUqpHlATkcyjpDCA9u4oJQWqJYhI5lFSGEB7T1RNRyKSkZQUBtDWHaVYSUFEMpCSwgA6eqOUKimISAZSUujH3dWnICIZS0mhn55onGjc1XwkIhnpdZOCmb3bzDImebT3RAHUfCQiGWkoF/v3A1vM7BtmtiDVAaVbe3eQFNR8JCKZ6HWTgrv/JXA2sA34qZk9FU50MyGHDO2rKRTnKSmISOYZUrOQu7cCdwO3A7XAe4F1ZvbJFMaWFn1JQTUFEclEQ+lTeI+Z3QOsBnKBc9z9HcBZwD+kNrzR19d8VJqfm+ZIRERG31C+Dv85cJO7P5pc6O6d4dwHE0qi+Sg/O82RiIiMvqEkhS8B+/tWzKwQqHH3He6+KlWBpUubmo9EJIMNpU/hLiCetB4Ly4bNzMrN7G4z22xmm8zsfDOrMLOHzGxL+Dp5JMcYro6+pKBbUkUkAw0lKeS4e2/fSricN8Ljfgd4wN0XEPRNbAJuAFa5+3xgVbg+6tq7o2QZFOaq+UhEMs9QkkK9mb2nb8XMlgOHhntAM5sEvAm4BYIk4+7NwHJgZfi2lcAVwz3GSOxv6aa6NB8zS8fhRUTSaihtJB8DbjWz7wMG7AY+NIJjzgXqgf8xs7OA54BPEfRT9PVdHABqBvqwmV0PXA8wa9asEYQxsB0NHcytKj7h+xURGQ+G8vDaNnc/DzgNWOjuF7j71hEcMwdYAvzQ3c8GOujXVOTuDvgg8dzs7kvdfWl1dfUIwhjYa4eUFEQkcw2pN9XM3gksAgr6mlXc/SvDPOYeYI+7rwnX7yZICgfNrNbd95tZLVA3zP0PW0tXhMaOXiUFEclYQ3l47UcE4x99kqD56H3A7OEe0N0PALvN7NSwaBnwMnAfsCIsWwHcO9xjDNeOQx0AzKlUUhCRzDSUmsIF7n6mmb3g7l82s28Cvx/hcT9J0E+RB2wHPkyQoO4MH4jbCVw9wmMctx0NQVJQTUFEMtVQkkJ3+NppZtOABoLxj4bN3dcDSwfYtGwk+x2pAy3Br1pbXpjOMERE0mYoSeE3ZlYO/AewjqAD+McpjSpNGjt7ycvJojhPzyiISGY6ZlIIJ9dZFT5H8Esz+y1Q4O4toxLdKGts76WyOE/PKIhIxjpmR7O7x4EfJK33TNSEANDU2cvkopE+rC0iMn4N5YnmVWb255YBX58bOnqpLFFSEJHMNZSk8DcEA+D1mFmrmbWZWWuK40qLxg7VFEQks71uR7O7T8hpNwfS2NFLRbGSgohkrtdNCmb2poHK+0+6M971RuO0dUeVFEQkow3lltT/k7RcAJxDMIjdW1MSUZo0dwajgyspiEgmG0rz0buT181sJvDtlEWUJg0dSgoiIkPpaO5vD7DwRAeSbk1hUlBHs4hksqH0KXyPw8NYZwGLCZ5snlCauyIAlBflpjkSEZH0GUqfwtqk5Shwm7s/kaJ40qY1TAqTCpUURCRzDSUp3A10u3sMwMyyzazI3TtTG9roalFSEBEZ2hPNQPKwoYXAH1MTTvq0dEXIzjKKNBieiGSwoSSFAndv71sJl4tSF1J6tHZHmFSYq8HwRCSjDSUpdJjZkr4VM3sD0DWSg5rZDjN70czWm9nasKzCzB4ysy3h6+SRHON4tXRF1XQkIhlvKH0KnwbuMrN9BNNxTiWYnnOk3uLuh5LWbyAYpvtGM7shXP/CCTjOkLR0RSgrGNKU1SIiE9ZQHl571swWAH1zKr/i7pEUxLIcuDhcXgmsZhSTQmtXhDLVFEQkw71u85GZfRwodveN7r4RKDGzvxvhcR140MyeM7Prw7Iad98fLh8AakZ4jOPS2hVR85GIZLyh9Cl8NJx5DQB3bwI+OsLjvtHdlwDvAD7ef9A9d3cOPzB3BDO73szWmtna+vr6EYZxWGu3agoiIkNJCtnJE+yYWTYworEg3H1v+FoH3EMwyN5BM6sNj1EL1A3y2Zvdfam7L62urh5JGMn7pEU1BRGRISWFB4A7zGyZmS0DbgN+P9wDmlmxmZX2LQOXAhuB+4AV4dtWAPcO9xjHqysSIxJzJQURyXhDud3mC8D1wMfC9RcI7kAarhrgnrDykQP8r7s/YGbPAnea2XXATuDqERxjyG566FWK84MH1soKlBREJLMN5e6juJmtAU4iuFBXAb8c7gHdfTtw1gDlDcCy4e53uL6zaktiuXZSwWgfXkRkTBk0KZjZKcA14c8h4A4Ad3/L6IQ2+ubXlKQ7BBGRtDpWTWEz8BjwLnffCmBmnxmVqEZJcJNToDgvm+nlhcd4t4jIxHesjuYrgf3AI2b247CTeUINDNQTjSeWT64p1bhHIpLxBk0K7v5rd/8AsAB4hGC4iylm9kMzu3S0Akylzt5YYvmUKWo6EhF53VtS3b3D3f83nKt5BvA8ozj8RCp19EQTy+88szaNkYiIjA3HNUezuzeFD4+N+l1CqdAVCWoKP7h2CRefOiXN0YiIpN9xJYWJpq+moIl1REQCGZ0U+voUlBRERAIZnRT6agrF+ZpHQUQEMjwpqKYgIsUd374AAA4ISURBVHIkJQVUUxAR6ZPhSSFoPipUTUFEBMjwpNDREzYf5SopiIhAhieFzt4o+TlZ5GRn9GkQEUnI6KthR29U/QkiIkkyOil09sR055GISJK0JQUzyzaz583st+H6XDNbY2ZbzewOMxvRPNBD0dkbozhPNQURkT7prCl8CtiUtP514CZ3PxloAq5LdQAdvVEKVFMQEUlIS1IwsxnAO4H/DtcNeCtwd/iWlcAVqY6jNxonPyejW9BERI6Qrivit4HPA32z3FQCze7eN5b1HmD6QB80s+vNbK2Zra2vrx9RENG4k5utiXVERPqMelIws3cBde7+3HA+Hw7dvdTdl1ZXV48olmgsTq5uRxURSUhHL+uFwHvM7HKgACgDvgOUm1lOWFuYAexNdSC9MScnS0lBRKTPqF8R3f0f3X2Gu88BPgA87O5/QTDl51Xh21YA96Y6lmgsTl6Omo9ERPqMpa/JXwA+a2ZbCfoYbkn1ASOxuGoKIiJJ0nqTvruvBlaHy9uBc0bz+JGYk6OOZhGRhIz+mhyJxclTR7OISEJGXxGjcdUURESSZXRSiER1S6qISLKMviJG4koKIiLJMvqKGI3piWYRkWQZmxTcPehT0C2pIiIJGXtFjMQcgDwNiCcikpCxV8RILBiLLydLzUciIn0yNilEw5qC5mcWETksY6+IvWFNIU8dzSIiCRmbFKLxsPlINQURkYSMvSJGokHzkZ5TEBE5LGOviJGwpqDnFEREDsvcpBDrSwoZewpERI6SsVfExN1HuiVVRCQhY5NCoqagh9dERBJG/YpoZgVm9oyZbTCzl8zsy2H5XDNbY2ZbzewOM8tLZRx9TzTnapgLEZGEdFwRe4C3uvtZwGLgMjM7D/g6cJO7nww0AdelMoho3xPN6mgWEUkY9aTggfZwNTf8ceCtwN1h+UrgilTG0auOZhGRo6Tlimhm2Wa2HqgDHgK2Ac3uHg3fsgeYPshnrzeztWa2tr6+ftgx9HU065ZUEZHD0pIU3D3m7ouBGcA5wILj+OzN7r7U3ZdWV1cPOwbdkioicrS0XhHdvRl4BDgfKDeznHDTDGBvKo8diaumICLSXzruPqo2s/JwuRB4G7CJIDlcFb5tBXBvKuOIRFVTEBHpL+f133LC1QIrzSybICnd6e6/NbOXgdvN7N+A54FbUhmEBsQTETnaqCcFd38BOHuA8u0E/QujIqKOZhGRo2Ts1+RER7MeXhMRScjYK+LhmddUUxAR6ZOxSUEPr4mIHC1jr4iHH17L2FMgInKUjL0iRmJxsgyyNXS2iEhC5iaFeFy3o4qI9JOxV8VI1MlTUhAROULGXhWj8bjuPBIR6Sdjk0IkFlcns4hIPxl7VezsjVGUl53uMERExpSMTgqFuUoKIiLJMjYpdPXGKFRNQUTkCJmbFCJqPhIR6S9jk0LQfJSOkcNFRMaujE0KXb1R1RRERPpJx8xrM83sETN72cxeMrNPheUVZvaQmW0JXyenMg51NIuIHC0dNYUo8A/ufhpwHvBxMzsNuAFY5e7zgVXhesp0RdTRLCLS36gnBXff7+7rwuU2gvmZpwPLgZXh21YCV6Qyji49pyAicpS09imY2RyCqTnXADXuvj/cdACoGeQz15vZWjNbW19fP6zj9kbjROOupCAi0k/akoKZlQC/BD7t7q3J29zdAR/oc+5+s7svdfel1dXVwzp2V28MgAL1KYiIHCEtScHMcgkSwq3u/quw+KCZ1Ybba4G6VB2/KxIkhaI83ZIqIpIsHXcfGXALsMndv5W06T5gRbi8Arg3VTF09kYB1HwkItJPOr4qXwh8EHjRzNaHZV8EbgTuNLPrgJ3A1akKoDNsPtLdRyIiRxr1pODujwODTWSwbDRi6Gs+0nMKIiJHysgnmvtqCmo+EhE5UkYmhS41H4mIDCgzk0Kkr6NZdx+JiCTLyKSQ6GhWn4KIyBEyMimo+UhEZGAZmRRmVRRx2aKp6mgWEeknIxvVL100lUsXTU13GCIiY05G1hRERGRgSgoiIpKgpCAiIglKCiIikqCkICIiCUoKIiKSoKQgIiIJSgoiIpJgwXTI45OZ1RNMyDMcVcChExhOqo2neBVraijW1BlP8Z6IWGe7+4CT3I/rpDASZrbW3ZemO46hGk/xKtbUUKypM57iTXWsaj4SEZEEJQUREUnI5KRwc7oDOE7jKV7FmhqKNXXGU7wpjTVj+xRERORomVxTEBGRfpQUREQkISOTgpldZmavmNlWM7sh3fH0Z2Y7zOxFM1tvZmvDsgoze8jMtoSvk9MU20/MrM7MNiaVDRibBb4bnucXzGzJGIn3S2a2Nzy/683s8qRt/xjG+4qZvX2UY51pZo+Y2ctm9pKZfSosH3Pn9xixjrlza2YFZvaMmW0IY/1yWD7XzNaEMd1hZnlheX64vjXcPmcMxPpTM3st6bwuDstP/N+Au2fUD5ANbAPmAXnABuC0dMfVL8YdQFW/sm8AN4TLNwBfT1NsbwKWABtfLzbgcuD3gAHnAWvGSLxfAj43wHtPC/8e8oG54d9J9ijGWgssCZdLgVfDmMbc+T1GrGPu3IbnpyRczgXWhOfrTuADYfmPgL8Nl/8O+FG4/AHgjlE8r4PF+lPgqgHef8L/BjKxpnAOsNXdt7t7L3A7sDzNMQ3FcmBluLwSuCIdQbj7o0Bjv+LBYlsO/MwDTwPlZlY7OpEGBol3MMuB2929x91fA7YS/L2MCnff7+7rwuU2YBMwnTF4fo8R62DSdm7D89MeruaGPw68Fbg7LO9/XvvO993AMjOzNMc6mBP+N5CJSWE6sDtpfQ/H/mNOBwceNLPnzOz6sKzG3feHyweAmvSENqDBYhvL5/oTYXX7J0lNcWMm3rDJ4myCb4pj+vz2ixXG4Lk1s2wzWw/UAQ8R1FSa3T06QDyJWMPtLUBlumJ1977z+tXwvN5kZvn9Yw2N+LxmYlIYD97o7kuAdwAfN7M3JW/0oN44Ju8lHsuxJfkhcBKwGNgPfDO94RzJzEqAXwKfdvfW5G1j7fwOEOuYPLfuHnP3xcAMghrKgjSHNKj+sZrZ6cA/EsT8Z0AF8IVUHT8Tk8JeYGbS+oywbMxw973hax1wD8Ef8cG+amH4Wpe+CI8yWGxj8ly7+8HwP14c+DGHmzHSHq+Z5RJcZG9191+FxWPy/A4U61g+t2F8zcAjwPkETS05A8STiDXcPgloGOVQk2O9LGyuc3fvAf6HFJ7XTEwKzwLzwzsP8gg6ku5Lc0wJZlZsZqV9y8ClwEaCGFeEb1sB3JueCAc0WGz3AR8K75A4D2hJagZJm35tru8lOL8QxPuB8O6TucB84JlRjMuAW4BN7v6tpE1j7vwOFutYPLdmVm1m5eFyIfA2gj6QR4Crwrf1P6995/sq4OGwhpauWDcnfSkwgr6P5PN6Yv8GUt2bPhZ/CHrsXyVoV/yndMfTL7Z5BHdpbABe6ouPoE1zFbAF+CNQkab4biNoFogQtF9eN1hsBHdE/CA8zy8CS8dIvD8P43kh/E9Vm/T+fwrjfQV4xyjH+kaCpqEXgPXhz+Vj8fweI9Yxd26BM4Hnw5g2Av8Sls8jSExbgbuA/LC8IFzfGm6fNwZifTg8rxuBX3D4DqUT/jegYS5ERCQhE5uPRERkEEoKIiKSoKQgIiIJSgoiIpKgpCAiIglKCjLumFl7+DrHzK49wfv+Yr/1J0/Qfn8ajh6aH65XmdmOE7Tvi83stydiXyJKCjKezQGOKykkPcE6mCOSgrtfcJwxHUsM+OsTuL8Twsyy0x2DjB1KCjKe3QhcFI4v/5lwILH/MLNnw4HD/gYS36QfM7P7gJfDsl+HAw6+1DfooJndCBSG+7s1LOurlVi4740WzHXx/qR9rzazu81ss5ndeowRNb8NfKZ/Yur/Td/Mvm9mfxUu7zCzr4UxrTWzJWb2BzPbZmYfS9pNmZndb8FcBT8ys6zw85ea2VNmts7M7grHKurb79fNbB3wvpH8I8jE8nrfmkTGshsIxu5/F0B4cW9x9z8Lm2meMLMHw/cuAU73YNhmgL9298ZwKIFnzeyX7n6DmX3Cg8HI+ruSYJC3s4Cq8DOPhtvOBhYB+4AngAuBxwfYx66w/IPAb47j99zl7ovN7CaCcfUvJHjqdiPBPAAQjIVzGrATeAC40sxWA/8MXOLuHWb2BeCzwFfCzzR4MPCiSIKSgkwklwJnmlnfeDaTCMbY6QWeSUoIAH9vZu8Nl2eG7zvWoGdvBG5z9xjBAHV/IhixsjXc9x4AC4Y8nsPASQHgawRj7Nx/HL9X39hcLxIMb9AGtJlZT984OWEM28MYbgvj7SZIFE+ElZc84Kmk/d5xHDFIhlBSkInEgE+6+x+OKDS7GOjot34JcL67d4bfqAtGcNyepOUYx/h/5e5bwsRxdVJxlCObcvvH0rf/eL9jxZOO1X+8Gic4Hw+5+zWDhNMxSLlkMPUpyHjWRjAVZJ8/AH9rwZDOmNkpFow0298koClMCAsIpjHsE+n7fD+PAe8P+y2qCab5HO4on18FPpe0vhM4LRxBtBxYNox9nmPByL9ZwPsJaipPAxea2cmQGIH3lGHGLBlCSUHGsxeAmAWTnH8G+G+CjuR1ZrYR+C8G/tb+AJBjZpsIOqufTtp2M/BCX0dzknvC420gGLHy8+5+YDhBu/tLwLqk9d0E8wVvDF+fH8ZunwW+TzAk9GvAPe5eD/wVcJuZvUDQdDRmJ5eRsUGjpIqISIJqCiIikqCkICIiCUoKIiKSoKQgIiIJSgoiIpKgpCAiIglKCiIikvD/AWkYOJGRJdu7AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "avg_train_losses = []\n",
        "for i in range(len(train_losses[0])):\n",
        "    avg_loss = 0\n",
        "    for j in range(num_workers):\n",
        "        avg_loss += train_losses[j][i]\n",
        "\n",
        "    avg_train_losses.append(avg_loss / num_workers)    \n"
      ],
      "metadata": {
        "id": "x7_UnDuWCkkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_losses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hf6TWOPQVtlt",
        "outputId": "3e11a23f-3a3e-4099-972b-7fe53046761b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "353"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(avg_train_losses, label='train')\n",
        "plt.plot(test_losses, label='test')\n",
        "plt.xlabel('Iteration Number')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Loss function');"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "6LkNIlbTW0oG",
        "outputId": "debbc37b-99b4-4a32-96b8-f0db11a81a12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdd3xV9f348df7Zu8NIQlksPdGBAe4RcWt1VpHrVqrtbbWOtq6vm21+qtatXXUPUrdioqKKFOQHWYYARLIXmTvm8/vj3NyCZBAwNzcjPfz8bgPzj3n3HPf9xjv+362GGNQSinVezk8HYBSSinP0kSglFK9nCYCpZTq5TQRKKVUL6eJQCmlejlNBEop1ctpIlCqnUQkQEQ+E5EyEXm/k997i4jM6Mz3VL2Ht6cDUOpYiUgG8AtjzIJOfuvLgL5AlDGm0V1vIiKvA1nGmD817zPGjHTX+ymlJQKl2i8R2OHOJKCUJ2giUD2GiPiJyNMikmM/nhYRP/tYtIh8LiKlIlIiIktFxGEfu0dEskWkQkS2i8jprVz7YeAB4EoRqRSRG0XkIRF5u8U5SSJiRMTbfr5IRP5PRL63rz1fRKJbnH+SiCy3Y9onIteLyM3AT4E/2O/zmX1uhoic0Y7POUNEskTkLhEpEJFcEbnBXfdc9QyaCFRP8kdgKjAOGAtMAZqrV+4CsoAYrOqd+wEjIkOB24HJxpgQ4Gwg49ALG2MeBP4GvGuMCTbGvNLOmK4GbgD6AL7A7wFEJBH4EnjWjmkckGqMeQl4B3jcfp8LjvFzAsQCYUA8cCPwLxGJaGe8qhfSRKB6kp8CjxhjCowxhcDDwM/sYw1APyDRGNNgjFlqrIm2nIAfMEJEfIwxGcaYXR0Y02vGmB3GmBrgPawvb7ASxAJjzBw7nmJjTGo7r3mkzwnWZ33Evu48oBIY2jEfR/VEmghUTxIHZLZ4nmnvA3gCSAfmi8huEbkXwBiTDtwJPAQUiMj/RCSOjpPXYrsaCLa3+wPHm3CO9DkBig9px2j5vkodRhOB6klysBp0mw2w92GMqTDG3GWMSQFmA79rbgswxvzXGHOS/VoD/L2d71cFBLZ4HnsMse4DBrZx7GhTArf5OZU6HpoIVHflIyL+LR7ewBzgTyISYzfKPgC8DSAi54vIIBERoAyrSqhJRIaKyGl2Y2stUAM0tTOGVOAUERkgImHAfccQ/zvAGSJyhYh4i0iUiDRXG+UDKUd4bZufU6njoYlAdVfzsL60mx8PAX8B1gAbgU3AOnsfwGBgAVZ9+Qrg38aYhVjtA48BRVjVOH1o5xe6MeYb4F37/dYCn7c3eGPMXmAWViN2CVZSGWsffgWrzaJURD5p5eVH+pxKHTPRhWmUUqp30xKBUkr1cpoIlFKql9NEoJRSvZwmAqWU6uW63eyj0dHRJikpydNhKKVUt7J27doiY0xMa8e6XSJISkpizZo1ng5DKaW6FRHJbOuYVg0ppVQvp4lAKaV6OU0ESinVy3W7NgKllDoeDQ0NZGVlUVtb6+lQ3Mrf35+EhAR8fHza/RpNBEqpXiErK4uQkBCSkpKw5h7seYwxFBcXk5WVRXJycrtfp1VDSqleoba2lqioqB6bBABEhKioqGMu9WgiUEr1Gj05CTQ7ns/YaxJBYUUdD3+2hfrG9k41r5RSvUOvSQRr07Op/eEV7vtwIzr1tlKqs5WWlvLvf//7mF83a9YsSktL3RDRAb0mEZzDch71eQXHxnf4eH22p8NRSvUybSWCxsbGVs4+YN68eYSHh7srLKAXJQLGXo1JPImHfd/m358uJqe0xtMRKaV6kXvvvZddu3Yxbtw4Jk+ezMknn8zs2bMZMWIEABdddBETJ05k5MiRvPTSS67XJSUlUVRUREZGBsOHD+emm25i5MiRnHXWWdTUdMz3WO/pPupwIBc+h//z03iw/kXufj+Ft26cisPR8xuPlFIHe/izLWzNKe/Qa46IC+XBC0a2efyxxx5j8+bNpKamsmjRIs477zw2b97s6ub56quvEhkZSU1NDZMnT+bSSy8lKirqoGvs3LmTOXPm8J///IcrrriCDz/8kGuuueZHx957SgQAkck4znyEk2UDCRkf8uaKDE9HpJTqpaZMmXJQX/9nnnmGsWPHMnXqVPbt28fOnTsPe01ycjLjxo0DYOLEiWRkZHRILL2nRNBs0o2YtLk8lPEO5345lpMGxzCoT7Cno1JKdaIj/XLvLEFBQa7tRYsWsWDBAlasWEFgYCAzZsxodSyAn5+fa9vLy6vDqoZ6V4kArCqi2c/h5y085v0it7+zlpp6p6ejUkr1cCEhIVRUVLR6rKysjIiICAIDA9m2bRs//PBDp8bW+xIBQEQijrP/wlQ2Mbroc15astvTESmlerioqCimT5/OqFGjuPvuuw86ds4559DY2Mjw4cO59957mTp1aqfGJt2tT/2kSZNMhyxMYwy8cibFeZmc0fA0i+45i7DA9k/SpJTqXtLS0hg+fLinw+gUrX1WEVlrjJnU2vm9s0QAIAIz7iOqsYBZjQv4YF2WpyNSSimP6L2JAGDgadD/BH7rN5f3f0jXEcdKqV6pdycCETj1HqKbihhRskBHHCuleqXenQgABp6GiR7CLwO/49Evt+Fs0lKBUqp30UQggky6kSGNO4is3MmGLPdO7qSUUl2NJgKAUZdixIsLvZazaFuBp6NRSqlOpYkAIDgGSZnBpX6rmL8lTxuNlVId7ninoQZ4+umnqa6u7uCIDtBE0GzEbPo682gs2MbKPSWejkYp1cN05UTQ++YaasugMwGY5b+JOaumMDUl6igvUEqp9ms5DfWZZ55Jnz59eO+996irq+Piiy/m4YcfpqqqiiuuuIKsrCycTid//vOfyc/PJycnh5kzZxIdHc3ChQs7PDZNBM3C4qHPSGaVb+KX+7TBWKke7ct7IW9Tx14zdjSc+1ibh1tOQz1//nw++OADVq1ahTGG2bNns2TJEgoLC4mLi+OLL74ArDmIwsLCePLJJ1m4cCHR0dEdG7NNq4ZaSpnBoPo0corLqKht8HQ0Sqkeav78+cyfP5/x48czYcIEtm3bxs6dOxk9ejTffPMN99xzD0uXLiUsLKxT4tESQUuJJ+L9w78YJXtIy61gSnKkpyNSSrnDEX65dwZjDPfddx+33HLLYcfWrVvHvHnz+NOf/sTpp5/OAw884PZ4tETQ0oATAZji2MbWnDIPB6OU6klaTkN99tln8+qrr1JZWQlAdnY2BQUF5OTkEBgYyDXXXMPdd9/NunXrDnutO2iJoKWgaEzUYKaVpPPhvlKu93Q8Sqkeo+U01Oeeey5XX301J55o/fgMDg7m7bffJj09nbvvvhuHw4GPjw/PP/88ADfffDPnnHMOcXFxbmks7r3TULflo1so2zKfsx3/YcX9pyOiaxor1RPoNNQ6DXX7xU8gzFmCqcgls9h9/XaVUqqrcFsiEJH+IrJQRLaKyBYR+U0r54iIPCMi6SKyUUQmuCuedosbD8BYxy5W7in2cDBKKeV+7iwRNAJ3GWNGAFOB20RkxCHnnAsMth83A8+7MZ72iR2NES8m+WayIUsbjJXqSbpbVfjxOJ7P6LZEYIzJNcass7crgDQg/pDTLgTeNJYfgHAR6eeumNrFJwCJGsh4/zw2Z2siUKqn8Pf3p7i4uEcnA2MMxcXF+Pv7H9PrOqXXkIgkAeOBlYccigf2tXieZe/LPeT1N2OVGBgwYIC7wjwgZigplRtIyy2nrtGJn7eX+99TKeVWCQkJZGVlUVhY6OlQ3Mrf35+EhIRjeo3bE4GIBAMfAncaY8qP5xrGmJeAl8DqNdSB4bUuZhiRaV8gznp25FUyOqFzRvcppdzHx8eH5ORkT4fRJbm115CI+GAlgXeMMR+1cko20L/F8wR7n2fFDENoIlly2VngvkEcSinVFbiz15AArwBpxpgn2zhtLnCt3XtoKlBmjMlt49zOEzMUgCGObDKKqjwcjFJKuZc7q4amAz8DNolIqr3vfmAAgDHmBWAeMAtIB6qBG9wYT/tFDQJxMDGwgDWaCJRSPZzbEoExZhlwxGG5xmq+v81dMRw3nwCISGJkbQ7vayJQSvVwOrK4LTHDSWzKYk9RVY/ubqaUUpoI2hIzlOi6fdTX11FQUefpaJRSym00EbQlZhgO00ii5JNdWuPpaJRSym00EbQlejAAgySH3NJaDwejlFLuo4mgLZHWwJMBkk9umZYIlFI9lyaCtgREYAIiGOhdSI6WCJRSPZgmgiOQiGQGeRdoiUAp1aNpIjiSyBQGkE9OmZYIlFI9lyaCI4lMJtpZQFHpcc2Vp5RS3YImgiOJSMZBE75V2ZTXNng6GqWUcgtNBEcSkQhAPEUsTy/ycDBKKeUemgiOJNxKBIN9ilm4rWcvZqGU6r00ERxJaBw4vJkSUcHy3VoiUEr1TJoIjsThBWEJJHsVkVNaS6OzydMRKaVUh9NEcDThiUQ783E2GXK1G6lSqgfSRHA0EYmE1lqrZ+7bX+3hYJRSquNpIjia8ER8a4sJoJas/TrCWCnV82giOBq751B/RxFZJVoiUEr1PJoIjsYeSzAmsFRLBEqpHkkTwdHYJYLhAaXaRqCU6pE0ERxNcB/w9megT5GWCJRSPZImgqMRgfABxEsheeW11DfqWAKlVM+iiaA9whOJbsjDGMjR9YuVUj2MJoL2iEgkxB5LoNVDSqmeRhNBe4Qn4l1fTihV2mCslOpxNBG0h92FNNGrkCxNBEqpHkYTQXvYXUhHB5bqQvZKqR5HE0F7hA8AYLBfCQUVmgiUUj2LJoL2CIgAv1CSvIrIL6/zdDRKKdWhNBG0hwiEJxJnCsgv1xKBUqpn0UTQXhGJxDTmUVHbSE2909PRKKVUh9FE0F7hiYTW5QJG2wmUUj2KJoL2ikjE21lDNOXaTqCU6lE0EbSX3YU0QQq1nUAp1aNoImgve1BZfykgT9cuVkr1IJoI2iusPwAjAvaTmlXq4WCUUqrjuC0RiMirIlIgIpvbOD5DRMpEJNV+POCuWDqEXzAERjMmuJzVe0owxng6IqWU6hDuLBG8DpxzlHOWGmPG2Y9H3BhLxwiNI8m3lIKKOvbq+sVKqR7CbYnAGLMEKHHX9T0iNJ7IpmIANmWXeTgYpZTqGJ5uIzhRRDaIyJciMtLDsRxdaD/8qvMAKKzQLqRKqZ7B24PvvQ5INMZUisgs4BNgcGsnisjNwM0AAwYM6LwIDxUah6OmhABHA0WVmgiUUj2Dx0oExphyY0ylvT0P8BGR6DbOfckYM8kYMykmJqZT4zxISBwAwwIrtUSglOoxPJYIRCRWRMTenmLHUuypeNol1EoEg/0rKKqs93AwSinVMdxWNSQic4AZQLSIZAEPAj4AxpgXgMuAW0WkEagBfmK6ep9MOxEk+5WxTauGlFI9hNsSgTHmqqMcfw54zl3v7xZ2IujvVUJRmSYCpVTP4OleQ92LXwgE9aG/yaGosl4HlSmlegRNBMcqegix9XupdzZRXtvo6WiUUupH00RwrKIHE1GdAehYAqVUz6CJ4FhFD8G3oYwIyskoqvJ0NEop9aNpIjhW0daYt4GSQ1puuYeDUUqpH08TwbGKGgjApJAStuVVeDgYpZT68TQRHKuw/iBejAksIS1PSwRKqe5PE8Gx8vKB8P4M9C4ko6iK2ganpyNSSqkfRRPB8YhIJqYxlyYDWftrPB2NUkr9KJoIjkdkMqE1WQBk7dcFapRS3ZsmguMRkYx3XSmhVJFdqiUCpVT3pongeEQmA5DsVaBVQ0qpbq9diUBEgkTEYW8PEZHZIuLj3tC6sAgrEYwL2q+JQCnV7bW3RLAE8BeReGA+8DOsxel7p4hEAIb5lZCtbQRKqW6uvYlAjDHVwCXAv40xlwNdf41hd/ELgaAYUrwK2KclAqVUN9fuRCAiJwI/Bb6w93m5J6RuIiKZePIorKijorbB09EopdRxa28iuBO4D/jYGLNFRFKAhe4LqxuITCaqPgeA3YU6+ZxSqvtqVyIwxiw2xsw2xvzdbjQuMsbc4ebYuraIZPyrc/GlgV2FlZ6ORimljlt7ew39V0RCRSQI2AxsFZG73RtaFxeZgmBIdBRpIlBKdWvtrRoaYYwpBy4CvgSSsXoO9V6RKQBMCislvUATgVKq+2pvIvCxxw1cBMw1xjQAvXvBXjsRjA0s1jYCpVS31t5E8CKQAQQBS0QkEejdczAHRoJfGAO9CthbUk1TU+/Oi0qp7qu9jcXPGGPijTGzjCUTmOnm2Lo2EYhMJq4ph7rGJgp0/WKlVDfV3sbiMBF5UkTW2I9/YJUOerfIFCJr9wGQUazVQ0qp7qm9VUOvAhXAFfajHHjNXUF1GzFD8a/MIoBa9hbrVBNKqe7Ju53nDTTGXNri+cMikuqOgLqV2NEIhlFe+8gs6b0zbiilurf2lghqROSk5iciMh3QSXZixwBwUnAO6/eWejgYpZQ6Pu0tEfwSeFNEwuzn+4Hr3BNSNxKWAAERnBGWz1O7ikkvqGBQnxBPR6WUUsekvb2GNhhjxgJjgDHGmPHAaW6NrDsQgdgxDDUZ+Ho7ePKbHZ6OSCmljtkxrVBmjCm3RxgD/M4N8XQ//cbgXZTGnTOSmLcpj+W7ijwdkVJKHZMfs1SldFgU3VnsGHDWcf1Qayrq1H3aVqCU6l5+TCLQobTgajAOLN5CeKAP2bpQjVKqmzliY7GIVND6F74AAW6JqLuJHgzeAZC3kYSIWWSXaiJQSnUvR0wExhjtAnM0Di/oMxwKthIffqlOQKeU6nZ+TNWQahYzDAq3Ex8eSNb+GozRWjOlVPehiaAjxAyFilySQhqpaXCyv1rXMFZKdR+aCDpCzDAAhjisNYy1wVgp1Z24LRGIyKsiUiAim9s4LiLyjIiki8hGEZngrljcLmYIAAmNewHILtUJ6JRS3Yc7SwSvA+cc4fi5wGD7cTPwvBtjca/wRPAJJKbKGlmcpSUCpVQ34rZEYIxZApQc4ZQLgTfthW5+AMJFpJ+74nErhxf0G4dvfipBvl6aCJRS3Yon2wjigX0tnmfZ+w4jIjc3L4pTWFjYKcEds4SJSO5GksJ9dCyBUqpb6RaNxcaYl4wxk4wxk2JiYjwdTuviJ4GzjhOCcrREoJTqVjyZCLKB/i2eJ9j7uqeEyQBMdOwkLbecm99cQ0WtdiNVSnV9nkwEc4Fr7d5DU4EyY0yuB+P5ccLiITyRobUbAJi/NZ+fv76a4kpd1F4p1bW5s/voHGAFMFREskTkRhH5pYj80j5lHrAbSAf+A/zKXbF0mqSTSa5M5apJ8fz90tFszCrjz5+22ntWKaW6jPauUHbMjDFXHeW4AW5z1/t7RNJ0vFLf5tGTvCB2ACt3l/C9rk+glOriukVjcbeRMsP6N/1bAEbEhZJfXkdhhVYPKaW6Lk0EHSk0DvqOhp3zARgVby3xvCWnzJNRKaXUEWki6GhDzoK9P0BNKSPiQgG4/rXVrM080tg6pZTyHE0EHW3wWWCcsHshof4+/GSy1UP2+/RiDwemlFKt00TQ0eIngX847LCqhx67dAyh/t4UaTdSpVQXpYmgo3l5w6AzIP0baGoCIDrEj+LKeg8HppRSrdNE4A6Dz4KqQshNBSA62I9CLREopbooTQTuMOgMQFy9h6KDfbVqSCnVZWkicIegKEiYBNu+AGOIDtaqIaVU16WJwF3GXAl5GyFrNdHBfpTVNFDf2OTpqJRS6jCaCNxl7FXgHwYr/kV0sB8AxVVaPaSU6no0EbiLXzBMuA7S5pIg1mI623IrPByUUkodThOBO025GRCGZb0HwA2vr+ZPn2zCmm9PKaW6BrfNPqqA8P4w9Fxi0t/nxZ/8im92lvL2D3sZ3CeEEXGhTE6K9HSESimlJQK3m3QDUl3M2V6ruffcYQA8OHcLl7+wAoCK2gYtISilPEoTgbulnAbhibDmNaKD/fDxEtehfy1MZ/RD81m4vcCDASqlejtNBO7mcMDE6yFzGRTu4H83T+WyiQkAPPH1dgBW7tGZSZVSnqOJoDOMuxoQ2PIxExMj+dvFo12H4sL82Z6nvYmUUp6jiaAzhMRC/xMg7TMAfL0dPHXlWN7/5YlMTYkiLbfcwwEqpXozTQSdZfj5kL8JstcCcPH4BCYnRTK8n7Wc5aLtBbyxPMOzMSqleiVNBJ1l/DUQGg8f3QwNta7doxOs5Sx/PWc9D322hf1VOieRUqpzaSLoLAERMPtZKE6HVS+6dk9KjCA62JeK2kaMgXP/uZS/f7UNZ5N2KVVKdQ5NBJ1p0Okw+GxY8v+gqggAby8H543u5zolr7yW5xft4tVlezwVpVKql9FE0NnO+j+or4KFf3XtunXGIB68YAQnJFsjjRMiAlibud9TESqlehlNBJ0tZihMuQnWvOZqOI4N8+eG6cm8dsNkNjx4FiPjQtlZoF1KlVKdQxOBJ8y83+pS+sGNrioigEBfb8ICfBjUJ5jdRVV8uSmXukYneWW1R7iYUkr9OJoIPME/DK54Cypy4ZUzoXD7QYcH9wnBGLj1nXWc/dQSpj76LR+ty6Km3smqPSXUNjg9FLhSqifSROAp/SfDtZ9CXQX853QoSncdGtQn2LWdUVwNwENzt3DRv77nihdX8PYPmZ0erlKq59JE4EkDpsJN3wEGvnvEtXtobAiXTIhnRL9QAKamRFJe28j2fKvdYEe+th8opTqOJgJPCx8A034NWz+Fta9DbRk+Xg6evGIcD1wwgqSoQB6ePQoAEUiJDiKjqPq43mp/VT3LdhYd/USlVK+iiaArOOm3ED8RPvsNPDvJ1WYwNSWKRXfPZGhsCCP6hTK+fzgTEyNYlVHCjCcW8pZdRfTKsj0s2JrP79/fQGZxVZtv88aKDK57bZW2MSilDqIrlHUF3n5We0H6Apj3B3jvWrhlKXj7uk556dqJeDmEj9ZlA1bbwZ8/2UxNfSN/m7fNdV5lbSMv/Gxiq2+zr6QGZ5OhuKqe+PAA934mpVS3oSWCrsIvBEZeDLOfgcJt8MmtsPkjaLJ+vSdEBNIvLIDEqMCDXvbYl9sOeu7lENqSW1YDQHFlXQcHr5TqzjQRdDVDz4VT74HNH8AHN8DGdw86PKRvCAB/Om84Xg6hyUBydJDreFV9Y5uXzrXHI6zJ2M/uwko3BK+U6o40EXRFM++Hu3dBcF+rAbnFmsZD+oaw6v7T+cXJKa6kcOupA13HF20v5NQnFh42i6kxhpxSq0TwyOdb+c3/Ut3/OZRS3YImgq4qKBqm/wb2rYRnxsPql12H+oT6AzA5KYLoYD8uGh9PfHgAIX5Wk09mcTWfb8plZ34Ff/hgA4/OS2NXYRV1jU2ua2zJKaOqru3Sg1Kq9xBjutd0x5MmTTJr1qzxdBido8kJqe/AurcgaxVc8yEMOsN1uKqukfLaBvqFWQ2/Ly/dzV++SAMgPjyA/dX1CFDX2ERjK9Nav/OLE5g+KPqw/Z+mZjM2IZykFlVOSqnuTUTWGmMmtXbMrSUCETlHRLaLSLqI3NvK8etFpFBEUu3HL9wZT7fj8IIJ11o9iqIGw9uXWaWDeX+A2nKC/LxdSQBgWKw1AO2M4X3JLq2hut7Jx7dN55XrJ7d6+TUZ+3E2Gd76IZN7P9zIQ3O3cP1rq7jz3VReXra7Uz6iUsrz3FYiEBEvYAdwJpAFrAauMsZsbXHO9cAkY8zt7b1uryoRtFRdAitfhLxNsOMrGHYeXPGmNcqshbyyWmLD/Fm1p4QGZ5PrF3/qvlK+3JTLi0usL3h/Hwfj+ofj6+3Fkh2Fh73diSlRzLl5qvs/l1KqUxypRODOcQRTgHRjzG47iP8BFwJbj/gq1brASJh5n7X9/TPwzZ/hxZOhtgwm3gAn/w6wprQGmGKvbdBsXP9wxvUPdyWCs0bEMndDDmCVIBak5R90/i7tVaRUr+HOqqF4YF+L51n2vkNdKiIbReQDEenf2oVE5GYRWSMiawoLD//12utM+zVMucWaqC40Hr59GBY9BhnLwHnkBuDmcQYzhsYAEOrvzf2zhh12XkFFHeW1Da1eo6Sqnifnb6fB2XTYsdb2KaW6Nk+PLP4MmGOMqRORW4A3gNMOPckY8xLwElhVQ50bYhckArMeh7P+AuKAD2+ERY9ax6b92trfhu/uOpX88jpi7Z5HZwzvS//IQBwCh7Yn7yqoZPyACPYUVbG/uh5jDH+bt43M4iqKKutxOIS4sADCAn3oG+pPRW0DP3tlFfPuOJkRcaHu+vRKqQ7mzkSQDbT8hZ9g73MxxhS3ePoy8Lgb4+l5mqeguOINyF4H3/0FVvwL+o2D0Ze1+pLEqCASo4IwxnDPOcM4c0RffLwcxIb6k2MPOEuJDmJ3URUvLN5FWIAPe0uq2V1Y5Zqeotk/v92Jl4irR9LkpAgAVu4p/tGJoMHZhI+X9m5WqjO4MxGsBgaLSDJWAvgJcHXLE0SknzEm1346G0hzYzw9W/wEKyH890qrhLDqJYgeAvszoP8UGPMTiBniOl1EuHXGgYFoCRGB5JTV8tSVYxkZF8bLS3fz3pqsw94mJSaI3YXWxHbGQGOLzgarM6x1ljOLq/lkfTYFFbXcfMrAw67RUl2jk3mbcpk9Nt5VbVVaXc/0x77jmavGc/rwvsd9S5RS7eO2n1zGmEbgduBrrC/494wxW0TkERGZbZ92h4hsEZENwB3A9e6Kp1fwC4GffQznPm6tfrblE2iohqVPwr8mHzQo7VDxEQE4BC4YE8eQviE8eskY3vz5FPx9Dv4TeeiCkVw5qT9xdqM0gEPg7rOHup6n5Zbz70Xp/L/5O446aO2tFZn89t0NvLv6QHPSzoJKquqdrMncf6x3QCl1HNxa9jbGzDPGDDHGDDTG/NXe94AxZq69fZ8xZqQxZqwxZqYxZtuRr6iOytsPTrgFfrMR7s20Fr757RYYfDZ8cRe8fj5s/wr2H7zK2SUT4rl1xkC87eoYL4dwypCYg36Ri8CkpAj+ftkYThpsdUvtG+rH5KRIbps5iFV/PJ2rpgxg5Z4SduRXUt/YxFPf7GB/VT3pBZXcMWc9FYc0QGfaK7B926LX0r4Sa9+uAu25pFRn8HRjsXIXERAvazssHqyDKa4AACAASURBVC5/HVa+AMufhTlXgpev1ZbQ1AB+IZw8+1lOHnx476HfnD6YyYkRPLdwF6H+3gT6Wn8yJyRH8W1aAZ//+mRXqaFPiD+j48OY0+L1Ly/bw3fbC4gM9GVN5n6GxoZw28xBruObc8oAWLKzkIraBkL8rTYJ0C6sSnUWTQS9hW+gNdZgwnWQvwk2f2iVCrzDYe8P8M+xMOBECOtvtSlMuQmwJrkb0jeEkuoGgny9XJe7ZEI8F46Lc5Ugml06MZ7nF6ezr6SG/1w7iV2Flfxj/nZ2F1bh4yW8umwPP5+ezIrdRfQLC2BrTjkDY4LYVVjF1pxyTkiJYl+JNTleZnH1YY3GxZV1vLkikxtPTibU34fN2WXUO5uoqXdSXe9kxtAYbWRW6hhpIuhtgqIgZYb1aLbja1j8OFTmQ/Za2PSe1R116Cw451EQB787c8hBlxERvL0OX/vAz9uLBb87laJKa/GbM+nLmSP68vYPmZyQHMUv317LC4t38fyiXYQF+lDX2MQ1UxN5+LOtbMousxOBVSJobDLsK6kmJSaY+z/eRHlNA+kFlWzLq6BPqB8/PSGRy19YQU2LFdeeuGwMl0/qz9rM/VTWNXLqEGu8hLPJHHGtBqV6M00ECoacbT0AGmph3u+tEcup/7XWUjYGRl0CpsmaGnvCzyAiqc3L+Xl7HbQC2sCYYB68YCQAU5Ii+ee3OwEorKgjxN+bKyf354XFu1i5p4Svt+SxOmM/g/oEk15QyabsMlJigvnvyr2ANTVGsJ836zJLuXxif1cSmJoSyZqM/fywu4RzR/fj0ueXA7D7b7PYXVTFrGeW8tGt0xgVH9bRd0+pbk/L0OpgPv5w4XNw5Vtw0fPWF39oHGyYA9s+h6X/gDcvgrVvwKr/QOOxrXb26KWjAYgO9sMhcNnEBAJ9vRkdH8Y3W/NZt7eUiEAfbjwpmWA/b+75cCNnP7UEgEF9gplz01SmpkSxfu9+dhZUAPDYJaN55xdTOW1YH1ZnlPDWigMN4dvzK1ibWUJ9YxPLdxUdMbas/dXsKarigU83s2JX8WHHX1m2h7Xak0n1QFoiUG0be6X1aKyDxlrwD4N9q6xZUD+7wzrnq/us0sGUm2H3IgjuYy2sE9zn8OtV5DOwZB1r/mRNpZ1XVsvAmGAArj5hAA4Rrpzc39VT6YuNuSxLL2J7vvWF/8fzhjN+QAQTEsNZkJbPYnuyvCnJkXg5hCnJkczfms/iHQV4O6yBbt+nF5FTag2U25JTfsSPe9o/FlNvr9mQUVzNiQOjXMcanU38bV4aF42LZ2JixHHdTqW6Kk0E6ui8/awHWA3Jv99hDVQrzbS+/DOWwZd3g18YOOushuiYYVaDc20ZxI2HhEnwxe9g2+dE/3IZxI4mOtjP9RanDevLacMOHjw2uG8wy9IP/IofZCeN04f15fGvtvP4V9sJ8fMmKcpaN2Fs/3AAVu0pYXJSJIWVdSxLL3LNf3SkRGCMcSUBgOXpRZRW1xMeaI3ezimtxdlkyNpf3e7b9vH6LKKC/DhlSAzGGMprGwkL8Gn365XqLJoI1LHz8Yc+w6zHkLOtBXQ2fWAliZr9sPoV2PUdfHTTgdfET7QaogFeOMnqvXT+U9aaC4eqrwKfQH575hDGJITx23c3WJew2x2GxoZw1ZT+zFm1j/tmDcdhNwI3r93cZKwBckNjQ/hgbZarF9GuwkpWZ5QQG+pP/8jAg96ysOJAFVd0sC9FlfV8tC6b0AAfZo+Nc5VKsvbXHPS6x77cRpMx/Hx6Mqc8vpC3f3GCa+bXv3+5nZSYIE4ZEsOCtAJu++86lv5hJn1D/VGqK9FEoH48h5dVhQRAsjXdRX01lOwGv2DY9gVs+B8MO98qKaTNhXVvQOb3kHwqJJ8CMUOhLAsQa4qMhEmEDjyNi0ddyp7TB5NZXOX6wgd45MJR/OLkFFfVEkBUkC8h/t5U1DYSHx7AqPgw3lyRCTi5dEICX2zK4fIXVtAnxI9v7zqVn7++mp9PT6aq3sn2PKu0cMupKVwwJo4bXl/NI59bM6a/vnwPm7Ot47llNSzeUUh8eACD+gSzIC2fBmcTCREB1DubePuHTLy9hJW7S8ivqHX1VFqdYbVTbM4u00SguhxNBMo9fAMhdpS1feJt1qPZ6X+G1Dmw+QMrQax55eDXegdA+reQvgAW/Z3fnf5niIuBLRkQNgBiR+Hj7XdQEgCrS2tydBAbs8qIDw9gasqBOv4Hzh/BnWcM5q9fpPHVljzumLOe1Rn7Sd1XSoPzwHxJN5+cQlSwH6cMjuHDddZcS81JAKzSxnWvrgJg0e9nkLW/mkanYf3eUgD8vB1c8u/lrvNzy2qob2wiLde6xvb8Ck4f3pfb/7uOEwdG8dMTEkndV8p/V2by6CVjDuviWl3fSHFlPct3FREZ5MeZIzpv7iVnkyF7fw0DogKPfrLq1jQRKM8Yd5X1cDZA1hoo2QVRgyB3I/QbA5EpUFUIn95udWdtyeFjnZswCRKngW+w1bspZijJUYHsyCogPiKAsAAfXrhmIsNiQwgL9CEs0IenrhzHVw98xcLthfh6Oah3NjF+QDi+Xg4KKuqIststTh16IBG05bttBdQ2WO0KH6+3Jtb9anPeQec0Gas30rY8q2ppR14FhRV1fL4xl8835nL+mDiueHEF9Y1NXHtiEqPiw9hTVEVsqD8Bvl48PHcr7645MA9TxmPnubZfWLyLVXtKeLWNpUh/rA/XZfHHjzex/N7TiQnxO/oLVLeliUB5lpcPJJ5oPQAGtFgeM7gP3PgNFO2wdxgoToec9VCwzWqXWP/WgfND4/mdI4En/NZQk3o1bPflnII0GHiaNara4UWArxcPnD+C3UWV/GTyAP7wwUYePG8YYwdEUt9iUZ1Zo2JpunIc324r4DN7Jbdmvl4OEJi/9eAvfYCKVibZW7mnxNUGsT2/krWZJa5jYx+e79r+YXcxvt4OznpqCRePj2f22LiDGssP9W1aPmsz91NT78Tfx8HnG3M5bVgfgvw65n/rrTnlNDgN6QWVR0wEz323k5FxYcwc1kpPMdUtaCJQXZuXN/QdceB535Ew4kJruywbyrOtEdG15bD2NRLyt7AvaDiJaXPALxQCwmHhX6xpufuOgPBEfh6RCAMToaGceQM/hrcuh1lP4DfhZ6638fZycNH4eIb1C2FSYgRnjuiLt0N49rt0fjrVSiA/7C45KNTmBuyTBkUf9AV+30ebEIGpyVGszdzPgrQC1/Ngf282ZpWSX17Hyj0lriVDP16f7Spl+Hk7MAbqnU38/attxIUHcPWUAWzLq6DJwM6CChqcTfx6znqumJTA45eN7ZBb3zzX056iqoO60rZkjOG5hemcOiRGE0E3polAdV9h8daj2fif4mUMSSLQ1AQOhzUqOu0zq4G6ZLc1KK665WAxsaqZ5t5uNWoPnGl1la0th5x1DEuczrCxl0CQ1WPp/y6y2j1GxoWyMavsoHBG9LMW4zl1SIwrEfQL88fLITx39QT8vB2c+8+lfLA2i2kDo/jvTQdKP/d9tJE5q6wqoPNG9+OLTbmuY49eMhp/Hy9+9c46nl+0C4CC8loqaq3Sx2XPr3CtVf3emixmj43npMHRlNsrxt0wLYmLxsfzxvIMnvxmB9dPS+Lc0bEkRQXh6+Vgp/2LPzLIl4fmbiExKpAbpie71p1YkJbPiLhQxtndc/dX1fNpajY/mTKAyrpGahuaXPNDHavdhZX88ePNvHjtREL9tWutp2giUD2L2I2tDseB5yNmW49mdRVQutcqUfQdaVVBLXsaVj4PO748cF5gNGz5GL661xpMFxAJoy+HqgLuKt7NXX5rWOgcR+Cg6TgGzuD00M34npfIJZP7cG4SVPpGkxQVhJ+3A7HjOmtEX77dVsBDs0ceFPavZgxyJYInLh/DLaem8P/m72DJjkLGJIRT13hgPqWYED+e/S7d9bze2cTekmriwwPw83Zwz4cbefX6yby0ZDcb9pXy9g+ZXDgujrd/yKSpyfDPb3fyz2930jfUjyF9Q1i6s4ggXy8+v+NkXl+eAViTDeaUWV/u320rIHVfKTeelExUkC9zN+SwfFcxy9KLuf00aybZfccwvqKl5buKWbG7mO15FUxOirT3FTE8NpS88lr+s2Q3j106Bl9vnQTBncSY7rUE8KRJk8yaNWs8HYbqiZqc1jiIxjqrVBAYBfmbYdP7UFVklSj2rrB6NYlQGDmRkJJN+DeUHrhGoF2FUl8FsaOtxYLGXwO+IRA+gLqIQdQ5DaGOeqvRu3m5UWBTljWTavPI5aq6RpalF3H2yFgq6xoZ9eDXANw/axh/m3f40h2nDevDHacP5sbXVyMiVNc3Ul3vZHR8GDeelMyd76byfxeOZHJyJGm55by3OouVe4oZFhvK1txyZo+NY+4h7SFtGdo3hO35Fdw2cyD/WmiVUj7+1TTu/XAT101L4uoTBhx+e5sMtY1O11TmAE98vY1/LdzFc1eP5/wxcWSX1jD9se84bVgfhsWG8O9Fu/jw1mntHs3d1GQO6mbcVe0rqSYiyJfgDmrPaQ8RWWuMmdTaMS0RKNXM4QVB0Qfvix1tPZpVFlglA4cXMSJW1dPeFVC4zSpBbP3UmpyvLAvKc6BxD+z6uevlfmED8ItIhH0rQRzWtN/jr4GBMxldnmnN61TaFwIjCfIN5OyRsQAE+3lz1oi+zBjah4vHxxMW4ENkkJ89iM6wNbeCE5Ij6Rvqz5/PH8Gd76YCEOLvzabsMu58N5X48ADOGxNHZJAvw2JDuXh8ArUNTirrGpn0lwXM3ZBDcnQQb/58Cr9/fwO5ZbXMHBrDGysyCfbzxt/HwT+uGEdFbQNTkiM58dHveHHxbtdn++/KvWzPr+D+jzcRG+bHacP6kltWwwXPfs9TV45lTcZ+/vntTjY+dJarGijXnv6joNxqTG9umN+WW07z1/n6vfsZmxDG4h2FjIgLpV9YAJ+mZvPy0j189KtprgGD36cXceMbq/nijpMP61rcEZxNhkfnpXHtiUk/qkuts8lwwXPLuHJyf+47d3gHRnj8NBEodSwOnUNJxOrCmjjNet5cBdVk90AyTqtU4WywksW2L6zG7Uk3Wg3hOamw4EHr0ZJ3ADQ1QlCMNc/TqEt5KdYf6iNhSx+u3DwHLvoXRFjjCgb1CXG9tHlkM8ClExJc1T0LfncqAb4Hj+T29/HC3+fAvssmJtA/MpB3bznRte/BC0by9ZY8QgN8mD7oQKI8aVC0a74nsKqQAn29SIgI4P6PNvPZr8N5acluiirreGN5BjvtFef+t2qvay3r5uqn/AorIXy+0UoEpTUNbLXHXqzfW8qKXWv5dlsB0cG+/O/mE3lvzT42ZZexak8JJVX1jIwL5eWlu6ltaOLT1JzDpk0/msKKOkqq6hkaG9LmOTvyK3h52R4ignxdiys1OptwiBxTKWRHfgWl1Q3ssdtgugJNBEq5Q3MbBQ5rriWwpuCYcO3h5xZuh4I0COkHexZb+6oKrRXmKvOsJJL6jlV15Wwx2+srZ1vdb2vLrOqsmCEQEkdc0kn8I3g5zsZGJsiJxHqvp8nLl4C8KKuqqniXlbhalH7iwwPILq3hikn9W/kowrmj+x22//bTBh2UCIqr6pmUGMFDs0dy2QvLufLFFWSVWl/02aW1BNgJ560fMrnp5BREhNwyKwEUltdRXFnH5uwDCxVV11vtIs0N55dNTGDeplye/W4nq/ZYPbZ++vJKwGq835pbjgi8uHgXM4fGMH5ABGU1DYT4ebu+qL/YmEtOaQ03nZJy0Gf50yebWJ5ezKo/nnFYsmzW3Isqo8j6AjfG8NOXV+Lr7eC2mYMYmxDe5mtbSt1nVSXmldceduzNFRkM6hPMtIHRhx1zJ00ESnlazFDrATDghNbPaS5hVORA2ucQlgBrX7casYOiraSRvwkKtsLOrznfO4QmbwcBa79hgJcX3mLg1Q8PXM8nyKqaGnAChA/gy4H1lDr9iVm1zupym/G9NbbD4Q1Rg8FZDymnWonENjkpkrvOHEKIvzefpOaQuq+UwX2DGRUfxks/m8R9H21i4oAIBvcNtqf6gLAAH/aV1LAgrYCTBkW7EkFBhdV9FuCO0wfzm/9ZVVs/mdyf/622GtFvnTGQRmcTn6RapYZAXy9XsmieUPCG6Um89n0G17y8kjvPGMITX2/ntGF9+O2ZQxgQGcifP93M/up6Zg6LcZWiymsbWLitkHpnE/O35nHhuBY90bDq88tqGthVYCWAjGLr32+25rtiXrqziDtOG8TvzhrK+2v2sXRnEc9cNb7V/5Qb7ESQU1pLXlktLyzexb3nDqOxyfDAp1us92gxcLAzaCJQqjtoLmGEJcDUX1rbw88//DxnI1Tk4Bdm/7Lfv4fixhBiQ31h5zdWtVT8RGs9CWc9ZCyFnFRCMYTWV8H2Bqs6yy/s4B5UAAERkDAFIhKtxNVQy6/LN0PASIL7xRKSnc6M/ath+WJOyVrDshv/CNXFrC734s0V1iVuOTWFx7/azk1vrmH6oCjXjK/L0otYll5EgI8Xs0b3IyEigKU7i7h1xkBWZ5RQ29DEwJhgLp6QwCepOZw0KJonLh9DcWU9i3cU8sTX2wn09eKPs4YzKi6Mu97fwF/npZESE8RXW/L4aksek5MiKKmqx9shPPNtOrfNHET/yAC+2ZJPvbOJQF8vXl+ewRnD+/LFplxC/b25/+PN1Dc2UdlioOCeomqamgxPfrOD5OggJiZG8MHaLFZlWEnhg7VZrNxTwl1nDaFvqP9BVW9woERQVFnH419v46N12YyKDyMi8MjdZwsr6ogO9nX1QOtI2mtIKXWAsxGqi6zeT4Xbrd5TZfusHk6rXoL9e6BkD9Rb1SQE97WSy6HE/vIzToy3P0UNvqxoGsmJQ+JozE5lQ1UknzqnkW8iMP7hOGsryDVR3Dcki0tOnQIhsdY0I95+1DY4aXA2EWI3MG/Pq2BQn2DXvEyLdxRy3aurXGMzquoaGffIfBqchs9uPwmD4Ymvt7N0ZxEXjoujf0Qgzy20ut9GB/tSWt1AQkQAv5o5iD98sNH1EcIDfSitbmBYbAiJUYF8veXA5zxjeF8WpOXz9JXjuGh8PA/N3cK7q/ex/oEzGf/IN66V83y9HTx95TiC/bxdPZ9GP/Q1saH+5JTVMrxfKGm55a4OAM3tOWmPnOOqZnp6wQ4qaxuZs2ovV04ewAMXtBhgeQy015BSqn28vK0vYTgwaWD0YOvf5JOtf5uarLYLsHo57VlidZf19rNKEnkbrPWulzwBgdFITQm1OdlMzVpBVN4uHPGjid23knPqV1vXMEDzDBZ7geZZQ3yDITIZ/4Qp+O/PsKrAQuMZGhILFSngHw4OL8ZJLcmOXC4LLYSaoQQFRDBtYDQFFXWMig9FRHjuqgks3lnIeaP7Udvg5PONOQzqE8La9Gwam7y5bloSl09MYEdeBTllNczblEdpdQND+gbz1Z2nkFtWw9db8gny9aKq3smCtHwuHh/PBWPjAKua7PXlGZz51OKD1tCub2zizndTqW9sIjzQh4dnj6TJwNmjYnnt+wzXZIQL0vLxazFW4oXFu9hZUMHVUxJ5esFO1/6UmKAf/9+4FVoiUEp1vrpKa3Gj8hzqKovIr/VlgDMTgmOtAX9ePlYvq5xUqyE9MtmaTqS2zOpN1RafQIgZirOuGqmvwOHlYyWQwCirvaMiD/qMoKnPCBxZKzFb57I85krGn/VTAk2tlcxiR/PJM3fyefkgAobO5NnrrAS4p6gKh8AfP97M788e6hppDVBW3cCNb6xmjb2U6ZTkSOoanFw0Pp6HP7OmM/fzdlBnV4W9d8uJXPGiVV92yfh4Pt+US31jE49fNuagUsmh3r15KiektD7dx9EcqUSgiUAp1X0YY/WoKtkNNaXQUG114a3Is0oyGd9bExP6Bdsr5tVbVV3VxVbPqpB+1hiOhmprPEif4da6GC35BFrHAad44zXoNOv63n7WBIYBkdZ1/UOtdpOGWogaCF6+7N1fy7ZyX84aEoZpclIaMIBrXlnJn84bwebsMv46L42kyAC++u2pXPDsMirrGnnrxinsLalmd2EV101L4oJnlzGufzhXTRnAx+uz6Rfmz6NfWgMI1//5TCKCfA+9K+2iiUAppZrVlltf5EHRVmIp3gXFO63uuRW5kL+Zjxun8enqndw7tIBh+xeBtz9goGin9W97xU2w2jqKtoNPEHU1FfiU7cEx7DxrMGFppnU8NB5qSqzxI7GjrMb8yb8Abz/KSvIZ904jUcEBrvW+j4e2ESilVDP/0APbIhA9yHq0ELYtn0Urg7lj5jQY0GJ6C2cj1JYemJiwpsSqcsrdYCULESjdZ433cDZavbIyl1u9rOoq8BMnjLrUWtfbWQ8hcbD5I1pNLps/sGIB1vsHU2PCYMUhizx1EC0RKKXUIYwxrM7Yz+SkCLd016S+2kog3r5WwijbZyWRygKrGip2lDVeJDASHD7sXv8twaaaPhMvhDGXH9dbatWQUkr1ckdKBDq3q1JK9XKaCJRSqpfTRKCUUr2cJgKllOrlNBEopVQvp4lAKaV6OU0ESinVy2kiUEqpXq7bDSgTkUIg8zhfHg0UdWA47tad4tVY3aM7xQrdK97eFmuiMSamtQPdLhH8GCKypq2RdV1Rd4pXY3WP7hQrdK94NdYDtGpIKaV6OU0ESinVy/W2RPCSpwM4Rt0pXo3VPbpTrNC94tVYbb2qjUAppdTheluJQCml1CE0ESilVC/XaxKBiJwjIttFJF1E7vV0PIcSkQwR2SQiqSKyxt4XKSLfiMhO+9+Io13HjfG9KiIFIrK5xb5W4xPLM/a93igiE7pArA+JSLZ9f1NFZFaLY/fZsW4XkbM7Odb+IrJQRLaKyBYR+Y29v8vd2yPE2uXurYj4i8gqEdlgx/qwvT9ZRFbaMb0rIr72fj/7ebp9PKkLxPq6iOxpcV/H2fs7/m/AGNPjH4AXsAtIAXyBDcAIT8d1SIwZQPQh+x4H7rW37wX+7sH4TgEmAJuPFh8wC/gSEGAqsLILxPoQ8PtWzh1h/z34Acn234lXJ8baD5hgb4cAO+yYuty9PUKsXe7e2vcn2N72AVba9+s94Cf2/heAW+3tXwEv2Ns/Ad7txPvaVqyvA5e1cn6H/w30lhLBFCDdGLPbGFMP/A+40MMxtceFwBv29hvARZ4KxBizBCg5ZHdb8V0IvGksPwDhItKvcyJtM9a2XAj8zxhTZ4zZA6Rj/b10CmNMrjFmnb1dAaQB8XTBe3uEWNvisXtr359K+6mP/TDAacAH9v5D72vz/f4AOF3csljxMcXalg7/G+gtiSAe2NfieRZH/gP2BAPMF5G1InKzva+vMSbX3s4D+nomtDa1FV9Xvd+320XpV1tUs3WZWO3qiPFYvwi79L09JFbogvdWRLxEJBUoAL7BKpGUGmMaW4nHFat9vAyI8lSsxpjm+/pX+74+JSJ+h8Zq+9H3tbckgu7gJGPMBOBc4DYROaXlQWOVCbtsX9+uHh/wPDAQGAfkAv/wbDgHE5Fg4EPgTmNMectjXe3ethJrl7y3xhinMWYckIBVEhnm4ZDadGisIjIKuA8r5slAJHCPu96/tySCbKB/i+cJ9r4uwxiTbf9bAHyM9Yeb31zks/8t8FyErWorvi53v40x+fb/bE3AfzhQReHxWEXEB+uL9R1jzEf27i55b1uLtSvfWzu+UmAhcCJWNYp3K/G4YrWPhwHFnRxqy1jPsavijDGmDngNN97X3pIIVgOD7R4DvliNQXM9HJOLiASJSEjzNnAWsBkrxuvs064DPvVMhG1qK765wLV274apQFmLag6POKQO9WKs+wtWrD+xe40kA4OBVZ0YlwCvAGnGmCdbHOpy97atWLvivRWRGBEJt7cDgDOx2jQWApfZpx16X5vv92XAd3ZJzFOxbmvxQ0Cw2jJa3teO/Rtwd4t4V3lgtbTvwKon/KOn4zkkthSs3hUbgC3N8WHVUX4L7AQWAJEejHEOVrG/AatO8sa24sPqzfAv+15vAiZ1gVjfsmPZaP+P1K/F+X+0Y90OnNvJsZ6EVe2zEUi1H7O64r09Qqxd7t4CY4D1dkybgQfs/SlYySgdeB/ws/f728/T7eMpXSDW7+z7uhl4mwM9izr8b0CnmFBKqV6ut1QNKaWUaoMmAqWU6uU0ESilVC+niUAppXo5TQRKKdXLaSJQ3YKIVNr/JonI1R187fsPeb68g677uj0rp5/9PFpEMjro2jNE5POOuJZSmghUd5MEHFMiaDGStC0HJQJjzLRjjOlInMDPO/B6HUJEvDwdg+o6NBGo7uYx4GR7fvbf2pN1PSEiq+3JuW4B1y/mpSIyF9hq7/vEntRvS/PEfiLyGBBgX+8de19z6UPsa28Wa62IK1tce5GIfCAi20TknSPMVPk08NtDk9Ghv+hF5DkRud7ezhCRR+2Y1ojIBBH5WkR2icgvW1wmVES+EGuu/xdExGG//iwRWSEi60TkfXtuoObr/l1E1gGX/5j/CKpnOdovJaW6mnux5r4/H8D+Qi8zxky2q2C+F5H59rkTgFHGmgIZ4OfGmBJ7GP9qEfnQGHOviNxurAm/DnUJ1kRqY4Fo+zVL7GPjgZFADvA9MB1Y1so19tr7fwZ8dgyfc68xZpyIPIU1L/10rNGvm7Hm0Qdr7pkRQCbwFXCJiCwC/gScYYypEpF7gN8Bj9ivKTbW5IZKuWgiUN3dWcAYEWmePyYMa06bemBViyQAcIeIXGxv97fPO9LEYicBc4wxTqxJ4BZjzQRZbl87C/j/7d2xS1tRFMfx7yki3eri7iBS/AMK4lKwf4Pi1tXBoVD8D6T/gIviHjplKjQ4iigISqPSoVARF6FDh5BiaJPjcO6Tl4cW8lwM9/dZkvdC7j0JhJP3LvwuFvHBMzzcuiCkuQAAAWZJREFUCAA+EZk2X0b4XEUW1hkRLdABOmbWK3JpUg0/Uw2NVO8t0RwO0kXKJHBYGvfzCDVIJtQIZNwZsO7uraGTZm+BbuX4HbDg7n/SP+eXT5i3V3re5z+/JXf/kZrFcun0P4ZvzVZrKcYfVOYalOaq5sM48X3sufvqI+V0HzkvGdMagYybDrFNYqEFrFnEI2NmcxYJrlWvgN+pCbwmtvgr/C3eX7EPrKR1iGliC8y66ZmbwMfS8RUwn5I5p4ClGmO+sUjUfQGsEFckR8Cimc3CfbLtXM2aJRNqBDJu2kDfYqPvD8AusRh8YrFZ/TYP/zv/CkyY2Xdiwfmo9NoO0C4Wi0uaab5vRBLkhrvf1Cna3S+Ak9LxNbF/7nl6PK0x7DGwRcQrXwJNd/8FvAcaZtYmbgs92w1Z5HlQ+qiISOZ0RSAikjk1AhGRzKkRiIhkTo1ARCRzagQiIplTIxARyZwagYhI5u4AkeqI8V+sau8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "DWw5o12lXGZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Noisy connections"
      ],
      "metadata": {
        "id": "BV6DAYJxZyLh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Noise = 0.01"
      ],
      "metadata": {
        "id": "DEVirHyfgLiL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_workers = 8\n",
        "all_workers = [i for i in range (num_workers)]"
      ],
      "metadata": {
        "id": "UW2IJ0FLbSc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nets_list = [CNN().to(device)]\n",
        "optimizers = [optim.SGD(nets_list[0].parameters(), lr = 0.1)]\n",
        "# schedulers = [optim.lr_scheduler.MultiStepLR(optimizers[0], milestones=[210], gamma=0.1)]\n",
        "for i in range(1,num_workers):\n",
        "    new_net = CNN().to(device)\n",
        "    new_net.load_state_dict(nets_list[0].state_dict())\n",
        "    nets_list.append(new_net)\n",
        "    optimizer = optim.SGD(new_net.parameters(), lr = 0.1) \n",
        "    optimizers.append(optimizer)\n",
        "    # scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[210], gamma=0.1)\n",
        "    # schedulers.append(scheduler)"
      ],
      "metadata": {
        "id": "SrHazPv5bSc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_network = CNN().to(device)"
      ],
      "metadata": {
        "id": "--558Mp6bSdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "queues_list = [[] for i in range (num_workers)]\n",
        "alpha_list = [1 / num_workers for i in range (num_workers)]"
      ],
      "metadata": {
        "id": "3VNrAny_bSdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_iters = 1000\n",
        "p = 0.5\n",
        "batch_size = 128"
      ],
      "metadata": {
        "id": "pEYH6X8_bSdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_iters = []\n",
        "for i in range(num_workers):\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "                 dataset=mnist_trainset,\n",
        "                 batch_size=batch_size,\n",
        "                 shuffle=True\n",
        "                 )\n",
        "    \n",
        "    data_iters.append(iter(train_loader))"
      ],
      "metadata": {
        "id": "KMmPaga3bSdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader = torch.utils.data.DataLoader(\n",
        "                dataset=mnist_testset,\n",
        "                batch_size=batch_size,\n",
        "                shuffle=False\n",
        "                )"
      ],
      "metadata": {
        "id": "RD8w4TKIbSdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "train_losses = [[] for i in range (num_workers)]\n",
        "test_losses = []\n",
        "test_accuracy = []"
      ],
      "metadata": {
        "id": "0N4wrkGnbSdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comm_matrix = []\n",
        "for i in range (num_workers):\n",
        "    adj_nodes = list(range(num_workers))\n",
        "    del adj_nodes[i]\n",
        "    comm_matrix.append(adj_nodes)"
      ],
      "metadata": {
        "id": "295WPa_SbSdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for iter_no in range(max_iters):\n",
        "    for worker_num in range(num_workers):\n",
        "        train(iter_no, worker_num, with_noise=True)\n",
        "    acc = test(iter_no, nets_list)\n",
        "    if acc >= 96:\n",
        "        break\n",
        "\n",
        "    # for scheduler in schedulers:\n",
        "    #     scheduler.step()    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d4faa33-41a7-41ca-f26e-67d1bb755060",
        "id": "sdcRzcVhbSdA"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training:  Iteration No:  1 Worker Num:  0 \n",
            " Loss:  2.3080592155456543\n",
            "Training:  Iteration No:  1 Worker Num:  1 \n",
            " Loss:  2.3078372478485107\n",
            "Training:  Iteration No:  1 Worker Num:  2 \n",
            " Loss:  2.30560040473938\n",
            "Training:  Iteration No:  1 Worker Num:  3 \n",
            " Loss:  2.3064398765563965\n",
            "Training:  Iteration No:  1 Worker Num:  4 \n",
            " Loss:  2.2940196990966797\n",
            "Training:  Iteration No:  1 Worker Num:  5 \n",
            " Loss:  2.3083548545837402\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            " Loss:  0.4114264742105822\n",
            "Test accuracy:  88.62\n",
            "Training:  Iteration No:  85 Worker Num:  0 \n",
            " Loss:  0.5685581564903259\n",
            "Training:  Iteration No:  85 Worker Num:  1 \n",
            " Loss:  0.5412850975990295\n",
            "Training:  Iteration No:  85 Worker Num:  2 \n",
            " Loss:  0.5660219192504883\n",
            "Training:  Iteration No:  85 Worker Num:  3 \n",
            " Loss:  0.5304107069969177\n",
            "Training:  Iteration No:  85 Worker Num:  4 \n",
            " Loss:  0.5256035327911377\n",
            "Training:  Iteration No:  85 Worker Num:  5 \n",
            " Loss:  0.5269055366516113\n",
            "Training:  Iteration No:  85 Worker Num:  6 \n",
            " Loss:  0.6042199730873108\n",
            "Training:  Iteration No:  85 Worker Num:  7 \n",
            " Loss:  0.5524095296859741\n",
            "Test:  Iteration No:  85 \n",
            " Loss:  0.41026012627761577\n",
            "Test accuracy:  88.72\n",
            "Training:  Iteration No:  86 Worker Num:  0 \n",
            " Loss:  0.589164137840271\n",
            "Training:  Iteration No:  86 Worker Num:  1 \n",
            " Loss:  0.44098833203315735\n",
            "Training:  Iteration No:  86 Worker Num:  2 \n",
            " Loss:  0.5265860557556152\n",
            "Training:  Iteration No:  86 Worker Num:  3 \n",
            " Loss:  0.41395652294158936\n",
            "Training:  Iteration No:  86 Worker Num:  4 \n",
            " Loss:  0.44690272212028503\n",
            "Training:  Iteration No:  86 Worker Num:  5 \n",
            " Loss:  0.5200897455215454\n",
            "Training:  Iteration No:  86 Worker Num:  6 \n",
            " Loss:  0.5335245728492737\n",
            "Training:  Iteration No:  86 Worker Num:  7 \n",
            " Loss:  0.6023864150047302\n",
            "Test:  Iteration No:  86 \n",
            " Loss:  0.4108499132002456\n",
            "Test accuracy:  88.69\n",
            "Training:  Iteration No:  87 Worker Num:  0 \n",
            " Loss:  0.517616868019104\n",
            "Training:  Iteration No:  87 Worker Num:  1 \n",
            " Loss:  0.582964301109314\n",
            "Training:  Iteration No:  87 Worker Num:  2 \n",
            " Loss:  0.3597356975078583\n",
            "Training:  Iteration No:  87 Worker Num:  3 \n",
            " Loss:  0.47649943828582764\n",
            "Training:  Iteration No:  87 Worker Num:  4 \n",
            " Loss:  0.5361695885658264\n",
            "Training:  Iteration No:  87 Worker Num:  5 \n",
            " Loss:  0.43840810656547546\n",
            "Training:  Iteration No:  87 Worker Num:  6 \n",
            " Loss:  0.5616641044616699\n",
            "Training:  Iteration No:  87 Worker Num:  7 \n",
            " Loss:  0.49033188819885254\n",
            "Test:  Iteration No:  87 \n",
            " Loss:  0.4056208254415778\n",
            "Test accuracy:  88.76\n",
            "Training:  Iteration No:  88 Worker Num:  0 \n",
            " Loss:  0.4409131705760956\n",
            "Training:  Iteration No:  88 Worker Num:  1 \n",
            " Loss:  0.3392724096775055\n",
            "Training:  Iteration No:  88 Worker Num:  2 \n",
            " Loss:  0.5529047250747681\n",
            "Training:  Iteration No:  88 Worker Num:  3 \n",
            " Loss:  0.418768048286438\n",
            "Training:  Iteration No:  88 Worker Num:  4 \n",
            " Loss:  0.28589412569999695\n",
            "Training:  Iteration No:  88 Worker Num:  5 \n",
            " Loss:  0.4853478968143463\n",
            "Training:  Iteration No:  88 Worker Num:  6 \n",
            " Loss:  0.5174624919891357\n",
            "Training:  Iteration No:  88 Worker Num:  7 \n",
            " Loss:  0.47396352887153625\n",
            "Test:  Iteration No:  88 \n",
            " Loss:  0.40160768828060056\n",
            "Test accuracy:  88.97\n",
            "Training:  Iteration No:  89 Worker Num:  0 \n",
            " Loss:  0.4140911400318146\n",
            "Training:  Iteration No:  89 Worker Num:  1 \n",
            " Loss:  0.5205097198486328\n",
            "Training:  Iteration No:  89 Worker Num:  2 \n",
            " Loss:  0.6038825511932373\n",
            "Training:  Iteration No:  89 Worker Num:  3 \n",
            " Loss:  0.3978346884250641\n",
            "Training:  Iteration No:  89 Worker Num:  4 \n",
            " Loss:  0.5890278816223145\n",
            "Training:  Iteration No:  89 Worker Num:  5 \n",
            " Loss:  0.5090612173080444\n",
            "Training:  Iteration No:  89 Worker Num:  6 \n",
            " Loss:  0.3160392940044403\n",
            "Training:  Iteration No:  89 Worker Num:  7 \n",
            " Loss:  0.5225951671600342\n",
            "Test:  Iteration No:  89 \n",
            " Loss:  0.39718357539629634\n",
            "Test accuracy:  88.87\n",
            "Training:  Iteration No:  90 Worker Num:  0 \n",
            " Loss:  0.5000755786895752\n",
            "Training:  Iteration No:  90 Worker Num:  1 \n",
            " Loss:  0.3924970030784607\n",
            "Training:  Iteration No:  90 Worker Num:  2 \n",
            " Loss:  0.6201865673065186\n",
            "Training:  Iteration No:  90 Worker Num:  3 \n",
            " Loss:  0.4401992857456207\n",
            "Training:  Iteration No:  90 Worker Num:  4 \n",
            " Loss:  0.4432479441165924\n",
            "Training:  Iteration No:  90 Worker Num:  5 \n",
            " Loss:  0.4245608448982239\n",
            "Training:  Iteration No:  90 Worker Num:  6 \n",
            " Loss:  0.44517993927001953\n",
            "Training:  Iteration No:  90 Worker Num:  7 \n",
            " Loss:  0.5079790353775024\n",
            "Test:  Iteration No:  90 \n",
            " Loss:  0.3911818800659119\n",
            "Test accuracy:  89.14\n",
            "Training:  Iteration No:  91 Worker Num:  0 \n",
            " Loss:  0.5032034516334534\n",
            "Training:  Iteration No:  91 Worker Num:  1 \n",
            " Loss:  0.5814211964607239\n",
            "Training:  Iteration No:  91 Worker Num:  2 \n",
            " Loss:  0.5860657095909119\n",
            "Training:  Iteration No:  91 Worker Num:  3 \n",
            " Loss:  0.4276617169380188\n",
            "Training:  Iteration No:  91 Worker Num:  4 \n",
            " Loss:  0.5727235078811646\n",
            "Training:  Iteration No:  91 Worker Num:  5 \n",
            " Loss:  0.43082886934280396\n",
            "Training:  Iteration No:  91 Worker Num:  6 \n",
            " Loss:  0.4357876181602478\n",
            "Training:  Iteration No:  91 Worker Num:  7 \n",
            " Loss:  0.5991842150688171\n",
            "Test:  Iteration No:  91 \n",
            " Loss:  0.391400761053532\n",
            "Test accuracy:  89.13\n",
            "Training:  Iteration No:  92 Worker Num:  0 \n",
            " Loss:  0.45509645342826843\n",
            "Training:  Iteration No:  92 Worker Num:  1 \n",
            " Loss:  0.49120593070983887\n",
            "Training:  Iteration No:  92 Worker Num:  2 \n",
            " Loss:  0.5067776441574097\n",
            "Training:  Iteration No:  92 Worker Num:  3 \n",
            " Loss:  0.6772189140319824\n",
            "Training:  Iteration No:  92 Worker Num:  4 \n",
            " Loss:  0.36497947573661804\n",
            "Training:  Iteration No:  92 Worker Num:  5 \n",
            " Loss:  0.4554167687892914\n",
            "Training:  Iteration No:  92 Worker Num:  6 \n",
            " Loss:  0.39916059374809265\n",
            "Training:  Iteration No:  92 Worker Num:  7 \n",
            " Loss:  0.42432063817977905\n",
            "Test:  Iteration No:  92 \n",
            " Loss:  0.39046029954016964\n",
            "Test accuracy:  88.73\n",
            "Training:  Iteration No:  93 Worker Num:  0 \n",
            " Loss:  0.5578800439834595\n",
            "Training:  Iteration No:  93 Worker Num:  1 \n",
            " Loss:  0.48947009444236755\n",
            "Training:  Iteration No:  93 Worker Num:  2 \n",
            " Loss:  0.46001410484313965\n",
            "Training:  Iteration No:  93 Worker Num:  3 \n",
            " Loss:  0.5588070154190063\n",
            "Training:  Iteration No:  93 Worker Num:  4 \n",
            " Loss:  0.45285844802856445\n",
            "Training:  Iteration No:  93 Worker Num:  5 \n",
            " Loss:  0.3719915747642517\n",
            "Training:  Iteration No:  93 Worker Num:  6 \n",
            " Loss:  0.3754768967628479\n",
            "Training:  Iteration No:  93 Worker Num:  7 \n",
            " Loss:  0.5428170561790466\n",
            "Test:  Iteration No:  93 \n",
            " Loss:  0.3851531006490128\n",
            "Test accuracy:  89.36\n",
            "Training:  Iteration No:  94 Worker Num:  0 \n",
            " Loss:  0.45392709970474243\n",
            "Training:  Iteration No:  94 Worker Num:  1 \n",
            " Loss:  0.4196529984474182\n",
            "Training:  Iteration No:  94 Worker Num:  2 \n",
            " Loss:  0.6232847571372986\n",
            "Training:  Iteration No:  94 Worker Num:  3 \n",
            " Loss:  0.47103238105773926\n",
            "Training:  Iteration No:  94 Worker Num:  4 \n",
            " Loss:  0.4835488498210907\n",
            "Training:  Iteration No:  94 Worker Num:  5 \n",
            " Loss:  0.5854713916778564\n",
            "Training:  Iteration No:  94 Worker Num:  6 \n",
            " Loss:  0.45013827085494995\n",
            "Training:  Iteration No:  94 Worker Num:  7 \n",
            " Loss:  0.499920517206192\n",
            "Test:  Iteration No:  94 \n",
            " Loss:  0.3819277295017544\n",
            "Test accuracy:  89.25\n",
            "Training:  Iteration No:  95 Worker Num:  0 \n",
            " Loss:  0.47536227107048035\n",
            "Training:  Iteration No:  95 Worker Num:  1 \n",
            " Loss:  0.47181186079978943\n",
            "Training:  Iteration No:  95 Worker Num:  2 \n",
            " Loss:  0.4648408889770508\n",
            "Training:  Iteration No:  95 Worker Num:  3 \n",
            " Loss:  0.384947806596756\n",
            "Training:  Iteration No:  95 Worker Num:  4 \n",
            " Loss:  0.44824013113975525\n",
            "Training:  Iteration No:  95 Worker Num:  5 \n",
            " Loss:  0.34681427478790283\n",
            "Training:  Iteration No:  95 Worker Num:  6 \n",
            " Loss:  0.4358506500720978\n",
            "Training:  Iteration No:  95 Worker Num:  7 \n",
            " Loss:  0.42313647270202637\n",
            "Test:  Iteration No:  95 \n",
            " Loss:  0.37840949876021734\n",
            "Test accuracy:  89.41\n",
            "Training:  Iteration No:  96 Worker Num:  0 \n",
            " Loss:  0.4119056165218353\n",
            "Training:  Iteration No:  96 Worker Num:  1 \n",
            " Loss:  0.4315890967845917\n",
            "Training:  Iteration No:  96 Worker Num:  2 \n",
            " Loss:  0.3472197651863098\n",
            "Training:  Iteration No:  96 Worker Num:  3 \n",
            " Loss:  0.5074952244758606\n",
            "Training:  Iteration No:  96 Worker Num:  4 \n",
            " Loss:  0.3701125383377075\n",
            "Training:  Iteration No:  96 Worker Num:  5 \n",
            " Loss:  0.5286307334899902\n",
            "Training:  Iteration No:  96 Worker Num:  6 \n",
            " Loss:  0.6135156750679016\n",
            "Training:  Iteration No:  96 Worker Num:  7 \n",
            " Loss:  0.46286073327064514\n",
            "Test:  Iteration No:  96 \n",
            " Loss:  0.3830999824442441\n",
            "Test accuracy:  89.41\n",
            "Training:  Iteration No:  97 Worker Num:  0 \n",
            " Loss:  0.43416500091552734\n",
            "Training:  Iteration No:  97 Worker Num:  1 \n",
            " Loss:  0.398553729057312\n",
            "Training:  Iteration No:  97 Worker Num:  2 \n",
            " Loss:  0.5618754625320435\n",
            "Training:  Iteration No:  97 Worker Num:  3 \n",
            " Loss:  0.5414456129074097\n",
            "Training:  Iteration No:  97 Worker Num:  4 \n",
            " Loss:  0.48407992720603943\n",
            "Training:  Iteration No:  97 Worker Num:  5 \n",
            " Loss:  0.34054067730903625\n",
            "Training:  Iteration No:  97 Worker Num:  6 \n",
            " Loss:  0.4012489914894104\n",
            "Training:  Iteration No:  97 Worker Num:  7 \n",
            " Loss:  0.4787853956222534\n",
            "Test:  Iteration No:  97 \n",
            " Loss:  0.3781892650678188\n",
            "Test accuracy:  89.39\n",
            "Training:  Iteration No:  98 Worker Num:  0 \n",
            " Loss:  0.40161579847335815\n",
            "Training:  Iteration No:  98 Worker Num:  1 \n",
            " Loss:  0.48283499479293823\n",
            "Training:  Iteration No:  98 Worker Num:  2 \n",
            " Loss:  0.4354795813560486\n",
            "Training:  Iteration No:  98 Worker Num:  3 \n",
            " Loss:  0.4477405846118927\n",
            "Training:  Iteration No:  98 Worker Num:  4 \n",
            " Loss:  0.36744117736816406\n",
            "Training:  Iteration No:  98 Worker Num:  5 \n",
            " Loss:  0.39331409335136414\n",
            "Training:  Iteration No:  98 Worker Num:  6 \n",
            " Loss:  0.31614598631858826\n",
            "Training:  Iteration No:  98 Worker Num:  7 \n",
            " Loss:  0.5005639791488647\n",
            "Test:  Iteration No:  98 \n",
            " Loss:  0.3818269714335852\n",
            "Test accuracy:  89.02\n",
            "Training:  Iteration No:  99 Worker Num:  0 \n",
            " Loss:  0.3768805265426636\n",
            "Training:  Iteration No:  99 Worker Num:  1 \n",
            " Loss:  0.5269739031791687\n",
            "Training:  Iteration No:  99 Worker Num:  2 \n",
            " Loss:  0.4121726155281067\n",
            "Training:  Iteration No:  99 Worker Num:  3 \n",
            " Loss:  0.36771896481513977\n",
            "Training:  Iteration No:  99 Worker Num:  4 \n",
            " Loss:  0.38355040550231934\n",
            "Training:  Iteration No:  99 Worker Num:  5 \n",
            " Loss:  0.5636204481124878\n",
            "Training:  Iteration No:  99 Worker Num:  6 \n",
            " Loss:  0.35871225595474243\n",
            "Training:  Iteration No:  99 Worker Num:  7 \n",
            " Loss:  0.442471444606781\n",
            "Test:  Iteration No:  99 \n",
            " Loss:  0.3729051883462109\n",
            "Test accuracy:  89.6\n",
            "Training:  Iteration No:  100 Worker Num:  0 \n",
            " Loss:  0.4511388838291168\n",
            "Training:  Iteration No:  100 Worker Num:  1 \n",
            " Loss:  0.35522738099098206\n",
            "Training:  Iteration No:  100 Worker Num:  2 \n",
            " Loss:  0.36572161316871643\n",
            "Training:  Iteration No:  100 Worker Num:  3 \n",
            " Loss:  0.5067676901817322\n",
            "Training:  Iteration No:  100 Worker Num:  4 \n",
            " Loss:  0.46154987812042236\n",
            "Training:  Iteration No:  100 Worker Num:  5 \n",
            " Loss:  0.5056594014167786\n",
            "Training:  Iteration No:  100 Worker Num:  6 \n",
            " Loss:  0.5048732757568359\n",
            "Training:  Iteration No:  100 Worker Num:  7 \n",
            " Loss:  0.3853137493133545\n",
            "Test:  Iteration No:  100 \n",
            " Loss:  0.37861689840313756\n",
            "Test accuracy:  88.85\n",
            "Training:  Iteration No:  101 Worker Num:  0 \n",
            " Loss:  0.4494364559650421\n",
            "Training:  Iteration No:  101 Worker Num:  1 \n",
            " Loss:  0.4320484399795532\n",
            "Training:  Iteration No:  101 Worker Num:  2 \n",
            " Loss:  0.4584411382675171\n",
            "Training:  Iteration No:  101 Worker Num:  3 \n",
            " Loss:  0.3475622832775116\n",
            "Training:  Iteration No:  101 Worker Num:  4 \n",
            " Loss:  0.4441019594669342\n",
            "Training:  Iteration No:  101 Worker Num:  5 \n",
            " Loss:  0.49258217215538025\n",
            "Training:  Iteration No:  101 Worker Num:  6 \n",
            " Loss:  0.5071752667427063\n",
            "Training:  Iteration No:  101 Worker Num:  7 \n",
            " Loss:  0.4332967698574066\n",
            "Test:  Iteration No:  101 \n",
            " Loss:  0.3781314032930362\n",
            "Test accuracy:  89.38\n",
            "Training:  Iteration No:  102 Worker Num:  0 \n",
            " Loss:  0.5076696872711182\n",
            "Training:  Iteration No:  102 Worker Num:  1 \n",
            " Loss:  0.5529308915138245\n",
            "Training:  Iteration No:  102 Worker Num:  2 \n",
            " Loss:  0.5427460670471191\n",
            "Training:  Iteration No:  102 Worker Num:  3 \n",
            " Loss:  0.40406376123428345\n",
            "Training:  Iteration No:  102 Worker Num:  4 \n",
            " Loss:  0.4349857568740845\n",
            "Training:  Iteration No:  102 Worker Num:  5 \n",
            " Loss:  0.4225250780582428\n",
            "Training:  Iteration No:  102 Worker Num:  6 \n",
            " Loss:  0.33620280027389526\n",
            "Training:  Iteration No:  102 Worker Num:  7 \n",
            " Loss:  0.45500481128692627\n",
            "Test:  Iteration No:  102 \n",
            " Loss:  0.3741290047764778\n",
            "Test accuracy:  89.19\n",
            "Training:  Iteration No:  103 Worker Num:  0 \n",
            " Loss:  0.41510844230651855\n",
            "Training:  Iteration No:  103 Worker Num:  1 \n",
            " Loss:  0.3699875473976135\n",
            "Training:  Iteration No:  103 Worker Num:  2 \n",
            " Loss:  0.34856265783309937\n",
            "Training:  Iteration No:  103 Worker Num:  3 \n",
            " Loss:  0.28894782066345215\n",
            "Training:  Iteration No:  103 Worker Num:  4 \n",
            " Loss:  0.3580191433429718\n",
            "Training:  Iteration No:  103 Worker Num:  5 \n",
            " Loss:  0.43383729457855225\n",
            "Training:  Iteration No:  103 Worker Num:  6 \n",
            " Loss:  0.5343992710113525\n",
            "Training:  Iteration No:  103 Worker Num:  7 \n",
            " Loss:  0.3674001693725586\n",
            "Test:  Iteration No:  103 \n",
            " Loss:  0.3714877303831185\n",
            "Test accuracy:  89.45\n",
            "Training:  Iteration No:  104 Worker Num:  0 \n",
            " Loss:  0.5960933566093445\n",
            "Training:  Iteration No:  104 Worker Num:  1 \n",
            " Loss:  0.511297345161438\n",
            "Training:  Iteration No:  104 Worker Num:  2 \n",
            " Loss:  0.5572493672370911\n",
            "Training:  Iteration No:  104 Worker Num:  3 \n",
            " Loss:  0.28929421305656433\n",
            "Training:  Iteration No:  104 Worker Num:  4 \n",
            " Loss:  0.46112072467803955\n",
            "Training:  Iteration No:  104 Worker Num:  5 \n",
            " Loss:  0.45949000120162964\n",
            "Training:  Iteration No:  104 Worker Num:  6 \n",
            " Loss:  0.4456791281700134\n",
            "Training:  Iteration No:  104 Worker Num:  7 \n",
            " Loss:  0.3977762460708618\n",
            "Test:  Iteration No:  104 \n",
            " Loss:  0.36343827829519404\n",
            "Test accuracy:  89.84\n",
            "Training:  Iteration No:  105 Worker Num:  0 \n",
            " Loss:  0.5026838779449463\n",
            "Training:  Iteration No:  105 Worker Num:  1 \n",
            " Loss:  0.3021410405635834\n",
            "Training:  Iteration No:  105 Worker Num:  2 \n",
            " Loss:  0.447805255651474\n",
            "Training:  Iteration No:  105 Worker Num:  3 \n",
            " Loss:  0.3872000575065613\n",
            "Training:  Iteration No:  105 Worker Num:  4 \n",
            " Loss:  0.33861044049263\n",
            "Training:  Iteration No:  105 Worker Num:  5 \n",
            " Loss:  0.47884225845336914\n",
            "Training:  Iteration No:  105 Worker Num:  6 \n",
            " Loss:  0.5761706829071045\n",
            "Training:  Iteration No:  105 Worker Num:  7 \n",
            " Loss:  0.3341269791126251\n",
            "Test:  Iteration No:  105 \n",
            " Loss:  0.3576155354422105\n",
            "Test accuracy:  89.81\n",
            "Training:  Iteration No:  106 Worker Num:  0 \n",
            " Loss:  0.5889544486999512\n",
            "Training:  Iteration No:  106 Worker Num:  1 \n",
            " Loss:  0.5344871878623962\n",
            "Training:  Iteration No:  106 Worker Num:  2 \n",
            " Loss:  0.41750770807266235\n",
            "Training:  Iteration No:  106 Worker Num:  3 \n",
            " Loss:  0.4650763273239136\n",
            "Training:  Iteration No:  106 Worker Num:  4 \n",
            " Loss:  0.4869404137134552\n",
            "Training:  Iteration No:  106 Worker Num:  5 \n",
            " Loss:  0.37600213289260864\n",
            "Training:  Iteration No:  106 Worker Num:  6 \n",
            " Loss:  0.5542713403701782\n",
            "Training:  Iteration No:  106 Worker Num:  7 \n",
            " Loss:  0.35585373640060425\n",
            "Test:  Iteration No:  106 \n",
            " Loss:  0.3616800040855438\n",
            "Test accuracy:  89.9\n",
            "Training:  Iteration No:  107 Worker Num:  0 \n",
            " Loss:  0.4763394296169281\n",
            "Training:  Iteration No:  107 Worker Num:  1 \n",
            " Loss:  0.4630620777606964\n",
            "Training:  Iteration No:  107 Worker Num:  2 \n",
            " Loss:  0.4123302698135376\n",
            "Training:  Iteration No:  107 Worker Num:  3 \n",
            " Loss:  0.5158308744430542\n",
            "Training:  Iteration No:  107 Worker Num:  4 \n",
            " Loss:  0.4723149240016937\n",
            "Training:  Iteration No:  107 Worker Num:  5 \n",
            " Loss:  0.5748963952064514\n",
            "Training:  Iteration No:  107 Worker Num:  6 \n",
            " Loss:  0.3759753704071045\n",
            "Training:  Iteration No:  107 Worker Num:  7 \n",
            " Loss:  0.2897016406059265\n",
            "Test:  Iteration No:  107 \n",
            " Loss:  0.36044112998473493\n",
            "Test accuracy:  89.63\n",
            "Training:  Iteration No:  108 Worker Num:  0 \n",
            " Loss:  0.34029921889305115\n",
            "Training:  Iteration No:  108 Worker Num:  1 \n",
            " Loss:  0.5017938017845154\n",
            "Training:  Iteration No:  108 Worker Num:  2 \n",
            " Loss:  0.4662088453769684\n",
            "Training:  Iteration No:  108 Worker Num:  3 \n",
            " Loss:  0.548708975315094\n",
            "Training:  Iteration No:  108 Worker Num:  4 \n",
            " Loss:  0.3677949905395508\n",
            "Training:  Iteration No:  108 Worker Num:  5 \n",
            " Loss:  0.47313162684440613\n",
            "Training:  Iteration No:  108 Worker Num:  6 \n",
            " Loss:  0.49578967690467834\n",
            "Training:  Iteration No:  108 Worker Num:  7 \n",
            " Loss:  0.5109789967536926\n",
            "Test:  Iteration No:  108 \n",
            " Loss:  0.3669419361254837\n",
            "Test accuracy:  89.58\n",
            "Training:  Iteration No:  109 Worker Num:  0 \n",
            " Loss:  0.43135783076286316\n",
            "Training:  Iteration No:  109 Worker Num:  1 \n",
            " Loss:  0.3861245810985565\n",
            "Training:  Iteration No:  109 Worker Num:  2 \n",
            " Loss:  0.5626035332679749\n",
            "Training:  Iteration No:  109 Worker Num:  3 \n",
            " Loss:  0.40815964341163635\n",
            "Training:  Iteration No:  109 Worker Num:  4 \n",
            " Loss:  0.5003379583358765\n",
            "Training:  Iteration No:  109 Worker Num:  5 \n",
            " Loss:  0.46968257427215576\n",
            "Training:  Iteration No:  109 Worker Num:  6 \n",
            " Loss:  0.5290619134902954\n",
            "Training:  Iteration No:  109 Worker Num:  7 \n",
            " Loss:  0.46295297145843506\n",
            "Test:  Iteration No:  109 \n",
            " Loss:  0.3569801830321173\n",
            "Test accuracy:  89.58\n",
            "Training:  Iteration No:  110 Worker Num:  0 \n",
            " Loss:  0.4930129945278168\n",
            "Training:  Iteration No:  110 Worker Num:  1 \n",
            " Loss:  0.37047579884529114\n",
            "Training:  Iteration No:  110 Worker Num:  2 \n",
            " Loss:  0.4328700006008148\n",
            "Training:  Iteration No:  110 Worker Num:  3 \n",
            " Loss:  0.4540550112724304\n",
            "Training:  Iteration No:  110 Worker Num:  4 \n",
            " Loss:  0.4230271577835083\n",
            "Training:  Iteration No:  110 Worker Num:  5 \n",
            " Loss:  0.46288979053497314\n",
            "Training:  Iteration No:  110 Worker Num:  6 \n",
            " Loss:  0.2911147475242615\n",
            "Training:  Iteration No:  110 Worker Num:  7 \n",
            " Loss:  0.35790401697158813\n",
            "Test:  Iteration No:  110 \n",
            " Loss:  0.3626687196613867\n",
            "Test accuracy:  89.35\n",
            "Training:  Iteration No:  111 Worker Num:  0 \n",
            " Loss:  0.4002387523651123\n",
            "Training:  Iteration No:  111 Worker Num:  1 \n",
            " Loss:  0.5578000545501709\n",
            "Training:  Iteration No:  111 Worker Num:  2 \n",
            " Loss:  0.41325274109840393\n",
            "Training:  Iteration No:  111 Worker Num:  3 \n",
            " Loss:  0.4640989601612091\n",
            "Training:  Iteration No:  111 Worker Num:  4 \n",
            " Loss:  0.5119286179542542\n",
            "Training:  Iteration No:  111 Worker Num:  5 \n",
            " Loss:  0.3435589373111725\n",
            "Training:  Iteration No:  111 Worker Num:  6 \n",
            " Loss:  0.4931739270687103\n",
            "Training:  Iteration No:  111 Worker Num:  7 \n",
            " Loss:  0.39237794280052185\n",
            "Test:  Iteration No:  111 \n",
            " Loss:  0.3518317369154737\n",
            "Test accuracy:  89.83\n",
            "Training:  Iteration No:  112 Worker Num:  0 \n",
            " Loss:  0.37309056520462036\n",
            "Training:  Iteration No:  112 Worker Num:  1 \n",
            " Loss:  0.4345885217189789\n",
            "Training:  Iteration No:  112 Worker Num:  2 \n",
            " Loss:  0.3475176990032196\n",
            "Training:  Iteration No:  112 Worker Num:  3 \n",
            " Loss:  0.443430095911026\n",
            "Training:  Iteration No:  112 Worker Num:  4 \n",
            " Loss:  0.31465616822242737\n",
            "Training:  Iteration No:  112 Worker Num:  5 \n",
            " Loss:  0.4204360544681549\n",
            "Training:  Iteration No:  112 Worker Num:  6 \n",
            " Loss:  0.39228034019470215\n",
            "Training:  Iteration No:  112 Worker Num:  7 \n",
            " Loss:  0.370874285697937\n",
            "Test:  Iteration No:  112 \n",
            " Loss:  0.35014956666133074\n",
            "Test accuracy:  90.16\n",
            "Training:  Iteration No:  113 Worker Num:  0 \n",
            " Loss:  0.3274184465408325\n",
            "Training:  Iteration No:  113 Worker Num:  1 \n",
            " Loss:  0.4801050126552582\n",
            "Training:  Iteration No:  113 Worker Num:  2 \n",
            " Loss:  0.5251989364624023\n",
            "Training:  Iteration No:  113 Worker Num:  3 \n",
            " Loss:  0.4319390058517456\n",
            "Training:  Iteration No:  113 Worker Num:  4 \n",
            " Loss:  0.40541550517082214\n",
            "Training:  Iteration No:  113 Worker Num:  5 \n",
            " Loss:  0.4988371431827545\n",
            "Training:  Iteration No:  113 Worker Num:  6 \n",
            " Loss:  0.4047699570655823\n",
            "Training:  Iteration No:  113 Worker Num:  7 \n",
            " Loss:  0.4021162986755371\n",
            "Test:  Iteration No:  113 \n",
            " Loss:  0.3455227736996699\n",
            "Test accuracy:  90.14\n",
            "Training:  Iteration No:  114 Worker Num:  0 \n",
            " Loss:  0.42717331647872925\n",
            "Training:  Iteration No:  114 Worker Num:  1 \n",
            " Loss:  0.35631805658340454\n",
            "Training:  Iteration No:  114 Worker Num:  2 \n",
            " Loss:  0.4328867495059967\n",
            "Training:  Iteration No:  114 Worker Num:  3 \n",
            " Loss:  0.773630678653717\n",
            "Training:  Iteration No:  114 Worker Num:  4 \n",
            " Loss:  0.39404112100601196\n",
            "Training:  Iteration No:  114 Worker Num:  5 \n",
            " Loss:  0.2670062780380249\n",
            "Training:  Iteration No:  114 Worker Num:  6 \n",
            " Loss:  0.37201792001724243\n",
            "Training:  Iteration No:  114 Worker Num:  7 \n",
            " Loss:  0.4031197428703308\n",
            "Test:  Iteration No:  114 \n",
            " Loss:  0.34325062771198117\n",
            "Test accuracy:  90.26\n",
            "Training:  Iteration No:  115 Worker Num:  0 \n",
            " Loss:  0.3664083480834961\n",
            "Training:  Iteration No:  115 Worker Num:  1 \n",
            " Loss:  0.38824254274368286\n",
            "Training:  Iteration No:  115 Worker Num:  2 \n",
            " Loss:  0.35817915201187134\n",
            "Training:  Iteration No:  115 Worker Num:  3 \n",
            " Loss:  0.3840511441230774\n",
            "Training:  Iteration No:  115 Worker Num:  4 \n",
            " Loss:  0.2899929881095886\n",
            "Training:  Iteration No:  115 Worker Num:  5 \n",
            " Loss:  0.41865164041519165\n",
            "Training:  Iteration No:  115 Worker Num:  6 \n",
            " Loss:  0.3922739624977112\n",
            "Training:  Iteration No:  115 Worker Num:  7 \n",
            " Loss:  0.41862380504608154\n",
            "Test:  Iteration No:  115 \n",
            " Loss:  0.3452038655081127\n",
            "Test accuracy:  89.81\n",
            "Training:  Iteration No:  116 Worker Num:  0 \n",
            " Loss:  0.4078187644481659\n",
            "Training:  Iteration No:  116 Worker Num:  1 \n",
            " Loss:  0.34836867451667786\n",
            "Training:  Iteration No:  116 Worker Num:  2 \n",
            " Loss:  0.4261379539966583\n",
            "Training:  Iteration No:  116 Worker Num:  3 \n",
            " Loss:  0.39072808623313904\n",
            "Training:  Iteration No:  116 Worker Num:  4 \n",
            " Loss:  0.3947696387767792\n",
            "Training:  Iteration No:  116 Worker Num:  5 \n",
            " Loss:  0.42077261209487915\n",
            "Training:  Iteration No:  116 Worker Num:  6 \n",
            " Loss:  0.5717504620552063\n",
            "Training:  Iteration No:  116 Worker Num:  7 \n",
            " Loss:  0.3328981101512909\n",
            "Test:  Iteration No:  116 \n",
            " Loss:  0.34355695484371124\n",
            "Test accuracy:  90.23\n",
            "Training:  Iteration No:  117 Worker Num:  0 \n",
            " Loss:  0.5204576849937439\n",
            "Training:  Iteration No:  117 Worker Num:  1 \n",
            " Loss:  0.3918418288230896\n",
            "Training:  Iteration No:  117 Worker Num:  2 \n",
            " Loss:  0.40789172053337097\n",
            "Training:  Iteration No:  117 Worker Num:  3 \n",
            " Loss:  0.5261842012405396\n",
            "Training:  Iteration No:  117 Worker Num:  4 \n",
            " Loss:  0.5218904614448547\n",
            "Training:  Iteration No:  117 Worker Num:  5 \n",
            " Loss:  0.35959306359291077\n",
            "Training:  Iteration No:  117 Worker Num:  6 \n",
            " Loss:  0.3498622477054596\n",
            "Training:  Iteration No:  117 Worker Num:  7 \n",
            " Loss:  0.4846489429473877\n",
            "Test:  Iteration No:  117 \n",
            " Loss:  0.3399938733988925\n",
            "Test accuracy:  90.22\n",
            "Training:  Iteration No:  118 Worker Num:  0 \n",
            " Loss:  0.40163224935531616\n",
            "Training:  Iteration No:  118 Worker Num:  1 \n",
            " Loss:  0.342966765165329\n",
            "Training:  Iteration No:  118 Worker Num:  2 \n",
            " Loss:  0.3639233410358429\n",
            "Training:  Iteration No:  118 Worker Num:  3 \n",
            " Loss:  0.4000248610973358\n",
            "Training:  Iteration No:  118 Worker Num:  4 \n",
            " Loss:  0.32764872908592224\n",
            "Training:  Iteration No:  118 Worker Num:  5 \n",
            " Loss:  0.48147130012512207\n",
            "Training:  Iteration No:  118 Worker Num:  6 \n",
            " Loss:  0.4698377847671509\n",
            "Training:  Iteration No:  118 Worker Num:  7 \n",
            " Loss:  0.3838748335838318\n",
            "Test:  Iteration No:  118 \n",
            " Loss:  0.337606299055528\n",
            "Test accuracy:  90.29\n",
            "Training:  Iteration No:  119 Worker Num:  0 \n",
            " Loss:  0.6264371275901794\n",
            "Training:  Iteration No:  119 Worker Num:  1 \n",
            " Loss:  0.4844159781932831\n",
            "Training:  Iteration No:  119 Worker Num:  2 \n",
            " Loss:  0.49690261483192444\n",
            "Training:  Iteration No:  119 Worker Num:  3 \n",
            " Loss:  0.3499503433704376\n",
            "Training:  Iteration No:  119 Worker Num:  4 \n",
            " Loss:  0.3727920353412628\n",
            "Training:  Iteration No:  119 Worker Num:  5 \n",
            " Loss:  0.5001592040061951\n",
            "Training:  Iteration No:  119 Worker Num:  6 \n",
            " Loss:  0.5383772253990173\n",
            "Training:  Iteration No:  119 Worker Num:  7 \n",
            " Loss:  0.3790513277053833\n",
            "Test:  Iteration No:  119 \n",
            " Loss:  0.34047834858109677\n",
            "Test accuracy:  90.25\n",
            "Training:  Iteration No:  120 Worker Num:  0 \n",
            " Loss:  0.5170836448669434\n",
            "Training:  Iteration No:  120 Worker Num:  1 \n",
            " Loss:  0.35623297095298767\n",
            "Training:  Iteration No:  120 Worker Num:  2 \n",
            " Loss:  0.5161781311035156\n",
            "Training:  Iteration No:  120 Worker Num:  3 \n",
            " Loss:  0.38757431507110596\n",
            "Training:  Iteration No:  120 Worker Num:  4 \n",
            " Loss:  0.4265786111354828\n",
            "Training:  Iteration No:  120 Worker Num:  5 \n",
            " Loss:  0.4108162820339203\n",
            "Training:  Iteration No:  120 Worker Num:  6 \n",
            " Loss:  0.41502994298934937\n",
            "Training:  Iteration No:  120 Worker Num:  7 \n",
            " Loss:  0.3211011588573456\n",
            "Test:  Iteration No:  120 \n",
            " Loss:  0.334527719718746\n",
            "Test accuracy:  90.36\n",
            "Training:  Iteration No:  121 Worker Num:  0 \n",
            " Loss:  0.404616117477417\n",
            "Training:  Iteration No:  121 Worker Num:  1 \n",
            " Loss:  0.5436691641807556\n",
            "Training:  Iteration No:  121 Worker Num:  2 \n",
            " Loss:  0.44805261492729187\n",
            "Training:  Iteration No:  121 Worker Num:  3 \n",
            " Loss:  0.3806968927383423\n",
            "Training:  Iteration No:  121 Worker Num:  4 \n",
            " Loss:  0.33579012751579285\n",
            "Training:  Iteration No:  121 Worker Num:  5 \n",
            " Loss:  0.35593488812446594\n",
            "Training:  Iteration No:  121 Worker Num:  6 \n",
            " Loss:  0.3365202248096466\n",
            "Training:  Iteration No:  121 Worker Num:  7 \n",
            " Loss:  0.4265280067920685\n",
            "Test:  Iteration No:  121 \n",
            " Loss:  0.33545865750388254\n",
            "Test accuracy:  90.31\n",
            "Training:  Iteration No:  122 Worker Num:  0 \n",
            " Loss:  0.5136222839355469\n",
            "Training:  Iteration No:  122 Worker Num:  1 \n",
            " Loss:  0.30581551790237427\n",
            "Training:  Iteration No:  122 Worker Num:  2 \n",
            " Loss:  0.4219904839992523\n",
            "Training:  Iteration No:  122 Worker Num:  3 \n",
            " Loss:  0.4733441174030304\n",
            "Training:  Iteration No:  122 Worker Num:  4 \n",
            " Loss:  0.3835969865322113\n",
            "Training:  Iteration No:  122 Worker Num:  5 \n",
            " Loss:  0.48124822974205017\n",
            "Training:  Iteration No:  122 Worker Num:  6 \n",
            " Loss:  0.40932372212409973\n",
            "Training:  Iteration No:  122 Worker Num:  7 \n",
            " Loss:  0.4659501910209656\n",
            "Test:  Iteration No:  122 \n",
            " Loss:  0.33122466231071496\n",
            "Test accuracy:  90.34\n",
            "Training:  Iteration No:  123 Worker Num:  0 \n",
            " Loss:  0.4401054084300995\n",
            "Training:  Iteration No:  123 Worker Num:  1 \n",
            " Loss:  0.3675537407398224\n",
            "Training:  Iteration No:  123 Worker Num:  2 \n",
            " Loss:  0.4601813554763794\n",
            "Training:  Iteration No:  123 Worker Num:  3 \n",
            " Loss:  0.3318158984184265\n",
            "Training:  Iteration No:  123 Worker Num:  4 \n",
            " Loss:  0.4846455156803131\n",
            "Training:  Iteration No:  123 Worker Num:  5 \n",
            " Loss:  0.42398709058761597\n",
            "Training:  Iteration No:  123 Worker Num:  6 \n",
            " Loss:  0.47708266973495483\n",
            "Training:  Iteration No:  123 Worker Num:  7 \n",
            " Loss:  0.4158191382884979\n",
            "Test:  Iteration No:  123 \n",
            " Loss:  0.32822042694197423\n",
            "Test accuracy:  90.64\n",
            "Training:  Iteration No:  124 Worker Num:  0 \n",
            " Loss:  0.3816968500614166\n",
            "Training:  Iteration No:  124 Worker Num:  1 \n",
            " Loss:  0.5059499740600586\n",
            "Training:  Iteration No:  124 Worker Num:  2 \n",
            " Loss:  0.4307926595211029\n",
            "Training:  Iteration No:  124 Worker Num:  3 \n",
            " Loss:  0.3516691327095032\n",
            "Training:  Iteration No:  124 Worker Num:  4 \n",
            " Loss:  0.4034463167190552\n",
            "Training:  Iteration No:  124 Worker Num:  5 \n",
            " Loss:  0.45118555426597595\n",
            "Training:  Iteration No:  124 Worker Num:  6 \n",
            " Loss:  0.3632391393184662\n",
            "Training:  Iteration No:  124 Worker Num:  7 \n",
            " Loss:  0.29701539874076843\n",
            "Test:  Iteration No:  124 \n",
            " Loss:  0.327271450923968\n",
            "Test accuracy:  90.61\n",
            "Training:  Iteration No:  125 Worker Num:  0 \n",
            " Loss:  0.3965664803981781\n",
            "Training:  Iteration No:  125 Worker Num:  1 \n",
            " Loss:  0.3474316895008087\n",
            "Training:  Iteration No:  125 Worker Num:  2 \n",
            " Loss:  0.5081539750099182\n",
            "Training:  Iteration No:  125 Worker Num:  3 \n",
            " Loss:  0.48326513171195984\n",
            "Training:  Iteration No:  125 Worker Num:  4 \n",
            " Loss:  0.4432896673679352\n",
            "Training:  Iteration No:  125 Worker Num:  5 \n",
            " Loss:  0.44306352734565735\n",
            "Training:  Iteration No:  125 Worker Num:  6 \n",
            " Loss:  0.497461199760437\n",
            "Training:  Iteration No:  125 Worker Num:  7 \n",
            " Loss:  0.4681380093097687\n",
            "Test:  Iteration No:  125 \n",
            " Loss:  0.33269999746846246\n",
            "Test accuracy:  90.59\n",
            "Training:  Iteration No:  126 Worker Num:  0 \n",
            " Loss:  0.36856594681739807\n",
            "Training:  Iteration No:  126 Worker Num:  1 \n",
            " Loss:  0.4963521957397461\n",
            "Training:  Iteration No:  126 Worker Num:  2 \n",
            " Loss:  0.33273589611053467\n",
            "Training:  Iteration No:  126 Worker Num:  3 \n",
            " Loss:  0.3921144902706146\n",
            "Training:  Iteration No:  126 Worker Num:  4 \n",
            " Loss:  0.3554244935512543\n",
            "Training:  Iteration No:  126 Worker Num:  5 \n",
            " Loss:  0.37701404094696045\n",
            "Training:  Iteration No:  126 Worker Num:  6 \n",
            " Loss:  0.3516015410423279\n",
            "Training:  Iteration No:  126 Worker Num:  7 \n",
            " Loss:  0.3751450777053833\n",
            "Test:  Iteration No:  126 \n",
            " Loss:  0.3236042257822767\n",
            "Test accuracy:  90.52\n",
            "Training:  Iteration No:  127 Worker Num:  0 \n",
            " Loss:  0.37580016255378723\n",
            "Training:  Iteration No:  127 Worker Num:  1 \n",
            " Loss:  0.33231958746910095\n",
            "Training:  Iteration No:  127 Worker Num:  2 \n",
            " Loss:  0.5166302919387817\n",
            "Training:  Iteration No:  127 Worker Num:  3 \n",
            " Loss:  0.29150691628456116\n",
            "Training:  Iteration No:  127 Worker Num:  4 \n",
            " Loss:  0.44397297501564026\n",
            "Training:  Iteration No:  127 Worker Num:  5 \n",
            " Loss:  0.3235560953617096\n",
            "Training:  Iteration No:  127 Worker Num:  6 \n",
            " Loss:  0.3029667139053345\n",
            "Training:  Iteration No:  127 Worker Num:  7 \n",
            " Loss:  0.36183881759643555\n",
            "Test:  Iteration No:  127 \n",
            " Loss:  0.3245360672568219\n",
            "Test accuracy:  90.64\n",
            "Training:  Iteration No:  128 Worker Num:  0 \n",
            " Loss:  0.46844029426574707\n",
            "Training:  Iteration No:  128 Worker Num:  1 \n",
            " Loss:  0.4133940041065216\n",
            "Training:  Iteration No:  128 Worker Num:  2 \n",
            " Loss:  0.38103604316711426\n",
            "Training:  Iteration No:  128 Worker Num:  3 \n",
            " Loss:  0.40018776059150696\n",
            "Training:  Iteration No:  128 Worker Num:  4 \n",
            " Loss:  0.41079747676849365\n",
            "Training:  Iteration No:  128 Worker Num:  5 \n",
            " Loss:  0.4537920355796814\n",
            "Training:  Iteration No:  128 Worker Num:  6 \n",
            " Loss:  0.3860224485397339\n",
            "Training:  Iteration No:  128 Worker Num:  7 \n",
            " Loss:  0.3277580142021179\n",
            "Test:  Iteration No:  128 \n",
            " Loss:  0.32379946056046066\n",
            "Test accuracy:  90.57\n",
            "Training:  Iteration No:  129 Worker Num:  0 \n",
            " Loss:  0.4525849223136902\n",
            "Training:  Iteration No:  129 Worker Num:  1 \n",
            " Loss:  0.3990611135959625\n",
            "Training:  Iteration No:  129 Worker Num:  2 \n",
            " Loss:  0.29635873436927795\n",
            "Training:  Iteration No:  129 Worker Num:  3 \n",
            " Loss:  0.31296664476394653\n",
            "Training:  Iteration No:  129 Worker Num:  4 \n",
            " Loss:  0.39701300859451294\n",
            "Training:  Iteration No:  129 Worker Num:  5 \n",
            " Loss:  0.3115217089653015\n",
            "Training:  Iteration No:  129 Worker Num:  6 \n",
            " Loss:  0.503359854221344\n",
            "Training:  Iteration No:  129 Worker Num:  7 \n",
            " Loss:  0.35848933458328247\n",
            "Test:  Iteration No:  129 \n",
            " Loss:  0.3190370411906816\n",
            "Test accuracy:  90.68\n",
            "Training:  Iteration No:  130 Worker Num:  0 \n",
            " Loss:  0.3139813542366028\n",
            "Training:  Iteration No:  130 Worker Num:  1 \n",
            " Loss:  0.3843725025653839\n",
            "Training:  Iteration No:  130 Worker Num:  2 \n",
            " Loss:  0.36022669076919556\n",
            "Training:  Iteration No:  130 Worker Num:  3 \n",
            " Loss:  0.3838556408882141\n",
            "Training:  Iteration No:  130 Worker Num:  4 \n",
            " Loss:  0.4459717869758606\n",
            "Training:  Iteration No:  130 Worker Num:  5 \n",
            " Loss:  0.32560256123542786\n",
            "Training:  Iteration No:  130 Worker Num:  6 \n",
            " Loss:  0.35627415776252747\n",
            "Training:  Iteration No:  130 Worker Num:  7 \n",
            " Loss:  0.3478242754936218\n",
            "Test:  Iteration No:  130 \n",
            " Loss:  0.31977394487284405\n",
            "Test accuracy:  90.77\n",
            "Training:  Iteration No:  131 Worker Num:  0 \n",
            " Loss:  0.5614237785339355\n",
            "Training:  Iteration No:  131 Worker Num:  1 \n",
            " Loss:  0.5693809390068054\n",
            "Training:  Iteration No:  131 Worker Num:  2 \n",
            " Loss:  0.3512824475765228\n",
            "Training:  Iteration No:  131 Worker Num:  3 \n",
            " Loss:  0.5277174711227417\n",
            "Training:  Iteration No:  131 Worker Num:  4 \n",
            " Loss:  0.35926640033721924\n",
            "Training:  Iteration No:  131 Worker Num:  5 \n",
            " Loss:  0.36243167519569397\n",
            "Training:  Iteration No:  131 Worker Num:  6 \n",
            " Loss:  0.32007911801338196\n",
            "Training:  Iteration No:  131 Worker Num:  7 \n",
            " Loss:  0.5063248872756958\n",
            "Test:  Iteration No:  131 \n",
            " Loss:  0.31498051326297505\n",
            "Test accuracy:  90.88\n",
            "Training:  Iteration No:  132 Worker Num:  0 \n",
            " Loss:  0.4174884557723999\n",
            "Training:  Iteration No:  132 Worker Num:  1 \n",
            " Loss:  0.38577795028686523\n",
            "Training:  Iteration No:  132 Worker Num:  2 \n",
            " Loss:  0.46539542078971863\n",
            "Training:  Iteration No:  132 Worker Num:  3 \n",
            " Loss:  0.3592638075351715\n",
            "Training:  Iteration No:  132 Worker Num:  4 \n",
            " Loss:  0.41868582367897034\n",
            "Training:  Iteration No:  132 Worker Num:  5 \n",
            " Loss:  0.40409916639328003\n",
            "Training:  Iteration No:  132 Worker Num:  6 \n",
            " Loss:  0.3863513171672821\n",
            "Training:  Iteration No:  132 Worker Num:  7 \n",
            " Loss:  0.36410951614379883\n",
            "Test:  Iteration No:  132 \n",
            " Loss:  0.3213890490558329\n",
            "Test accuracy:  90.74\n",
            "Training:  Iteration No:  133 Worker Num:  0 \n",
            " Loss:  0.3770807683467865\n",
            "Training:  Iteration No:  133 Worker Num:  1 \n",
            " Loss:  0.4285775125026703\n",
            "Training:  Iteration No:  133 Worker Num:  2 \n",
            " Loss:  0.4006524085998535\n",
            "Training:  Iteration No:  133 Worker Num:  3 \n",
            " Loss:  0.34733250737190247\n",
            "Training:  Iteration No:  133 Worker Num:  4 \n",
            " Loss:  0.5206336975097656\n",
            "Training:  Iteration No:  133 Worker Num:  5 \n",
            " Loss:  0.36457547545433044\n",
            "Training:  Iteration No:  133 Worker Num:  6 \n",
            " Loss:  0.449513703584671\n",
            "Training:  Iteration No:  133 Worker Num:  7 \n",
            " Loss:  0.336666464805603\n",
            "Test:  Iteration No:  133 \n",
            " Loss:  0.31259321944811674\n",
            "Test accuracy:  91.2\n",
            "Training:  Iteration No:  134 Worker Num:  0 \n",
            " Loss:  0.2815110683441162\n",
            "Training:  Iteration No:  134 Worker Num:  1 \n",
            " Loss:  0.5458188056945801\n",
            "Training:  Iteration No:  134 Worker Num:  2 \n",
            " Loss:  0.4484916031360626\n",
            "Training:  Iteration No:  134 Worker Num:  3 \n",
            " Loss:  0.3150302469730377\n",
            "Training:  Iteration No:  134 Worker Num:  4 \n",
            " Loss:  0.34649890661239624\n",
            "Training:  Iteration No:  134 Worker Num:  5 \n",
            " Loss:  0.4248413145542145\n",
            "Training:  Iteration No:  134 Worker Num:  6 \n",
            " Loss:  0.2671363949775696\n",
            "Training:  Iteration No:  134 Worker Num:  7 \n",
            " Loss:  0.34457436203956604\n",
            "Test:  Iteration No:  134 \n",
            " Loss:  0.31210857761811606\n",
            "Test accuracy:  91.05\n",
            "Training:  Iteration No:  135 Worker Num:  0 \n",
            " Loss:  0.44395890831947327\n",
            "Training:  Iteration No:  135 Worker Num:  1 \n",
            " Loss:  0.349802166223526\n",
            "Training:  Iteration No:  135 Worker Num:  2 \n",
            " Loss:  0.350606232881546\n",
            "Training:  Iteration No:  135 Worker Num:  3 \n",
            " Loss:  0.35833388566970825\n",
            "Training:  Iteration No:  135 Worker Num:  4 \n",
            " Loss:  0.32988372445106506\n",
            "Training:  Iteration No:  135 Worker Num:  5 \n",
            " Loss:  0.3794674575328827\n",
            "Training:  Iteration No:  135 Worker Num:  6 \n",
            " Loss:  0.3697074055671692\n",
            "Training:  Iteration No:  135 Worker Num:  7 \n",
            " Loss:  0.49999815225601196\n",
            "Test:  Iteration No:  135 \n",
            " Loss:  0.3114044306493258\n",
            "Test accuracy:  91.18\n",
            "Training:  Iteration No:  136 Worker Num:  0 \n",
            " Loss:  0.31789499521255493\n",
            "Training:  Iteration No:  136 Worker Num:  1 \n",
            " Loss:  0.3486878573894501\n",
            "Training:  Iteration No:  136 Worker Num:  2 \n",
            " Loss:  0.37346351146698\n",
            "Training:  Iteration No:  136 Worker Num:  3 \n",
            " Loss:  0.541125476360321\n",
            "Training:  Iteration No:  136 Worker Num:  4 \n",
            " Loss:  0.37729987502098083\n",
            "Training:  Iteration No:  136 Worker Num:  5 \n",
            " Loss:  0.4495430290699005\n",
            "Training:  Iteration No:  136 Worker Num:  6 \n",
            " Loss:  0.41732028126716614\n",
            "Training:  Iteration No:  136 Worker Num:  7 \n",
            " Loss:  0.24181097745895386\n",
            "Test:  Iteration No:  136 \n",
            " Loss:  0.3091546039132378\n",
            "Test accuracy:  90.94\n",
            "Training:  Iteration No:  137 Worker Num:  0 \n",
            " Loss:  0.3181081712245941\n",
            "Training:  Iteration No:  137 Worker Num:  1 \n",
            " Loss:  0.2970714867115021\n",
            "Training:  Iteration No:  137 Worker Num:  2 \n",
            " Loss:  0.3044164478778839\n",
            "Training:  Iteration No:  137 Worker Num:  3 \n",
            " Loss:  0.4126085638999939\n",
            "Training:  Iteration No:  137 Worker Num:  4 \n",
            " Loss:  0.42120808362960815\n",
            "Training:  Iteration No:  137 Worker Num:  5 \n",
            " Loss:  0.38976994156837463\n",
            "Training:  Iteration No:  137 Worker Num:  6 \n",
            " Loss:  0.3737691640853882\n",
            "Training:  Iteration No:  137 Worker Num:  7 \n",
            " Loss:  0.34809625148773193\n",
            "Test:  Iteration No:  137 \n",
            " Loss:  0.31174491888171507\n",
            "Test accuracy:  90.87\n",
            "Training:  Iteration No:  138 Worker Num:  0 \n",
            " Loss:  0.45421329140663147\n",
            "Training:  Iteration No:  138 Worker Num:  1 \n",
            " Loss:  0.2659268081188202\n",
            "Training:  Iteration No:  138 Worker Num:  2 \n",
            " Loss:  0.4748315215110779\n",
            "Training:  Iteration No:  138 Worker Num:  3 \n",
            " Loss:  0.4466820955276489\n",
            "Training:  Iteration No:  138 Worker Num:  4 \n",
            " Loss:  0.5568307042121887\n",
            "Training:  Iteration No:  138 Worker Num:  5 \n",
            " Loss:  0.46636757254600525\n",
            "Training:  Iteration No:  138 Worker Num:  6 \n",
            " Loss:  0.3843083083629608\n",
            "Training:  Iteration No:  138 Worker Num:  7 \n",
            " Loss:  0.3260888159275055\n",
            "Test:  Iteration No:  138 \n",
            " Loss:  0.30640161768356455\n",
            "Test accuracy:  91.2\n",
            "Training:  Iteration No:  139 Worker Num:  0 \n",
            " Loss:  0.38044294714927673\n",
            "Training:  Iteration No:  139 Worker Num:  1 \n",
            " Loss:  0.30409735441207886\n",
            "Training:  Iteration No:  139 Worker Num:  2 \n",
            " Loss:  0.24046623706817627\n",
            "Training:  Iteration No:  139 Worker Num:  3 \n",
            " Loss:  0.35319703817367554\n",
            "Training:  Iteration No:  139 Worker Num:  4 \n",
            " Loss:  0.348965585231781\n",
            "Training:  Iteration No:  139 Worker Num:  5 \n",
            " Loss:  0.3495417535305023\n",
            "Training:  Iteration No:  139 Worker Num:  6 \n",
            " Loss:  0.4599743187427521\n",
            "Training:  Iteration No:  139 Worker Num:  7 \n",
            " Loss:  0.39195680618286133\n",
            "Test:  Iteration No:  139 \n",
            " Loss:  0.30891296508002886\n",
            "Test accuracy:  90.76\n",
            "Training:  Iteration No:  140 Worker Num:  0 \n",
            " Loss:  0.4842560291290283\n",
            "Training:  Iteration No:  140 Worker Num:  1 \n",
            " Loss:  0.33963096141815186\n",
            "Training:  Iteration No:  140 Worker Num:  2 \n",
            " Loss:  0.44438374042510986\n",
            "Training:  Iteration No:  140 Worker Num:  3 \n",
            " Loss:  0.33540260791778564\n",
            "Training:  Iteration No:  140 Worker Num:  4 \n",
            " Loss:  0.4754841923713684\n",
            "Training:  Iteration No:  140 Worker Num:  5 \n",
            " Loss:  0.3770720660686493\n",
            "Training:  Iteration No:  140 Worker Num:  6 \n",
            " Loss:  0.42296937108039856\n",
            "Training:  Iteration No:  140 Worker Num:  7 \n",
            " Loss:  0.6192021369934082\n",
            "Test:  Iteration No:  140 \n",
            " Loss:  0.3021441318000419\n",
            "Test accuracy:  91.43\n",
            "Training:  Iteration No:  141 Worker Num:  0 \n",
            " Loss:  0.4026770293712616\n",
            "Training:  Iteration No:  141 Worker Num:  1 \n",
            " Loss:  0.3449263870716095\n",
            "Training:  Iteration No:  141 Worker Num:  2 \n",
            " Loss:  0.2571490705013275\n",
            "Training:  Iteration No:  141 Worker Num:  3 \n",
            " Loss:  0.25383540987968445\n",
            "Training:  Iteration No:  141 Worker Num:  4 \n",
            " Loss:  0.5835128426551819\n",
            "Training:  Iteration No:  141 Worker Num:  5 \n",
            " Loss:  0.44092416763305664\n",
            "Training:  Iteration No:  141 Worker Num:  6 \n",
            " Loss:  0.29890352487564087\n",
            "Training:  Iteration No:  141 Worker Num:  7 \n",
            " Loss:  0.5142807960510254\n",
            "Test:  Iteration No:  141 \n",
            " Loss:  0.3056706032138082\n",
            "Test accuracy:  91.17\n",
            "Training:  Iteration No:  142 Worker Num:  0 \n",
            " Loss:  0.49721643328666687\n",
            "Training:  Iteration No:  142 Worker Num:  1 \n",
            " Loss:  0.5808718800544739\n",
            "Training:  Iteration No:  142 Worker Num:  2 \n",
            " Loss:  0.43001314997673035\n",
            "Training:  Iteration No:  142 Worker Num:  3 \n",
            " Loss:  0.39869239926338196\n",
            "Training:  Iteration No:  142 Worker Num:  4 \n",
            " Loss:  0.3399161994457245\n",
            "Training:  Iteration No:  142 Worker Num:  5 \n",
            " Loss:  0.3455007076263428\n",
            "Training:  Iteration No:  142 Worker Num:  6 \n",
            " Loss:  0.2771466374397278\n",
            "Training:  Iteration No:  142 Worker Num:  7 \n",
            " Loss:  0.3312903344631195\n",
            "Test:  Iteration No:  142 \n",
            " Loss:  0.29707994514816927\n",
            "Test accuracy:  91.4\n",
            "Training:  Iteration No:  143 Worker Num:  0 \n",
            " Loss:  0.27886632084846497\n",
            "Training:  Iteration No:  143 Worker Num:  1 \n",
            " Loss:  0.35037803649902344\n",
            "Training:  Iteration No:  143 Worker Num:  2 \n",
            " Loss:  0.3999550938606262\n",
            "Training:  Iteration No:  143 Worker Num:  3 \n",
            " Loss:  0.3799825608730316\n",
            "Training:  Iteration No:  143 Worker Num:  4 \n",
            " Loss:  0.36778882145881653\n",
            "Training:  Iteration No:  143 Worker Num:  5 \n",
            " Loss:  0.35352540016174316\n",
            "Training:  Iteration No:  143 Worker Num:  6 \n",
            " Loss:  0.2663843333721161\n",
            "Training:  Iteration No:  143 Worker Num:  7 \n",
            " Loss:  0.3582489490509033\n",
            "Test:  Iteration No:  143 \n",
            " Loss:  0.2957200758819339\n",
            "Test accuracy:  91.42\n",
            "Training:  Iteration No:  144 Worker Num:  0 \n",
            " Loss:  0.40384554862976074\n",
            "Training:  Iteration No:  144 Worker Num:  1 \n",
            " Loss:  0.3579421639442444\n",
            "Training:  Iteration No:  144 Worker Num:  2 \n",
            " Loss:  0.4592968225479126\n",
            "Training:  Iteration No:  144 Worker Num:  3 \n",
            " Loss:  0.3144500255584717\n",
            "Training:  Iteration No:  144 Worker Num:  4 \n",
            " Loss:  0.3631998896598816\n",
            "Training:  Iteration No:  144 Worker Num:  5 \n",
            " Loss:  0.30432218313217163\n",
            "Training:  Iteration No:  144 Worker Num:  6 \n",
            " Loss:  0.41719678044319153\n",
            "Training:  Iteration No:  144 Worker Num:  7 \n",
            " Loss:  0.2944703996181488\n",
            "Test:  Iteration No:  144 \n",
            " Loss:  0.2964533299584932\n",
            "Test accuracy:  91.39\n",
            "Training:  Iteration No:  145 Worker Num:  0 \n",
            " Loss:  0.332407683134079\n",
            "Training:  Iteration No:  145 Worker Num:  1 \n",
            " Loss:  0.2648433446884155\n",
            "Training:  Iteration No:  145 Worker Num:  2 \n",
            " Loss:  0.2560728192329407\n",
            "Training:  Iteration No:  145 Worker Num:  3 \n",
            " Loss:  0.42727088928222656\n",
            "Training:  Iteration No:  145 Worker Num:  4 \n",
            " Loss:  0.38991498947143555\n",
            "Training:  Iteration No:  145 Worker Num:  5 \n",
            " Loss:  0.40065208077430725\n",
            "Training:  Iteration No:  145 Worker Num:  6 \n",
            " Loss:  0.39788880944252014\n",
            "Training:  Iteration No:  145 Worker Num:  7 \n",
            " Loss:  0.46310314536094666\n",
            "Test:  Iteration No:  145 \n",
            " Loss:  0.29502652236555194\n",
            "Test accuracy:  91.54\n",
            "Training:  Iteration No:  146 Worker Num:  0 \n",
            " Loss:  0.41162267327308655\n",
            "Training:  Iteration No:  146 Worker Num:  1 \n",
            " Loss:  0.25441521406173706\n",
            "Training:  Iteration No:  146 Worker Num:  2 \n",
            " Loss:  0.4259527623653412\n",
            "Training:  Iteration No:  146 Worker Num:  3 \n",
            " Loss:  0.41675904393196106\n",
            "Training:  Iteration No:  146 Worker Num:  4 \n",
            " Loss:  0.3849335014820099\n",
            "Training:  Iteration No:  146 Worker Num:  5 \n",
            " Loss:  0.39542171359062195\n",
            "Training:  Iteration No:  146 Worker Num:  6 \n",
            " Loss:  0.35448306798934937\n",
            "Training:  Iteration No:  146 Worker Num:  7 \n",
            " Loss:  0.4349062740802765\n",
            "Test:  Iteration No:  146 \n",
            " Loss:  0.296174564528503\n",
            "Test accuracy:  91.27\n",
            "Training:  Iteration No:  147 Worker Num:  0 \n",
            " Loss:  0.3576517403125763\n",
            "Training:  Iteration No:  147 Worker Num:  1 \n",
            " Loss:  0.32326415181159973\n",
            "Training:  Iteration No:  147 Worker Num:  2 \n",
            " Loss:  0.2332375943660736\n",
            "Training:  Iteration No:  147 Worker Num:  3 \n",
            " Loss:  0.4341442286968231\n",
            "Training:  Iteration No:  147 Worker Num:  4 \n",
            " Loss:  0.4248496890068054\n",
            "Training:  Iteration No:  147 Worker Num:  5 \n",
            " Loss:  0.27671095728874207\n",
            "Training:  Iteration No:  147 Worker Num:  6 \n",
            " Loss:  0.20847363770008087\n",
            "Training:  Iteration No:  147 Worker Num:  7 \n",
            " Loss:  0.243323415517807\n",
            "Test:  Iteration No:  147 \n",
            " Loss:  0.2942878226219099\n",
            "Test accuracy:  91.37\n",
            "Training:  Iteration No:  148 Worker Num:  0 \n",
            " Loss:  0.37767183780670166\n",
            "Training:  Iteration No:  148 Worker Num:  1 \n",
            " Loss:  0.4382741451263428\n",
            "Training:  Iteration No:  148 Worker Num:  2 \n",
            " Loss:  0.39194929599761963\n",
            "Training:  Iteration No:  148 Worker Num:  3 \n",
            " Loss:  0.3548089265823364\n",
            "Training:  Iteration No:  148 Worker Num:  4 \n",
            " Loss:  0.4040004014968872\n",
            "Training:  Iteration No:  148 Worker Num:  5 \n",
            " Loss:  0.41867366433143616\n",
            "Training:  Iteration No:  148 Worker Num:  6 \n",
            " Loss:  0.37250208854675293\n",
            "Training:  Iteration No:  148 Worker Num:  7 \n",
            " Loss:  0.2791517674922943\n",
            "Test:  Iteration No:  148 \n",
            " Loss:  0.29775046660930293\n",
            "Test accuracy:  91.19\n",
            "Training:  Iteration No:  149 Worker Num:  0 \n",
            " Loss:  0.2635626196861267\n",
            "Training:  Iteration No:  149 Worker Num:  1 \n",
            " Loss:  0.3973042964935303\n",
            "Training:  Iteration No:  149 Worker Num:  2 \n",
            " Loss:  0.27448904514312744\n",
            "Training:  Iteration No:  149 Worker Num:  3 \n",
            " Loss:  0.327181875705719\n",
            "Training:  Iteration No:  149 Worker Num:  4 \n",
            " Loss:  0.5579759478569031\n",
            "Training:  Iteration No:  149 Worker Num:  5 \n",
            " Loss:  0.3756149113178253\n",
            "Training:  Iteration No:  149 Worker Num:  6 \n",
            " Loss:  0.3329997658729553\n",
            "Training:  Iteration No:  149 Worker Num:  7 \n",
            " Loss:  0.30464258790016174\n",
            "Test:  Iteration No:  149 \n",
            " Loss:  0.29178408526261396\n",
            "Test accuracy:  91.44\n",
            "Training:  Iteration No:  150 Worker Num:  0 \n",
            " Loss:  0.30780765414237976\n",
            "Training:  Iteration No:  150 Worker Num:  1 \n",
            " Loss:  0.38282930850982666\n",
            "Training:  Iteration No:  150 Worker Num:  2 \n",
            " Loss:  0.5309692025184631\n",
            "Training:  Iteration No:  150 Worker Num:  3 \n",
            " Loss:  0.3083360195159912\n",
            "Training:  Iteration No:  150 Worker Num:  4 \n",
            " Loss:  0.2574562132358551\n",
            "Training:  Iteration No:  150 Worker Num:  5 \n",
            " Loss:  0.4167247414588928\n",
            "Training:  Iteration No:  150 Worker Num:  6 \n",
            " Loss:  0.37468966841697693\n",
            "Training:  Iteration No:  150 Worker Num:  7 \n",
            " Loss:  0.38317859172821045\n",
            "Test:  Iteration No:  150 \n",
            " Loss:  0.29945036773628825\n",
            "Test accuracy:  91.21\n",
            "Training:  Iteration No:  151 Worker Num:  0 \n",
            " Loss:  0.4257909059524536\n",
            "Training:  Iteration No:  151 Worker Num:  1 \n",
            " Loss:  0.28449591994285583\n",
            "Training:  Iteration No:  151 Worker Num:  2 \n",
            " Loss:  0.3123633563518524\n",
            "Training:  Iteration No:  151 Worker Num:  3 \n",
            " Loss:  0.3569953441619873\n",
            "Training:  Iteration No:  151 Worker Num:  4 \n",
            " Loss:  0.39954227209091187\n",
            "Training:  Iteration No:  151 Worker Num:  5 \n",
            " Loss:  0.49626532196998596\n",
            "Training:  Iteration No:  151 Worker Num:  6 \n",
            " Loss:  0.3635348975658417\n",
            "Training:  Iteration No:  151 Worker Num:  7 \n",
            " Loss:  0.2875949442386627\n",
            "Test:  Iteration No:  151 \n",
            " Loss:  0.2877189797125285\n",
            "Test accuracy:  91.63\n",
            "Training:  Iteration No:  152 Worker Num:  0 \n",
            " Loss:  0.4112290143966675\n",
            "Training:  Iteration No:  152 Worker Num:  1 \n",
            " Loss:  0.3929094672203064\n",
            "Training:  Iteration No:  152 Worker Num:  2 \n",
            " Loss:  0.4193851947784424\n",
            "Training:  Iteration No:  152 Worker Num:  3 \n",
            " Loss:  0.2974427342414856\n",
            "Training:  Iteration No:  152 Worker Num:  4 \n",
            " Loss:  0.372403085231781\n",
            "Training:  Iteration No:  152 Worker Num:  5 \n",
            " Loss:  0.31472864747047424\n",
            "Training:  Iteration No:  152 Worker Num:  6 \n",
            " Loss:  0.27093374729156494\n",
            "Training:  Iteration No:  152 Worker Num:  7 \n",
            " Loss:  0.4347924590110779\n",
            "Test:  Iteration No:  152 \n",
            " Loss:  0.28733939243645606\n",
            "Test accuracy:  91.39\n",
            "Training:  Iteration No:  153 Worker Num:  0 \n",
            " Loss:  0.2902088761329651\n",
            "Training:  Iteration No:  153 Worker Num:  1 \n",
            " Loss:  0.3178217113018036\n",
            "Training:  Iteration No:  153 Worker Num:  2 \n",
            " Loss:  0.41386711597442627\n",
            "Training:  Iteration No:  153 Worker Num:  3 \n",
            " Loss:  0.40070781111717224\n",
            "Training:  Iteration No:  153 Worker Num:  4 \n",
            " Loss:  0.27983358502388\n",
            "Training:  Iteration No:  153 Worker Num:  5 \n",
            " Loss:  0.2500181794166565\n",
            "Training:  Iteration No:  153 Worker Num:  6 \n",
            " Loss:  0.2833271324634552\n",
            "Training:  Iteration No:  153 Worker Num:  7 \n",
            " Loss:  0.3527336120605469\n",
            "Test:  Iteration No:  153 \n",
            " Loss:  0.28378761533789243\n",
            "Test accuracy:  91.74\n",
            "Training:  Iteration No:  154 Worker Num:  0 \n",
            " Loss:  0.37170863151550293\n",
            "Training:  Iteration No:  154 Worker Num:  1 \n",
            " Loss:  0.4913782477378845\n",
            "Training:  Iteration No:  154 Worker Num:  2 \n",
            " Loss:  0.3856368362903595\n",
            "Training:  Iteration No:  154 Worker Num:  3 \n",
            " Loss:  0.2983119785785675\n",
            "Training:  Iteration No:  154 Worker Num:  4 \n",
            " Loss:  0.5010201334953308\n",
            "Training:  Iteration No:  154 Worker Num:  5 \n",
            " Loss:  0.4411168396472931\n",
            "Training:  Iteration No:  154 Worker Num:  6 \n",
            " Loss:  0.4936766028404236\n",
            "Training:  Iteration No:  154 Worker Num:  7 \n",
            " Loss:  0.35547956824302673\n",
            "Test:  Iteration No:  154 \n",
            " Loss:  0.28759952240823944\n",
            "Test accuracy:  91.55\n",
            "Training:  Iteration No:  155 Worker Num:  0 \n",
            " Loss:  0.4016586244106293\n",
            "Training:  Iteration No:  155 Worker Num:  1 \n",
            " Loss:  0.3944114148616791\n",
            "Training:  Iteration No:  155 Worker Num:  2 \n",
            " Loss:  0.3402317464351654\n",
            "Training:  Iteration No:  155 Worker Num:  3 \n",
            " Loss:  0.3673090636730194\n",
            "Training:  Iteration No:  155 Worker Num:  4 \n",
            " Loss:  0.4045785963535309\n",
            "Training:  Iteration No:  155 Worker Num:  5 \n",
            " Loss:  0.2862221598625183\n",
            "Training:  Iteration No:  155 Worker Num:  6 \n",
            " Loss:  0.2969985604286194\n",
            "Training:  Iteration No:  155 Worker Num:  7 \n",
            " Loss:  0.3185715079307556\n",
            "Test:  Iteration No:  155 \n",
            " Loss:  0.28526782407130624\n",
            "Test accuracy:  91.87\n",
            "Training:  Iteration No:  156 Worker Num:  0 \n",
            " Loss:  0.2976286709308624\n",
            "Training:  Iteration No:  156 Worker Num:  1 \n",
            " Loss:  0.29509180784225464\n",
            "Training:  Iteration No:  156 Worker Num:  2 \n",
            " Loss:  0.21569181978702545\n",
            "Training:  Iteration No:  156 Worker Num:  3 \n",
            " Loss:  0.26699066162109375\n",
            "Training:  Iteration No:  156 Worker Num:  4 \n",
            " Loss:  0.23129841685295105\n",
            "Training:  Iteration No:  156 Worker Num:  5 \n",
            " Loss:  0.45438507199287415\n",
            "Training:  Iteration No:  156 Worker Num:  6 \n",
            " Loss:  0.33198603987693787\n",
            "Training:  Iteration No:  156 Worker Num:  7 \n",
            " Loss:  0.39420464634895325\n",
            "Test:  Iteration No:  156 \n",
            " Loss:  0.28369017747006836\n",
            "Test accuracy:  91.72\n",
            "Training:  Iteration No:  157 Worker Num:  0 \n",
            " Loss:  0.2610878646373749\n",
            "Training:  Iteration No:  157 Worker Num:  1 \n",
            " Loss:  0.39208662509918213\n",
            "Training:  Iteration No:  157 Worker Num:  2 \n",
            " Loss:  0.2860049307346344\n",
            "Training:  Iteration No:  157 Worker Num:  3 \n",
            " Loss:  0.259464830160141\n",
            "Training:  Iteration No:  157 Worker Num:  4 \n",
            " Loss:  0.7526042461395264\n",
            "Training:  Iteration No:  157 Worker Num:  5 \n",
            " Loss:  0.4125458002090454\n",
            "Training:  Iteration No:  157 Worker Num:  6 \n",
            " Loss:  0.3457028567790985\n",
            "Training:  Iteration No:  157 Worker Num:  7 \n",
            " Loss:  0.4696767330169678\n",
            "Test:  Iteration No:  157 \n",
            " Loss:  0.2833980323770378\n",
            "Test accuracy:  91.56\n",
            "Training:  Iteration No:  158 Worker Num:  0 \n",
            " Loss:  0.37799569964408875\n",
            "Training:  Iteration No:  158 Worker Num:  1 \n",
            " Loss:  0.44790178537368774\n",
            "Training:  Iteration No:  158 Worker Num:  2 \n",
            " Loss:  0.26893651485443115\n",
            "Training:  Iteration No:  158 Worker Num:  3 \n",
            " Loss:  0.3359769284725189\n",
            "Training:  Iteration No:  158 Worker Num:  4 \n",
            " Loss:  0.27327704429626465\n",
            "Training:  Iteration No:  158 Worker Num:  5 \n",
            " Loss:  0.36273616552352905\n",
            "Training:  Iteration No:  158 Worker Num:  6 \n",
            " Loss:  0.4751177132129669\n",
            "Training:  Iteration No:  158 Worker Num:  7 \n",
            " Loss:  0.2980678677558899\n",
            "Test:  Iteration No:  158 \n",
            " Loss:  0.27973643397983117\n",
            "Test accuracy:  91.93\n",
            "Training:  Iteration No:  159 Worker Num:  0 \n",
            " Loss:  0.3014146089553833\n",
            "Training:  Iteration No:  159 Worker Num:  1 \n",
            " Loss:  0.347465455532074\n",
            "Training:  Iteration No:  159 Worker Num:  2 \n",
            " Loss:  0.3169468641281128\n",
            "Training:  Iteration No:  159 Worker Num:  3 \n",
            " Loss:  0.43061015009880066\n",
            "Training:  Iteration No:  159 Worker Num:  4 \n",
            " Loss:  0.35259348154067993\n",
            "Training:  Iteration No:  159 Worker Num:  5 \n",
            " Loss:  0.2663966417312622\n",
            "Training:  Iteration No:  159 Worker Num:  6 \n",
            " Loss:  0.23889537155628204\n",
            "Training:  Iteration No:  159 Worker Num:  7 \n",
            " Loss:  0.31813961267471313\n",
            "Test:  Iteration No:  159 \n",
            " Loss:  0.2797997193764659\n",
            "Test accuracy:  91.69\n",
            "Training:  Iteration No:  160 Worker Num:  0 \n",
            " Loss:  0.21859994530677795\n",
            "Training:  Iteration No:  160 Worker Num:  1 \n",
            " Loss:  0.3664579689502716\n",
            "Training:  Iteration No:  160 Worker Num:  2 \n",
            " Loss:  0.3039158582687378\n",
            "Training:  Iteration No:  160 Worker Num:  3 \n",
            " Loss:  0.3107793629169464\n",
            "Training:  Iteration No:  160 Worker Num:  4 \n",
            " Loss:  0.24724021553993225\n",
            "Training:  Iteration No:  160 Worker Num:  5 \n",
            " Loss:  0.285536527633667\n",
            "Training:  Iteration No:  160 Worker Num:  6 \n",
            " Loss:  0.27180975675582886\n",
            "Training:  Iteration No:  160 Worker Num:  7 \n",
            " Loss:  0.26271578669548035\n",
            "Test:  Iteration No:  160 \n",
            " Loss:  0.27403375699738913\n",
            "Test accuracy:  92.0\n",
            "Training:  Iteration No:  161 Worker Num:  0 \n",
            " Loss:  0.29487451910972595\n",
            "Training:  Iteration No:  161 Worker Num:  1 \n",
            " Loss:  0.352303147315979\n",
            "Training:  Iteration No:  161 Worker Num:  2 \n",
            " Loss:  0.24628949165344238\n",
            "Training:  Iteration No:  161 Worker Num:  3 \n",
            " Loss:  0.34936994314193726\n",
            "Training:  Iteration No:  161 Worker Num:  4 \n",
            " Loss:  0.4075581431388855\n",
            "Training:  Iteration No:  161 Worker Num:  5 \n",
            " Loss:  0.2648630440235138\n",
            "Training:  Iteration No:  161 Worker Num:  6 \n",
            " Loss:  0.35417428612709045\n",
            "Training:  Iteration No:  161 Worker Num:  7 \n",
            " Loss:  0.3056299388408661\n",
            "Test:  Iteration No:  161 \n",
            " Loss:  0.27593612998629674\n",
            "Test accuracy:  91.96\n",
            "Training:  Iteration No:  162 Worker Num:  0 \n",
            " Loss:  0.38524705171585083\n",
            "Training:  Iteration No:  162 Worker Num:  1 \n",
            " Loss:  0.2865215837955475\n",
            "Training:  Iteration No:  162 Worker Num:  2 \n",
            " Loss:  0.45349180698394775\n",
            "Training:  Iteration No:  162 Worker Num:  3 \n",
            " Loss:  0.3725072741508484\n",
            "Training:  Iteration No:  162 Worker Num:  4 \n",
            " Loss:  0.4309099316596985\n",
            "Training:  Iteration No:  162 Worker Num:  5 \n",
            " Loss:  0.21429026126861572\n",
            "Training:  Iteration No:  162 Worker Num:  6 \n",
            " Loss:  0.2896937429904938\n",
            "Training:  Iteration No:  162 Worker Num:  7 \n",
            " Loss:  0.38466691970825195\n",
            "Test:  Iteration No:  162 \n",
            " Loss:  0.27601611331293857\n",
            "Test accuracy:  91.94\n",
            "Training:  Iteration No:  163 Worker Num:  0 \n",
            " Loss:  0.38672804832458496\n",
            "Training:  Iteration No:  163 Worker Num:  1 \n",
            " Loss:  0.4195004105567932\n",
            "Training:  Iteration No:  163 Worker Num:  2 \n",
            " Loss:  0.372088223695755\n",
            "Training:  Iteration No:  163 Worker Num:  3 \n",
            " Loss:  0.37091365456581116\n",
            "Training:  Iteration No:  163 Worker Num:  4 \n",
            " Loss:  0.316664457321167\n",
            "Training:  Iteration No:  163 Worker Num:  5 \n",
            " Loss:  0.3209272623062134\n",
            "Training:  Iteration No:  163 Worker Num:  6 \n",
            " Loss:  0.39394649863243103\n",
            "Training:  Iteration No:  163 Worker Num:  7 \n",
            " Loss:  0.26482391357421875\n",
            "Test:  Iteration No:  163 \n",
            " Loss:  0.28119626219231114\n",
            "Test accuracy:  91.79\n",
            "Training:  Iteration No:  164 Worker Num:  0 \n",
            " Loss:  0.2708906829357147\n",
            "Training:  Iteration No:  164 Worker Num:  1 \n",
            " Loss:  0.3451361060142517\n",
            "Training:  Iteration No:  164 Worker Num:  2 \n",
            " Loss:  0.33098071813583374\n",
            "Training:  Iteration No:  164 Worker Num:  3 \n",
            " Loss:  0.2055237591266632\n",
            "Training:  Iteration No:  164 Worker Num:  4 \n",
            " Loss:  0.45624732971191406\n",
            "Training:  Iteration No:  164 Worker Num:  5 \n",
            " Loss:  0.22737015783786774\n",
            "Training:  Iteration No:  164 Worker Num:  6 \n",
            " Loss:  0.40440961718559265\n",
            "Training:  Iteration No:  164 Worker Num:  7 \n",
            " Loss:  0.43476060032844543\n",
            "Test:  Iteration No:  164 \n",
            " Loss:  0.2762706354967778\n",
            "Test accuracy:  91.91\n",
            "Training:  Iteration No:  165 Worker Num:  0 \n",
            " Loss:  0.25681108236312866\n",
            "Training:  Iteration No:  165 Worker Num:  1 \n",
            " Loss:  0.3968783915042877\n",
            "Training:  Iteration No:  165 Worker Num:  2 \n",
            " Loss:  0.31116634607315063\n",
            "Training:  Iteration No:  165 Worker Num:  3 \n",
            " Loss:  0.37481215596199036\n",
            "Training:  Iteration No:  165 Worker Num:  4 \n",
            " Loss:  0.31125080585479736\n",
            "Training:  Iteration No:  165 Worker Num:  5 \n",
            " Loss:  0.299416184425354\n",
            "Training:  Iteration No:  165 Worker Num:  6 \n",
            " Loss:  0.244026318192482\n",
            "Training:  Iteration No:  165 Worker Num:  7 \n",
            " Loss:  0.31920239329338074\n",
            "Test:  Iteration No:  165 \n",
            " Loss:  0.2718658615375244\n",
            "Test accuracy:  92.18\n",
            "Training:  Iteration No:  166 Worker Num:  0 \n",
            " Loss:  0.2925550937652588\n",
            "Training:  Iteration No:  166 Worker Num:  1 \n",
            " Loss:  0.44089367985725403\n",
            "Training:  Iteration No:  166 Worker Num:  2 \n",
            " Loss:  0.4273812770843506\n",
            "Training:  Iteration No:  166 Worker Num:  3 \n",
            " Loss:  0.22137701511383057\n",
            "Training:  Iteration No:  166 Worker Num:  4 \n",
            " Loss:  0.34651562571525574\n",
            "Training:  Iteration No:  166 Worker Num:  5 \n",
            " Loss:  0.3006022274494171\n",
            "Training:  Iteration No:  166 Worker Num:  6 \n",
            " Loss:  0.44421225786209106\n",
            "Training:  Iteration No:  166 Worker Num:  7 \n",
            " Loss:  0.45227324962615967\n",
            "Test:  Iteration No:  166 \n",
            " Loss:  0.2695396213167453\n",
            "Test accuracy:  91.97\n",
            "Training:  Iteration No:  167 Worker Num:  0 \n",
            " Loss:  0.2268732637166977\n",
            "Training:  Iteration No:  167 Worker Num:  1 \n",
            " Loss:  0.3721183240413666\n",
            "Training:  Iteration No:  167 Worker Num:  2 \n",
            " Loss:  0.33294638991355896\n",
            "Training:  Iteration No:  167 Worker Num:  3 \n",
            " Loss:  0.26070094108581543\n",
            "Training:  Iteration No:  167 Worker Num:  4 \n",
            " Loss:  0.24070271849632263\n",
            "Training:  Iteration No:  167 Worker Num:  5 \n",
            " Loss:  0.4403831958770752\n",
            "Training:  Iteration No:  167 Worker Num:  6 \n",
            " Loss:  0.2923872768878937\n",
            "Training:  Iteration No:  167 Worker Num:  7 \n",
            " Loss:  0.2841988801956177\n",
            "Test:  Iteration No:  167 \n",
            " Loss:  0.27350188433369504\n",
            "Test accuracy:  91.95\n",
            "Training:  Iteration No:  168 Worker Num:  0 \n",
            " Loss:  0.4358840882778168\n",
            "Training:  Iteration No:  168 Worker Num:  1 \n",
            " Loss:  0.31446439027786255\n",
            "Training:  Iteration No:  168 Worker Num:  2 \n",
            " Loss:  0.35047826170921326\n",
            "Training:  Iteration No:  168 Worker Num:  3 \n",
            " Loss:  0.29534971714019775\n",
            "Training:  Iteration No:  168 Worker Num:  4 \n",
            " Loss:  0.5364689826965332\n",
            "Training:  Iteration No:  168 Worker Num:  5 \n",
            " Loss:  0.3529422879219055\n",
            "Training:  Iteration No:  168 Worker Num:  6 \n",
            " Loss:  0.3298928737640381\n",
            "Training:  Iteration No:  168 Worker Num:  7 \n",
            " Loss:  0.3856869339942932\n",
            "Test:  Iteration No:  168 \n",
            " Loss:  0.2741410447921179\n",
            "Test accuracy:  91.62\n",
            "Training:  Iteration No:  169 Worker Num:  0 \n",
            " Loss:  0.2411891222000122\n",
            "Training:  Iteration No:  169 Worker Num:  1 \n",
            " Loss:  0.33689555525779724\n",
            "Training:  Iteration No:  169 Worker Num:  2 \n",
            " Loss:  0.33023184537887573\n",
            "Training:  Iteration No:  169 Worker Num:  3 \n",
            " Loss:  0.5365840792655945\n",
            "Training:  Iteration No:  169 Worker Num:  4 \n",
            " Loss:  0.33369794487953186\n",
            "Training:  Iteration No:  169 Worker Num:  5 \n",
            " Loss:  0.3350203037261963\n",
            "Training:  Iteration No:  169 Worker Num:  6 \n",
            " Loss:  0.4499760866165161\n",
            "Training:  Iteration No:  169 Worker Num:  7 \n",
            " Loss:  0.3524746894836426\n",
            "Test:  Iteration No:  169 \n",
            " Loss:  0.26924686284759375\n",
            "Test accuracy:  92.26\n",
            "Training:  Iteration No:  170 Worker Num:  0 \n",
            " Loss:  0.26284223794937134\n",
            "Training:  Iteration No:  170 Worker Num:  1 \n",
            " Loss:  0.33404263854026794\n",
            "Training:  Iteration No:  170 Worker Num:  2 \n",
            " Loss:  0.31667089462280273\n",
            "Training:  Iteration No:  170 Worker Num:  3 \n",
            " Loss:  0.3550838232040405\n",
            "Training:  Iteration No:  170 Worker Num:  4 \n",
            " Loss:  0.3313237130641937\n",
            "Training:  Iteration No:  170 Worker Num:  5 \n",
            " Loss:  0.24921253323554993\n",
            "Training:  Iteration No:  170 Worker Num:  6 \n",
            " Loss:  0.3125876486301422\n",
            "Training:  Iteration No:  170 Worker Num:  7 \n",
            " Loss:  0.29290881752967834\n",
            "Test:  Iteration No:  170 \n",
            " Loss:  0.2646742880438702\n",
            "Test accuracy:  92.26\n",
            "Training:  Iteration No:  171 Worker Num:  0 \n",
            " Loss:  0.2508004307746887\n",
            "Training:  Iteration No:  171 Worker Num:  1 \n",
            " Loss:  0.35267287492752075\n",
            "Training:  Iteration No:  171 Worker Num:  2 \n",
            " Loss:  0.18882054090499878\n",
            "Training:  Iteration No:  171 Worker Num:  3 \n",
            " Loss:  0.38371992111206055\n",
            "Training:  Iteration No:  171 Worker Num:  4 \n",
            " Loss:  0.3665165603160858\n",
            "Training:  Iteration No:  171 Worker Num:  5 \n",
            " Loss:  0.3924025893211365\n",
            "Training:  Iteration No:  171 Worker Num:  6 \n",
            " Loss:  0.1993398517370224\n",
            "Training:  Iteration No:  171 Worker Num:  7 \n",
            " Loss:  0.2798857092857361\n",
            "Test:  Iteration No:  171 \n",
            " Loss:  0.2680018459788606\n",
            "Test accuracy:  92.11\n",
            "Training:  Iteration No:  172 Worker Num:  0 \n",
            " Loss:  0.4369346499443054\n",
            "Training:  Iteration No:  172 Worker Num:  1 \n",
            " Loss:  0.2811068296432495\n",
            "Training:  Iteration No:  172 Worker Num:  2 \n",
            " Loss:  0.33249497413635254\n",
            "Training:  Iteration No:  172 Worker Num:  3 \n",
            " Loss:  0.26511120796203613\n",
            "Training:  Iteration No:  172 Worker Num:  4 \n",
            " Loss:  0.38659098744392395\n",
            "Training:  Iteration No:  172 Worker Num:  5 \n",
            " Loss:  0.34101977944374084\n",
            "Training:  Iteration No:  172 Worker Num:  6 \n",
            " Loss:  0.35551801323890686\n",
            "Training:  Iteration No:  172 Worker Num:  7 \n",
            " Loss:  0.4011026620864868\n",
            "Test:  Iteration No:  172 \n",
            " Loss:  0.2679650308210639\n",
            "Test accuracy:  92.37\n",
            "Training:  Iteration No:  173 Worker Num:  0 \n",
            " Loss:  0.3942698836326599\n",
            "Training:  Iteration No:  173 Worker Num:  1 \n",
            " Loss:  0.2266910970211029\n",
            "Training:  Iteration No:  173 Worker Num:  2 \n",
            " Loss:  0.31137362122535706\n",
            "Training:  Iteration No:  173 Worker Num:  3 \n",
            " Loss:  0.31890201568603516\n",
            "Training:  Iteration No:  173 Worker Num:  4 \n",
            " Loss:  0.33426469564437866\n",
            "Training:  Iteration No:  173 Worker Num:  5 \n",
            " Loss:  0.32195571064949036\n",
            "Training:  Iteration No:  173 Worker Num:  6 \n",
            " Loss:  0.29305586218833923\n",
            "Training:  Iteration No:  173 Worker Num:  7 \n",
            " Loss:  0.26233091950416565\n",
            "Test:  Iteration No:  173 \n",
            " Loss:  0.2680636174693892\n",
            "Test accuracy:  91.85\n",
            "Training:  Iteration No:  174 Worker Num:  0 \n",
            " Loss:  0.2686425447463989\n",
            "Training:  Iteration No:  174 Worker Num:  1 \n",
            " Loss:  0.3273582458496094\n",
            "Training:  Iteration No:  174 Worker Num:  2 \n",
            " Loss:  0.39739352464675903\n",
            "Training:  Iteration No:  174 Worker Num:  3 \n",
            " Loss:  0.3181571066379547\n",
            "Training:  Iteration No:  174 Worker Num:  4 \n",
            " Loss:  0.3234156668186188\n",
            "Training:  Iteration No:  174 Worker Num:  5 \n",
            " Loss:  0.3098691999912262\n",
            "Training:  Iteration No:  174 Worker Num:  6 \n",
            " Loss:  0.24494397640228271\n",
            "Training:  Iteration No:  174 Worker Num:  7 \n",
            " Loss:  0.3029868006706238\n",
            "Test:  Iteration No:  174 \n",
            " Loss:  0.2619755035384169\n",
            "Test accuracy:  92.31\n",
            "Training:  Iteration No:  175 Worker Num:  0 \n",
            " Loss:  0.28862544894218445\n",
            "Training:  Iteration No:  175 Worker Num:  1 \n",
            " Loss:  0.3080054819583893\n",
            "Training:  Iteration No:  175 Worker Num:  2 \n",
            " Loss:  0.4101508557796478\n",
            "Training:  Iteration No:  175 Worker Num:  3 \n",
            " Loss:  0.33996137976646423\n",
            "Training:  Iteration No:  175 Worker Num:  4 \n",
            " Loss:  0.2553585171699524\n",
            "Training:  Iteration No:  175 Worker Num:  5 \n",
            " Loss:  0.4024793207645416\n",
            "Training:  Iteration No:  175 Worker Num:  6 \n",
            " Loss:  0.3208911716938019\n",
            "Training:  Iteration No:  175 Worker Num:  7 \n",
            " Loss:  0.35245558619499207\n",
            "Test:  Iteration No:  175 \n",
            " Loss:  0.26023878390558913\n",
            "Test accuracy:  92.36\n",
            "Training:  Iteration No:  176 Worker Num:  0 \n",
            " Loss:  0.2392660677433014\n",
            "Training:  Iteration No:  176 Worker Num:  1 \n",
            " Loss:  0.3641088902950287\n",
            "Training:  Iteration No:  176 Worker Num:  2 \n",
            " Loss:  0.23350270092487335\n",
            "Training:  Iteration No:  176 Worker Num:  3 \n",
            " Loss:  0.4333655834197998\n",
            "Training:  Iteration No:  176 Worker Num:  4 \n",
            " Loss:  0.31215327978134155\n",
            "Training:  Iteration No:  176 Worker Num:  5 \n",
            " Loss:  0.2817484736442566\n",
            "Training:  Iteration No:  176 Worker Num:  6 \n",
            " Loss:  0.3132283687591553\n",
            "Training:  Iteration No:  176 Worker Num:  7 \n",
            " Loss:  0.2509353756904602\n",
            "Test:  Iteration No:  176 \n",
            " Loss:  0.26444092965861665\n",
            "Test accuracy:  91.95\n",
            "Training:  Iteration No:  177 Worker Num:  0 \n",
            " Loss:  0.3459189236164093\n",
            "Training:  Iteration No:  177 Worker Num:  1 \n",
            " Loss:  0.2596190869808197\n",
            "Training:  Iteration No:  177 Worker Num:  2 \n",
            " Loss:  0.3337770998477936\n",
            "Training:  Iteration No:  177 Worker Num:  3 \n",
            " Loss:  0.40713047981262207\n",
            "Training:  Iteration No:  177 Worker Num:  4 \n",
            " Loss:  0.31416308879852295\n",
            "Training:  Iteration No:  177 Worker Num:  5 \n",
            " Loss:  0.4373212158679962\n",
            "Training:  Iteration No:  177 Worker Num:  6 \n",
            " Loss:  0.3548581004142761\n",
            "Training:  Iteration No:  177 Worker Num:  7 \n",
            " Loss:  0.3175395131111145\n",
            "Test:  Iteration No:  177 \n",
            " Loss:  0.2609796060508565\n",
            "Test accuracy:  92.31\n",
            "Training:  Iteration No:  178 Worker Num:  0 \n",
            " Loss:  0.3373824954032898\n",
            "Training:  Iteration No:  178 Worker Num:  1 \n",
            " Loss:  0.20944884419441223\n",
            "Training:  Iteration No:  178 Worker Num:  2 \n",
            " Loss:  0.3014693558216095\n",
            "Training:  Iteration No:  178 Worker Num:  3 \n",
            " Loss:  0.34019696712493896\n",
            "Training:  Iteration No:  178 Worker Num:  4 \n",
            " Loss:  0.4065280556678772\n",
            "Training:  Iteration No:  178 Worker Num:  5 \n",
            " Loss:  0.393510639667511\n",
            "Training:  Iteration No:  178 Worker Num:  6 \n",
            " Loss:  0.35762640833854675\n",
            "Training:  Iteration No:  178 Worker Num:  7 \n",
            " Loss:  0.24325865507125854\n",
            "Test:  Iteration No:  178 \n",
            " Loss:  0.25448087987171697\n",
            "Test accuracy:  92.53\n",
            "Training:  Iteration No:  179 Worker Num:  0 \n",
            " Loss:  0.25896933674812317\n",
            "Training:  Iteration No:  179 Worker Num:  1 \n",
            " Loss:  0.17795419692993164\n",
            "Training:  Iteration No:  179 Worker Num:  2 \n",
            " Loss:  0.34986478090286255\n",
            "Training:  Iteration No:  179 Worker Num:  3 \n",
            " Loss:  0.3239130675792694\n",
            "Training:  Iteration No:  179 Worker Num:  4 \n",
            " Loss:  0.29919418692588806\n",
            "Training:  Iteration No:  179 Worker Num:  5 \n",
            " Loss:  0.3611617088317871\n",
            "Training:  Iteration No:  179 Worker Num:  6 \n",
            " Loss:  0.24627920985221863\n",
            "Training:  Iteration No:  179 Worker Num:  7 \n",
            " Loss:  0.3715793192386627\n",
            "Test:  Iteration No:  179 \n",
            " Loss:  0.2521314611776343\n",
            "Test accuracy:  92.6\n",
            "Training:  Iteration No:  180 Worker Num:  0 \n",
            " Loss:  0.2771450877189636\n",
            "Training:  Iteration No:  180 Worker Num:  1 \n",
            " Loss:  0.35478106141090393\n",
            "Training:  Iteration No:  180 Worker Num:  2 \n",
            " Loss:  0.34242478013038635\n",
            "Training:  Iteration No:  180 Worker Num:  3 \n",
            " Loss:  0.2964100241661072\n",
            "Training:  Iteration No:  180 Worker Num:  4 \n",
            " Loss:  0.387883722782135\n",
            "Training:  Iteration No:  180 Worker Num:  5 \n",
            " Loss:  0.33532601594924927\n",
            "Training:  Iteration No:  180 Worker Num:  6 \n",
            " Loss:  0.2581494450569153\n",
            "Training:  Iteration No:  180 Worker Num:  7 \n",
            " Loss:  0.33619335293769836\n",
            "Test:  Iteration No:  180 \n",
            " Loss:  0.2555473312216846\n",
            "Test accuracy:  92.57\n",
            "Training:  Iteration No:  181 Worker Num:  0 \n",
            " Loss:  0.3928315341472626\n",
            "Training:  Iteration No:  181 Worker Num:  1 \n",
            " Loss:  0.2842160165309906\n",
            "Training:  Iteration No:  181 Worker Num:  2 \n",
            " Loss:  0.2516425848007202\n",
            "Training:  Iteration No:  181 Worker Num:  3 \n",
            " Loss:  0.34915342926979065\n",
            "Training:  Iteration No:  181 Worker Num:  4 \n",
            " Loss:  0.24552962183952332\n",
            "Training:  Iteration No:  181 Worker Num:  5 \n",
            " Loss:  0.3571570813655853\n",
            "Training:  Iteration No:  181 Worker Num:  6 \n",
            " Loss:  0.3536478281021118\n",
            "Training:  Iteration No:  181 Worker Num:  7 \n",
            " Loss:  0.30673637986183167\n",
            "Test:  Iteration No:  181 \n",
            " Loss:  0.24949206965916518\n",
            "Test accuracy:  92.72\n",
            "Training:  Iteration No:  182 Worker Num:  0 \n",
            " Loss:  0.33986979722976685\n",
            "Training:  Iteration No:  182 Worker Num:  1 \n",
            " Loss:  0.29009154438972473\n",
            "Training:  Iteration No:  182 Worker Num:  2 \n",
            " Loss:  0.31590816378593445\n",
            "Training:  Iteration No:  182 Worker Num:  3 \n",
            " Loss:  0.26449692249298096\n",
            "Training:  Iteration No:  182 Worker Num:  4 \n",
            " Loss:  0.3216572403907776\n",
            "Training:  Iteration No:  182 Worker Num:  5 \n",
            " Loss:  0.39089763164520264\n",
            "Training:  Iteration No:  182 Worker Num:  6 \n",
            " Loss:  0.39826589822769165\n",
            "Training:  Iteration No:  182 Worker Num:  7 \n",
            " Loss:  0.426840215921402\n",
            "Test:  Iteration No:  182 \n",
            " Loss:  0.2497265624424702\n",
            "Test accuracy:  92.68\n",
            "Training:  Iteration No:  183 Worker Num:  0 \n",
            " Loss:  0.3273552656173706\n",
            "Training:  Iteration No:  183 Worker Num:  1 \n",
            " Loss:  0.266533762216568\n",
            "Training:  Iteration No:  183 Worker Num:  2 \n",
            " Loss:  0.31405413150787354\n",
            "Training:  Iteration No:  183 Worker Num:  3 \n",
            " Loss:  0.2720184028148651\n",
            "Training:  Iteration No:  183 Worker Num:  4 \n",
            " Loss:  0.42001432180404663\n",
            "Training:  Iteration No:  183 Worker Num:  5 \n",
            " Loss:  0.3143920302391052\n",
            "Training:  Iteration No:  183 Worker Num:  6 \n",
            " Loss:  0.27189314365386963\n",
            "Training:  Iteration No:  183 Worker Num:  7 \n",
            " Loss:  0.27095597982406616\n",
            "Test:  Iteration No:  183 \n",
            " Loss:  0.25064852956352357\n",
            "Test accuracy:  92.55\n",
            "Training:  Iteration No:  184 Worker Num:  0 \n",
            " Loss:  0.39208948612213135\n",
            "Training:  Iteration No:  184 Worker Num:  1 \n",
            " Loss:  0.34573933482170105\n",
            "Training:  Iteration No:  184 Worker Num:  2 \n",
            " Loss:  0.2695704996585846\n",
            "Training:  Iteration No:  184 Worker Num:  3 \n",
            " Loss:  0.2920301854610443\n",
            "Training:  Iteration No:  184 Worker Num:  4 \n",
            " Loss:  0.36889418959617615\n",
            "Training:  Iteration No:  184 Worker Num:  5 \n",
            " Loss:  0.24965831637382507\n",
            "Training:  Iteration No:  184 Worker Num:  6 \n",
            " Loss:  0.2573659420013428\n",
            "Training:  Iteration No:  184 Worker Num:  7 \n",
            " Loss:  0.18533314764499664\n",
            "Test:  Iteration No:  184 \n",
            " Loss:  0.2525572437745861\n",
            "Test accuracy:  92.51\n",
            "Training:  Iteration No:  185 Worker Num:  0 \n",
            " Loss:  0.36576196551322937\n",
            "Training:  Iteration No:  185 Worker Num:  1 \n",
            " Loss:  0.32824161648750305\n",
            "Training:  Iteration No:  185 Worker Num:  2 \n",
            " Loss:  0.27711403369903564\n",
            "Training:  Iteration No:  185 Worker Num:  3 \n",
            " Loss:  0.3575754165649414\n",
            "Training:  Iteration No:  185 Worker Num:  4 \n",
            " Loss:  0.25998079776763916\n",
            "Training:  Iteration No:  185 Worker Num:  5 \n",
            " Loss:  0.3137885332107544\n",
            "Training:  Iteration No:  185 Worker Num:  6 \n",
            " Loss:  0.2969565987586975\n",
            "Training:  Iteration No:  185 Worker Num:  7 \n",
            " Loss:  0.2144288271665573\n",
            "Test:  Iteration No:  185 \n",
            " Loss:  0.24981217303230793\n",
            "Test accuracy:  92.43\n",
            "Training:  Iteration No:  186 Worker Num:  0 \n",
            " Loss:  0.2662474513053894\n",
            "Training:  Iteration No:  186 Worker Num:  1 \n",
            " Loss:  0.2786663770675659\n",
            "Training:  Iteration No:  186 Worker Num:  2 \n",
            " Loss:  0.3210233747959137\n",
            "Training:  Iteration No:  186 Worker Num:  3 \n",
            " Loss:  0.2980242371559143\n",
            "Training:  Iteration No:  186 Worker Num:  4 \n",
            " Loss:  0.2130560725927353\n",
            "Training:  Iteration No:  186 Worker Num:  5 \n",
            " Loss:  0.34292250871658325\n",
            "Training:  Iteration No:  186 Worker Num:  6 \n",
            " Loss:  0.3073917329311371\n",
            "Training:  Iteration No:  186 Worker Num:  7 \n",
            " Loss:  0.23156534135341644\n",
            "Test:  Iteration No:  186 \n",
            " Loss:  0.24503233901496174\n",
            "Test accuracy:  92.7\n",
            "Training:  Iteration No:  187 Worker Num:  0 \n",
            " Loss:  0.2773013412952423\n",
            "Training:  Iteration No:  187 Worker Num:  1 \n",
            " Loss:  0.26297345757484436\n",
            "Training:  Iteration No:  187 Worker Num:  2 \n",
            " Loss:  0.188771590590477\n",
            "Training:  Iteration No:  187 Worker Num:  3 \n",
            " Loss:  0.24057608842849731\n",
            "Training:  Iteration No:  187 Worker Num:  4 \n",
            " Loss:  0.1530236303806305\n",
            "Training:  Iteration No:  187 Worker Num:  5 \n",
            " Loss:  0.3359608054161072\n",
            "Training:  Iteration No:  187 Worker Num:  6 \n",
            " Loss:  0.26154521107673645\n",
            "Training:  Iteration No:  187 Worker Num:  7 \n",
            " Loss:  0.32114413380622864\n",
            "Test:  Iteration No:  187 \n",
            " Loss:  0.2504615972077922\n",
            "Test accuracy:  92.43\n",
            "Training:  Iteration No:  188 Worker Num:  0 \n",
            " Loss:  0.4047456681728363\n",
            "Training:  Iteration No:  188 Worker Num:  1 \n",
            " Loss:  0.24592454731464386\n",
            "Training:  Iteration No:  188 Worker Num:  2 \n",
            " Loss:  0.3102900981903076\n",
            "Training:  Iteration No:  188 Worker Num:  3 \n",
            " Loss:  0.31665876507759094\n",
            "Training:  Iteration No:  188 Worker Num:  4 \n",
            " Loss:  0.3759893774986267\n",
            "Training:  Iteration No:  188 Worker Num:  5 \n",
            " Loss:  0.289346843957901\n",
            "Training:  Iteration No:  188 Worker Num:  6 \n",
            " Loss:  0.308832585811615\n",
            "Training:  Iteration No:  188 Worker Num:  7 \n",
            " Loss:  0.3462860882282257\n",
            "Test:  Iteration No:  188 \n",
            " Loss:  0.2493765224950223\n",
            "Test accuracy:  92.78\n",
            "Training:  Iteration No:  189 Worker Num:  0 \n",
            " Loss:  0.24745270609855652\n",
            "Training:  Iteration No:  189 Worker Num:  1 \n",
            " Loss:  0.3928975760936737\n",
            "Training:  Iteration No:  189 Worker Num:  2 \n",
            " Loss:  0.2877752184867859\n",
            "Training:  Iteration No:  189 Worker Num:  3 \n",
            " Loss:  0.36181163787841797\n",
            "Training:  Iteration No:  189 Worker Num:  4 \n",
            " Loss:  0.21595628559589386\n",
            "Training:  Iteration No:  189 Worker Num:  5 \n",
            " Loss:  0.2226296216249466\n",
            "Training:  Iteration No:  189 Worker Num:  6 \n",
            " Loss:  0.25493761897087097\n",
            "Training:  Iteration No:  189 Worker Num:  7 \n",
            " Loss:  0.35903987288475037\n",
            "Test:  Iteration No:  189 \n",
            " Loss:  0.2509550319489422\n",
            "Test accuracy:  92.62\n",
            "Training:  Iteration No:  190 Worker Num:  0 \n",
            " Loss:  0.5152097940444946\n",
            "Training:  Iteration No:  190 Worker Num:  1 \n",
            " Loss:  0.3526310920715332\n",
            "Training:  Iteration No:  190 Worker Num:  2 \n",
            " Loss:  0.2860429286956787\n",
            "Training:  Iteration No:  190 Worker Num:  3 \n",
            " Loss:  0.4215817153453827\n",
            "Training:  Iteration No:  190 Worker Num:  4 \n",
            " Loss:  0.2701203525066376\n",
            "Training:  Iteration No:  190 Worker Num:  5 \n",
            " Loss:  0.35503339767456055\n",
            "Training:  Iteration No:  190 Worker Num:  6 \n",
            " Loss:  0.24113886058330536\n",
            "Training:  Iteration No:  190 Worker Num:  7 \n",
            " Loss:  0.31736212968826294\n",
            "Test:  Iteration No:  190 \n",
            " Loss:  0.24244868293215957\n",
            "Test accuracy:  92.83\n",
            "Training:  Iteration No:  191 Worker Num:  0 \n",
            " Loss:  0.2950543463230133\n",
            "Training:  Iteration No:  191 Worker Num:  1 \n",
            " Loss:  0.26775646209716797\n",
            "Training:  Iteration No:  191 Worker Num:  2 \n",
            " Loss:  0.301226407289505\n",
            "Training:  Iteration No:  191 Worker Num:  3 \n",
            " Loss:  0.2841843068599701\n",
            "Training:  Iteration No:  191 Worker Num:  4 \n",
            " Loss:  0.41653016209602356\n",
            "Training:  Iteration No:  191 Worker Num:  5 \n",
            " Loss:  0.27143967151641846\n",
            "Training:  Iteration No:  191 Worker Num:  6 \n",
            " Loss:  0.21540670096874237\n",
            "Training:  Iteration No:  191 Worker Num:  7 \n",
            " Loss:  0.3292085826396942\n",
            "Test:  Iteration No:  191 \n",
            " Loss:  0.24411050687673724\n",
            "Test accuracy:  92.7\n",
            "Training:  Iteration No:  192 Worker Num:  0 \n",
            " Loss:  0.2520604133605957\n",
            "Training:  Iteration No:  192 Worker Num:  1 \n",
            " Loss:  0.278668612241745\n",
            "Training:  Iteration No:  192 Worker Num:  2 \n",
            " Loss:  0.30425456166267395\n",
            "Training:  Iteration No:  192 Worker Num:  3 \n",
            " Loss:  0.34929296374320984\n",
            "Training:  Iteration No:  192 Worker Num:  4 \n",
            " Loss:  0.3326224684715271\n",
            "Training:  Iteration No:  192 Worker Num:  5 \n",
            " Loss:  0.3094114661216736\n",
            "Training:  Iteration No:  192 Worker Num:  6 \n",
            " Loss:  0.281329482793808\n",
            "Training:  Iteration No:  192 Worker Num:  7 \n",
            " Loss:  0.31702661514282227\n",
            "Test:  Iteration No:  192 \n",
            " Loss:  0.24703704278114474\n",
            "Test accuracy:  92.66\n",
            "Training:  Iteration No:  193 Worker Num:  0 \n",
            " Loss:  0.3896554708480835\n",
            "Training:  Iteration No:  193 Worker Num:  1 \n",
            " Loss:  0.3073621094226837\n",
            "Training:  Iteration No:  193 Worker Num:  2 \n",
            " Loss:  0.20376847684383392\n",
            "Training:  Iteration No:  193 Worker Num:  3 \n",
            " Loss:  0.3182618319988251\n",
            "Training:  Iteration No:  193 Worker Num:  4 \n",
            " Loss:  0.2853378355503082\n",
            "Training:  Iteration No:  193 Worker Num:  5 \n",
            " Loss:  0.3785998225212097\n",
            "Training:  Iteration No:  193 Worker Num:  6 \n",
            " Loss:  0.15688009560108185\n",
            "Training:  Iteration No:  193 Worker Num:  7 \n",
            " Loss:  0.28414440155029297\n",
            "Test:  Iteration No:  193 \n",
            " Loss:  0.2423678802302744\n",
            "Test accuracy:  92.73\n",
            "Training:  Iteration No:  194 Worker Num:  0 \n",
            " Loss:  0.3492879569530487\n",
            "Training:  Iteration No:  194 Worker Num:  1 \n",
            " Loss:  0.1842721551656723\n",
            "Training:  Iteration No:  194 Worker Num:  2 \n",
            " Loss:  0.1989428848028183\n",
            "Training:  Iteration No:  194 Worker Num:  3 \n",
            " Loss:  0.2633371353149414\n",
            "Training:  Iteration No:  194 Worker Num:  4 \n",
            " Loss:  0.4451155662536621\n",
            "Training:  Iteration No:  194 Worker Num:  5 \n",
            " Loss:  0.3151487112045288\n",
            "Training:  Iteration No:  194 Worker Num:  6 \n",
            " Loss:  0.3235810697078705\n",
            "Training:  Iteration No:  194 Worker Num:  7 \n",
            " Loss:  0.29760798811912537\n",
            "Test:  Iteration No:  194 \n",
            " Loss:  0.24356409751727612\n",
            "Test accuracy:  92.86\n",
            "Training:  Iteration No:  195 Worker Num:  0 \n",
            " Loss:  0.30208727717399597\n",
            "Training:  Iteration No:  195 Worker Num:  1 \n",
            " Loss:  0.3440465033054352\n",
            "Training:  Iteration No:  195 Worker Num:  2 \n",
            " Loss:  0.4382629692554474\n",
            "Training:  Iteration No:  195 Worker Num:  3 \n",
            " Loss:  0.20164690911769867\n",
            "Training:  Iteration No:  195 Worker Num:  4 \n",
            " Loss:  0.1535637527704239\n",
            "Training:  Iteration No:  195 Worker Num:  5 \n",
            " Loss:  0.2451123148202896\n",
            "Training:  Iteration No:  195 Worker Num:  6 \n",
            " Loss:  0.22442284226417542\n",
            "Training:  Iteration No:  195 Worker Num:  7 \n",
            " Loss:  0.33471518754959106\n",
            "Test:  Iteration No:  195 \n",
            " Loss:  0.23790446178445332\n",
            "Test accuracy:  92.9\n",
            "Training:  Iteration No:  196 Worker Num:  0 \n",
            " Loss:  0.2597762942314148\n",
            "Training:  Iteration No:  196 Worker Num:  1 \n",
            " Loss:  0.28046542406082153\n",
            "Training:  Iteration No:  196 Worker Num:  2 \n",
            " Loss:  0.2468927502632141\n",
            "Training:  Iteration No:  196 Worker Num:  3 \n",
            " Loss:  0.3093932569026947\n",
            "Training:  Iteration No:  196 Worker Num:  4 \n",
            " Loss:  0.33876848220825195\n",
            "Training:  Iteration No:  196 Worker Num:  5 \n",
            " Loss:  0.34354740381240845\n",
            "Training:  Iteration No:  196 Worker Num:  6 \n",
            " Loss:  0.3575865924358368\n",
            "Training:  Iteration No:  196 Worker Num:  7 \n",
            " Loss:  0.21770264208316803\n",
            "Test:  Iteration No:  196 \n",
            " Loss:  0.23739878484342672\n",
            "Test accuracy:  93.01\n",
            "Training:  Iteration No:  197 Worker Num:  0 \n",
            " Loss:  0.309856653213501\n",
            "Training:  Iteration No:  197 Worker Num:  1 \n",
            " Loss:  0.24574755132198334\n",
            "Training:  Iteration No:  197 Worker Num:  2 \n",
            " Loss:  0.5530393123626709\n",
            "Training:  Iteration No:  197 Worker Num:  3 \n",
            " Loss:  0.24510948359966278\n",
            "Training:  Iteration No:  197 Worker Num:  4 \n",
            " Loss:  0.16760626435279846\n",
            "Training:  Iteration No:  197 Worker Num:  5 \n",
            " Loss:  0.38388878107070923\n",
            "Training:  Iteration No:  197 Worker Num:  6 \n",
            " Loss:  0.1907947063446045\n",
            "Training:  Iteration No:  197 Worker Num:  7 \n",
            " Loss:  0.352018803358078\n",
            "Test:  Iteration No:  197 \n",
            " Loss:  0.23741777995719185\n",
            "Test accuracy:  92.87\n",
            "Training:  Iteration No:  198 Worker Num:  0 \n",
            " Loss:  0.3150073289871216\n",
            "Training:  Iteration No:  198 Worker Num:  1 \n",
            " Loss:  0.17051060497760773\n",
            "Training:  Iteration No:  198 Worker Num:  2 \n",
            " Loss:  0.24636076390743256\n",
            "Training:  Iteration No:  198 Worker Num:  3 \n",
            " Loss:  0.3404413163661957\n",
            "Training:  Iteration No:  198 Worker Num:  4 \n",
            " Loss:  0.32776883244514465\n",
            "Training:  Iteration No:  198 Worker Num:  5 \n",
            " Loss:  0.2989347279071808\n",
            "Training:  Iteration No:  198 Worker Num:  6 \n",
            " Loss:  0.28283625841140747\n",
            "Training:  Iteration No:  198 Worker Num:  7 \n",
            " Loss:  0.16729547083377838\n",
            "Test:  Iteration No:  198 \n",
            " Loss:  0.23828084745644768\n",
            "Test accuracy:  92.97\n",
            "Training:  Iteration No:  199 Worker Num:  0 \n",
            " Loss:  0.24276092648506165\n",
            "Training:  Iteration No:  199 Worker Num:  1 \n",
            " Loss:  0.32224491238594055\n",
            "Training:  Iteration No:  199 Worker Num:  2 \n",
            " Loss:  0.246822789311409\n",
            "Training:  Iteration No:  199 Worker Num:  3 \n",
            " Loss:  0.18317484855651855\n",
            "Training:  Iteration No:  199 Worker Num:  4 \n",
            " Loss:  0.3568277657032013\n",
            "Training:  Iteration No:  199 Worker Num:  5 \n",
            " Loss:  0.3083999752998352\n",
            "Training:  Iteration No:  199 Worker Num:  6 \n",
            " Loss:  0.26255330443382263\n",
            "Training:  Iteration No:  199 Worker Num:  7 \n",
            " Loss:  0.28761735558509827\n",
            "Test:  Iteration No:  199 \n",
            " Loss:  0.23430007993231847\n",
            "Test accuracy:  93.12\n",
            "Training:  Iteration No:  200 Worker Num:  0 \n",
            " Loss:  0.19909444451332092\n",
            "Training:  Iteration No:  200 Worker Num:  1 \n",
            " Loss:  0.28810641169548035\n",
            "Training:  Iteration No:  200 Worker Num:  2 \n",
            " Loss:  0.44517600536346436\n",
            "Training:  Iteration No:  200 Worker Num:  3 \n",
            " Loss:  0.30791175365448\n",
            "Training:  Iteration No:  200 Worker Num:  4 \n",
            " Loss:  0.32498353719711304\n",
            "Training:  Iteration No:  200 Worker Num:  5 \n",
            " Loss:  0.27630195021629333\n",
            "Training:  Iteration No:  200 Worker Num:  6 \n",
            " Loss:  0.22663404047489166\n",
            "Training:  Iteration No:  200 Worker Num:  7 \n",
            " Loss:  0.31385886669158936\n",
            "Test:  Iteration No:  200 \n",
            " Loss:  0.23869751285337196\n",
            "Test accuracy:  93.0\n",
            "Training:  Iteration No:  201 Worker Num:  0 \n",
            " Loss:  0.4465187191963196\n",
            "Training:  Iteration No:  201 Worker Num:  1 \n",
            " Loss:  0.2578268051147461\n",
            "Training:  Iteration No:  201 Worker Num:  2 \n",
            " Loss:  0.22811762988567352\n",
            "Training:  Iteration No:  201 Worker Num:  3 \n",
            " Loss:  0.3270142674446106\n",
            "Training:  Iteration No:  201 Worker Num:  4 \n",
            " Loss:  0.27895814180374146\n",
            "Training:  Iteration No:  201 Worker Num:  5 \n",
            " Loss:  0.174678772687912\n",
            "Training:  Iteration No:  201 Worker Num:  6 \n",
            " Loss:  0.19445717334747314\n",
            "Training:  Iteration No:  201 Worker Num:  7 \n",
            " Loss:  0.3191452622413635\n",
            "Test:  Iteration No:  201 \n",
            " Loss:  0.23609082636576664\n",
            "Test accuracy:  92.9\n",
            "Training:  Iteration No:  202 Worker Num:  0 \n",
            " Loss:  0.24467018246650696\n",
            "Training:  Iteration No:  202 Worker Num:  1 \n",
            " Loss:  0.3860350251197815\n",
            "Training:  Iteration No:  202 Worker Num:  2 \n",
            " Loss:  0.3542075455188751\n",
            "Training:  Iteration No:  202 Worker Num:  3 \n",
            " Loss:  0.2732868194580078\n",
            "Training:  Iteration No:  202 Worker Num:  4 \n",
            " Loss:  0.2925693392753601\n",
            "Training:  Iteration No:  202 Worker Num:  5 \n",
            " Loss:  0.1662093997001648\n",
            "Training:  Iteration No:  202 Worker Num:  6 \n",
            " Loss:  0.34980469942092896\n",
            "Training:  Iteration No:  202 Worker Num:  7 \n",
            " Loss:  0.27767640352249146\n",
            "Test:  Iteration No:  202 \n",
            " Loss:  0.2289525736712768\n",
            "Test accuracy:  93.27\n",
            "Training:  Iteration No:  203 Worker Num:  0 \n",
            " Loss:  0.20403434336185455\n",
            "Training:  Iteration No:  203 Worker Num:  1 \n",
            " Loss:  0.19878818094730377\n",
            "Training:  Iteration No:  203 Worker Num:  2 \n",
            " Loss:  0.18898002803325653\n",
            "Training:  Iteration No:  203 Worker Num:  3 \n",
            " Loss:  0.30786827206611633\n",
            "Training:  Iteration No:  203 Worker Num:  4 \n",
            " Loss:  0.2378416359424591\n",
            "Training:  Iteration No:  203 Worker Num:  5 \n",
            " Loss:  0.39911460876464844\n",
            "Training:  Iteration No:  203 Worker Num:  6 \n",
            " Loss:  0.22020649909973145\n",
            "Training:  Iteration No:  203 Worker Num:  7 \n",
            " Loss:  0.3190959692001343\n",
            "Test:  Iteration No:  203 \n",
            " Loss:  0.2277831219466804\n",
            "Test accuracy:  93.28\n",
            "Training:  Iteration No:  204 Worker Num:  0 \n",
            " Loss:  0.23780296742916107\n",
            "Training:  Iteration No:  204 Worker Num:  1 \n",
            " Loss:  0.21933414041996002\n",
            "Training:  Iteration No:  204 Worker Num:  2 \n",
            " Loss:  0.27672135829925537\n",
            "Training:  Iteration No:  204 Worker Num:  3 \n",
            " Loss:  0.22518104314804077\n",
            "Training:  Iteration No:  204 Worker Num:  4 \n",
            " Loss:  0.21221067011356354\n",
            "Training:  Iteration No:  204 Worker Num:  5 \n",
            " Loss:  0.28890129923820496\n",
            "Training:  Iteration No:  204 Worker Num:  6 \n",
            " Loss:  0.2121412307024002\n",
            "Training:  Iteration No:  204 Worker Num:  7 \n",
            " Loss:  0.2690807282924652\n",
            "Test:  Iteration No:  204 \n",
            " Loss:  0.23092103499589087\n",
            "Test accuracy:  93.15\n",
            "Training:  Iteration No:  205 Worker Num:  0 \n",
            " Loss:  0.2716948688030243\n",
            "Training:  Iteration No:  205 Worker Num:  1 \n",
            " Loss:  0.24810227751731873\n",
            "Training:  Iteration No:  205 Worker Num:  2 \n",
            " Loss:  0.27434271574020386\n",
            "Training:  Iteration No:  205 Worker Num:  3 \n",
            " Loss:  0.4389581084251404\n",
            "Training:  Iteration No:  205 Worker Num:  4 \n",
            " Loss:  0.20300999283790588\n",
            "Training:  Iteration No:  205 Worker Num:  5 \n",
            " Loss:  0.25415337085723877\n",
            "Training:  Iteration No:  205 Worker Num:  6 \n",
            " Loss:  0.23012252151966095\n",
            "Training:  Iteration No:  205 Worker Num:  7 \n",
            " Loss:  0.3023776412010193\n",
            "Test:  Iteration No:  205 \n",
            " Loss:  0.23266841530139687\n",
            "Test accuracy:  93.0\n",
            "Training:  Iteration No:  206 Worker Num:  0 \n",
            " Loss:  0.17906001210212708\n",
            "Training:  Iteration No:  206 Worker Num:  1 \n",
            " Loss:  0.3542569577693939\n",
            "Training:  Iteration No:  206 Worker Num:  2 \n",
            " Loss:  0.27988743782043457\n",
            "Training:  Iteration No:  206 Worker Num:  3 \n",
            " Loss:  0.3347083330154419\n",
            "Training:  Iteration No:  206 Worker Num:  4 \n",
            " Loss:  0.3496696949005127\n",
            "Training:  Iteration No:  206 Worker Num:  5 \n",
            " Loss:  0.20197793841362\n",
            "Training:  Iteration No:  206 Worker Num:  6 \n",
            " Loss:  0.23840393126010895\n",
            "Training:  Iteration No:  206 Worker Num:  7 \n",
            " Loss:  0.2830624282360077\n",
            "Test:  Iteration No:  206 \n",
            " Loss:  0.23520811987875762\n",
            "Test accuracy:  93.15\n",
            "Training:  Iteration No:  207 Worker Num:  0 \n",
            " Loss:  0.23956607282161713\n",
            "Training:  Iteration No:  207 Worker Num:  1 \n",
            " Loss:  0.3866075873374939\n",
            "Training:  Iteration No:  207 Worker Num:  2 \n",
            " Loss:  0.35973307490348816\n",
            "Training:  Iteration No:  207 Worker Num:  3 \n",
            " Loss:  0.2095116227865219\n",
            "Training:  Iteration No:  207 Worker Num:  4 \n",
            " Loss:  0.369688481092453\n",
            "Training:  Iteration No:  207 Worker Num:  5 \n",
            " Loss:  0.43872150778770447\n",
            "Training:  Iteration No:  207 Worker Num:  6 \n",
            " Loss:  0.23874971270561218\n",
            "Training:  Iteration No:  207 Worker Num:  7 \n",
            " Loss:  0.19883641600608826\n",
            "Test:  Iteration No:  207 \n",
            " Loss:  0.22606406444565783\n",
            "Test accuracy:  93.36\n",
            "Training:  Iteration No:  208 Worker Num:  0 \n",
            " Loss:  0.2132885903120041\n",
            "Training:  Iteration No:  208 Worker Num:  1 \n",
            " Loss:  0.19245103001594543\n",
            "Training:  Iteration No:  208 Worker Num:  2 \n",
            " Loss:  0.27187588810920715\n",
            "Training:  Iteration No:  208 Worker Num:  3 \n",
            " Loss:  0.22923961281776428\n",
            "Training:  Iteration No:  208 Worker Num:  4 \n",
            " Loss:  0.3258819282054901\n",
            "Training:  Iteration No:  208 Worker Num:  5 \n",
            " Loss:  0.22773322463035583\n",
            "Training:  Iteration No:  208 Worker Num:  6 \n",
            " Loss:  0.27706024050712585\n",
            "Training:  Iteration No:  208 Worker Num:  7 \n",
            " Loss:  0.17355762422084808\n",
            "Test:  Iteration No:  208 \n",
            " Loss:  0.22688782167962834\n",
            "Test accuracy:  93.25\n",
            "Training:  Iteration No:  209 Worker Num:  0 \n",
            " Loss:  0.2452714592218399\n",
            "Training:  Iteration No:  209 Worker Num:  1 \n",
            " Loss:  0.4220597743988037\n",
            "Training:  Iteration No:  209 Worker Num:  2 \n",
            " Loss:  0.3580673933029175\n",
            "Training:  Iteration No:  209 Worker Num:  3 \n",
            " Loss:  0.24842803180217743\n",
            "Training:  Iteration No:  209 Worker Num:  4 \n",
            " Loss:  0.19596801698207855\n",
            "Training:  Iteration No:  209 Worker Num:  5 \n",
            " Loss:  0.21425995230674744\n",
            "Training:  Iteration No:  209 Worker Num:  6 \n",
            " Loss:  0.2821028530597687\n",
            "Training:  Iteration No:  209 Worker Num:  7 \n",
            " Loss:  0.2554611563682556\n",
            "Test:  Iteration No:  209 \n",
            " Loss:  0.22553842732847867\n",
            "Test accuracy:  93.26\n",
            "Training:  Iteration No:  210 Worker Num:  0 \n",
            " Loss:  0.21936872601509094\n",
            "Training:  Iteration No:  210 Worker Num:  1 \n",
            " Loss:  0.24601782858371735\n",
            "Training:  Iteration No:  210 Worker Num:  2 \n",
            " Loss:  0.19604022800922394\n",
            "Training:  Iteration No:  210 Worker Num:  3 \n",
            " Loss:  0.3956844210624695\n",
            "Training:  Iteration No:  210 Worker Num:  4 \n",
            " Loss:  0.2502692937850952\n",
            "Training:  Iteration No:  210 Worker Num:  5 \n",
            " Loss:  0.24968449771404266\n",
            "Training:  Iteration No:  210 Worker Num:  6 \n",
            " Loss:  0.19946995377540588\n",
            "Training:  Iteration No:  210 Worker Num:  7 \n",
            " Loss:  0.2220676988363266\n",
            "Test:  Iteration No:  210 \n",
            " Loss:  0.22947102996131666\n",
            "Test accuracy:  93.2\n",
            "Training:  Iteration No:  211 Worker Num:  0 \n",
            " Loss:  0.3114563822746277\n",
            "Training:  Iteration No:  211 Worker Num:  1 \n",
            " Loss:  0.2492072582244873\n",
            "Training:  Iteration No:  211 Worker Num:  2 \n",
            " Loss:  0.3330102264881134\n",
            "Training:  Iteration No:  211 Worker Num:  3 \n",
            " Loss:  0.1652221977710724\n",
            "Training:  Iteration No:  211 Worker Num:  4 \n",
            " Loss:  0.23355448246002197\n",
            "Training:  Iteration No:  211 Worker Num:  5 \n",
            " Loss:  0.19630023837089539\n",
            "Training:  Iteration No:  211 Worker Num:  6 \n",
            " Loss:  0.3893018960952759\n",
            "Training:  Iteration No:  211 Worker Num:  7 \n",
            " Loss:  0.17825418710708618\n",
            "Test:  Iteration No:  211 \n",
            " Loss:  0.22507247263919325\n",
            "Test accuracy:  93.25\n",
            "Training:  Iteration No:  212 Worker Num:  0 \n",
            " Loss:  0.38822486996650696\n",
            "Training:  Iteration No:  212 Worker Num:  1 \n",
            " Loss:  0.2908916175365448\n",
            "Training:  Iteration No:  212 Worker Num:  2 \n",
            " Loss:  0.3320156931877136\n",
            "Training:  Iteration No:  212 Worker Num:  3 \n",
            " Loss:  0.22478017210960388\n",
            "Training:  Iteration No:  212 Worker Num:  4 \n",
            " Loss:  0.3023029565811157\n",
            "Training:  Iteration No:  212 Worker Num:  5 \n",
            " Loss:  0.29412660002708435\n",
            "Training:  Iteration No:  212 Worker Num:  6 \n",
            " Loss:  0.15381665527820587\n",
            "Training:  Iteration No:  212 Worker Num:  7 \n",
            " Loss:  0.2836422324180603\n",
            "Test:  Iteration No:  212 \n",
            " Loss:  0.21920460970671493\n",
            "Test accuracy:  93.64\n",
            "Training:  Iteration No:  213 Worker Num:  0 \n",
            " Loss:  0.21743571758270264\n",
            "Training:  Iteration No:  213 Worker Num:  1 \n",
            " Loss:  0.31315311789512634\n",
            "Training:  Iteration No:  213 Worker Num:  2 \n",
            " Loss:  0.3132022023200989\n",
            "Training:  Iteration No:  213 Worker Num:  3 \n",
            " Loss:  0.3683665096759796\n",
            "Training:  Iteration No:  213 Worker Num:  4 \n",
            " Loss:  0.19158752262592316\n",
            "Training:  Iteration No:  213 Worker Num:  5 \n",
            " Loss:  0.33637651801109314\n",
            "Training:  Iteration No:  213 Worker Num:  6 \n",
            " Loss:  0.29252901673316956\n",
            "Training:  Iteration No:  213 Worker Num:  7 \n",
            " Loss:  0.19615554809570312\n",
            "Test:  Iteration No:  213 \n",
            " Loss:  0.22080889532837686\n",
            "Test accuracy:  93.43\n",
            "Training:  Iteration No:  214 Worker Num:  0 \n",
            " Loss:  0.2873048782348633\n",
            "Training:  Iteration No:  214 Worker Num:  1 \n",
            " Loss:  0.2874304950237274\n",
            "Training:  Iteration No:  214 Worker Num:  2 \n",
            " Loss:  0.20545530319213867\n",
            "Training:  Iteration No:  214 Worker Num:  3 \n",
            " Loss:  0.4006982445716858\n",
            "Training:  Iteration No:  214 Worker Num:  4 \n",
            " Loss:  0.22669464349746704\n",
            "Training:  Iteration No:  214 Worker Num:  5 \n",
            " Loss:  0.2158265858888626\n",
            "Training:  Iteration No:  214 Worker Num:  6 \n",
            " Loss:  0.15600089728832245\n",
            "Training:  Iteration No:  214 Worker Num:  7 \n",
            " Loss:  0.35237953066825867\n",
            "Test:  Iteration No:  214 \n",
            " Loss:  0.2225473100390238\n",
            "Test accuracy:  93.35\n",
            "Training:  Iteration No:  215 Worker Num:  0 \n",
            " Loss:  0.28052422404289246\n",
            "Training:  Iteration No:  215 Worker Num:  1 \n",
            " Loss:  0.26165324449539185\n",
            "Training:  Iteration No:  215 Worker Num:  2 \n",
            " Loss:  0.30493584275245667\n",
            "Training:  Iteration No:  215 Worker Num:  3 \n",
            " Loss:  0.43161317706108093\n",
            "Training:  Iteration No:  215 Worker Num:  4 \n",
            " Loss:  0.22521299123764038\n",
            "Training:  Iteration No:  215 Worker Num:  5 \n",
            " Loss:  0.4233444333076477\n",
            "Training:  Iteration No:  215 Worker Num:  6 \n",
            " Loss:  0.19747799634933472\n",
            "Training:  Iteration No:  215 Worker Num:  7 \n",
            " Loss:  0.27394863963127136\n",
            "Test:  Iteration No:  215 \n",
            " Loss:  0.2221177047401478\n",
            "Test accuracy:  93.52\n",
            "Training:  Iteration No:  216 Worker Num:  0 \n",
            " Loss:  0.24162347614765167\n",
            "Training:  Iteration No:  216 Worker Num:  1 \n",
            " Loss:  0.23965035378932953\n",
            "Training:  Iteration No:  216 Worker Num:  2 \n",
            " Loss:  0.17922940850257874\n",
            "Training:  Iteration No:  216 Worker Num:  3 \n",
            " Loss:  0.2793806493282318\n",
            "Training:  Iteration No:  216 Worker Num:  4 \n",
            " Loss:  0.3279110789299011\n",
            "Training:  Iteration No:  216 Worker Num:  5 \n",
            " Loss:  0.2166435420513153\n",
            "Training:  Iteration No:  216 Worker Num:  6 \n",
            " Loss:  0.27451738715171814\n",
            "Training:  Iteration No:  216 Worker Num:  7 \n",
            " Loss:  0.3835320770740509\n",
            "Test:  Iteration No:  216 \n",
            " Loss:  0.219582140139198\n",
            "Test accuracy:  93.41\n",
            "Training:  Iteration No:  217 Worker Num:  0 \n",
            " Loss:  0.30055665969848633\n",
            "Training:  Iteration No:  217 Worker Num:  1 \n",
            " Loss:  0.23195135593414307\n",
            "Training:  Iteration No:  217 Worker Num:  2 \n",
            " Loss:  0.24661850929260254\n",
            "Training:  Iteration No:  217 Worker Num:  3 \n",
            " Loss:  0.33145537972450256\n",
            "Training:  Iteration No:  217 Worker Num:  4 \n",
            " Loss:  0.3165537416934967\n",
            "Training:  Iteration No:  217 Worker Num:  5 \n",
            " Loss:  0.33035042881965637\n",
            "Training:  Iteration No:  217 Worker Num:  6 \n",
            " Loss:  0.35946542024612427\n",
            "Training:  Iteration No:  217 Worker Num:  7 \n",
            " Loss:  0.24474576115608215\n",
            "Test:  Iteration No:  217 \n",
            " Loss:  0.2210533874038654\n",
            "Test accuracy:  93.51\n",
            "Training:  Iteration No:  218 Worker Num:  0 \n",
            " Loss:  0.24456624686717987\n",
            "Training:  Iteration No:  218 Worker Num:  1 \n",
            " Loss:  0.2569544017314911\n",
            "Training:  Iteration No:  218 Worker Num:  2 \n",
            " Loss:  0.19398532807826996\n",
            "Training:  Iteration No:  218 Worker Num:  3 \n",
            " Loss:  0.30985525250434875\n",
            "Training:  Iteration No:  218 Worker Num:  4 \n",
            " Loss:  0.33253705501556396\n",
            "Training:  Iteration No:  218 Worker Num:  5 \n",
            " Loss:  0.2316509187221527\n",
            "Training:  Iteration No:  218 Worker Num:  6 \n",
            " Loss:  0.30893805623054504\n",
            "Training:  Iteration No:  218 Worker Num:  7 \n",
            " Loss:  0.38669705390930176\n",
            "Test:  Iteration No:  218 \n",
            " Loss:  0.21866927360762148\n",
            "Test accuracy:  93.42\n",
            "Training:  Iteration No:  219 Worker Num:  0 \n",
            " Loss:  0.21616259217262268\n",
            "Training:  Iteration No:  219 Worker Num:  1 \n",
            " Loss:  0.1902959942817688\n",
            "Training:  Iteration No:  219 Worker Num:  2 \n",
            " Loss:  0.2175772786140442\n",
            "Training:  Iteration No:  219 Worker Num:  3 \n",
            " Loss:  0.4198228716850281\n",
            "Training:  Iteration No:  219 Worker Num:  4 \n",
            " Loss:  0.22765672206878662\n",
            "Training:  Iteration No:  219 Worker Num:  5 \n",
            " Loss:  0.2836194336414337\n",
            "Training:  Iteration No:  219 Worker Num:  6 \n",
            " Loss:  0.2622557580471039\n",
            "Training:  Iteration No:  219 Worker Num:  7 \n",
            " Loss:  0.19976204633712769\n",
            "Test:  Iteration No:  219 \n",
            " Loss:  0.2139590343436863\n",
            "Test accuracy:  93.8\n",
            "Training:  Iteration No:  220 Worker Num:  0 \n",
            " Loss:  0.23277857899665833\n",
            "Training:  Iteration No:  220 Worker Num:  1 \n",
            " Loss:  0.30142658948898315\n",
            "Training:  Iteration No:  220 Worker Num:  2 \n",
            " Loss:  0.23179322481155396\n",
            "Training:  Iteration No:  220 Worker Num:  3 \n",
            " Loss:  0.1850375086069107\n",
            "Training:  Iteration No:  220 Worker Num:  4 \n",
            " Loss:  0.2584076523780823\n",
            "Training:  Iteration No:  220 Worker Num:  5 \n",
            " Loss:  0.19797047972679138\n",
            "Training:  Iteration No:  220 Worker Num:  6 \n",
            " Loss:  0.19896601140499115\n",
            "Training:  Iteration No:  220 Worker Num:  7 \n",
            " Loss:  0.19230714440345764\n",
            "Test:  Iteration No:  220 \n",
            " Loss:  0.2162267377340718\n",
            "Test accuracy:  93.55\n",
            "Training:  Iteration No:  221 Worker Num:  0 \n",
            " Loss:  0.2617444097995758\n",
            "Training:  Iteration No:  221 Worker Num:  1 \n",
            " Loss:  0.21745547652244568\n",
            "Training:  Iteration No:  221 Worker Num:  2 \n",
            " Loss:  0.3298235833644867\n",
            "Training:  Iteration No:  221 Worker Num:  3 \n",
            " Loss:  0.30910271406173706\n",
            "Training:  Iteration No:  221 Worker Num:  4 \n",
            " Loss:  0.2751430571079254\n",
            "Training:  Iteration No:  221 Worker Num:  5 \n",
            " Loss:  0.2832447588443756\n",
            "Training:  Iteration No:  221 Worker Num:  6 \n",
            " Loss:  0.1513841599225998\n",
            "Training:  Iteration No:  221 Worker Num:  7 \n",
            " Loss:  0.11786830425262451\n",
            "Test:  Iteration No:  221 \n",
            " Loss:  0.2125214090735852\n",
            "Test accuracy:  93.72\n",
            "Training:  Iteration No:  222 Worker Num:  0 \n",
            " Loss:  0.2892065644264221\n",
            "Training:  Iteration No:  222 Worker Num:  1 \n",
            " Loss:  0.2741585075855255\n",
            "Training:  Iteration No:  222 Worker Num:  2 \n",
            " Loss:  0.28298521041870117\n",
            "Training:  Iteration No:  222 Worker Num:  3 \n",
            " Loss:  0.2617218792438507\n",
            "Training:  Iteration No:  222 Worker Num:  4 \n",
            " Loss:  0.16904261708259583\n",
            "Training:  Iteration No:  222 Worker Num:  5 \n",
            " Loss:  0.3763797879219055\n",
            "Training:  Iteration No:  222 Worker Num:  6 \n",
            " Loss:  0.33286598324775696\n",
            "Training:  Iteration No:  222 Worker Num:  7 \n",
            " Loss:  0.22446392476558685\n",
            "Test:  Iteration No:  222 \n",
            " Loss:  0.22006661474374653\n",
            "Test accuracy:  93.51\n",
            "Training:  Iteration No:  223 Worker Num:  0 \n",
            " Loss:  0.3528895080089569\n",
            "Training:  Iteration No:  223 Worker Num:  1 \n",
            " Loss:  0.19884540140628815\n",
            "Training:  Iteration No:  223 Worker Num:  2 \n",
            " Loss:  0.3902702331542969\n",
            "Training:  Iteration No:  223 Worker Num:  3 \n",
            " Loss:  0.15296129882335663\n",
            "Training:  Iteration No:  223 Worker Num:  4 \n",
            " Loss:  0.3051360249519348\n",
            "Training:  Iteration No:  223 Worker Num:  5 \n",
            " Loss:  0.23698283731937408\n",
            "Training:  Iteration No:  223 Worker Num:  6 \n",
            " Loss:  0.25504380464553833\n",
            "Training:  Iteration No:  223 Worker Num:  7 \n",
            " Loss:  0.22632849216461182\n",
            "Test:  Iteration No:  223 \n",
            " Loss:  0.21318661449830742\n",
            "Test accuracy:  93.76\n",
            "Training:  Iteration No:  224 Worker Num:  0 \n",
            " Loss:  0.22942639887332916\n",
            "Training:  Iteration No:  224 Worker Num:  1 \n",
            " Loss:  0.32092323899269104\n",
            "Training:  Iteration No:  224 Worker Num:  2 \n",
            " Loss:  0.24502864480018616\n",
            "Training:  Iteration No:  224 Worker Num:  3 \n",
            " Loss:  0.32407742738723755\n",
            "Training:  Iteration No:  224 Worker Num:  4 \n",
            " Loss:  0.3460617661476135\n",
            "Training:  Iteration No:  224 Worker Num:  5 \n",
            " Loss:  0.30714213848114014\n",
            "Training:  Iteration No:  224 Worker Num:  6 \n",
            " Loss:  0.27344486117362976\n",
            "Training:  Iteration No:  224 Worker Num:  7 \n",
            " Loss:  0.38204899430274963\n",
            "Test:  Iteration No:  224 \n",
            " Loss:  0.21299933477104466\n",
            "Test accuracy:  93.69\n",
            "Training:  Iteration No:  225 Worker Num:  0 \n",
            " Loss:  0.2991795539855957\n",
            "Training:  Iteration No:  225 Worker Num:  1 \n",
            " Loss:  0.30723756551742554\n",
            "Training:  Iteration No:  225 Worker Num:  2 \n",
            " Loss:  0.24389240145683289\n",
            "Training:  Iteration No:  225 Worker Num:  3 \n",
            " Loss:  0.25904160737991333\n",
            "Training:  Iteration No:  225 Worker Num:  4 \n",
            " Loss:  0.27995967864990234\n",
            "Training:  Iteration No:  225 Worker Num:  5 \n",
            " Loss:  0.38298499584198\n",
            "Training:  Iteration No:  225 Worker Num:  6 \n",
            " Loss:  0.15285511314868927\n",
            "Training:  Iteration No:  225 Worker Num:  7 \n",
            " Loss:  0.20197443664073944\n",
            "Test:  Iteration No:  225 \n",
            " Loss:  0.2150971032207525\n",
            "Test accuracy:  93.81\n",
            "Training:  Iteration No:  226 Worker Num:  0 \n",
            " Loss:  0.22614479064941406\n",
            "Training:  Iteration No:  226 Worker Num:  1 \n",
            " Loss:  0.2366371601819992\n",
            "Training:  Iteration No:  226 Worker Num:  2 \n",
            " Loss:  0.24876555800437927\n",
            "Training:  Iteration No:  226 Worker Num:  3 \n",
            " Loss:  0.2733108103275299\n",
            "Training:  Iteration No:  226 Worker Num:  4 \n",
            " Loss:  0.2190699726343155\n",
            "Training:  Iteration No:  226 Worker Num:  5 \n",
            " Loss:  0.2548741102218628\n",
            "Training:  Iteration No:  226 Worker Num:  6 \n",
            " Loss:  0.3245328962802887\n",
            "Training:  Iteration No:  226 Worker Num:  7 \n",
            " Loss:  0.3029293119907379\n",
            "Test:  Iteration No:  226 \n",
            " Loss:  0.20995928825739818\n",
            "Test accuracy:  93.88\n",
            "Training:  Iteration No:  227 Worker Num:  0 \n",
            " Loss:  0.25887367129325867\n",
            "Training:  Iteration No:  227 Worker Num:  1 \n",
            " Loss:  0.27072322368621826\n",
            "Training:  Iteration No:  227 Worker Num:  2 \n",
            " Loss:  0.2540951669216156\n",
            "Training:  Iteration No:  227 Worker Num:  3 \n",
            " Loss:  0.2482508420944214\n",
            "Training:  Iteration No:  227 Worker Num:  4 \n",
            " Loss:  0.2164023071527481\n",
            "Training:  Iteration No:  227 Worker Num:  5 \n",
            " Loss:  0.2731506824493408\n",
            "Training:  Iteration No:  227 Worker Num:  6 \n",
            " Loss:  0.2907774746417999\n",
            "Training:  Iteration No:  227 Worker Num:  7 \n",
            " Loss:  0.19670885801315308\n",
            "Test:  Iteration No:  227 \n",
            " Loss:  0.21462555710650696\n",
            "Test accuracy:  93.64\n",
            "Training:  Iteration No:  228 Worker Num:  0 \n",
            " Loss:  0.23938506841659546\n",
            "Training:  Iteration No:  228 Worker Num:  1 \n",
            " Loss:  0.20563028752803802\n",
            "Training:  Iteration No:  228 Worker Num:  2 \n",
            " Loss:  0.30602747201919556\n",
            "Training:  Iteration No:  228 Worker Num:  3 \n",
            " Loss:  0.29810860753059387\n",
            "Training:  Iteration No:  228 Worker Num:  4 \n",
            " Loss:  0.32362526655197144\n",
            "Training:  Iteration No:  228 Worker Num:  5 \n",
            " Loss:  0.20955877006053925\n",
            "Training:  Iteration No:  228 Worker Num:  6 \n",
            " Loss:  0.2945469319820404\n",
            "Training:  Iteration No:  228 Worker Num:  7 \n",
            " Loss:  0.25986847281455994\n",
            "Test:  Iteration No:  228 \n",
            " Loss:  0.21406666708143451\n",
            "Test accuracy:  93.55\n",
            "Training:  Iteration No:  229 Worker Num:  0 \n",
            " Loss:  0.24077576398849487\n",
            "Training:  Iteration No:  229 Worker Num:  1 \n",
            " Loss:  0.21402029693126678\n",
            "Training:  Iteration No:  229 Worker Num:  2 \n",
            " Loss:  0.27204978466033936\n",
            "Training:  Iteration No:  229 Worker Num:  3 \n",
            " Loss:  0.17280083894729614\n",
            "Training:  Iteration No:  229 Worker Num:  4 \n",
            " Loss:  0.30539777874946594\n",
            "Training:  Iteration No:  229 Worker Num:  5 \n",
            " Loss:  0.3119569718837738\n",
            "Training:  Iteration No:  229 Worker Num:  6 \n",
            " Loss:  0.25829827785491943\n",
            "Training:  Iteration No:  229 Worker Num:  7 \n",
            " Loss:  0.31008148193359375\n",
            "Test:  Iteration No:  229 \n",
            " Loss:  0.21552434128626616\n",
            "Test accuracy:  93.63\n",
            "Training:  Iteration No:  230 Worker Num:  0 \n",
            " Loss:  0.16086386144161224\n",
            "Training:  Iteration No:  230 Worker Num:  1 \n",
            " Loss:  0.25451603531837463\n",
            "Training:  Iteration No:  230 Worker Num:  2 \n",
            " Loss:  0.3064231872558594\n",
            "Training:  Iteration No:  230 Worker Num:  3 \n",
            " Loss:  0.20407851040363312\n",
            "Training:  Iteration No:  230 Worker Num:  4 \n",
            " Loss:  0.22651171684265137\n",
            "Training:  Iteration No:  230 Worker Num:  5 \n",
            " Loss:  0.24513506889343262\n",
            "Training:  Iteration No:  230 Worker Num:  6 \n",
            " Loss:  0.3102872371673584\n",
            "Training:  Iteration No:  230 Worker Num:  7 \n",
            " Loss:  0.18913936614990234\n",
            "Test:  Iteration No:  230 \n",
            " Loss:  0.21026137866126962\n",
            "Test accuracy:  93.71\n",
            "Training:  Iteration No:  231 Worker Num:  0 \n",
            " Loss:  0.23612576723098755\n",
            "Training:  Iteration No:  231 Worker Num:  1 \n",
            " Loss:  0.25259217619895935\n",
            "Training:  Iteration No:  231 Worker Num:  2 \n",
            " Loss:  0.27310800552368164\n",
            "Training:  Iteration No:  231 Worker Num:  3 \n",
            " Loss:  0.15215526521205902\n",
            "Training:  Iteration No:  231 Worker Num:  4 \n",
            " Loss:  0.23819205164909363\n",
            "Training:  Iteration No:  231 Worker Num:  5 \n",
            " Loss:  0.32465124130249023\n",
            "Training:  Iteration No:  231 Worker Num:  6 \n",
            " Loss:  0.23492810130119324\n",
            "Training:  Iteration No:  231 Worker Num:  7 \n",
            " Loss:  0.3311326205730438\n",
            "Test:  Iteration No:  231 \n",
            " Loss:  0.20840092506731236\n",
            "Test accuracy:  93.95\n",
            "Training:  Iteration No:  232 Worker Num:  0 \n",
            " Loss:  0.27515390515327454\n",
            "Training:  Iteration No:  232 Worker Num:  1 \n",
            " Loss:  0.21935519576072693\n",
            "Training:  Iteration No:  232 Worker Num:  2 \n",
            " Loss:  0.2610257565975189\n",
            "Training:  Iteration No:  232 Worker Num:  3 \n",
            " Loss:  0.264242023229599\n",
            "Training:  Iteration No:  232 Worker Num:  4 \n",
            " Loss:  0.2530146539211273\n",
            "Training:  Iteration No:  232 Worker Num:  5 \n",
            " Loss:  0.24755215644836426\n",
            "Training:  Iteration No:  232 Worker Num:  6 \n",
            " Loss:  0.21083959937095642\n",
            "Training:  Iteration No:  232 Worker Num:  7 \n",
            " Loss:  0.23584915697574615\n",
            "Test:  Iteration No:  232 \n",
            " Loss:  0.2072061117196196\n",
            "Test accuracy:  94.01\n",
            "Training:  Iteration No:  233 Worker Num:  0 \n",
            " Loss:  0.329026997089386\n",
            "Training:  Iteration No:  233 Worker Num:  1 \n",
            " Loss:  0.31111940741539\n",
            "Training:  Iteration No:  233 Worker Num:  2 \n",
            " Loss:  0.30748865008354187\n",
            "Training:  Iteration No:  233 Worker Num:  3 \n",
            " Loss:  0.2212265133857727\n",
            "Training:  Iteration No:  233 Worker Num:  4 \n",
            " Loss:  0.14900092780590057\n",
            "Training:  Iteration No:  233 Worker Num:  5 \n",
            " Loss:  0.3063516914844513\n",
            "Training:  Iteration No:  233 Worker Num:  6 \n",
            " Loss:  0.29655295610427856\n",
            "Training:  Iteration No:  233 Worker Num:  7 \n",
            " Loss:  0.34829482436180115\n",
            "Test:  Iteration No:  233 \n",
            " Loss:  0.20414887457095746\n",
            "Test accuracy:  94.04\n",
            "Training:  Iteration No:  234 Worker Num:  0 \n",
            " Loss:  0.19307203590869904\n",
            "Training:  Iteration No:  234 Worker Num:  1 \n",
            " Loss:  0.2466336488723755\n",
            "Training:  Iteration No:  234 Worker Num:  2 \n",
            " Loss:  0.2059178352355957\n",
            "Training:  Iteration No:  234 Worker Num:  3 \n",
            " Loss:  0.14975358545780182\n",
            "Training:  Iteration No:  234 Worker Num:  4 \n",
            " Loss:  0.17528361082077026\n",
            "Training:  Iteration No:  234 Worker Num:  5 \n",
            " Loss:  0.2853470742702484\n",
            "Training:  Iteration No:  234 Worker Num:  6 \n",
            " Loss:  0.29928669333457947\n",
            "Training:  Iteration No:  234 Worker Num:  7 \n",
            " Loss:  0.2000688910484314\n",
            "Test:  Iteration No:  234 \n",
            " Loss:  0.20328170315751545\n",
            "Test accuracy:  94.0\n",
            "Training:  Iteration No:  235 Worker Num:  0 \n",
            " Loss:  0.20014262199401855\n",
            "Training:  Iteration No:  235 Worker Num:  1 \n",
            " Loss:  0.1933213770389557\n",
            "Training:  Iteration No:  235 Worker Num:  2 \n",
            " Loss:  0.23427033424377441\n",
            "Training:  Iteration No:  235 Worker Num:  3 \n",
            " Loss:  0.2150300294160843\n",
            "Training:  Iteration No:  235 Worker Num:  4 \n",
            " Loss:  0.40366533398628235\n",
            "Training:  Iteration No:  235 Worker Num:  5 \n",
            " Loss:  0.21373434364795685\n",
            "Training:  Iteration No:  235 Worker Num:  6 \n",
            " Loss:  0.4155728220939636\n",
            "Training:  Iteration No:  235 Worker Num:  7 \n",
            " Loss:  0.2871348559856415\n",
            "Test:  Iteration No:  235 \n",
            " Loss:  0.20253001296114695\n",
            "Test accuracy:  94.08\n",
            "Training:  Iteration No:  236 Worker Num:  0 \n",
            " Loss:  0.3008517920970917\n",
            "Training:  Iteration No:  236 Worker Num:  1 \n",
            " Loss:  0.26210206747055054\n",
            "Training:  Iteration No:  236 Worker Num:  2 \n",
            " Loss:  0.19158031046390533\n",
            "Training:  Iteration No:  236 Worker Num:  3 \n",
            " Loss:  0.21373973786830902\n",
            "Training:  Iteration No:  236 Worker Num:  4 \n",
            " Loss:  0.2595057189464569\n",
            "Training:  Iteration No:  236 Worker Num:  5 \n",
            " Loss:  0.16504885256290436\n",
            "Training:  Iteration No:  236 Worker Num:  6 \n",
            " Loss:  0.3166688084602356\n",
            "Training:  Iteration No:  236 Worker Num:  7 \n",
            " Loss:  0.3413951098918915\n",
            "Test:  Iteration No:  236 \n",
            " Loss:  0.2057557726939079\n",
            "Test accuracy:  93.99\n",
            "Training:  Iteration No:  237 Worker Num:  0 \n",
            " Loss:  0.27736997604370117\n",
            "Training:  Iteration No:  237 Worker Num:  1 \n",
            " Loss:  0.22453957796096802\n",
            "Training:  Iteration No:  237 Worker Num:  2 \n",
            " Loss:  0.27153220772743225\n",
            "Training:  Iteration No:  237 Worker Num:  3 \n",
            " Loss:  0.22017812728881836\n",
            "Training:  Iteration No:  237 Worker Num:  4 \n",
            " Loss:  0.2570485472679138\n",
            "Training:  Iteration No:  237 Worker Num:  5 \n",
            " Loss:  0.2501257359981537\n",
            "Training:  Iteration No:  237 Worker Num:  6 \n",
            " Loss:  0.38074415922164917\n",
            "Training:  Iteration No:  237 Worker Num:  7 \n",
            " Loss:  0.21587686240673065\n",
            "Test:  Iteration No:  237 \n",
            " Loss:  0.20357326620930358\n",
            "Test accuracy:  93.89\n",
            "Training:  Iteration No:  238 Worker Num:  0 \n",
            " Loss:  0.28710752725601196\n",
            "Training:  Iteration No:  238 Worker Num:  1 \n",
            " Loss:  0.20203667879104614\n",
            "Training:  Iteration No:  238 Worker Num:  2 \n",
            " Loss:  0.326986700296402\n",
            "Training:  Iteration No:  238 Worker Num:  3 \n",
            " Loss:  0.2254997193813324\n",
            "Training:  Iteration No:  238 Worker Num:  4 \n",
            " Loss:  0.22305560111999512\n",
            "Training:  Iteration No:  238 Worker Num:  5 \n",
            " Loss:  0.25872886180877686\n",
            "Training:  Iteration No:  238 Worker Num:  6 \n",
            " Loss:  0.30474239587783813\n",
            "Training:  Iteration No:  238 Worker Num:  7 \n",
            " Loss:  0.27673768997192383\n",
            "Test:  Iteration No:  238 \n",
            " Loss:  0.20237283986297588\n",
            "Test accuracy:  94.0\n",
            "Training:  Iteration No:  239 Worker Num:  0 \n",
            " Loss:  0.22768104076385498\n",
            "Training:  Iteration No:  239 Worker Num:  1 \n",
            " Loss:  0.14573393762111664\n",
            "Training:  Iteration No:  239 Worker Num:  2 \n",
            " Loss:  0.3364897668361664\n",
            "Training:  Iteration No:  239 Worker Num:  3 \n",
            " Loss:  0.19251590967178345\n",
            "Training:  Iteration No:  239 Worker Num:  4 \n",
            " Loss:  0.20134417712688446\n",
            "Training:  Iteration No:  239 Worker Num:  5 \n",
            " Loss:  0.2833736538887024\n",
            "Training:  Iteration No:  239 Worker Num:  6 \n",
            " Loss:  0.20093175768852234\n",
            "Training:  Iteration No:  239 Worker Num:  7 \n",
            " Loss:  0.22911377251148224\n",
            "Test:  Iteration No:  239 \n",
            " Loss:  0.2037581398705893\n",
            "Test accuracy:  94.01\n",
            "Training:  Iteration No:  240 Worker Num:  0 \n",
            " Loss:  0.25540462136268616\n",
            "Training:  Iteration No:  240 Worker Num:  1 \n",
            " Loss:  0.24645256996154785\n",
            "Training:  Iteration No:  240 Worker Num:  2 \n",
            " Loss:  0.33704692125320435\n",
            "Training:  Iteration No:  240 Worker Num:  3 \n",
            " Loss:  0.2821054458618164\n",
            "Training:  Iteration No:  240 Worker Num:  4 \n",
            " Loss:  0.26890820264816284\n",
            "Training:  Iteration No:  240 Worker Num:  5 \n",
            " Loss:  0.2289668619632721\n",
            "Training:  Iteration No:  240 Worker Num:  6 \n",
            " Loss:  0.22600731253623962\n",
            "Training:  Iteration No:  240 Worker Num:  7 \n",
            " Loss:  0.23645488917827606\n",
            "Test:  Iteration No:  240 \n",
            " Loss:  0.19918101625163345\n",
            "Test accuracy:  94.05\n",
            "Training:  Iteration No:  241 Worker Num:  0 \n",
            " Loss:  0.18664871156215668\n",
            "Training:  Iteration No:  241 Worker Num:  1 \n",
            " Loss:  0.3351312577724457\n",
            "Training:  Iteration No:  241 Worker Num:  2 \n",
            " Loss:  0.2769623398780823\n",
            "Training:  Iteration No:  241 Worker Num:  3 \n",
            " Loss:  0.2099560648202896\n",
            "Training:  Iteration No:  241 Worker Num:  4 \n",
            " Loss:  0.24433733522891998\n",
            "Training:  Iteration No:  241 Worker Num:  5 \n",
            " Loss:  0.3433302640914917\n",
            "Training:  Iteration No:  241 Worker Num:  6 \n",
            " Loss:  0.3152030408382416\n",
            "Training:  Iteration No:  241 Worker Num:  7 \n",
            " Loss:  0.22384998202323914\n",
            "Test:  Iteration No:  241 \n",
            " Loss:  0.2006461644380153\n",
            "Test accuracy:  94.06\n",
            "Training:  Iteration No:  242 Worker Num:  0 \n",
            " Loss:  0.25863590836524963\n",
            "Training:  Iteration No:  242 Worker Num:  1 \n",
            " Loss:  0.4441363215446472\n",
            "Training:  Iteration No:  242 Worker Num:  2 \n",
            " Loss:  0.2756573557853699\n",
            "Training:  Iteration No:  242 Worker Num:  3 \n",
            " Loss:  0.2729628086090088\n",
            "Training:  Iteration No:  242 Worker Num:  4 \n",
            " Loss:  0.3694765567779541\n",
            "Training:  Iteration No:  242 Worker Num:  5 \n",
            " Loss:  0.3056553900241852\n",
            "Training:  Iteration No:  242 Worker Num:  6 \n",
            " Loss:  0.2751680910587311\n",
            "Training:  Iteration No:  242 Worker Num:  7 \n",
            " Loss:  0.2087055742740631\n",
            "Test:  Iteration No:  242 \n",
            " Loss:  0.2023199240005092\n",
            "Test accuracy:  94.09\n",
            "Training:  Iteration No:  243 Worker Num:  0 \n",
            " Loss:  0.19151683151721954\n",
            "Training:  Iteration No:  243 Worker Num:  1 \n",
            " Loss:  0.20317314565181732\n",
            "Training:  Iteration No:  243 Worker Num:  2 \n",
            " Loss:  0.2668672502040863\n",
            "Training:  Iteration No:  243 Worker Num:  3 \n",
            " Loss:  0.17093534767627716\n",
            "Training:  Iteration No:  243 Worker Num:  4 \n",
            " Loss:  0.1381133496761322\n",
            "Training:  Iteration No:  243 Worker Num:  5 \n",
            " Loss:  0.19218413531780243\n",
            "Training:  Iteration No:  243 Worker Num:  6 \n",
            " Loss:  0.25207439064979553\n",
            "Training:  Iteration No:  243 Worker Num:  7 \n",
            " Loss:  0.2945083975791931\n",
            "Test:  Iteration No:  243 \n",
            " Loss:  0.19682801481855067\n",
            "Test accuracy:  94.16\n",
            "Training:  Iteration No:  244 Worker Num:  0 \n",
            " Loss:  0.20507009327411652\n",
            "Training:  Iteration No:  244 Worker Num:  1 \n",
            " Loss:  0.3922376036643982\n",
            "Training:  Iteration No:  244 Worker Num:  2 \n",
            " Loss:  0.3371158242225647\n",
            "Training:  Iteration No:  244 Worker Num:  3 \n",
            " Loss:  0.2104319930076599\n",
            "Training:  Iteration No:  244 Worker Num:  4 \n",
            " Loss:  0.31346067786216736\n",
            "Training:  Iteration No:  244 Worker Num:  5 \n",
            " Loss:  0.18361009657382965\n",
            "Training:  Iteration No:  244 Worker Num:  6 \n",
            " Loss:  0.19184201955795288\n",
            "Training:  Iteration No:  244 Worker Num:  7 \n",
            " Loss:  0.3648446202278137\n",
            "Test:  Iteration No:  244 \n",
            " Loss:  0.19891589883526292\n",
            "Test accuracy:  94.19\n",
            "Training:  Iteration No:  245 Worker Num:  0 \n",
            " Loss:  0.23600934445858002\n",
            "Training:  Iteration No:  245 Worker Num:  1 \n",
            " Loss:  0.21297356486320496\n",
            "Training:  Iteration No:  245 Worker Num:  2 \n",
            " Loss:  0.19789859652519226\n",
            "Training:  Iteration No:  245 Worker Num:  3 \n",
            " Loss:  0.1254558563232422\n",
            "Training:  Iteration No:  245 Worker Num:  4 \n",
            " Loss:  0.18515673279762268\n",
            "Training:  Iteration No:  245 Worker Num:  5 \n",
            " Loss:  0.26403528451919556\n",
            "Training:  Iteration No:  245 Worker Num:  6 \n",
            " Loss:  0.24995604157447815\n",
            "Training:  Iteration No:  245 Worker Num:  7 \n",
            " Loss:  0.22584672272205353\n",
            "Test:  Iteration No:  245 \n",
            " Loss:  0.19467808788382931\n",
            "Test accuracy:  94.22\n",
            "Training:  Iteration No:  246 Worker Num:  0 \n",
            " Loss:  0.2533935606479645\n",
            "Training:  Iteration No:  246 Worker Num:  1 \n",
            " Loss:  0.10477535426616669\n",
            "Training:  Iteration No:  246 Worker Num:  2 \n",
            " Loss:  0.2561706006526947\n",
            "Training:  Iteration No:  246 Worker Num:  3 \n",
            " Loss:  0.3433382213115692\n",
            "Training:  Iteration No:  246 Worker Num:  4 \n",
            " Loss:  0.2902914881706238\n",
            "Training:  Iteration No:  246 Worker Num:  5 \n",
            " Loss:  0.23583045601844788\n",
            "Training:  Iteration No:  246 Worker Num:  6 \n",
            " Loss:  0.29358410835266113\n",
            "Training:  Iteration No:  246 Worker Num:  7 \n",
            " Loss:  0.14857026934623718\n",
            "Test:  Iteration No:  246 \n",
            " Loss:  0.19569978606002994\n",
            "Test accuracy:  94.19\n",
            "Training:  Iteration No:  247 Worker Num:  0 \n",
            " Loss:  0.1989845186471939\n",
            "Training:  Iteration No:  247 Worker Num:  1 \n",
            " Loss:  0.18395398557186127\n",
            "Training:  Iteration No:  247 Worker Num:  2 \n",
            " Loss:  0.26989343762397766\n",
            "Training:  Iteration No:  247 Worker Num:  3 \n",
            " Loss:  0.19704540073871613\n",
            "Training:  Iteration No:  247 Worker Num:  4 \n",
            " Loss:  0.14647914469242096\n",
            "Training:  Iteration No:  247 Worker Num:  5 \n",
            " Loss:  0.3143656253814697\n",
            "Training:  Iteration No:  247 Worker Num:  6 \n",
            " Loss:  0.2048925906419754\n",
            "Training:  Iteration No:  247 Worker Num:  7 \n",
            " Loss:  0.3128998577594757\n",
            "Test:  Iteration No:  247 \n",
            " Loss:  0.19934216779479874\n",
            "Test accuracy:  94.03\n",
            "Training:  Iteration No:  248 Worker Num:  0 \n",
            " Loss:  0.20723676681518555\n",
            "Training:  Iteration No:  248 Worker Num:  1 \n",
            " Loss:  0.26590216159820557\n",
            "Training:  Iteration No:  248 Worker Num:  2 \n",
            " Loss:  0.15461964905261993\n",
            "Training:  Iteration No:  248 Worker Num:  3 \n",
            " Loss:  0.19869478046894073\n",
            "Training:  Iteration No:  248 Worker Num:  4 \n",
            " Loss:  0.2587306499481201\n",
            "Training:  Iteration No:  248 Worker Num:  5 \n",
            " Loss:  0.1617998629808426\n",
            "Training:  Iteration No:  248 Worker Num:  6 \n",
            " Loss:  0.2422817200422287\n",
            "Training:  Iteration No:  248 Worker Num:  7 \n",
            " Loss:  0.14809991419315338\n",
            "Test:  Iteration No:  248 \n",
            " Loss:  0.19707068525066104\n",
            "Test accuracy:  94.21\n",
            "Training:  Iteration No:  249 Worker Num:  0 \n",
            " Loss:  0.21628306806087494\n",
            "Training:  Iteration No:  249 Worker Num:  1 \n",
            " Loss:  0.1119084507226944\n",
            "Training:  Iteration No:  249 Worker Num:  2 \n",
            " Loss:  0.4157772958278656\n",
            "Training:  Iteration No:  249 Worker Num:  3 \n",
            " Loss:  0.20142416656017303\n",
            "Training:  Iteration No:  249 Worker Num:  4 \n",
            " Loss:  0.2465842068195343\n",
            "Training:  Iteration No:  249 Worker Num:  5 \n",
            " Loss:  0.16288527846336365\n",
            "Training:  Iteration No:  249 Worker Num:  6 \n",
            " Loss:  0.18629658222198486\n",
            "Training:  Iteration No:  249 Worker Num:  7 \n",
            " Loss:  0.31978195905685425\n",
            "Test:  Iteration No:  249 \n",
            " Loss:  0.19391487772234633\n",
            "Test accuracy:  94.1\n",
            "Training:  Iteration No:  250 Worker Num:  0 \n",
            " Loss:  0.2265290766954422\n",
            "Training:  Iteration No:  250 Worker Num:  1 \n",
            " Loss:  0.22362631559371948\n",
            "Training:  Iteration No:  250 Worker Num:  2 \n",
            " Loss:  0.15059615671634674\n",
            "Training:  Iteration No:  250 Worker Num:  3 \n",
            " Loss:  0.2339685708284378\n",
            "Training:  Iteration No:  250 Worker Num:  4 \n",
            " Loss:  0.3243017792701721\n",
            "Training:  Iteration No:  250 Worker Num:  5 \n",
            " Loss:  0.28296807408332825\n",
            "Training:  Iteration No:  250 Worker Num:  6 \n",
            " Loss:  0.2494240701198578\n",
            "Training:  Iteration No:  250 Worker Num:  7 \n",
            " Loss:  0.3565364181995392\n",
            "Test:  Iteration No:  250 \n",
            " Loss:  0.19396365525884718\n",
            "Test accuracy:  94.17\n",
            "Training:  Iteration No:  251 Worker Num:  0 \n",
            " Loss:  0.24065305292606354\n",
            "Training:  Iteration No:  251 Worker Num:  1 \n",
            " Loss:  0.12985378503799438\n",
            "Training:  Iteration No:  251 Worker Num:  2 \n",
            " Loss:  0.2068692296743393\n",
            "Training:  Iteration No:  251 Worker Num:  3 \n",
            " Loss:  0.30083611607551575\n",
            "Training:  Iteration No:  251 Worker Num:  4 \n",
            " Loss:  0.31217530369758606\n",
            "Training:  Iteration No:  251 Worker Num:  5 \n",
            " Loss:  0.2515242099761963\n",
            "Training:  Iteration No:  251 Worker Num:  6 \n",
            " Loss:  0.2824026942253113\n",
            "Training:  Iteration No:  251 Worker Num:  7 \n",
            " Loss:  0.31149518489837646\n",
            "Test:  Iteration No:  251 \n",
            " Loss:  0.19364584690030617\n",
            "Test accuracy:  94.17\n",
            "Training:  Iteration No:  252 Worker Num:  0 \n",
            " Loss:  0.21743471920490265\n",
            "Training:  Iteration No:  252 Worker Num:  1 \n",
            " Loss:  0.30921831727027893\n",
            "Training:  Iteration No:  252 Worker Num:  2 \n",
            " Loss:  0.2370394915342331\n",
            "Training:  Iteration No:  252 Worker Num:  3 \n",
            " Loss:  0.3003109097480774\n",
            "Training:  Iteration No:  252 Worker Num:  4 \n",
            " Loss:  0.33037933707237244\n",
            "Training:  Iteration No:  252 Worker Num:  5 \n",
            " Loss:  0.35858598351478577\n",
            "Training:  Iteration No:  252 Worker Num:  6 \n",
            " Loss:  0.21419429779052734\n",
            "Training:  Iteration No:  252 Worker Num:  7 \n",
            " Loss:  0.19993779063224792\n",
            "Test:  Iteration No:  252 \n",
            " Loss:  0.1914131122676632\n",
            "Test accuracy:  94.12\n",
            "Training:  Iteration No:  253 Worker Num:  0 \n",
            " Loss:  0.29942429065704346\n",
            "Training:  Iteration No:  253 Worker Num:  1 \n",
            " Loss:  0.33409133553504944\n",
            "Training:  Iteration No:  253 Worker Num:  2 \n",
            " Loss:  0.280004620552063\n",
            "Training:  Iteration No:  253 Worker Num:  3 \n",
            " Loss:  0.2239743173122406\n",
            "Training:  Iteration No:  253 Worker Num:  4 \n",
            " Loss:  0.32140278816223145\n",
            "Training:  Iteration No:  253 Worker Num:  5 \n",
            " Loss:  0.17011015117168427\n",
            "Training:  Iteration No:  253 Worker Num:  6 \n",
            " Loss:  0.17699971795082092\n",
            "Training:  Iteration No:  253 Worker Num:  7 \n",
            " Loss:  0.3249063491821289\n",
            "Test:  Iteration No:  253 \n",
            " Loss:  0.1897121088673608\n",
            "Test accuracy:  94.19\n",
            "Training:  Iteration No:  254 Worker Num:  0 \n",
            " Loss:  0.31995564699172974\n",
            "Training:  Iteration No:  254 Worker Num:  1 \n",
            " Loss:  0.17295727133750916\n",
            "Training:  Iteration No:  254 Worker Num:  2 \n",
            " Loss:  0.26895374059677124\n",
            "Training:  Iteration No:  254 Worker Num:  3 \n",
            " Loss:  0.28899240493774414\n",
            "Training:  Iteration No:  254 Worker Num:  4 \n",
            " Loss:  0.35112378001213074\n",
            "Training:  Iteration No:  254 Worker Num:  5 \n",
            " Loss:  0.19938890635967255\n",
            "Training:  Iteration No:  254 Worker Num:  6 \n",
            " Loss:  0.3110158145427704\n",
            "Training:  Iteration No:  254 Worker Num:  7 \n",
            " Loss:  0.33724522590637207\n",
            "Test:  Iteration No:  254 \n",
            " Loss:  0.18864936009049416\n",
            "Test accuracy:  94.41\n",
            "Training:  Iteration No:  255 Worker Num:  0 \n",
            " Loss:  0.262748658657074\n",
            "Training:  Iteration No:  255 Worker Num:  1 \n",
            " Loss:  0.3449585735797882\n",
            "Training:  Iteration No:  255 Worker Num:  2 \n",
            " Loss:  0.32728150486946106\n",
            "Training:  Iteration No:  255 Worker Num:  3 \n",
            " Loss:  0.33030810952186584\n",
            "Training:  Iteration No:  255 Worker Num:  4 \n",
            " Loss:  0.24821370840072632\n",
            "Training:  Iteration No:  255 Worker Num:  5 \n",
            " Loss:  0.15353935956954956\n",
            "Training:  Iteration No:  255 Worker Num:  6 \n",
            " Loss:  0.23127058148384094\n",
            "Training:  Iteration No:  255 Worker Num:  7 \n",
            " Loss:  0.19605331122875214\n",
            "Test:  Iteration No:  255 \n",
            " Loss:  0.1881147890198457\n",
            "Test accuracy:  94.48\n",
            "Training:  Iteration No:  256 Worker Num:  0 \n",
            " Loss:  0.25567826628685\n",
            "Training:  Iteration No:  256 Worker Num:  1 \n",
            " Loss:  0.1967087835073471\n",
            "Training:  Iteration No:  256 Worker Num:  2 \n",
            " Loss:  0.23363657295703888\n",
            "Training:  Iteration No:  256 Worker Num:  3 \n",
            " Loss:  0.2277924120426178\n",
            "Training:  Iteration No:  256 Worker Num:  4 \n",
            " Loss:  0.17912225425243378\n",
            "Training:  Iteration No:  256 Worker Num:  5 \n",
            " Loss:  0.17249776422977448\n",
            "Training:  Iteration No:  256 Worker Num:  6 \n",
            " Loss:  0.21395571529865265\n",
            "Training:  Iteration No:  256 Worker Num:  7 \n",
            " Loss:  0.27007725834846497\n",
            "Test:  Iteration No:  256 \n",
            " Loss:  0.19100936519901587\n",
            "Test accuracy:  94.3\n",
            "Training:  Iteration No:  257 Worker Num:  0 \n",
            " Loss:  0.2128656804561615\n",
            "Training:  Iteration No:  257 Worker Num:  1 \n",
            " Loss:  0.34787020087242126\n",
            "Training:  Iteration No:  257 Worker Num:  2 \n",
            " Loss:  0.21715231239795685\n",
            "Training:  Iteration No:  257 Worker Num:  3 \n",
            " Loss:  0.24940967559814453\n",
            "Training:  Iteration No:  257 Worker Num:  4 \n",
            " Loss:  0.31336820125579834\n",
            "Training:  Iteration No:  257 Worker Num:  5 \n",
            " Loss:  0.2430679202079773\n",
            "Training:  Iteration No:  257 Worker Num:  6 \n",
            " Loss:  0.17574860155582428\n",
            "Training:  Iteration No:  257 Worker Num:  7 \n",
            " Loss:  0.28985220193862915\n",
            "Test:  Iteration No:  257 \n",
            " Loss:  0.18908128222521348\n",
            "Test accuracy:  94.33\n",
            "Training:  Iteration No:  258 Worker Num:  0 \n",
            " Loss:  0.19418568909168243\n",
            "Training:  Iteration No:  258 Worker Num:  1 \n",
            " Loss:  0.15730154514312744\n",
            "Training:  Iteration No:  258 Worker Num:  2 \n",
            " Loss:  0.2075454741716385\n",
            "Training:  Iteration No:  258 Worker Num:  3 \n",
            " Loss:  0.22749066352844238\n",
            "Training:  Iteration No:  258 Worker Num:  4 \n",
            " Loss:  0.14105193316936493\n",
            "Training:  Iteration No:  258 Worker Num:  5 \n",
            " Loss:  0.207268625497818\n",
            "Training:  Iteration No:  258 Worker Num:  6 \n",
            " Loss:  0.3342309594154358\n",
            "Training:  Iteration No:  258 Worker Num:  7 \n",
            " Loss:  0.27609288692474365\n",
            "Test:  Iteration No:  258 \n",
            " Loss:  0.19087694544062206\n",
            "Test accuracy:  94.21\n",
            "Training:  Iteration No:  259 Worker Num:  0 \n",
            " Loss:  0.19690194725990295\n",
            "Training:  Iteration No:  259 Worker Num:  1 \n",
            " Loss:  0.3312450051307678\n",
            "Training:  Iteration No:  259 Worker Num:  2 \n",
            " Loss:  0.18618933856487274\n",
            "Training:  Iteration No:  259 Worker Num:  3 \n",
            " Loss:  0.22559109330177307\n",
            "Training:  Iteration No:  259 Worker Num:  4 \n",
            " Loss:  0.31677231192588806\n",
            "Training:  Iteration No:  259 Worker Num:  5 \n",
            " Loss:  0.17012980580329895\n",
            "Training:  Iteration No:  259 Worker Num:  6 \n",
            " Loss:  0.3728908896446228\n",
            "Training:  Iteration No:  259 Worker Num:  7 \n",
            " Loss:  0.29915690422058105\n",
            "Test:  Iteration No:  259 \n",
            " Loss:  0.1906558938819585\n",
            "Test accuracy:  94.27\n",
            "Training:  Iteration No:  260 Worker Num:  0 \n",
            " Loss:  0.1537061482667923\n",
            "Training:  Iteration No:  260 Worker Num:  1 \n",
            " Loss:  0.20371486246585846\n",
            "Training:  Iteration No:  260 Worker Num:  2 \n",
            " Loss:  0.28874608874320984\n",
            "Training:  Iteration No:  260 Worker Num:  3 \n",
            " Loss:  0.20097705721855164\n",
            "Training:  Iteration No:  260 Worker Num:  4 \n",
            " Loss:  0.14158697426319122\n",
            "Training:  Iteration No:  260 Worker Num:  5 \n",
            " Loss:  0.23176133632659912\n",
            "Training:  Iteration No:  260 Worker Num:  6 \n",
            " Loss:  0.36255621910095215\n",
            "Training:  Iteration No:  260 Worker Num:  7 \n",
            " Loss:  0.1333671659231186\n",
            "Test:  Iteration No:  260 \n",
            " Loss:  0.1953759335358686\n",
            "Test accuracy:  94.04\n",
            "Training:  Iteration No:  261 Worker Num:  0 \n",
            " Loss:  0.1909397840499878\n",
            "Training:  Iteration No:  261 Worker Num:  1 \n",
            " Loss:  0.21574287116527557\n",
            "Training:  Iteration No:  261 Worker Num:  2 \n",
            " Loss:  0.30204078555107117\n",
            "Training:  Iteration No:  261 Worker Num:  3 \n",
            " Loss:  0.28722864389419556\n",
            "Training:  Iteration No:  261 Worker Num:  4 \n",
            " Loss:  0.2913164794445038\n",
            "Training:  Iteration No:  261 Worker Num:  5 \n",
            " Loss:  0.17855176329612732\n",
            "Training:  Iteration No:  261 Worker Num:  6 \n",
            " Loss:  0.23785951733589172\n",
            "Training:  Iteration No:  261 Worker Num:  7 \n",
            " Loss:  0.17493250966072083\n",
            "Test:  Iteration No:  261 \n",
            " Loss:  0.19169706293629318\n",
            "Test accuracy:  94.3\n",
            "Training:  Iteration No:  262 Worker Num:  0 \n",
            " Loss:  0.2015947550535202\n",
            "Training:  Iteration No:  262 Worker Num:  1 \n",
            " Loss:  0.24495629966259003\n",
            "Training:  Iteration No:  262 Worker Num:  2 \n",
            " Loss:  0.17583458125591278\n",
            "Training:  Iteration No:  262 Worker Num:  3 \n",
            " Loss:  0.32733461260795593\n",
            "Training:  Iteration No:  262 Worker Num:  4 \n",
            " Loss:  0.3171859085559845\n",
            "Training:  Iteration No:  262 Worker Num:  5 \n",
            " Loss:  0.23476970195770264\n",
            "Training:  Iteration No:  262 Worker Num:  6 \n",
            " Loss:  0.2528916001319885\n",
            "Training:  Iteration No:  262 Worker Num:  7 \n",
            " Loss:  0.19529154896736145\n",
            "Test:  Iteration No:  262 \n",
            " Loss:  0.1894348300050331\n",
            "Test accuracy:  94.28\n",
            "Training:  Iteration No:  263 Worker Num:  0 \n",
            " Loss:  0.22262616455554962\n",
            "Training:  Iteration No:  263 Worker Num:  1 \n",
            " Loss:  0.19412943720817566\n",
            "Training:  Iteration No:  263 Worker Num:  2 \n",
            " Loss:  0.33019179105758667\n",
            "Training:  Iteration No:  263 Worker Num:  3 \n",
            " Loss:  0.24318000674247742\n",
            "Training:  Iteration No:  263 Worker Num:  4 \n",
            " Loss:  0.18388549983501434\n",
            "Training:  Iteration No:  263 Worker Num:  5 \n",
            " Loss:  0.2644297480583191\n",
            "Training:  Iteration No:  263 Worker Num:  6 \n",
            " Loss:  0.20471638441085815\n",
            "Training:  Iteration No:  263 Worker Num:  7 \n",
            " Loss:  0.2998143434524536\n",
            "Test:  Iteration No:  263 \n",
            " Loss:  0.19134690235309962\n",
            "Test accuracy:  94.12\n",
            "Training:  Iteration No:  264 Worker Num:  0 \n",
            " Loss:  0.2703043818473816\n",
            "Training:  Iteration No:  264 Worker Num:  1 \n",
            " Loss:  0.24346330761909485\n",
            "Training:  Iteration No:  264 Worker Num:  2 \n",
            " Loss:  0.26694339513778687\n",
            "Training:  Iteration No:  264 Worker Num:  3 \n",
            " Loss:  0.21345661580562592\n",
            "Training:  Iteration No:  264 Worker Num:  4 \n",
            " Loss:  0.19620275497436523\n",
            "Training:  Iteration No:  264 Worker Num:  5 \n",
            " Loss:  0.16962634027004242\n",
            "Training:  Iteration No:  264 Worker Num:  6 \n",
            " Loss:  0.4129577875137329\n",
            "Training:  Iteration No:  264 Worker Num:  7 \n",
            " Loss:  0.31252196431159973\n",
            "Test:  Iteration No:  264 \n",
            " Loss:  0.1879378621454669\n",
            "Test accuracy:  94.31\n",
            "Training:  Iteration No:  265 Worker Num:  0 \n",
            " Loss:  0.2472987323999405\n",
            "Training:  Iteration No:  265 Worker Num:  1 \n",
            " Loss:  0.18998019397258759\n",
            "Training:  Iteration No:  265 Worker Num:  2 \n",
            " Loss:  0.30284222960472107\n",
            "Training:  Iteration No:  265 Worker Num:  3 \n",
            " Loss:  0.25872984528541565\n",
            "Training:  Iteration No:  265 Worker Num:  4 \n",
            " Loss:  0.20904967188835144\n",
            "Training:  Iteration No:  265 Worker Num:  5 \n",
            " Loss:  0.25351887941360474\n",
            "Training:  Iteration No:  265 Worker Num:  6 \n",
            " Loss:  0.17010816931724548\n",
            "Training:  Iteration No:  265 Worker Num:  7 \n",
            " Loss:  0.23360629379749298\n",
            "Test:  Iteration No:  265 \n",
            " Loss:  0.18538305487578052\n",
            "Test accuracy:  94.4\n",
            "Training:  Iteration No:  266 Worker Num:  0 \n",
            " Loss:  0.2893070578575134\n",
            "Training:  Iteration No:  266 Worker Num:  1 \n",
            " Loss:  0.2340724915266037\n",
            "Training:  Iteration No:  266 Worker Num:  2 \n",
            " Loss:  0.24632714688777924\n",
            "Training:  Iteration No:  266 Worker Num:  3 \n",
            " Loss:  0.3489348292350769\n",
            "Training:  Iteration No:  266 Worker Num:  4 \n",
            " Loss:  0.2088196575641632\n",
            "Training:  Iteration No:  266 Worker Num:  5 \n",
            " Loss:  0.2485474795103073\n",
            "Training:  Iteration No:  266 Worker Num:  6 \n",
            " Loss:  0.2567758560180664\n",
            "Training:  Iteration No:  266 Worker Num:  7 \n",
            " Loss:  0.1768675595521927\n",
            "Test:  Iteration No:  266 \n",
            " Loss:  0.18354198482783535\n",
            "Test accuracy:  94.46\n",
            "Training:  Iteration No:  267 Worker Num:  0 \n",
            " Loss:  0.21124553680419922\n",
            "Training:  Iteration No:  267 Worker Num:  1 \n",
            " Loss:  0.3466717004776001\n",
            "Training:  Iteration No:  267 Worker Num:  2 \n",
            " Loss:  0.13195520639419556\n",
            "Training:  Iteration No:  267 Worker Num:  3 \n",
            " Loss:  0.1708565652370453\n",
            "Training:  Iteration No:  267 Worker Num:  4 \n",
            " Loss:  0.2900843024253845\n",
            "Training:  Iteration No:  267 Worker Num:  5 \n",
            " Loss:  0.35467058420181274\n",
            "Training:  Iteration No:  267 Worker Num:  6 \n",
            " Loss:  0.14532703161239624\n",
            "Training:  Iteration No:  267 Worker Num:  7 \n",
            " Loss:  0.2525876462459564\n",
            "Test:  Iteration No:  267 \n",
            " Loss:  0.1845854214640169\n",
            "Test accuracy:  94.23\n",
            "Training:  Iteration No:  268 Worker Num:  0 \n",
            " Loss:  0.26495110988616943\n",
            "Training:  Iteration No:  268 Worker Num:  1 \n",
            " Loss:  0.17686592042446136\n",
            "Training:  Iteration No:  268 Worker Num:  2 \n",
            " Loss:  0.25916236639022827\n",
            "Training:  Iteration No:  268 Worker Num:  3 \n",
            " Loss:  0.2887588143348694\n",
            "Training:  Iteration No:  268 Worker Num:  4 \n",
            " Loss:  0.17790278792381287\n",
            "Training:  Iteration No:  268 Worker Num:  5 \n",
            " Loss:  0.33018386363983154\n",
            "Training:  Iteration No:  268 Worker Num:  6 \n",
            " Loss:  0.27061766386032104\n",
            "Training:  Iteration No:  268 Worker Num:  7 \n",
            " Loss:  0.283801406621933\n",
            "Test:  Iteration No:  268 \n",
            " Loss:  0.1828098252203457\n",
            "Test accuracy:  94.58\n",
            "Training:  Iteration No:  269 Worker Num:  0 \n",
            " Loss:  0.15478312969207764\n",
            "Training:  Iteration No:  269 Worker Num:  1 \n",
            " Loss:  0.18181483447551727\n",
            "Training:  Iteration No:  269 Worker Num:  2 \n",
            " Loss:  0.17975066602230072\n",
            "Training:  Iteration No:  269 Worker Num:  3 \n",
            " Loss:  0.2666100263595581\n",
            "Training:  Iteration No:  269 Worker Num:  4 \n",
            " Loss:  0.16375014185905457\n",
            "Training:  Iteration No:  269 Worker Num:  5 \n",
            " Loss:  0.23986145853996277\n",
            "Training:  Iteration No:  269 Worker Num:  6 \n",
            " Loss:  0.2358587235212326\n",
            "Training:  Iteration No:  269 Worker Num:  7 \n",
            " Loss:  0.15522988140583038\n",
            "Test:  Iteration No:  269 \n",
            " Loss:  0.18136510366125952\n",
            "Test accuracy:  94.5\n",
            "Training:  Iteration No:  270 Worker Num:  0 \n",
            " Loss:  0.2265769988298416\n",
            "Training:  Iteration No:  270 Worker Num:  1 \n",
            " Loss:  0.10949377715587616\n",
            "Training:  Iteration No:  270 Worker Num:  2 \n",
            " Loss:  0.18108244240283966\n",
            "Training:  Iteration No:  270 Worker Num:  3 \n",
            " Loss:  0.14285671710968018\n",
            "Training:  Iteration No:  270 Worker Num:  4 \n",
            " Loss:  0.1970146894454956\n",
            "Training:  Iteration No:  270 Worker Num:  5 \n",
            " Loss:  0.4366675615310669\n",
            "Training:  Iteration No:  270 Worker Num:  6 \n",
            " Loss:  0.11532358825206757\n",
            "Training:  Iteration No:  270 Worker Num:  7 \n",
            " Loss:  0.23045207560062408\n",
            "Test:  Iteration No:  270 \n",
            " Loss:  0.18097357670010267\n",
            "Test accuracy:  94.61\n",
            "Training:  Iteration No:  271 Worker Num:  0 \n",
            " Loss:  0.30494925379753113\n",
            "Training:  Iteration No:  271 Worker Num:  1 \n",
            " Loss:  0.16477933526039124\n",
            "Training:  Iteration No:  271 Worker Num:  2 \n",
            " Loss:  0.2724177837371826\n",
            "Training:  Iteration No:  271 Worker Num:  3 \n",
            " Loss:  0.46608278155326843\n",
            "Training:  Iteration No:  271 Worker Num:  4 \n",
            " Loss:  0.15399643778800964\n",
            "Training:  Iteration No:  271 Worker Num:  5 \n",
            " Loss:  0.16062162816524506\n",
            "Training:  Iteration No:  271 Worker Num:  6 \n",
            " Loss:  0.22082588076591492\n",
            "Training:  Iteration No:  271 Worker Num:  7 \n",
            " Loss:  0.16664178669452667\n",
            "Test:  Iteration No:  271 \n",
            " Loss:  0.18133398099460557\n",
            "Test accuracy:  94.5\n",
            "Training:  Iteration No:  272 Worker Num:  0 \n",
            " Loss:  0.24528740346431732\n",
            "Training:  Iteration No:  272 Worker Num:  1 \n",
            " Loss:  0.30863380432128906\n",
            "Training:  Iteration No:  272 Worker Num:  2 \n",
            " Loss:  0.3070514500141144\n",
            "Training:  Iteration No:  272 Worker Num:  3 \n",
            " Loss:  0.22065363824367523\n",
            "Training:  Iteration No:  272 Worker Num:  4 \n",
            " Loss:  0.21310390532016754\n",
            "Training:  Iteration No:  272 Worker Num:  5 \n",
            " Loss:  0.2038981318473816\n",
            "Training:  Iteration No:  272 Worker Num:  6 \n",
            " Loss:  0.1953321099281311\n",
            "Training:  Iteration No:  272 Worker Num:  7 \n",
            " Loss:  0.1934400498867035\n",
            "Test:  Iteration No:  272 \n",
            " Loss:  0.17750405211331724\n",
            "Test accuracy:  94.62\n",
            "Training:  Iteration No:  273 Worker Num:  0 \n",
            " Loss:  0.1616116166114807\n",
            "Training:  Iteration No:  273 Worker Num:  1 \n",
            " Loss:  0.29266688227653503\n",
            "Training:  Iteration No:  273 Worker Num:  2 \n",
            " Loss:  0.3074537515640259\n",
            "Training:  Iteration No:  273 Worker Num:  3 \n",
            " Loss:  0.22726786136627197\n",
            "Training:  Iteration No:  273 Worker Num:  4 \n",
            " Loss:  0.26608309149742126\n",
            "Training:  Iteration No:  273 Worker Num:  5 \n",
            " Loss:  0.25168323516845703\n",
            "Training:  Iteration No:  273 Worker Num:  6 \n",
            " Loss:  0.3728815019130707\n",
            "Training:  Iteration No:  273 Worker Num:  7 \n",
            " Loss:  0.2843399941921234\n",
            "Test:  Iteration No:  273 \n",
            " Loss:  0.17621387718127499\n",
            "Test accuracy:  94.73\n",
            "Training:  Iteration No:  274 Worker Num:  0 \n",
            " Loss:  0.20714473724365234\n",
            "Training:  Iteration No:  274 Worker Num:  1 \n",
            " Loss:  0.3214181661605835\n",
            "Training:  Iteration No:  274 Worker Num:  2 \n",
            " Loss:  0.14596986770629883\n",
            "Training:  Iteration No:  274 Worker Num:  3 \n",
            " Loss:  0.14761076867580414\n",
            "Training:  Iteration No:  274 Worker Num:  4 \n",
            " Loss:  0.25805559754371643\n",
            "Training:  Iteration No:  274 Worker Num:  5 \n",
            " Loss:  0.22794508934020996\n",
            "Training:  Iteration No:  274 Worker Num:  6 \n",
            " Loss:  0.3276056945323944\n",
            "Training:  Iteration No:  274 Worker Num:  7 \n",
            " Loss:  0.1536465436220169\n",
            "Test:  Iteration No:  274 \n",
            " Loss:  0.1781855718692458\n",
            "Test accuracy:  94.68\n",
            "Training:  Iteration No:  275 Worker Num:  0 \n",
            " Loss:  0.14834435284137726\n",
            "Training:  Iteration No:  275 Worker Num:  1 \n",
            " Loss:  0.25753384828567505\n",
            "Training:  Iteration No:  275 Worker Num:  2 \n",
            " Loss:  0.29120776057243347\n",
            "Training:  Iteration No:  275 Worker Num:  3 \n",
            " Loss:  0.27206575870513916\n",
            "Training:  Iteration No:  275 Worker Num:  4 \n",
            " Loss:  0.25472572445869446\n",
            "Training:  Iteration No:  275 Worker Num:  5 \n",
            " Loss:  0.22785350680351257\n",
            "Training:  Iteration No:  275 Worker Num:  6 \n",
            " Loss:  0.1958649754524231\n",
            "Training:  Iteration No:  275 Worker Num:  7 \n",
            " Loss:  0.23033706843852997\n",
            "Test:  Iteration No:  275 \n",
            " Loss:  0.17892320106348283\n",
            "Test accuracy:  94.65\n",
            "Training:  Iteration No:  276 Worker Num:  0 \n",
            " Loss:  0.24044471979141235\n",
            "Training:  Iteration No:  276 Worker Num:  1 \n",
            " Loss:  0.21461936831474304\n",
            "Training:  Iteration No:  276 Worker Num:  2 \n",
            " Loss:  0.30027955770492554\n",
            "Training:  Iteration No:  276 Worker Num:  3 \n",
            " Loss:  0.21483851969242096\n",
            "Training:  Iteration No:  276 Worker Num:  4 \n",
            " Loss:  0.14562565088272095\n",
            "Training:  Iteration No:  276 Worker Num:  5 \n",
            " Loss:  0.15539123117923737\n",
            "Training:  Iteration No:  276 Worker Num:  6 \n",
            " Loss:  0.11599443107843399\n",
            "Training:  Iteration No:  276 Worker Num:  7 \n",
            " Loss:  0.2024034708738327\n",
            "Test:  Iteration No:  276 \n",
            " Loss:  0.177595046602331\n",
            "Test accuracy:  94.64\n",
            "Training:  Iteration No:  277 Worker Num:  0 \n",
            " Loss:  0.1405806690454483\n",
            "Training:  Iteration No:  277 Worker Num:  1 \n",
            " Loss:  0.2574200928211212\n",
            "Training:  Iteration No:  277 Worker Num:  2 \n",
            " Loss:  0.14167940616607666\n",
            "Training:  Iteration No:  277 Worker Num:  3 \n",
            " Loss:  0.3209666907787323\n",
            "Training:  Iteration No:  277 Worker Num:  4 \n",
            " Loss:  0.2733164131641388\n",
            "Training:  Iteration No:  277 Worker Num:  5 \n",
            " Loss:  0.2560898959636688\n",
            "Training:  Iteration No:  277 Worker Num:  6 \n",
            " Loss:  0.07657158374786377\n",
            "Training:  Iteration No:  277 Worker Num:  7 \n",
            " Loss:  0.19982001185417175\n",
            "Test:  Iteration No:  277 \n",
            " Loss:  0.1744265353212817\n",
            "Test accuracy:  94.88\n",
            "Training:  Iteration No:  278 Worker Num:  0 \n",
            " Loss:  0.2752624750137329\n",
            "Training:  Iteration No:  278 Worker Num:  1 \n",
            " Loss:  0.32123863697052\n",
            "Training:  Iteration No:  278 Worker Num:  2 \n",
            " Loss:  0.2420588731765747\n",
            "Training:  Iteration No:  278 Worker Num:  3 \n",
            " Loss:  0.17622697353363037\n",
            "Training:  Iteration No:  278 Worker Num:  4 \n",
            " Loss:  0.2106717824935913\n",
            "Training:  Iteration No:  278 Worker Num:  5 \n",
            " Loss:  0.26445332169532776\n",
            "Training:  Iteration No:  278 Worker Num:  6 \n",
            " Loss:  0.24920547008514404\n",
            "Training:  Iteration No:  278 Worker Num:  7 \n",
            " Loss:  0.3007746636867523\n",
            "Test:  Iteration No:  278 \n",
            " Loss:  0.1731972130180537\n",
            "Test accuracy:  94.89\n",
            "Training:  Iteration No:  279 Worker Num:  0 \n",
            " Loss:  0.2305075079202652\n",
            "Training:  Iteration No:  279 Worker Num:  1 \n",
            " Loss:  0.17490585148334503\n",
            "Training:  Iteration No:  279 Worker Num:  2 \n",
            " Loss:  0.15877050161361694\n",
            "Training:  Iteration No:  279 Worker Num:  3 \n",
            " Loss:  0.18121697008609772\n",
            "Training:  Iteration No:  279 Worker Num:  4 \n",
            " Loss:  0.16403260827064514\n",
            "Training:  Iteration No:  279 Worker Num:  5 \n",
            " Loss:  0.2739178240299225\n",
            "Training:  Iteration No:  279 Worker Num:  6 \n",
            " Loss:  0.21045051515102386\n",
            "Training:  Iteration No:  279 Worker Num:  7 \n",
            " Loss:  0.1178586557507515\n",
            "Test:  Iteration No:  279 \n",
            " Loss:  0.1758245023356491\n",
            "Test accuracy:  94.76\n",
            "Training:  Iteration No:  280 Worker Num:  0 \n",
            " Loss:  0.18593725562095642\n",
            "Training:  Iteration No:  280 Worker Num:  1 \n",
            " Loss:  0.24357742071151733\n",
            "Training:  Iteration No:  280 Worker Num:  2 \n",
            " Loss:  0.2999131977558136\n",
            "Training:  Iteration No:  280 Worker Num:  3 \n",
            " Loss:  0.11451178044080734\n",
            "Training:  Iteration No:  280 Worker Num:  4 \n",
            " Loss:  0.21038997173309326\n",
            "Training:  Iteration No:  280 Worker Num:  5 \n",
            " Loss:  0.2471524327993393\n",
            "Training:  Iteration No:  280 Worker Num:  6 \n",
            " Loss:  0.20175626873970032\n",
            "Training:  Iteration No:  280 Worker Num:  7 \n",
            " Loss:  0.20763817429542542\n",
            "Test:  Iteration No:  280 \n",
            " Loss:  0.17423035295774478\n",
            "Test accuracy:  94.79\n",
            "Training:  Iteration No:  281 Worker Num:  0 \n",
            " Loss:  0.20627613365650177\n",
            "Training:  Iteration No:  281 Worker Num:  1 \n",
            " Loss:  0.17718158662319183\n",
            "Training:  Iteration No:  281 Worker Num:  2 \n",
            " Loss:  0.27010831236839294\n",
            "Training:  Iteration No:  281 Worker Num:  3 \n",
            " Loss:  0.27310603857040405\n",
            "Training:  Iteration No:  281 Worker Num:  4 \n",
            " Loss:  0.22567549347877502\n",
            "Training:  Iteration No:  281 Worker Num:  5 \n",
            " Loss:  0.21371985971927643\n",
            "Training:  Iteration No:  281 Worker Num:  6 \n",
            " Loss:  0.15823502838611603\n",
            "Training:  Iteration No:  281 Worker Num:  7 \n",
            " Loss:  0.18420152366161346\n",
            "Test:  Iteration No:  281 \n",
            " Loss:  0.17194199886267322\n",
            "Test accuracy:  94.87\n",
            "Training:  Iteration No:  282 Worker Num:  0 \n",
            " Loss:  0.30256617069244385\n",
            "Training:  Iteration No:  282 Worker Num:  1 \n",
            " Loss:  0.19934210181236267\n",
            "Training:  Iteration No:  282 Worker Num:  2 \n",
            " Loss:  0.16094747185707092\n",
            "Training:  Iteration No:  282 Worker Num:  3 \n",
            " Loss:  0.190190389752388\n",
            "Training:  Iteration No:  282 Worker Num:  4 \n",
            " Loss:  0.25111523270606995\n",
            "Training:  Iteration No:  282 Worker Num:  5 \n",
            " Loss:  0.18053065240383148\n",
            "Training:  Iteration No:  282 Worker Num:  6 \n",
            " Loss:  0.22518286108970642\n",
            "Training:  Iteration No:  282 Worker Num:  7 \n",
            " Loss:  0.2306199073791504\n",
            "Test:  Iteration No:  282 \n",
            " Loss:  0.17086886046925867\n",
            "Test accuracy:  94.98\n",
            "Training:  Iteration No:  283 Worker Num:  0 \n",
            " Loss:  0.2624590992927551\n",
            "Training:  Iteration No:  283 Worker Num:  1 \n",
            " Loss:  0.27913030982017517\n",
            "Training:  Iteration No:  283 Worker Num:  2 \n",
            " Loss:  0.23862788081169128\n",
            "Training:  Iteration No:  283 Worker Num:  3 \n",
            " Loss:  0.2929181158542633\n",
            "Training:  Iteration No:  283 Worker Num:  4 \n",
            " Loss:  0.2465929090976715\n",
            "Training:  Iteration No:  283 Worker Num:  5 \n",
            " Loss:  0.21970205008983612\n",
            "Training:  Iteration No:  283 Worker Num:  6 \n",
            " Loss:  0.23260939121246338\n",
            "Training:  Iteration No:  283 Worker Num:  7 \n",
            " Loss:  0.35547226667404175\n",
            "Test:  Iteration No:  283 \n",
            " Loss:  0.1722242812614275\n",
            "Test accuracy:  94.92\n",
            "Training:  Iteration No:  284 Worker Num:  0 \n",
            " Loss:  0.2806542217731476\n",
            "Training:  Iteration No:  284 Worker Num:  1 \n",
            " Loss:  0.16367553174495697\n",
            "Training:  Iteration No:  284 Worker Num:  2 \n",
            " Loss:  0.2482558637857437\n",
            "Training:  Iteration No:  284 Worker Num:  3 \n",
            " Loss:  0.11331719160079956\n",
            "Training:  Iteration No:  284 Worker Num:  4 \n",
            " Loss:  0.205131396651268\n",
            "Training:  Iteration No:  284 Worker Num:  5 \n",
            " Loss:  0.25296151638031006\n",
            "Training:  Iteration No:  284 Worker Num:  6 \n",
            " Loss:  0.13321805000305176\n",
            "Training:  Iteration No:  284 Worker Num:  7 \n",
            " Loss:  0.20032736659049988\n",
            "Test:  Iteration No:  284 \n",
            " Loss:  0.17444424162607028\n",
            "Test accuracy:  94.77\n",
            "Training:  Iteration No:  285 Worker Num:  0 \n",
            " Loss:  0.31350505352020264\n",
            "Training:  Iteration No:  285 Worker Num:  1 \n",
            " Loss:  0.193245992064476\n",
            "Training:  Iteration No:  285 Worker Num:  2 \n",
            " Loss:  0.20499008893966675\n",
            "Training:  Iteration No:  285 Worker Num:  3 \n",
            " Loss:  0.2005530744791031\n",
            "Training:  Iteration No:  285 Worker Num:  4 \n",
            " Loss:  0.1387554258108139\n",
            "Training:  Iteration No:  285 Worker Num:  5 \n",
            " Loss:  0.14929604530334473\n",
            "Training:  Iteration No:  285 Worker Num:  6 \n",
            " Loss:  0.1948620229959488\n",
            "Training:  Iteration No:  285 Worker Num:  7 \n",
            " Loss:  0.26474788784980774\n",
            "Test:  Iteration No:  285 \n",
            " Loss:  0.17067821579690598\n",
            "Test accuracy:  94.94\n",
            "Training:  Iteration No:  286 Worker Num:  0 \n",
            " Loss:  0.24486184120178223\n",
            "Training:  Iteration No:  286 Worker Num:  1 \n",
            " Loss:  0.14599774777889252\n",
            "Training:  Iteration No:  286 Worker Num:  2 \n",
            " Loss:  0.16602379083633423\n",
            "Training:  Iteration No:  286 Worker Num:  3 \n",
            " Loss:  0.2103373110294342\n",
            "Training:  Iteration No:  286 Worker Num:  4 \n",
            " Loss:  0.22429947555065155\n",
            "Training:  Iteration No:  286 Worker Num:  5 \n",
            " Loss:  0.2334236353635788\n",
            "Training:  Iteration No:  286 Worker Num:  6 \n",
            " Loss:  0.14648590981960297\n",
            "Training:  Iteration No:  286 Worker Num:  7 \n",
            " Loss:  0.171091690659523\n",
            "Test:  Iteration No:  286 \n",
            " Loss:  0.1730933168702582\n",
            "Test accuracy:  94.95\n",
            "Training:  Iteration No:  287 Worker Num:  0 \n",
            " Loss:  0.21893368661403656\n",
            "Training:  Iteration No:  287 Worker Num:  1 \n",
            " Loss:  0.3605975806713104\n",
            "Training:  Iteration No:  287 Worker Num:  2 \n",
            " Loss:  0.24814894795417786\n",
            "Training:  Iteration No:  287 Worker Num:  3 \n",
            " Loss:  0.2875021994113922\n",
            "Training:  Iteration No:  287 Worker Num:  4 \n",
            " Loss:  0.23693402111530304\n",
            "Training:  Iteration No:  287 Worker Num:  5 \n",
            " Loss:  0.16789792478084564\n",
            "Training:  Iteration No:  287 Worker Num:  6 \n",
            " Loss:  0.24815012514591217\n",
            "Training:  Iteration No:  287 Worker Num:  7 \n",
            " Loss:  0.2090577781200409\n",
            "Test:  Iteration No:  287 \n",
            " Loss:  0.17976913136677652\n",
            "Test accuracy:  94.26\n",
            "Training:  Iteration No:  288 Worker Num:  0 \n",
            " Loss:  0.2563893496990204\n",
            "Training:  Iteration No:  288 Worker Num:  1 \n",
            " Loss:  0.2082592248916626\n",
            "Training:  Iteration No:  288 Worker Num:  2 \n",
            " Loss:  0.14640334248542786\n",
            "Training:  Iteration No:  288 Worker Num:  3 \n",
            " Loss:  0.22118031978607178\n",
            "Training:  Iteration No:  288 Worker Num:  4 \n",
            " Loss:  0.22818776965141296\n",
            "Training:  Iteration No:  288 Worker Num:  5 \n",
            " Loss:  0.3713725805282593\n",
            "Training:  Iteration No:  288 Worker Num:  6 \n",
            " Loss:  0.21978908777236938\n",
            "Training:  Iteration No:  288 Worker Num:  7 \n",
            " Loss:  0.24928393959999084\n",
            "Test:  Iteration No:  288 \n",
            " Loss:  0.17845015975210485\n",
            "Test accuracy:  94.73\n",
            "Training:  Iteration No:  289 Worker Num:  0 \n",
            " Loss:  0.2195342481136322\n",
            "Training:  Iteration No:  289 Worker Num:  1 \n",
            " Loss:  0.15438443422317505\n",
            "Training:  Iteration No:  289 Worker Num:  2 \n",
            " Loss:  0.18664699792861938\n",
            "Training:  Iteration No:  289 Worker Num:  3 \n",
            " Loss:  0.22335781157016754\n",
            "Training:  Iteration No:  289 Worker Num:  4 \n",
            " Loss:  0.18741485476493835\n",
            "Training:  Iteration No:  289 Worker Num:  5 \n",
            " Loss:  0.27210932970046997\n",
            "Training:  Iteration No:  289 Worker Num:  6 \n",
            " Loss:  0.3590470254421234\n",
            "Training:  Iteration No:  289 Worker Num:  7 \n",
            " Loss:  0.1662040799856186\n",
            "Test:  Iteration No:  289 \n",
            " Loss:  0.16684013697071165\n",
            "Test accuracy:  95.05\n",
            "Training:  Iteration No:  290 Worker Num:  0 \n",
            " Loss:  0.19837334752082825\n",
            "Training:  Iteration No:  290 Worker Num:  1 \n",
            " Loss:  0.31932780146598816\n",
            "Training:  Iteration No:  290 Worker Num:  2 \n",
            " Loss:  0.20863796770572662\n",
            "Training:  Iteration No:  290 Worker Num:  3 \n",
            " Loss:  0.22076770663261414\n",
            "Training:  Iteration No:  290 Worker Num:  4 \n",
            " Loss:  0.33985787630081177\n",
            "Training:  Iteration No:  290 Worker Num:  5 \n",
            " Loss:  0.324482262134552\n",
            "Training:  Iteration No:  290 Worker Num:  6 \n",
            " Loss:  0.11815295368432999\n",
            "Training:  Iteration No:  290 Worker Num:  7 \n",
            " Loss:  0.14426717162132263\n",
            "Test:  Iteration No:  290 \n",
            " Loss:  0.1746696358711659\n",
            "Test accuracy:  94.7\n",
            "Training:  Iteration No:  291 Worker Num:  0 \n",
            " Loss:  0.17813704907894135\n",
            "Training:  Iteration No:  291 Worker Num:  1 \n",
            " Loss:  0.24194136261940002\n",
            "Training:  Iteration No:  291 Worker Num:  2 \n",
            " Loss:  0.15846239030361176\n",
            "Training:  Iteration No:  291 Worker Num:  3 \n",
            " Loss:  0.23122988641262054\n",
            "Training:  Iteration No:  291 Worker Num:  4 \n",
            " Loss:  0.15517765283584595\n",
            "Training:  Iteration No:  291 Worker Num:  5 \n",
            " Loss:  0.18978330492973328\n",
            "Training:  Iteration No:  291 Worker Num:  6 \n",
            " Loss:  0.18811601400375366\n",
            "Training:  Iteration No:  291 Worker Num:  7 \n",
            " Loss:  0.18394871056079865\n",
            "Test:  Iteration No:  291 \n",
            " Loss:  0.1665027832871751\n",
            "Test accuracy:  95.12\n",
            "Training:  Iteration No:  292 Worker Num:  0 \n",
            " Loss:  0.2312365025281906\n",
            "Training:  Iteration No:  292 Worker Num:  1 \n",
            " Loss:  0.30407872796058655\n",
            "Training:  Iteration No:  292 Worker Num:  2 \n",
            " Loss:  0.20559638738632202\n",
            "Training:  Iteration No:  292 Worker Num:  3 \n",
            " Loss:  0.16246247291564941\n",
            "Training:  Iteration No:  292 Worker Num:  4 \n",
            " Loss:  0.18325065076351166\n",
            "Training:  Iteration No:  292 Worker Num:  5 \n",
            " Loss:  0.25859716534614563\n",
            "Training:  Iteration No:  292 Worker Num:  6 \n",
            " Loss:  0.28266170620918274\n",
            "Training:  Iteration No:  292 Worker Num:  7 \n",
            " Loss:  0.1368703991174698\n",
            "Test:  Iteration No:  292 \n",
            " Loss:  0.166751017410874\n",
            "Test accuracy:  95.03\n",
            "Training:  Iteration No:  293 Worker Num:  0 \n",
            " Loss:  0.15707358717918396\n",
            "Training:  Iteration No:  293 Worker Num:  1 \n",
            " Loss:  0.42178529500961304\n",
            "Training:  Iteration No:  293 Worker Num:  2 \n",
            " Loss:  0.17080935835838318\n",
            "Training:  Iteration No:  293 Worker Num:  3 \n",
            " Loss:  0.28753718733787537\n",
            "Training:  Iteration No:  293 Worker Num:  4 \n",
            " Loss:  0.26566365361213684\n",
            "Training:  Iteration No:  293 Worker Num:  5 \n",
            " Loss:  0.1869792938232422\n",
            "Training:  Iteration No:  293 Worker Num:  6 \n",
            " Loss:  0.21283720433712006\n",
            "Training:  Iteration No:  293 Worker Num:  7 \n",
            " Loss:  0.17621223628520966\n",
            "Test:  Iteration No:  293 \n",
            " Loss:  0.16749788279633357\n",
            "Test accuracy:  94.92\n",
            "Training:  Iteration No:  294 Worker Num:  0 \n",
            " Loss:  0.14107933640480042\n",
            "Training:  Iteration No:  294 Worker Num:  1 \n",
            " Loss:  0.20914174616336823\n",
            "Training:  Iteration No:  294 Worker Num:  2 \n",
            " Loss:  0.1485978215932846\n",
            "Training:  Iteration No:  294 Worker Num:  3 \n",
            " Loss:  0.20977464318275452\n",
            "Training:  Iteration No:  294 Worker Num:  4 \n",
            " Loss:  0.21445544064044952\n",
            "Training:  Iteration No:  294 Worker Num:  5 \n",
            " Loss:  0.28343430161476135\n",
            "Training:  Iteration No:  294 Worker Num:  6 \n",
            " Loss:  0.1685415655374527\n",
            "Training:  Iteration No:  294 Worker Num:  7 \n",
            " Loss:  0.2635401785373688\n",
            "Test:  Iteration No:  294 \n",
            " Loss:  0.17091971810294104\n",
            "Test accuracy:  94.96\n",
            "Training:  Iteration No:  295 Worker Num:  0 \n",
            " Loss:  0.29899489879608154\n",
            "Training:  Iteration No:  295 Worker Num:  1 \n",
            " Loss:  0.24201861023902893\n",
            "Training:  Iteration No:  295 Worker Num:  2 \n",
            " Loss:  0.16826443374156952\n",
            "Training:  Iteration No:  295 Worker Num:  3 \n",
            " Loss:  0.30923423171043396\n",
            "Training:  Iteration No:  295 Worker Num:  4 \n",
            " Loss:  0.22117269039154053\n",
            "Training:  Iteration No:  295 Worker Num:  5 \n",
            " Loss:  0.19361576437950134\n",
            "Training:  Iteration No:  295 Worker Num:  6 \n",
            " Loss:  0.23169825971126556\n",
            "Training:  Iteration No:  295 Worker Num:  7 \n",
            " Loss:  0.15559186041355133\n",
            "Test:  Iteration No:  295 \n",
            " Loss:  0.1655309803760316\n",
            "Test accuracy:  94.97\n",
            "Training:  Iteration No:  296 Worker Num:  0 \n",
            " Loss:  0.09905301034450531\n",
            "Training:  Iteration No:  296 Worker Num:  1 \n",
            " Loss:  0.15128710865974426\n",
            "Training:  Iteration No:  296 Worker Num:  2 \n",
            " Loss:  0.18356890976428986\n",
            "Training:  Iteration No:  296 Worker Num:  3 \n",
            " Loss:  0.3271772265434265\n",
            "Training:  Iteration No:  296 Worker Num:  4 \n",
            " Loss:  0.18217843770980835\n",
            "Training:  Iteration No:  296 Worker Num:  5 \n",
            " Loss:  0.23656721413135529\n",
            "Training:  Iteration No:  296 Worker Num:  6 \n",
            " Loss:  0.21603360772132874\n",
            "Training:  Iteration No:  296 Worker Num:  7 \n",
            " Loss:  0.2371630221605301\n",
            "Test:  Iteration No:  296 \n",
            " Loss:  0.1664284264928178\n",
            "Test accuracy:  94.98\n",
            "Training:  Iteration No:  297 Worker Num:  0 \n",
            " Loss:  0.2711426913738251\n",
            "Training:  Iteration No:  297 Worker Num:  1 \n",
            " Loss:  0.19782203435897827\n",
            "Training:  Iteration No:  297 Worker Num:  2 \n",
            " Loss:  0.19637924432754517\n",
            "Training:  Iteration No:  297 Worker Num:  3 \n",
            " Loss:  0.3633652329444885\n",
            "Training:  Iteration No:  297 Worker Num:  4 \n",
            " Loss:  0.27175256609916687\n",
            "Training:  Iteration No:  297 Worker Num:  5 \n",
            " Loss:  0.22229154407978058\n",
            "Training:  Iteration No:  297 Worker Num:  6 \n",
            " Loss:  0.13478855788707733\n",
            "Training:  Iteration No:  297 Worker Num:  7 \n",
            " Loss:  0.11897536367177963\n",
            "Test:  Iteration No:  297 \n",
            " Loss:  0.16749144984479947\n",
            "Test accuracy:  95.21\n",
            "Training:  Iteration No:  298 Worker Num:  0 \n",
            " Loss:  0.20783792436122894\n",
            "Training:  Iteration No:  298 Worker Num:  1 \n",
            " Loss:  0.18527762591838837\n",
            "Training:  Iteration No:  298 Worker Num:  2 \n",
            " Loss:  0.16766688227653503\n",
            "Training:  Iteration No:  298 Worker Num:  3 \n",
            " Loss:  0.16342045366764069\n",
            "Training:  Iteration No:  298 Worker Num:  4 \n",
            " Loss:  0.2283187210559845\n",
            "Training:  Iteration No:  298 Worker Num:  5 \n",
            " Loss:  0.14819905161857605\n",
            "Training:  Iteration No:  298 Worker Num:  6 \n",
            " Loss:  0.2661058008670807\n",
            "Training:  Iteration No:  298 Worker Num:  7 \n",
            " Loss:  0.13800740242004395\n",
            "Test:  Iteration No:  298 \n",
            " Loss:  0.1639997684455748\n",
            "Test accuracy:  95.11\n",
            "Training:  Iteration No:  299 Worker Num:  0 \n",
            " Loss:  0.18830880522727966\n",
            "Training:  Iteration No:  299 Worker Num:  1 \n",
            " Loss:  0.19210350513458252\n",
            "Training:  Iteration No:  299 Worker Num:  2 \n",
            " Loss:  0.26336127519607544\n",
            "Training:  Iteration No:  299 Worker Num:  3 \n",
            " Loss:  0.20456728339195251\n",
            "Training:  Iteration No:  299 Worker Num:  4 \n",
            " Loss:  0.23325945436954498\n",
            "Training:  Iteration No:  299 Worker Num:  5 \n",
            " Loss:  0.2720286548137665\n",
            "Training:  Iteration No:  299 Worker Num:  6 \n",
            " Loss:  0.18357276916503906\n",
            "Training:  Iteration No:  299 Worker Num:  7 \n",
            " Loss:  0.21736204624176025\n",
            "Test:  Iteration No:  299 \n",
            " Loss:  0.16422561730576468\n",
            "Test accuracy:  95.16\n",
            "Training:  Iteration No:  300 Worker Num:  0 \n",
            " Loss:  0.14203190803527832\n",
            "Training:  Iteration No:  300 Worker Num:  1 \n",
            " Loss:  0.26230788230895996\n",
            "Training:  Iteration No:  300 Worker Num:  2 \n",
            " Loss:  0.19178123772144318\n",
            "Training:  Iteration No:  300 Worker Num:  3 \n",
            " Loss:  0.17946085333824158\n",
            "Training:  Iteration No:  300 Worker Num:  4 \n",
            " Loss:  0.20603328943252563\n",
            "Training:  Iteration No:  300 Worker Num:  5 \n",
            " Loss:  0.1582646369934082\n",
            "Training:  Iteration No:  300 Worker Num:  6 \n",
            " Loss:  0.2063409835100174\n",
            "Training:  Iteration No:  300 Worker Num:  7 \n",
            " Loss:  0.23149210214614868\n",
            "Test:  Iteration No:  300 \n",
            " Loss:  0.16592036857847361\n",
            "Test accuracy:  95.04\n",
            "Training:  Iteration No:  301 Worker Num:  0 \n",
            " Loss:  0.15657995641231537\n",
            "Training:  Iteration No:  301 Worker Num:  1 \n",
            " Loss:  0.1901906579732895\n",
            "Training:  Iteration No:  301 Worker Num:  2 \n",
            " Loss:  0.11268310248851776\n",
            "Training:  Iteration No:  301 Worker Num:  3 \n",
            " Loss:  0.1599394530057907\n",
            "Training:  Iteration No:  301 Worker Num:  4 \n",
            " Loss:  0.2976387143135071\n",
            "Training:  Iteration No:  301 Worker Num:  5 \n",
            " Loss:  0.31904444098472595\n",
            "Training:  Iteration No:  301 Worker Num:  6 \n",
            " Loss:  0.23426534235477448\n",
            "Training:  Iteration No:  301 Worker Num:  7 \n",
            " Loss:  0.2042505443096161\n",
            "Test:  Iteration No:  301 \n",
            " Loss:  0.16397142805206247\n",
            "Test accuracy:  94.94\n",
            "Training:  Iteration No:  302 Worker Num:  0 \n",
            " Loss:  0.12076160311698914\n",
            "Training:  Iteration No:  302 Worker Num:  1 \n",
            " Loss:  0.27727049589157104\n",
            "Training:  Iteration No:  302 Worker Num:  2 \n",
            " Loss:  0.1112104058265686\n",
            "Training:  Iteration No:  302 Worker Num:  3 \n",
            " Loss:  0.26544517278671265\n",
            "Training:  Iteration No:  302 Worker Num:  4 \n",
            " Loss:  0.16104979813098907\n",
            "Training:  Iteration No:  302 Worker Num:  5 \n",
            " Loss:  0.3513464629650116\n",
            "Training:  Iteration No:  302 Worker Num:  6 \n",
            " Loss:  0.23852597177028656\n",
            "Training:  Iteration No:  302 Worker Num:  7 \n",
            " Loss:  0.1679559201002121\n",
            "Test:  Iteration No:  302 \n",
            " Loss:  0.16126960695756576\n",
            "Test accuracy:  95.25\n",
            "Training:  Iteration No:  303 Worker Num:  0 \n",
            " Loss:  0.22217188775539398\n",
            "Training:  Iteration No:  303 Worker Num:  1 \n",
            " Loss:  0.1917697787284851\n",
            "Training:  Iteration No:  303 Worker Num:  2 \n",
            " Loss:  0.20559686422348022\n",
            "Training:  Iteration No:  303 Worker Num:  3 \n",
            " Loss:  0.12317978590726852\n",
            "Training:  Iteration No:  303 Worker Num:  4 \n",
            " Loss:  0.18085333704948425\n",
            "Training:  Iteration No:  303 Worker Num:  5 \n",
            " Loss:  0.28056207299232483\n",
            "Training:  Iteration No:  303 Worker Num:  6 \n",
            " Loss:  0.23929329216480255\n",
            "Training:  Iteration No:  303 Worker Num:  7 \n",
            " Loss:  0.23746204376220703\n",
            "Test:  Iteration No:  303 \n",
            " Loss:  0.16107732786714465\n",
            "Test accuracy:  95.28\n",
            "Training:  Iteration No:  304 Worker Num:  0 \n",
            " Loss:  0.17730487883090973\n",
            "Training:  Iteration No:  304 Worker Num:  1 \n",
            " Loss:  0.20659610629081726\n",
            "Training:  Iteration No:  304 Worker Num:  2 \n",
            " Loss:  0.18699069321155548\n",
            "Training:  Iteration No:  304 Worker Num:  3 \n",
            " Loss:  0.15889479219913483\n",
            "Training:  Iteration No:  304 Worker Num:  4 \n",
            " Loss:  0.1906384527683258\n",
            "Training:  Iteration No:  304 Worker Num:  5 \n",
            " Loss:  0.23951812088489532\n",
            "Training:  Iteration No:  304 Worker Num:  6 \n",
            " Loss:  0.19133150577545166\n",
            "Training:  Iteration No:  304 Worker Num:  7 \n",
            " Loss:  0.19048069417476654\n",
            "Test:  Iteration No:  304 \n",
            " Loss:  0.16244079067690087\n",
            "Test accuracy:  95.18\n",
            "Training:  Iteration No:  305 Worker Num:  0 \n",
            " Loss:  0.13185293972492218\n",
            "Training:  Iteration No:  305 Worker Num:  1 \n",
            " Loss:  0.18028230965137482\n",
            "Training:  Iteration No:  305 Worker Num:  2 \n",
            " Loss:  0.37485289573669434\n",
            "Training:  Iteration No:  305 Worker Num:  3 \n",
            " Loss:  0.242598757147789\n",
            "Training:  Iteration No:  305 Worker Num:  4 \n",
            " Loss:  0.18958064913749695\n",
            "Training:  Iteration No:  305 Worker Num:  5 \n",
            " Loss:  0.17509105801582336\n",
            "Training:  Iteration No:  305 Worker Num:  6 \n",
            " Loss:  0.26332736015319824\n",
            "Training:  Iteration No:  305 Worker Num:  7 \n",
            " Loss:  0.2537948489189148\n",
            "Test:  Iteration No:  305 \n",
            " Loss:  0.1614944426566834\n",
            "Test accuracy:  95.2\n",
            "Training:  Iteration No:  306 Worker Num:  0 \n",
            " Loss:  0.24943672120571136\n",
            "Training:  Iteration No:  306 Worker Num:  1 \n",
            " Loss:  0.25781723856925964\n",
            "Training:  Iteration No:  306 Worker Num:  2 \n",
            " Loss:  0.12211152911186218\n",
            "Training:  Iteration No:  306 Worker Num:  3 \n",
            " Loss:  0.19225110113620758\n",
            "Training:  Iteration No:  306 Worker Num:  4 \n",
            " Loss:  0.18038305640220642\n",
            "Training:  Iteration No:  306 Worker Num:  5 \n",
            " Loss:  0.18749910593032837\n",
            "Training:  Iteration No:  306 Worker Num:  6 \n",
            " Loss:  0.2000693678855896\n",
            "Training:  Iteration No:  306 Worker Num:  7 \n",
            " Loss:  0.17858988046646118\n",
            "Test:  Iteration No:  306 \n",
            " Loss:  0.15966295618328114\n",
            "Test accuracy:  95.18\n",
            "Training:  Iteration No:  307 Worker Num:  0 \n",
            " Loss:  0.2150847613811493\n",
            "Training:  Iteration No:  307 Worker Num:  1 \n",
            " Loss:  0.20004534721374512\n",
            "Training:  Iteration No:  307 Worker Num:  2 \n",
            " Loss:  0.29780876636505127\n",
            "Training:  Iteration No:  307 Worker Num:  3 \n",
            " Loss:  0.11589500308036804\n",
            "Training:  Iteration No:  307 Worker Num:  4 \n",
            " Loss:  0.26456302404403687\n",
            "Training:  Iteration No:  307 Worker Num:  5 \n",
            " Loss:  0.18507833778858185\n",
            "Training:  Iteration No:  307 Worker Num:  6 \n",
            " Loss:  0.2069481611251831\n",
            "Training:  Iteration No:  307 Worker Num:  7 \n",
            " Loss:  0.2321348935365677\n",
            "Test:  Iteration No:  307 \n",
            " Loss:  0.15784031034836285\n",
            "Test accuracy:  95.44\n",
            "Training:  Iteration No:  308 Worker Num:  0 \n",
            " Loss:  0.2581873834133148\n",
            "Training:  Iteration No:  308 Worker Num:  1 \n",
            " Loss:  0.20267054438591003\n",
            "Training:  Iteration No:  308 Worker Num:  2 \n",
            " Loss:  0.08011112362146378\n",
            "Training:  Iteration No:  308 Worker Num:  3 \n",
            " Loss:  0.22783957421779633\n",
            "Training:  Iteration No:  308 Worker Num:  4 \n",
            " Loss:  0.136873260140419\n",
            "Training:  Iteration No:  308 Worker Num:  5 \n",
            " Loss:  0.16133485734462738\n",
            "Training:  Iteration No:  308 Worker Num:  6 \n",
            " Loss:  0.21157461404800415\n",
            "Training:  Iteration No:  308 Worker Num:  7 \n",
            " Loss:  0.2201257348060608\n",
            "Test:  Iteration No:  308 \n",
            " Loss:  0.15989512077803852\n",
            "Test accuracy:  95.25\n",
            "Training:  Iteration No:  309 Worker Num:  0 \n",
            " Loss:  0.2595439553260803\n",
            "Training:  Iteration No:  309 Worker Num:  1 \n",
            " Loss:  0.18202176690101624\n",
            "Training:  Iteration No:  309 Worker Num:  2 \n",
            " Loss:  0.09371966123580933\n",
            "Training:  Iteration No:  309 Worker Num:  3 \n",
            " Loss:  0.1344839334487915\n",
            "Training:  Iteration No:  309 Worker Num:  4 \n",
            " Loss:  0.22257080674171448\n",
            "Training:  Iteration No:  309 Worker Num:  5 \n",
            " Loss:  0.3288742005825043\n",
            "Training:  Iteration No:  309 Worker Num:  6 \n",
            " Loss:  0.17363646626472473\n",
            "Training:  Iteration No:  309 Worker Num:  7 \n",
            " Loss:  0.2306467443704605\n",
            "Test:  Iteration No:  309 \n",
            " Loss:  0.1593870110161414\n",
            "Test accuracy:  95.29\n",
            "Training:  Iteration No:  310 Worker Num:  0 \n",
            " Loss:  0.23620733618736267\n",
            "Training:  Iteration No:  310 Worker Num:  1 \n",
            " Loss:  0.16759420931339264\n",
            "Training:  Iteration No:  310 Worker Num:  2 \n",
            " Loss:  0.16453436017036438\n",
            "Training:  Iteration No:  310 Worker Num:  3 \n",
            " Loss:  0.34751254320144653\n",
            "Training:  Iteration No:  310 Worker Num:  4 \n",
            " Loss:  0.22743260860443115\n",
            "Training:  Iteration No:  310 Worker Num:  5 \n",
            " Loss:  0.2856594920158386\n",
            "Training:  Iteration No:  310 Worker Num:  6 \n",
            " Loss:  0.20641450583934784\n",
            "Training:  Iteration No:  310 Worker Num:  7 \n",
            " Loss:  0.26346978545188904\n",
            "Test:  Iteration No:  310 \n",
            " Loss:  0.15558760027884494\n",
            "Test accuracy:  95.5\n",
            "Training:  Iteration No:  311 Worker Num:  0 \n",
            " Loss:  0.1759209781885147\n",
            "Training:  Iteration No:  311 Worker Num:  1 \n",
            " Loss:  0.14344896376132965\n",
            "Training:  Iteration No:  311 Worker Num:  2 \n",
            " Loss:  0.23355799913406372\n",
            "Training:  Iteration No:  311 Worker Num:  3 \n",
            " Loss:  0.19212959706783295\n",
            "Training:  Iteration No:  311 Worker Num:  4 \n",
            " Loss:  0.18362314999103546\n",
            "Training:  Iteration No:  311 Worker Num:  5 \n",
            " Loss:  0.24556274712085724\n",
            "Training:  Iteration No:  311 Worker Num:  6 \n",
            " Loss:  0.2267238050699234\n",
            "Training:  Iteration No:  311 Worker Num:  7 \n",
            " Loss:  0.159905344247818\n",
            "Test:  Iteration No:  311 \n",
            " Loss:  0.15865569653673262\n",
            "Test accuracy:  95.21\n",
            "Training:  Iteration No:  312 Worker Num:  0 \n",
            " Loss:  0.13725335896015167\n",
            "Training:  Iteration No:  312 Worker Num:  1 \n",
            " Loss:  0.18458083271980286\n",
            "Training:  Iteration No:  312 Worker Num:  2 \n",
            " Loss:  0.16816289722919464\n",
            "Training:  Iteration No:  312 Worker Num:  3 \n",
            " Loss:  0.17956289649009705\n",
            "Training:  Iteration No:  312 Worker Num:  4 \n",
            " Loss:  0.3037487268447876\n",
            "Training:  Iteration No:  312 Worker Num:  5 \n",
            " Loss:  0.22364771366119385\n",
            "Training:  Iteration No:  312 Worker Num:  6 \n",
            " Loss:  0.22921738028526306\n",
            "Training:  Iteration No:  312 Worker Num:  7 \n",
            " Loss:  0.12277905642986298\n",
            "Test:  Iteration No:  312 \n",
            " Loss:  0.1560994498844294\n",
            "Test accuracy:  95.42\n",
            "Training:  Iteration No:  313 Worker Num:  0 \n",
            " Loss:  0.2887689173221588\n",
            "Training:  Iteration No:  313 Worker Num:  1 \n",
            " Loss:  0.2506493330001831\n",
            "Training:  Iteration No:  313 Worker Num:  2 \n",
            " Loss:  0.1383688747882843\n",
            "Training:  Iteration No:  313 Worker Num:  3 \n",
            " Loss:  0.2551213502883911\n",
            "Training:  Iteration No:  313 Worker Num:  4 \n",
            " Loss:  0.2564164400100708\n",
            "Training:  Iteration No:  313 Worker Num:  5 \n",
            " Loss:  0.1128314658999443\n",
            "Training:  Iteration No:  313 Worker Num:  6 \n",
            " Loss:  0.15280082821846008\n",
            "Training:  Iteration No:  313 Worker Num:  7 \n",
            " Loss:  0.1891920268535614\n",
            "Test:  Iteration No:  313 \n",
            " Loss:  0.16000998143366055\n",
            "Test accuracy:  95.25\n",
            "Training:  Iteration No:  314 Worker Num:  0 \n",
            " Loss:  0.1567414402961731\n",
            "Training:  Iteration No:  314 Worker Num:  1 \n",
            " Loss:  0.1542041301727295\n",
            "Training:  Iteration No:  314 Worker Num:  2 \n",
            " Loss:  0.10676684975624084\n",
            "Training:  Iteration No:  314 Worker Num:  3 \n",
            " Loss:  0.24146699905395508\n",
            "Training:  Iteration No:  314 Worker Num:  4 \n",
            " Loss:  0.27613717317581177\n",
            "Training:  Iteration No:  314 Worker Num:  5 \n",
            " Loss:  0.22044315934181213\n",
            "Training:  Iteration No:  314 Worker Num:  6 \n",
            " Loss:  0.2769085466861725\n",
            "Training:  Iteration No:  314 Worker Num:  7 \n",
            " Loss:  0.205287903547287\n",
            "Test:  Iteration No:  314 \n",
            " Loss:  0.1558582983097604\n",
            "Test accuracy:  95.41\n",
            "Training:  Iteration No:  315 Worker Num:  0 \n",
            " Loss:  0.2726476490497589\n",
            "Training:  Iteration No:  315 Worker Num:  1 \n",
            " Loss:  0.1540645807981491\n",
            "Training:  Iteration No:  315 Worker Num:  2 \n",
            " Loss:  0.17624768614768982\n",
            "Training:  Iteration No:  315 Worker Num:  3 \n",
            " Loss:  0.149135559797287\n",
            "Training:  Iteration No:  315 Worker Num:  4 \n",
            " Loss:  0.23543794453144073\n",
            "Training:  Iteration No:  315 Worker Num:  5 \n",
            " Loss:  0.14483973383903503\n",
            "Training:  Iteration No:  315 Worker Num:  6 \n",
            " Loss:  0.20144860446453094\n",
            "Training:  Iteration No:  315 Worker Num:  7 \n",
            " Loss:  0.2340814620256424\n",
            "Test:  Iteration No:  315 \n",
            " Loss:  0.15412582520136162\n",
            "Test accuracy:  95.29\n",
            "Training:  Iteration No:  316 Worker Num:  0 \n",
            " Loss:  0.21912619471549988\n",
            "Training:  Iteration No:  316 Worker Num:  1 \n",
            " Loss:  0.2491992861032486\n",
            "Training:  Iteration No:  316 Worker Num:  2 \n",
            " Loss:  0.23165085911750793\n",
            "Training:  Iteration No:  316 Worker Num:  3 \n",
            " Loss:  0.1746407449245453\n",
            "Training:  Iteration No:  316 Worker Num:  4 \n",
            " Loss:  0.29073426127433777\n",
            "Training:  Iteration No:  316 Worker Num:  5 \n",
            " Loss:  0.16536745429039001\n",
            "Training:  Iteration No:  316 Worker Num:  6 \n",
            " Loss:  0.1378352791070938\n",
            "Training:  Iteration No:  316 Worker Num:  7 \n",
            " Loss:  0.1808442324399948\n",
            "Test:  Iteration No:  316 \n",
            " Loss:  0.15447121073456505\n",
            "Test accuracy:  95.35\n",
            "Training:  Iteration No:  317 Worker Num:  0 \n",
            " Loss:  0.3179839253425598\n",
            "Training:  Iteration No:  317 Worker Num:  1 \n",
            " Loss:  0.24294628202915192\n",
            "Training:  Iteration No:  317 Worker Num:  2 \n",
            " Loss:  0.25547754764556885\n",
            "Training:  Iteration No:  317 Worker Num:  3 \n",
            " Loss:  0.20448684692382812\n",
            "Training:  Iteration No:  317 Worker Num:  4 \n",
            " Loss:  0.21764926612377167\n",
            "Training:  Iteration No:  317 Worker Num:  5 \n",
            " Loss:  0.153311088681221\n",
            "Training:  Iteration No:  317 Worker Num:  6 \n",
            " Loss:  0.2677769064903259\n",
            "Training:  Iteration No:  317 Worker Num:  7 \n",
            " Loss:  0.20105965435504913\n",
            "Test:  Iteration No:  317 \n",
            " Loss:  0.15343894873144506\n",
            "Test accuracy:  95.36\n",
            "Training:  Iteration No:  318 Worker Num:  0 \n",
            " Loss:  0.19312524795532227\n",
            "Training:  Iteration No:  318 Worker Num:  1 \n",
            " Loss:  0.13563592731952667\n",
            "Training:  Iteration No:  318 Worker Num:  2 \n",
            " Loss:  0.17994965612888336\n",
            "Training:  Iteration No:  318 Worker Num:  3 \n",
            " Loss:  0.13233685493469238\n",
            "Training:  Iteration No:  318 Worker Num:  4 \n",
            " Loss:  0.11313655227422714\n",
            "Training:  Iteration No:  318 Worker Num:  5 \n",
            " Loss:  0.23079277575016022\n",
            "Training:  Iteration No:  318 Worker Num:  6 \n",
            " Loss:  0.21872122585773468\n",
            "Training:  Iteration No:  318 Worker Num:  7 \n",
            " Loss:  0.14662784337997437\n",
            "Test:  Iteration No:  318 \n",
            " Loss:  0.15165910969452956\n",
            "Test accuracy:  95.55\n",
            "Training:  Iteration No:  319 Worker Num:  0 \n",
            " Loss:  0.1536771059036255\n",
            "Training:  Iteration No:  319 Worker Num:  1 \n",
            " Loss:  0.3193553686141968\n",
            "Training:  Iteration No:  319 Worker Num:  2 \n",
            " Loss:  0.13416555523872375\n",
            "Training:  Iteration No:  319 Worker Num:  3 \n",
            " Loss:  0.10910764336585999\n",
            "Training:  Iteration No:  319 Worker Num:  4 \n",
            " Loss:  0.12235597521066666\n",
            "Training:  Iteration No:  319 Worker Num:  5 \n",
            " Loss:  0.20417064428329468\n",
            "Training:  Iteration No:  319 Worker Num:  6 \n",
            " Loss:  0.1535816639661789\n",
            "Training:  Iteration No:  319 Worker Num:  7 \n",
            " Loss:  0.21815331280231476\n",
            "Test:  Iteration No:  319 \n",
            " Loss:  0.15298889554400422\n",
            "Test accuracy:  95.35\n",
            "Training:  Iteration No:  320 Worker Num:  0 \n",
            " Loss:  0.27033111453056335\n",
            "Training:  Iteration No:  320 Worker Num:  1 \n",
            " Loss:  0.10425306111574173\n",
            "Training:  Iteration No:  320 Worker Num:  2 \n",
            " Loss:  0.13266712427139282\n",
            "Training:  Iteration No:  320 Worker Num:  3 \n",
            " Loss:  0.15837638080120087\n",
            "Training:  Iteration No:  320 Worker Num:  4 \n",
            " Loss:  0.15049639344215393\n",
            "Training:  Iteration No:  320 Worker Num:  5 \n",
            " Loss:  0.2446485459804535\n",
            "Training:  Iteration No:  320 Worker Num:  6 \n",
            " Loss:  0.13124868273735046\n",
            "Training:  Iteration No:  320 Worker Num:  7 \n",
            " Loss:  0.13277316093444824\n",
            "Test:  Iteration No:  320 \n",
            " Loss:  0.15068345880018005\n",
            "Test accuracy:  95.64\n",
            "Training:  Iteration No:  321 Worker Num:  0 \n",
            " Loss:  0.2119801789522171\n",
            "Training:  Iteration No:  321 Worker Num:  1 \n",
            " Loss:  0.11758628487586975\n",
            "Training:  Iteration No:  321 Worker Num:  2 \n",
            " Loss:  0.1417725831270218\n",
            "Training:  Iteration No:  321 Worker Num:  3 \n",
            " Loss:  0.1996658444404602\n",
            "Training:  Iteration No:  321 Worker Num:  4 \n",
            " Loss:  0.2392445206642151\n",
            "Training:  Iteration No:  321 Worker Num:  5 \n",
            " Loss:  0.2155035436153412\n",
            "Training:  Iteration No:  321 Worker Num:  6 \n",
            " Loss:  0.2662075161933899\n",
            "Training:  Iteration No:  321 Worker Num:  7 \n",
            " Loss:  0.1838676631450653\n",
            "Test:  Iteration No:  321 \n",
            " Loss:  0.15105701840329397\n",
            "Test accuracy:  95.4\n",
            "Training:  Iteration No:  322 Worker Num:  0 \n",
            " Loss:  0.15302857756614685\n",
            "Training:  Iteration No:  322 Worker Num:  1 \n",
            " Loss:  0.13857637345790863\n",
            "Training:  Iteration No:  322 Worker Num:  2 \n",
            " Loss:  0.11630389094352722\n",
            "Training:  Iteration No:  322 Worker Num:  3 \n",
            " Loss:  0.12998344004154205\n",
            "Training:  Iteration No:  322 Worker Num:  4 \n",
            " Loss:  0.22880707681179047\n",
            "Training:  Iteration No:  322 Worker Num:  5 \n",
            " Loss:  0.2262309342622757\n",
            "Training:  Iteration No:  322 Worker Num:  6 \n",
            " Loss:  0.1776016652584076\n",
            "Training:  Iteration No:  322 Worker Num:  7 \n",
            " Loss:  0.28570201992988586\n",
            "Test:  Iteration No:  322 \n",
            " Loss:  0.1510512904157933\n",
            "Test accuracy:  95.4\n",
            "Training:  Iteration No:  323 Worker Num:  0 \n",
            " Loss:  0.2744423449039459\n",
            "Training:  Iteration No:  323 Worker Num:  1 \n",
            " Loss:  0.22639325261116028\n",
            "Training:  Iteration No:  323 Worker Num:  2 \n",
            " Loss:  0.2076498121023178\n",
            "Training:  Iteration No:  323 Worker Num:  3 \n",
            " Loss:  0.23314006626605988\n",
            "Training:  Iteration No:  323 Worker Num:  4 \n",
            " Loss:  0.22860445082187653\n",
            "Training:  Iteration No:  323 Worker Num:  5 \n",
            " Loss:  0.13137288391590118\n",
            "Training:  Iteration No:  323 Worker Num:  6 \n",
            " Loss:  0.23200063407421112\n",
            "Training:  Iteration No:  323 Worker Num:  7 \n",
            " Loss:  0.13977289199829102\n",
            "Test:  Iteration No:  323 \n",
            " Loss:  0.1492523104129241\n",
            "Test accuracy:  95.67\n",
            "Training:  Iteration No:  324 Worker Num:  0 \n",
            " Loss:  0.14450988173484802\n",
            "Training:  Iteration No:  324 Worker Num:  1 \n",
            " Loss:  0.1269451379776001\n",
            "Training:  Iteration No:  324 Worker Num:  2 \n",
            " Loss:  0.18437010049819946\n",
            "Training:  Iteration No:  324 Worker Num:  3 \n",
            " Loss:  0.16071422398090363\n",
            "Training:  Iteration No:  324 Worker Num:  4 \n",
            " Loss:  0.19773517549037933\n",
            "Training:  Iteration No:  324 Worker Num:  5 \n",
            " Loss:  0.22164906561374664\n",
            "Training:  Iteration No:  324 Worker Num:  6 \n",
            " Loss:  0.18363061547279358\n",
            "Training:  Iteration No:  324 Worker Num:  7 \n",
            " Loss:  0.19225819408893585\n",
            "Test:  Iteration No:  324 \n",
            " Loss:  0.15036053957323295\n",
            "Test accuracy:  95.49\n",
            "Training:  Iteration No:  325 Worker Num:  0 \n",
            " Loss:  0.24210289120674133\n",
            "Training:  Iteration No:  325 Worker Num:  1 \n",
            " Loss:  0.2817250192165375\n",
            "Training:  Iteration No:  325 Worker Num:  2 \n",
            " Loss:  0.21410678327083588\n",
            "Training:  Iteration No:  325 Worker Num:  3 \n",
            " Loss:  0.22927606105804443\n",
            "Training:  Iteration No:  325 Worker Num:  4 \n",
            " Loss:  0.3318819999694824\n",
            "Training:  Iteration No:  325 Worker Num:  5 \n",
            " Loss:  0.15187318623065948\n",
            "Training:  Iteration No:  325 Worker Num:  6 \n",
            " Loss:  0.20850373804569244\n",
            "Training:  Iteration No:  325 Worker Num:  7 \n",
            " Loss:  0.20458610355854034\n",
            "Test:  Iteration No:  325 \n",
            " Loss:  0.1483979357312186\n",
            "Test accuracy:  95.68\n",
            "Training:  Iteration No:  326 Worker Num:  0 \n",
            " Loss:  0.15795834362506866\n",
            "Training:  Iteration No:  326 Worker Num:  1 \n",
            " Loss:  0.2785317897796631\n",
            "Training:  Iteration No:  326 Worker Num:  2 \n",
            " Loss:  0.17972835898399353\n",
            "Training:  Iteration No:  326 Worker Num:  3 \n",
            " Loss:  0.11568596959114075\n",
            "Training:  Iteration No:  326 Worker Num:  4 \n",
            " Loss:  0.2027723789215088\n",
            "Training:  Iteration No:  326 Worker Num:  5 \n",
            " Loss:  0.13650578260421753\n",
            "Training:  Iteration No:  326 Worker Num:  6 \n",
            " Loss:  0.19169209897518158\n",
            "Training:  Iteration No:  326 Worker Num:  7 \n",
            " Loss:  0.07335454970598221\n",
            "Test:  Iteration No:  326 \n",
            " Loss:  0.14862211385175964\n",
            "Test accuracy:  95.57\n",
            "Training:  Iteration No:  327 Worker Num:  0 \n",
            " Loss:  0.28440573811531067\n",
            "Training:  Iteration No:  327 Worker Num:  1 \n",
            " Loss:  0.2838228642940521\n",
            "Training:  Iteration No:  327 Worker Num:  2 \n",
            " Loss:  0.30019375681877136\n",
            "Training:  Iteration No:  327 Worker Num:  3 \n",
            " Loss:  0.23228801786899567\n",
            "Training:  Iteration No:  327 Worker Num:  4 \n",
            " Loss:  0.107130266726017\n",
            "Training:  Iteration No:  327 Worker Num:  5 \n",
            " Loss:  0.12496410310268402\n",
            "Training:  Iteration No:  327 Worker Num:  6 \n",
            " Loss:  0.15651167929172516\n",
            "Training:  Iteration No:  327 Worker Num:  7 \n",
            " Loss:  0.20588251948356628\n",
            "Test:  Iteration No:  327 \n",
            " Loss:  0.1512221408347729\n",
            "Test accuracy:  95.44\n",
            "Training:  Iteration No:  328 Worker Num:  0 \n",
            " Loss:  0.20066511631011963\n",
            "Training:  Iteration No:  328 Worker Num:  1 \n",
            " Loss:  0.2070348560810089\n",
            "Training:  Iteration No:  328 Worker Num:  2 \n",
            " Loss:  0.16395968198776245\n",
            "Training:  Iteration No:  328 Worker Num:  3 \n",
            " Loss:  0.3001020550727844\n",
            "Training:  Iteration No:  328 Worker Num:  4 \n",
            " Loss:  0.22829224169254303\n",
            "Training:  Iteration No:  328 Worker Num:  5 \n",
            " Loss:  0.1364225596189499\n",
            "Training:  Iteration No:  328 Worker Num:  6 \n",
            " Loss:  0.22750230133533478\n",
            "Training:  Iteration No:  328 Worker Num:  7 \n",
            " Loss:  0.22862160205841064\n",
            "Test:  Iteration No:  328 \n",
            " Loss:  0.147141408953297\n",
            "Test accuracy:  95.58\n",
            "Training:  Iteration No:  329 Worker Num:  0 \n",
            " Loss:  0.20309266448020935\n",
            "Training:  Iteration No:  329 Worker Num:  1 \n",
            " Loss:  0.07280181348323822\n",
            "Training:  Iteration No:  329 Worker Num:  2 \n",
            " Loss:  0.20339111983776093\n",
            "Training:  Iteration No:  329 Worker Num:  3 \n",
            " Loss:  0.25519222021102905\n",
            "Training:  Iteration No:  329 Worker Num:  4 \n",
            " Loss:  0.157457172870636\n",
            "Training:  Iteration No:  329 Worker Num:  5 \n",
            " Loss:  0.22540584206581116\n",
            "Training:  Iteration No:  329 Worker Num:  6 \n",
            " Loss:  0.17097355425357819\n",
            "Training:  Iteration No:  329 Worker Num:  7 \n",
            " Loss:  0.14325062930583954\n",
            "Test:  Iteration No:  329 \n",
            " Loss:  0.14445698938532908\n",
            "Test accuracy:  95.72\n",
            "Training:  Iteration No:  330 Worker Num:  0 \n",
            " Loss:  0.26249852776527405\n",
            "Training:  Iteration No:  330 Worker Num:  1 \n",
            " Loss:  0.15283042192459106\n",
            "Training:  Iteration No:  330 Worker Num:  2 \n",
            " Loss:  0.13356365263462067\n",
            "Training:  Iteration No:  330 Worker Num:  3 \n",
            " Loss:  0.29902610182762146\n",
            "Training:  Iteration No:  330 Worker Num:  4 \n",
            " Loss:  0.16005146503448486\n",
            "Training:  Iteration No:  330 Worker Num:  5 \n",
            " Loss:  0.16046206653118134\n",
            "Training:  Iteration No:  330 Worker Num:  6 \n",
            " Loss:  0.11553803086280823\n",
            "Training:  Iteration No:  330 Worker Num:  7 \n",
            " Loss:  0.19263531267642975\n",
            "Test:  Iteration No:  330 \n",
            " Loss:  0.14621230774806648\n",
            "Test accuracy:  95.58\n",
            "Training:  Iteration No:  331 Worker Num:  0 \n",
            " Loss:  0.2107802927494049\n",
            "Training:  Iteration No:  331 Worker Num:  1 \n",
            " Loss:  0.2648952007293701\n",
            "Training:  Iteration No:  331 Worker Num:  2 \n",
            " Loss:  0.25456780195236206\n",
            "Training:  Iteration No:  331 Worker Num:  3 \n",
            " Loss:  0.09660328179597855\n",
            "Training:  Iteration No:  331 Worker Num:  4 \n",
            " Loss:  0.1556398868560791\n",
            "Training:  Iteration No:  331 Worker Num:  5 \n",
            " Loss:  0.22415463626384735\n",
            "Training:  Iteration No:  331 Worker Num:  6 \n",
            " Loss:  0.12480713427066803\n",
            "Training:  Iteration No:  331 Worker Num:  7 \n",
            " Loss:  0.2972465455532074\n",
            "Test:  Iteration No:  331 \n",
            " Loss:  0.1455319473354877\n",
            "Test accuracy:  95.75\n",
            "Training:  Iteration No:  332 Worker Num:  0 \n",
            " Loss:  0.25769320130348206\n",
            "Training:  Iteration No:  332 Worker Num:  1 \n",
            " Loss:  0.20527207851409912\n",
            "Training:  Iteration No:  332 Worker Num:  2 \n",
            " Loss:  0.2513265907764435\n",
            "Training:  Iteration No:  332 Worker Num:  3 \n",
            " Loss:  0.16538751125335693\n",
            "Training:  Iteration No:  332 Worker Num:  4 \n",
            " Loss:  0.167993426322937\n",
            "Training:  Iteration No:  332 Worker Num:  5 \n",
            " Loss:  0.13541492819786072\n",
            "Training:  Iteration No:  332 Worker Num:  6 \n",
            " Loss:  0.15834681689739227\n",
            "Training:  Iteration No:  332 Worker Num:  7 \n",
            " Loss:  0.17601805925369263\n",
            "Test:  Iteration No:  332 \n",
            " Loss:  0.14288835427311214\n",
            "Test accuracy:  95.79\n",
            "Training:  Iteration No:  333 Worker Num:  0 \n",
            " Loss:  0.19309528172016144\n",
            "Training:  Iteration No:  333 Worker Num:  1 \n",
            " Loss:  0.14800305664539337\n",
            "Training:  Iteration No:  333 Worker Num:  2 \n",
            " Loss:  0.2376081496477127\n",
            "Training:  Iteration No:  333 Worker Num:  3 \n",
            " Loss:  0.1236536055803299\n",
            "Training:  Iteration No:  333 Worker Num:  4 \n",
            " Loss:  0.15427325665950775\n",
            "Training:  Iteration No:  333 Worker Num:  5 \n",
            " Loss:  0.10326021909713745\n",
            "Training:  Iteration No:  333 Worker Num:  6 \n",
            " Loss:  0.10528615862131119\n",
            "Training:  Iteration No:  333 Worker Num:  7 \n",
            " Loss:  0.10633622854948044\n",
            "Test:  Iteration No:  333 \n",
            " Loss:  0.1442516596363032\n",
            "Test accuracy:  95.78\n",
            "Training:  Iteration No:  334 Worker Num:  0 \n",
            " Loss:  0.3180142641067505\n",
            "Training:  Iteration No:  334 Worker Num:  1 \n",
            " Loss:  0.10316667705774307\n",
            "Training:  Iteration No:  334 Worker Num:  2 \n",
            " Loss:  0.19364866614341736\n",
            "Training:  Iteration No:  334 Worker Num:  3 \n",
            " Loss:  0.16323113441467285\n",
            "Training:  Iteration No:  334 Worker Num:  4 \n",
            " Loss:  0.18758068978786469\n",
            "Training:  Iteration No:  334 Worker Num:  5 \n",
            " Loss:  0.11040453612804413\n",
            "Training:  Iteration No:  334 Worker Num:  6 \n",
            " Loss:  0.10516860336065292\n",
            "Training:  Iteration No:  334 Worker Num:  7 \n",
            " Loss:  0.09586913883686066\n",
            "Test:  Iteration No:  334 \n",
            " Loss:  0.1418217852464111\n",
            "Test accuracy:  95.88\n",
            "Training:  Iteration No:  335 Worker Num:  0 \n",
            " Loss:  0.16760161519050598\n",
            "Training:  Iteration No:  335 Worker Num:  1 \n",
            " Loss:  0.15628144145011902\n",
            "Training:  Iteration No:  335 Worker Num:  2 \n",
            " Loss:  0.18648837506771088\n",
            "Training:  Iteration No:  335 Worker Num:  3 \n",
            " Loss:  0.1840297281742096\n",
            "Training:  Iteration No:  335 Worker Num:  4 \n",
            " Loss:  0.24262931942939758\n",
            "Training:  Iteration No:  335 Worker Num:  5 \n",
            " Loss:  0.1330636739730835\n",
            "Training:  Iteration No:  335 Worker Num:  6 \n",
            " Loss:  0.16970661282539368\n",
            "Training:  Iteration No:  335 Worker Num:  7 \n",
            " Loss:  0.1193646639585495\n",
            "Test:  Iteration No:  335 \n",
            " Loss:  0.142786524918732\n",
            "Test accuracy:  95.67\n",
            "Training:  Iteration No:  336 Worker Num:  0 \n",
            " Loss:  0.325930655002594\n",
            "Training:  Iteration No:  336 Worker Num:  1 \n",
            " Loss:  0.17017631232738495\n",
            "Training:  Iteration No:  336 Worker Num:  2 \n",
            " Loss:  0.28128135204315186\n",
            "Training:  Iteration No:  336 Worker Num:  3 \n",
            " Loss:  0.044969990849494934\n",
            "Training:  Iteration No:  336 Worker Num:  4 \n",
            " Loss:  0.1598813533782959\n",
            "Training:  Iteration No:  336 Worker Num:  5 \n",
            " Loss:  0.1102590337395668\n",
            "Training:  Iteration No:  336 Worker Num:  6 \n",
            " Loss:  0.2293408364057541\n",
            "Training:  Iteration No:  336 Worker Num:  7 \n",
            " Loss:  0.1447129100561142\n",
            "Test:  Iteration No:  336 \n",
            " Loss:  0.1420068147097113\n",
            "Test accuracy:  95.82\n",
            "Training:  Iteration No:  337 Worker Num:  0 \n",
            " Loss:  0.17987187206745148\n",
            "Training:  Iteration No:  337 Worker Num:  1 \n",
            " Loss:  0.24679528176784515\n",
            "Training:  Iteration No:  337 Worker Num:  2 \n",
            " Loss:  0.08977974951267242\n",
            "Training:  Iteration No:  337 Worker Num:  3 \n",
            " Loss:  0.2051757425069809\n",
            "Training:  Iteration No:  337 Worker Num:  4 \n",
            " Loss:  0.21472474932670593\n",
            "Training:  Iteration No:  337 Worker Num:  5 \n",
            " Loss:  0.14652469754219055\n",
            "Training:  Iteration No:  337 Worker Num:  6 \n",
            " Loss:  0.1907292604446411\n",
            "Training:  Iteration No:  337 Worker Num:  7 \n",
            " Loss:  0.17589235305786133\n",
            "Test:  Iteration No:  337 \n",
            " Loss:  0.14394229972334224\n",
            "Test accuracy:  95.71\n",
            "Training:  Iteration No:  338 Worker Num:  0 \n",
            " Loss:  0.116289883852005\n",
            "Training:  Iteration No:  338 Worker Num:  1 \n",
            " Loss:  0.09553864598274231\n",
            "Training:  Iteration No:  338 Worker Num:  2 \n",
            " Loss:  0.25299814343452454\n",
            "Training:  Iteration No:  338 Worker Num:  3 \n",
            " Loss:  0.19036296010017395\n",
            "Training:  Iteration No:  338 Worker Num:  4 \n",
            " Loss:  0.20356091856956482\n",
            "Training:  Iteration No:  338 Worker Num:  5 \n",
            " Loss:  0.13199950754642487\n",
            "Training:  Iteration No:  338 Worker Num:  6 \n",
            " Loss:  0.2460448294878006\n",
            "Training:  Iteration No:  338 Worker Num:  7 \n",
            " Loss:  0.39816275238990784\n",
            "Test:  Iteration No:  338 \n",
            " Loss:  0.14588965443729224\n",
            "Test accuracy:  95.46\n",
            "Training:  Iteration No:  339 Worker Num:  0 \n",
            " Loss:  0.17051006853580475\n",
            "Training:  Iteration No:  339 Worker Num:  1 \n",
            " Loss:  0.17072230577468872\n",
            "Training:  Iteration No:  339 Worker Num:  2 \n",
            " Loss:  0.11608798801898956\n",
            "Training:  Iteration No:  339 Worker Num:  3 \n",
            " Loss:  0.12345919758081436\n",
            "Training:  Iteration No:  339 Worker Num:  4 \n",
            " Loss:  0.11541007459163666\n",
            "Training:  Iteration No:  339 Worker Num:  5 \n",
            " Loss:  0.24118006229400635\n",
            "Training:  Iteration No:  339 Worker Num:  6 \n",
            " Loss:  0.18363532423973083\n",
            "Training:  Iteration No:  339 Worker Num:  7 \n",
            " Loss:  0.1568860411643982\n",
            "Test:  Iteration No:  339 \n",
            " Loss:  0.1407838850576855\n",
            "Test accuracy:  95.81\n",
            "Training:  Iteration No:  340 Worker Num:  0 \n",
            " Loss:  0.10215567797422409\n",
            "Training:  Iteration No:  340 Worker Num:  1 \n",
            " Loss:  0.1354404091835022\n",
            "Training:  Iteration No:  340 Worker Num:  2 \n",
            " Loss:  0.23179461061954498\n",
            "Training:  Iteration No:  340 Worker Num:  3 \n",
            " Loss:  0.34433814883232117\n",
            "Training:  Iteration No:  340 Worker Num:  4 \n",
            " Loss:  0.3674585223197937\n",
            "Training:  Iteration No:  340 Worker Num:  5 \n",
            " Loss:  0.11520944535732269\n",
            "Training:  Iteration No:  340 Worker Num:  6 \n",
            " Loss:  0.17428995668888092\n",
            "Training:  Iteration No:  340 Worker Num:  7 \n",
            " Loss:  0.21328014135360718\n",
            "Test:  Iteration No:  340 \n",
            " Loss:  0.1404918618794955\n",
            "Test accuracy:  95.78\n",
            "Training:  Iteration No:  341 Worker Num:  0 \n",
            " Loss:  0.24901680648326874\n",
            "Training:  Iteration No:  341 Worker Num:  1 \n",
            " Loss:  0.17735011875629425\n",
            "Training:  Iteration No:  341 Worker Num:  2 \n",
            " Loss:  0.23622006177902222\n",
            "Training:  Iteration No:  341 Worker Num:  3 \n",
            " Loss:  0.11378707736730576\n",
            "Training:  Iteration No:  341 Worker Num:  4 \n",
            " Loss:  0.10189075022935867\n",
            "Training:  Iteration No:  341 Worker Num:  5 \n",
            " Loss:  0.15395882725715637\n",
            "Training:  Iteration No:  341 Worker Num:  6 \n",
            " Loss:  0.17221993207931519\n",
            "Training:  Iteration No:  341 Worker Num:  7 \n",
            " Loss:  0.25324884057044983\n",
            "Test:  Iteration No:  341 \n",
            " Loss:  0.14157617494723276\n",
            "Test accuracy:  95.74\n",
            "Training:  Iteration No:  342 Worker Num:  0 \n",
            " Loss:  0.24514029920101166\n",
            "Training:  Iteration No:  342 Worker Num:  1 \n",
            " Loss:  0.16912932693958282\n",
            "Training:  Iteration No:  342 Worker Num:  2 \n",
            " Loss:  0.11985296756029129\n",
            "Training:  Iteration No:  342 Worker Num:  3 \n",
            " Loss:  0.24236641824245453\n",
            "Training:  Iteration No:  342 Worker Num:  4 \n",
            " Loss:  0.17387013137340546\n",
            "Training:  Iteration No:  342 Worker Num:  5 \n",
            " Loss:  0.2651979327201843\n",
            "Training:  Iteration No:  342 Worker Num:  6 \n",
            " Loss:  0.12965886294841766\n",
            "Training:  Iteration No:  342 Worker Num:  7 \n",
            " Loss:  0.20776543021202087\n",
            "Test:  Iteration No:  342 \n",
            " Loss:  0.14139929421458253\n",
            "Test accuracy:  95.91\n",
            "Training:  Iteration No:  343 Worker Num:  0 \n",
            " Loss:  0.23019441962242126\n",
            "Training:  Iteration No:  343 Worker Num:  1 \n",
            " Loss:  0.1538083702325821\n",
            "Training:  Iteration No:  343 Worker Num:  2 \n",
            " Loss:  0.14264923334121704\n",
            "Training:  Iteration No:  343 Worker Num:  3 \n",
            " Loss:  0.1750771403312683\n",
            "Training:  Iteration No:  343 Worker Num:  4 \n",
            " Loss:  0.2977904975414276\n",
            "Training:  Iteration No:  343 Worker Num:  5 \n",
            " Loss:  0.1005544513463974\n",
            "Training:  Iteration No:  343 Worker Num:  6 \n",
            " Loss:  0.17831005156040192\n",
            "Training:  Iteration No:  343 Worker Num:  7 \n",
            " Loss:  0.16765806078910828\n",
            "Test:  Iteration No:  343 \n",
            " Loss:  0.1404306045320781\n",
            "Test accuracy:  95.92\n",
            "Training:  Iteration No:  344 Worker Num:  0 \n",
            " Loss:  0.16888192296028137\n",
            "Training:  Iteration No:  344 Worker Num:  1 \n",
            " Loss:  0.24287569522857666\n",
            "Training:  Iteration No:  344 Worker Num:  2 \n",
            " Loss:  0.2502271234989166\n",
            "Training:  Iteration No:  344 Worker Num:  3 \n",
            " Loss:  0.08509249240159988\n",
            "Training:  Iteration No:  344 Worker Num:  4 \n",
            " Loss:  0.2084483802318573\n",
            "Training:  Iteration No:  344 Worker Num:  5 \n",
            " Loss:  0.1611843705177307\n",
            "Training:  Iteration No:  344 Worker Num:  6 \n",
            " Loss:  0.16819316148757935\n",
            "Training:  Iteration No:  344 Worker Num:  7 \n",
            " Loss:  0.20458351075649261\n",
            "Test:  Iteration No:  344 \n",
            " Loss:  0.14565047608464485\n",
            "Test accuracy:  95.59\n",
            "Training:  Iteration No:  345 Worker Num:  0 \n",
            " Loss:  0.18345345556735992\n",
            "Training:  Iteration No:  345 Worker Num:  1 \n",
            " Loss:  0.27812865376472473\n",
            "Training:  Iteration No:  345 Worker Num:  2 \n",
            " Loss:  0.1485547125339508\n",
            "Training:  Iteration No:  345 Worker Num:  3 \n",
            " Loss:  0.20948803424835205\n",
            "Training:  Iteration No:  345 Worker Num:  4 \n",
            " Loss:  0.16053061187267303\n",
            "Training:  Iteration No:  345 Worker Num:  5 \n",
            " Loss:  0.24365906417369843\n",
            "Training:  Iteration No:  345 Worker Num:  6 \n",
            " Loss:  0.12406688183546066\n",
            "Training:  Iteration No:  345 Worker Num:  7 \n",
            " Loss:  0.2126719057559967\n",
            "Test:  Iteration No:  345 \n",
            " Loss:  0.140055628463956\n",
            "Test accuracy:  95.92\n",
            "Training:  Iteration No:  346 Worker Num:  0 \n",
            " Loss:  0.14264515042304993\n",
            "Training:  Iteration No:  346 Worker Num:  1 \n",
            " Loss:  0.09247218072414398\n",
            "Training:  Iteration No:  346 Worker Num:  2 \n",
            " Loss:  0.27052536606788635\n",
            "Training:  Iteration No:  346 Worker Num:  3 \n",
            " Loss:  0.22200904786586761\n",
            "Training:  Iteration No:  346 Worker Num:  4 \n",
            " Loss:  0.16218562424182892\n",
            "Training:  Iteration No:  346 Worker Num:  5 \n",
            " Loss:  0.1360485851764679\n",
            "Training:  Iteration No:  346 Worker Num:  6 \n",
            " Loss:  0.24471436440944672\n",
            "Training:  Iteration No:  346 Worker Num:  7 \n",
            " Loss:  0.16311122477054596\n",
            "Test:  Iteration No:  346 \n",
            " Loss:  0.13704207521285625\n",
            "Test accuracy:  95.94\n",
            "Training:  Iteration No:  347 Worker Num:  0 \n",
            " Loss:  0.16972117125988007\n",
            "Training:  Iteration No:  347 Worker Num:  1 \n",
            " Loss:  0.26411914825439453\n",
            "Training:  Iteration No:  347 Worker Num:  2 \n",
            " Loss:  0.1627117246389389\n",
            "Training:  Iteration No:  347 Worker Num:  3 \n",
            " Loss:  0.3464556932449341\n",
            "Training:  Iteration No:  347 Worker Num:  4 \n",
            " Loss:  0.22054971754550934\n",
            "Training:  Iteration No:  347 Worker Num:  5 \n",
            " Loss:  0.14924617111682892\n",
            "Training:  Iteration No:  347 Worker Num:  6 \n",
            " Loss:  0.16863811016082764\n",
            "Training:  Iteration No:  347 Worker Num:  7 \n",
            " Loss:  0.08607309311628342\n",
            "Test:  Iteration No:  347 \n",
            " Loss:  0.13702621976432355\n",
            "Test accuracy:  96.06\n",
            "96 % accuracy reached!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(test_accuracy)\n",
        "plt.xlabel('Iteration Number')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Test accuracy');"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "59e2f7a6-42ec-4236-eb94-fa9523aeeca5",
        "id": "UMOvbdLSfvZV"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "avg_train_losses = []\n",
        "for i in range(len(train_losses[0])):\n",
        "    avg_loss = 0\n",
        "    for j in range(num_workers):\n",
        "        avg_loss += train_losses[j][i]\n",
        "\n",
        "    avg_train_losses.append(avg_loss / num_workers)    \n"
      ],
      "metadata": {
        "id": "Nl55t4KdfvZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_losses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7aa37856-0774-41e7-ef12-2ba4056d4f49",
        "id": "XhFpZabefvZV"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "347"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(avg_train_losses, label='train')\n",
        "plt.plot(test_losses, label='test')\n",
        "plt.xlabel('Iteration Number')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Loss function');"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "654b75b5-d8f1-4c87-e10c-6a701789a4c9",
        "id": "AecBnpA3fvZV"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Noise = 0.1"
      ],
      "metadata": {
        "id": "9kIqRzQJgQMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_workers = 8\n",
        "all_workers = [i for i in range (num_workers)]"
      ],
      "metadata": {
        "id": "ZGWCIu9BgWgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nets_list = [CNN().to(device)]\n",
        "optimizers = [optim.SGD(nets_list[0].parameters(), lr = 0.1)]\n",
        "# schedulers = [optim.lr_scheduler.MultiStepLR(optimizers[0], milestones=[210], gamma=0.1)]\n",
        "for i in range(1,num_workers):\n",
        "    new_net = CNN().to(device)\n",
        "    new_net.load_state_dict(nets_list[0].state_dict())\n",
        "    nets_list.append(new_net)\n",
        "    optimizer = optim.SGD(new_net.parameters(), lr = 0.1) \n",
        "    optimizers.append(optimizer)\n",
        "    # scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[210], gamma=0.1)\n",
        "    # schedulers.append(scheduler)"
      ],
      "metadata": {
        "id": "o5Q_aLhugWgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_network = CNN().to(device)"
      ],
      "metadata": {
        "id": "3bTJreW9gWgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "queues_list = [[] for i in range (num_workers)]\n",
        "alpha_list = [1 / num_workers for i in range (num_workers)]"
      ],
      "metadata": {
        "id": "LcpMYEoNgWgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_iters = 1000\n",
        "p = 0.5\n",
        "batch_size = 128"
      ],
      "metadata": {
        "id": "vgd_0BLhgWgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_iters = []\n",
        "for i in range(num_workers):\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "                 dataset=mnist_trainset,\n",
        "                 batch_size=batch_size,\n",
        "                 shuffle=True\n",
        "                 )\n",
        "    \n",
        "    data_iters.append(iter(train_loader))"
      ],
      "metadata": {
        "id": "sLyMBKuegWgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader = torch.utils.data.DataLoader(\n",
        "                dataset=mnist_testset,\n",
        "                batch_size=batch_size,\n",
        "                shuffle=False\n",
        "                )"
      ],
      "metadata": {
        "id": "AGP80aDNgWgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "train_losses = [[] for i in range (num_workers)]\n",
        "test_losses = []\n",
        "test_accuracy = []"
      ],
      "metadata": {
        "id": "owsg79bWgWgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comm_matrix = []\n",
        "for i in range (num_workers):\n",
        "    adj_nodes = list(range(num_workers))\n",
        "    del adj_nodes[i]\n",
        "    comm_matrix.append(adj_nodes)"
      ],
      "metadata": {
        "id": "9fAB1Up5gWgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for iter_no in range(max_iters):\n",
        "    for worker_num in range(num_workers):\n",
        "        train(iter_no, worker_num, with_noise=True)\n",
        "    acc = test(iter_no, nets_list)\n",
        "    if acc >= 96:\n",
        "        break\n",
        "\n",
        "    # for scheduler in schedulers:\n",
        "    #     scheduler.step()    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "19d77b79-7050-41be-d0ed-a70cdb809b28",
        "id": "5f6xpYthgWgl"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training:  Iteration No:  1 Worker Num:  0 \n",
            " Loss:  2.3095219135284424\n",
            "Training:  Iteration No:  1 Worker Num:  1 \n",
            " Loss:  2.304172992706299\n",
            "Training:  Iteration No:  1 Worker Num:  2 \n",
            " Loss:  2.30245304107666\n",
            "Training:  Iteration No:  1 Worker Num:  3 \n",
            " Loss:  2.310290575027466\n",
            "Training:  Iteration No:  1 Worker Num:  4 \n",
            " Loss:  2.307863235473633\n",
            "Training:  Iteration No:  1 Worker Num:  5 \n",
            " Loss:  2.3171119689941406\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            " Loss:  3.090336561203003\n",
            "Training:  Iteration No:  370 Worker Num:  7 \n",
            " Loss:  4.733135223388672\n",
            "Test:  Iteration No:  370 \n",
            " Loss:  1.3777252401732192\n",
            "Test accuracy:  87.0\n",
            "Training:  Iteration No:  371 Worker Num:  0 \n",
            " Loss:  3.164806604385376\n",
            "Training:  Iteration No:  371 Worker Num:  1 \n",
            " Loss:  2.3668408393859863\n",
            "Training:  Iteration No:  371 Worker Num:  2 \n",
            " Loss:  2.112806797027588\n",
            "Training:  Iteration No:  371 Worker Num:  3 \n",
            " Loss:  2.616762638092041\n",
            "Training:  Iteration No:  371 Worker Num:  4 \n",
            " Loss:  3.2156693935394287\n",
            "Training:  Iteration No:  371 Worker Num:  5 \n",
            " Loss:  1.790697693824768\n",
            "Training:  Iteration No:  371 Worker Num:  6 \n",
            " Loss:  3.7608156204223633\n",
            "Training:  Iteration No:  371 Worker Num:  7 \n",
            " Loss:  3.1508169174194336\n",
            "Test:  Iteration No:  371 \n",
            " Loss:  1.4818666618364522\n",
            "Test accuracy:  86.17\n",
            "Training:  Iteration No:  372 Worker Num:  0 \n",
            " Loss:  2.887624979019165\n",
            "Training:  Iteration No:  372 Worker Num:  1 \n",
            " Loss:  4.829021453857422\n",
            "Training:  Iteration No:  372 Worker Num:  2 \n",
            " Loss:  4.489429473876953\n",
            "Training:  Iteration No:  372 Worker Num:  3 \n",
            " Loss:  2.5901927947998047\n",
            "Training:  Iteration No:  372 Worker Num:  4 \n",
            " Loss:  2.8324804306030273\n",
            "Training:  Iteration No:  372 Worker Num:  5 \n",
            " Loss:  2.2731220722198486\n",
            "Training:  Iteration No:  372 Worker Num:  6 \n",
            " Loss:  2.9085304737091064\n",
            "Training:  Iteration No:  372 Worker Num:  7 \n",
            " Loss:  3.4524219036102295\n",
            "Test:  Iteration No:  372 \n",
            " Loss:  1.3896375329622739\n",
            "Test accuracy:  87.07\n",
            "Training:  Iteration No:  373 Worker Num:  0 \n",
            " Loss:  4.279148578643799\n",
            "Training:  Iteration No:  373 Worker Num:  1 \n",
            " Loss:  1.792854905128479\n",
            "Training:  Iteration No:  373 Worker Num:  2 \n",
            " Loss:  2.435683250427246\n",
            "Training:  Iteration No:  373 Worker Num:  3 \n",
            " Loss:  1.259026050567627\n",
            "Training:  Iteration No:  373 Worker Num:  4 \n",
            " Loss:  3.8078927993774414\n",
            "Training:  Iteration No:  373 Worker Num:  5 \n",
            " Loss:  4.234691619873047\n",
            "Training:  Iteration No:  373 Worker Num:  6 \n",
            " Loss:  2.57322096824646\n",
            "Training:  Iteration No:  373 Worker Num:  7 \n",
            " Loss:  4.85213565826416\n",
            "Test:  Iteration No:  373 \n",
            " Loss:  1.5301998783657444\n",
            "Test accuracy:  86.03\n",
            "Training:  Iteration No:  374 Worker Num:  0 \n",
            " Loss:  3.1353349685668945\n",
            "Training:  Iteration No:  374 Worker Num:  1 \n",
            " Loss:  2.337824821472168\n",
            "Training:  Iteration No:  374 Worker Num:  2 \n",
            " Loss:  3.2595982551574707\n",
            "Training:  Iteration No:  374 Worker Num:  3 \n",
            " Loss:  2.926532506942749\n",
            "Training:  Iteration No:  374 Worker Num:  4 \n",
            " Loss:  2.976201057434082\n",
            "Training:  Iteration No:  374 Worker Num:  5 \n",
            " Loss:  2.9784669876098633\n",
            "Training:  Iteration No:  374 Worker Num:  6 \n",
            " Loss:  2.772850751876831\n",
            "Training:  Iteration No:  374 Worker Num:  7 \n",
            " Loss:  3.717165946960449\n",
            "Test:  Iteration No:  374 \n",
            " Loss:  1.456633447280413\n",
            "Test accuracy:  86.86\n",
            "Training:  Iteration No:  375 Worker Num:  0 \n",
            " Loss:  2.916342258453369\n",
            "Training:  Iteration No:  375 Worker Num:  1 \n",
            " Loss:  3.1737220287323\n",
            "Training:  Iteration No:  375 Worker Num:  2 \n",
            " Loss:  3.164541006088257\n",
            "Training:  Iteration No:  375 Worker Num:  3 \n",
            " Loss:  1.9043197631835938\n",
            "Training:  Iteration No:  375 Worker Num:  4 \n",
            " Loss:  3.873220920562744\n",
            "Training:  Iteration No:  375 Worker Num:  5 \n",
            " Loss:  4.400590419769287\n",
            "Training:  Iteration No:  375 Worker Num:  6 \n",
            " Loss:  2.54716157913208\n",
            "Training:  Iteration No:  375 Worker Num:  7 \n",
            " Loss:  3.7396187782287598\n",
            "Test:  Iteration No:  375 \n",
            " Loss:  1.5517956822450403\n",
            "Test accuracy:  86.0\n",
            "Training:  Iteration No:  376 Worker Num:  0 \n",
            " Loss:  2.742455005645752\n",
            "Training:  Iteration No:  376 Worker Num:  1 \n",
            " Loss:  4.6126484870910645\n",
            "Training:  Iteration No:  376 Worker Num:  2 \n",
            " Loss:  3.001904249191284\n",
            "Training:  Iteration No:  376 Worker Num:  3 \n",
            " Loss:  3.042841911315918\n",
            "Training:  Iteration No:  376 Worker Num:  4 \n",
            " Loss:  2.073928117752075\n",
            "Training:  Iteration No:  376 Worker Num:  5 \n",
            " Loss:  3.09899640083313\n",
            "Training:  Iteration No:  376 Worker Num:  6 \n",
            " Loss:  2.7171928882598877\n",
            "Training:  Iteration No:  376 Worker Num:  7 \n",
            " Loss:  3.973038911819458\n",
            "Test:  Iteration No:  376 \n",
            " Loss:  1.4895695776124545\n",
            "Test accuracy:  87.03\n",
            "Training:  Iteration No:  377 Worker Num:  0 \n",
            " Loss:  2.2038872241973877\n",
            "Training:  Iteration No:  377 Worker Num:  1 \n",
            " Loss:  2.4789562225341797\n",
            "Training:  Iteration No:  377 Worker Num:  2 \n",
            " Loss:  2.785567283630371\n",
            "Training:  Iteration No:  377 Worker Num:  3 \n",
            " Loss:  5.250233173370361\n",
            "Training:  Iteration No:  377 Worker Num:  4 \n",
            " Loss:  4.38418436050415\n",
            "Training:  Iteration No:  377 Worker Num:  5 \n",
            " Loss:  3.049787998199463\n",
            "Training:  Iteration No:  377 Worker Num:  6 \n",
            " Loss:  3.295015811920166\n",
            "Training:  Iteration No:  377 Worker Num:  7 \n",
            " Loss:  4.034091472625732\n",
            "Test:  Iteration No:  377 \n",
            " Loss:  1.7156400266516059\n",
            "Test accuracy:  85.4\n",
            "Training:  Iteration No:  378 Worker Num:  0 \n",
            " Loss:  6.127532482147217\n",
            "Training:  Iteration No:  378 Worker Num:  1 \n",
            " Loss:  3.9194700717926025\n",
            "Training:  Iteration No:  378 Worker Num:  2 \n",
            " Loss:  1.9869476556777954\n",
            "Training:  Iteration No:  378 Worker Num:  3 \n",
            " Loss:  3.5128536224365234\n",
            "Training:  Iteration No:  378 Worker Num:  4 \n",
            " Loss:  2.8482515811920166\n",
            "Training:  Iteration No:  378 Worker Num:  5 \n",
            " Loss:  4.354485511779785\n",
            "Training:  Iteration No:  378 Worker Num:  6 \n",
            " Loss:  2.3033833503723145\n",
            "Training:  Iteration No:  378 Worker Num:  7 \n",
            " Loss:  2.9747231006622314\n",
            "Test:  Iteration No:  378 \n",
            " Loss:  1.5143264326490933\n",
            "Test accuracy:  86.75\n",
            "Training:  Iteration No:  379 Worker Num:  0 \n",
            " Loss:  3.334158182144165\n",
            "Training:  Iteration No:  379 Worker Num:  1 \n",
            " Loss:  2.365370273590088\n",
            "Training:  Iteration No:  379 Worker Num:  2 \n",
            " Loss:  2.721896171569824\n",
            "Training:  Iteration No:  379 Worker Num:  3 \n",
            " Loss:  5.110108375549316\n",
            "Training:  Iteration No:  379 Worker Num:  4 \n",
            " Loss:  2.2106828689575195\n",
            "Training:  Iteration No:  379 Worker Num:  5 \n",
            " Loss:  3.811981678009033\n",
            "Training:  Iteration No:  379 Worker Num:  6 \n",
            " Loss:  2.02004075050354\n",
            "Training:  Iteration No:  379 Worker Num:  7 \n",
            " Loss:  2.74094295501709\n",
            "Test:  Iteration No:  379 \n",
            " Loss:  1.4275967068616537\n",
            "Test accuracy:  87.05\n",
            "Training:  Iteration No:  380 Worker Num:  0 \n",
            " Loss:  3.738835096359253\n",
            "Training:  Iteration No:  380 Worker Num:  1 \n",
            " Loss:  4.080774784088135\n",
            "Training:  Iteration No:  380 Worker Num:  2 \n",
            " Loss:  4.1222615242004395\n",
            "Training:  Iteration No:  380 Worker Num:  3 \n",
            " Loss:  1.989700436592102\n",
            "Training:  Iteration No:  380 Worker Num:  4 \n",
            " Loss:  3.8936421871185303\n",
            "Training:  Iteration No:  380 Worker Num:  5 \n",
            " Loss:  3.2548611164093018\n",
            "Training:  Iteration No:  380 Worker Num:  6 \n",
            " Loss:  4.489592552185059\n",
            "Training:  Iteration No:  380 Worker Num:  7 \n",
            " Loss:  11.254342079162598\n",
            "Test:  Iteration No:  380 \n",
            " Loss:  1.5326744465487478\n",
            "Test accuracy:  85.84\n",
            "Training:  Iteration No:  381 Worker Num:  0 \n",
            " Loss:  2.301107406616211\n",
            "Training:  Iteration No:  381 Worker Num:  1 \n",
            " Loss:  3.7298200130462646\n",
            "Training:  Iteration No:  381 Worker Num:  2 \n",
            " Loss:  1.7346210479736328\n",
            "Training:  Iteration No:  381 Worker Num:  3 \n",
            " Loss:  4.756463050842285\n",
            "Training:  Iteration No:  381 Worker Num:  4 \n",
            " Loss:  2.347099542617798\n",
            "Training:  Iteration No:  381 Worker Num:  5 \n",
            " Loss:  7.528020858764648\n",
            "Training:  Iteration No:  381 Worker Num:  6 \n",
            " Loss:  1.509333610534668\n",
            "Training:  Iteration No:  381 Worker Num:  7 \n",
            " Loss:  2.16646671295166\n",
            "Test:  Iteration No:  381 \n",
            " Loss:  1.5577004858870296\n",
            "Test accuracy:  85.93\n",
            "Training:  Iteration No:  382 Worker Num:  0 \n",
            " Loss:  2.393585681915283\n",
            "Training:  Iteration No:  382 Worker Num:  1 \n",
            " Loss:  2.4316935539245605\n",
            "Training:  Iteration No:  382 Worker Num:  2 \n",
            " Loss:  3.07812237739563\n",
            "Training:  Iteration No:  382 Worker Num:  3 \n",
            " Loss:  4.046018123626709\n",
            "Training:  Iteration No:  382 Worker Num:  4 \n",
            " Loss:  2.8287196159362793\n",
            "Training:  Iteration No:  382 Worker Num:  5 \n",
            " Loss:  3.0267410278320312\n",
            "Training:  Iteration No:  382 Worker Num:  6 \n",
            " Loss:  3.3549630641937256\n",
            "Training:  Iteration No:  382 Worker Num:  7 \n",
            " Loss:  4.669925689697266\n",
            "Test:  Iteration No:  382 \n",
            " Loss:  1.6316278089262262\n",
            "Test accuracy:  85.5\n",
            "Training:  Iteration No:  383 Worker Num:  0 \n",
            " Loss:  3.2363765239715576\n",
            "Training:  Iteration No:  383 Worker Num:  1 \n",
            " Loss:  3.4803144931793213\n",
            "Training:  Iteration No:  383 Worker Num:  2 \n",
            " Loss:  1.275003433227539\n",
            "Training:  Iteration No:  383 Worker Num:  3 \n",
            " Loss:  4.007014751434326\n",
            "Training:  Iteration No:  383 Worker Num:  4 \n",
            " Loss:  3.6388912200927734\n",
            "Training:  Iteration No:  383 Worker Num:  5 \n",
            " Loss:  3.216742992401123\n",
            "Training:  Iteration No:  383 Worker Num:  6 \n",
            " Loss:  2.5506560802459717\n",
            "Training:  Iteration No:  383 Worker Num:  7 \n",
            " Loss:  2.6811106204986572\n",
            "Test:  Iteration No:  383 \n",
            " Loss:  1.5231644158145483\n",
            "Test accuracy:  86.25\n",
            "Training:  Iteration No:  384 Worker Num:  0 \n",
            " Loss:  1.4492944478988647\n",
            "Training:  Iteration No:  384 Worker Num:  1 \n",
            " Loss:  5.470177173614502\n",
            "Training:  Iteration No:  384 Worker Num:  2 \n",
            " Loss:  2.926447629928589\n",
            "Training:  Iteration No:  384 Worker Num:  3 \n",
            " Loss:  4.072409152984619\n",
            "Training:  Iteration No:  384 Worker Num:  4 \n",
            " Loss:  3.7225773334503174\n",
            "Training:  Iteration No:  384 Worker Num:  5 \n",
            " Loss:  4.1314544677734375\n",
            "Training:  Iteration No:  384 Worker Num:  6 \n",
            " Loss:  2.2394466400146484\n",
            "Training:  Iteration No:  384 Worker Num:  7 \n",
            " Loss:  2.4740936756134033\n",
            "Test:  Iteration No:  384 \n",
            " Loss:  1.5322240668185627\n",
            "Test accuracy:  86.13\n",
            "Training:  Iteration No:  385 Worker Num:  0 \n",
            " Loss:  6.302303791046143\n",
            "Training:  Iteration No:  385 Worker Num:  1 \n",
            " Loss:  2.8149235248565674\n",
            "Training:  Iteration No:  385 Worker Num:  2 \n",
            " Loss:  2.791206121444702\n",
            "Training:  Iteration No:  385 Worker Num:  3 \n",
            " Loss:  3.2912356853485107\n",
            "Training:  Iteration No:  385 Worker Num:  4 \n",
            " Loss:  2.8150198459625244\n",
            "Training:  Iteration No:  385 Worker Num:  5 \n",
            " Loss:  2.631890058517456\n",
            "Training:  Iteration No:  385 Worker Num:  6 \n",
            " Loss:  4.255927562713623\n",
            "Training:  Iteration No:  385 Worker Num:  7 \n",
            " Loss:  3.397817850112915\n",
            "Test:  Iteration No:  385 \n",
            " Loss:  1.6491947060143664\n",
            "Test accuracy:  85.55\n",
            "Training:  Iteration No:  386 Worker Num:  0 \n",
            " Loss:  2.9544014930725098\n",
            "Training:  Iteration No:  386 Worker Num:  1 \n",
            " Loss:  4.107841491699219\n",
            "Training:  Iteration No:  386 Worker Num:  2 \n",
            " Loss:  2.8923165798187256\n",
            "Training:  Iteration No:  386 Worker Num:  3 \n",
            " Loss:  4.2858967781066895\n",
            "Training:  Iteration No:  386 Worker Num:  4 \n",
            " Loss:  2.9212183952331543\n",
            "Training:  Iteration No:  386 Worker Num:  5 \n",
            " Loss:  4.374791145324707\n",
            "Training:  Iteration No:  386 Worker Num:  6 \n",
            " Loss:  2.760136365890503\n",
            "Training:  Iteration No:  386 Worker Num:  7 \n",
            " Loss:  3.8808016777038574\n",
            "Test:  Iteration No:  386 \n",
            " Loss:  1.6587721943572353\n",
            "Test accuracy:  85.42\n",
            "Training:  Iteration No:  387 Worker Num:  0 \n",
            " Loss:  3.5099761486053467\n",
            "Training:  Iteration No:  387 Worker Num:  1 \n",
            " Loss:  4.987871170043945\n",
            "Training:  Iteration No:  387 Worker Num:  2 \n",
            " Loss:  2.2776427268981934\n",
            "Training:  Iteration No:  387 Worker Num:  3 \n",
            " Loss:  3.6435599327087402\n",
            "Training:  Iteration No:  387 Worker Num:  4 \n",
            " Loss:  3.133504629135132\n",
            "Training:  Iteration No:  387 Worker Num:  5 \n",
            " Loss:  2.4585516452789307\n",
            "Training:  Iteration No:  387 Worker Num:  6 \n",
            " Loss:  5.666670322418213\n",
            "Training:  Iteration No:  387 Worker Num:  7 \n",
            " Loss:  3.7709896564483643\n",
            "Test:  Iteration No:  387 \n",
            " Loss:  1.5404700386810228\n",
            "Test accuracy:  86.52\n",
            "Training:  Iteration No:  388 Worker Num:  0 \n",
            " Loss:  2.559236526489258\n",
            "Training:  Iteration No:  388 Worker Num:  1 \n",
            " Loss:  3.4599242210388184\n",
            "Training:  Iteration No:  388 Worker Num:  2 \n",
            " Loss:  1.9688118696212769\n",
            "Training:  Iteration No:  388 Worker Num:  3 \n",
            " Loss:  4.271456241607666\n",
            "Training:  Iteration No:  388 Worker Num:  4 \n",
            " Loss:  2.5137548446655273\n",
            "Training:  Iteration No:  388 Worker Num:  5 \n",
            " Loss:  3.3213698863983154\n",
            "Training:  Iteration No:  388 Worker Num:  6 \n",
            " Loss:  3.270876407623291\n",
            "Training:  Iteration No:  388 Worker Num:  7 \n",
            " Loss:  2.8721582889556885\n",
            "Test:  Iteration No:  388 \n",
            " Loss:  1.538156055220509\n",
            "Test accuracy:  86.68\n",
            "Training:  Iteration No:  389 Worker Num:  0 \n",
            " Loss:  2.3524296283721924\n",
            "Training:  Iteration No:  389 Worker Num:  1 \n",
            " Loss:  1.4652944803237915\n",
            "Training:  Iteration No:  389 Worker Num:  2 \n",
            " Loss:  3.181820869445801\n",
            "Training:  Iteration No:  389 Worker Num:  3 \n",
            " Loss:  1.8149915933609009\n",
            "Training:  Iteration No:  389 Worker Num:  4 \n",
            " Loss:  4.174704551696777\n",
            "Training:  Iteration No:  389 Worker Num:  5 \n",
            " Loss:  2.7457926273345947\n",
            "Training:  Iteration No:  389 Worker Num:  6 \n",
            " Loss:  8.059198379516602\n",
            "Training:  Iteration No:  389 Worker Num:  7 \n",
            " Loss:  5.742887496948242\n",
            "Test:  Iteration No:  389 \n",
            " Loss:  1.5892847336354528\n",
            "Test accuracy:  86.25\n",
            "Training:  Iteration No:  390 Worker Num:  0 \n",
            " Loss:  2.4589297771453857\n",
            "Training:  Iteration No:  390 Worker Num:  1 \n",
            " Loss:  2.616802453994751\n",
            "Training:  Iteration No:  390 Worker Num:  2 \n",
            " Loss:  2.3770084381103516\n",
            "Training:  Iteration No:  390 Worker Num:  3 \n",
            " Loss:  3.676358699798584\n",
            "Training:  Iteration No:  390 Worker Num:  4 \n",
            " Loss:  2.641721725463867\n",
            "Training:  Iteration No:  390 Worker Num:  5 \n",
            " Loss:  1.5331753492355347\n",
            "Training:  Iteration No:  390 Worker Num:  6 \n",
            " Loss:  3.039478063583374\n",
            "Training:  Iteration No:  390 Worker Num:  7 \n",
            " Loss:  4.051048755645752\n",
            "Test:  Iteration No:  390 \n",
            " Loss:  1.5294931847671436\n",
            "Test accuracy:  86.85\n",
            "Training:  Iteration No:  391 Worker Num:  0 \n",
            " Loss:  2.9502902030944824\n",
            "Training:  Iteration No:  391 Worker Num:  1 \n",
            " Loss:  2.328740119934082\n",
            "Training:  Iteration No:  391 Worker Num:  2 \n",
            " Loss:  2.7331645488739014\n",
            "Training:  Iteration No:  391 Worker Num:  3 \n",
            " Loss:  3.3333628177642822\n",
            "Training:  Iteration No:  391 Worker Num:  4 \n",
            " Loss:  4.619547367095947\n",
            "Training:  Iteration No:  391 Worker Num:  5 \n",
            " Loss:  2.905486583709717\n",
            "Training:  Iteration No:  391 Worker Num:  6 \n",
            " Loss:  2.8728878498077393\n",
            "Training:  Iteration No:  391 Worker Num:  7 \n",
            " Loss:  2.4713916778564453\n",
            "Test:  Iteration No:  391 \n",
            " Loss:  1.874324900204245\n",
            "Test accuracy:  84.47\n",
            "Training:  Iteration No:  392 Worker Num:  0 \n",
            " Loss:  2.222905397415161\n",
            "Training:  Iteration No:  392 Worker Num:  1 \n",
            " Loss:  4.276081085205078\n",
            "Training:  Iteration No:  392 Worker Num:  2 \n",
            " Loss:  2.795083999633789\n",
            "Training:  Iteration No:  392 Worker Num:  3 \n",
            " Loss:  2.782893180847168\n",
            "Training:  Iteration No:  392 Worker Num:  4 \n",
            " Loss:  3.821871757507324\n",
            "Training:  Iteration No:  392 Worker Num:  5 \n",
            " Loss:  3.007199764251709\n",
            "Training:  Iteration No:  392 Worker Num:  6 \n",
            " Loss:  3.877101421356201\n",
            "Training:  Iteration No:  392 Worker Num:  7 \n",
            " Loss:  4.256241798400879\n",
            "Test:  Iteration No:  392 \n",
            " Loss:  1.4806360894155106\n",
            "Test accuracy:  87.17\n",
            "Training:  Iteration No:  393 Worker Num:  0 \n",
            " Loss:  2.806055784225464\n",
            "Training:  Iteration No:  393 Worker Num:  1 \n",
            " Loss:  2.2508156299591064\n",
            "Training:  Iteration No:  393 Worker Num:  2 \n",
            " Loss:  2.879856824874878\n",
            "Training:  Iteration No:  393 Worker Num:  3 \n",
            " Loss:  3.257768154144287\n",
            "Training:  Iteration No:  393 Worker Num:  4 \n",
            " Loss:  2.9757211208343506\n",
            "Training:  Iteration No:  393 Worker Num:  5 \n",
            " Loss:  4.573062419891357\n",
            "Training:  Iteration No:  393 Worker Num:  6 \n",
            " Loss:  2.2276782989501953\n",
            "Training:  Iteration No:  393 Worker Num:  7 \n",
            " Loss:  2.7852776050567627\n",
            "Test:  Iteration No:  393 \n",
            " Loss:  1.62459400167436\n",
            "Test accuracy:  85.97\n",
            "Training:  Iteration No:  394 Worker Num:  0 \n",
            " Loss:  3.1610782146453857\n",
            "Training:  Iteration No:  394 Worker Num:  1 \n",
            " Loss:  3.0296530723571777\n",
            "Training:  Iteration No:  394 Worker Num:  2 \n",
            " Loss:  4.632421970367432\n",
            "Training:  Iteration No:  394 Worker Num:  3 \n",
            " Loss:  3.0473780632019043\n",
            "Training:  Iteration No:  394 Worker Num:  4 \n",
            " Loss:  3.479987144470215\n",
            "Training:  Iteration No:  394 Worker Num:  5 \n",
            " Loss:  2.9412145614624023\n",
            "Training:  Iteration No:  394 Worker Num:  6 \n",
            " Loss:  2.9403076171875\n",
            "Training:  Iteration No:  394 Worker Num:  7 \n",
            " Loss:  2.6056089401245117\n",
            "Test:  Iteration No:  394 \n",
            " Loss:  1.550053982886035\n",
            "Test accuracy:  86.67\n",
            "Training:  Iteration No:  395 Worker Num:  0 \n",
            " Loss:  2.4289591312408447\n",
            "Training:  Iteration No:  395 Worker Num:  1 \n",
            " Loss:  3.4011659622192383\n",
            "Training:  Iteration No:  395 Worker Num:  2 \n",
            " Loss:  3.1293177604675293\n",
            "Training:  Iteration No:  395 Worker Num:  3 \n",
            " Loss:  4.903225421905518\n",
            "Training:  Iteration No:  395 Worker Num:  4 \n",
            " Loss:  2.5201947689056396\n",
            "Training:  Iteration No:  395 Worker Num:  5 \n",
            " Loss:  2.8546142578125\n",
            "Training:  Iteration No:  395 Worker Num:  6 \n",
            " Loss:  4.433996677398682\n",
            "Training:  Iteration No:  395 Worker Num:  7 \n",
            " Loss:  2.3448915481567383\n",
            "Test:  Iteration No:  395 \n",
            " Loss:  1.8910441888095457\n",
            "Test accuracy:  84.44\n",
            "Training:  Iteration No:  396 Worker Num:  0 \n",
            " Loss:  2.2126944065093994\n",
            "Training:  Iteration No:  396 Worker Num:  1 \n",
            " Loss:  3.2874832153320312\n",
            "Training:  Iteration No:  396 Worker Num:  2 \n",
            " Loss:  1.4060113430023193\n",
            "Training:  Iteration No:  396 Worker Num:  3 \n",
            " Loss:  3.57186222076416\n",
            "Training:  Iteration No:  396 Worker Num:  4 \n",
            " Loss:  2.490410804748535\n",
            "Training:  Iteration No:  396 Worker Num:  5 \n",
            " Loss:  2.2069976329803467\n",
            "Training:  Iteration No:  396 Worker Num:  6 \n",
            " Loss:  2.475283145904541\n",
            "Training:  Iteration No:  396 Worker Num:  7 \n",
            " Loss:  2.2528607845306396\n",
            "Test:  Iteration No:  396 \n",
            " Loss:  1.4266092322046184\n",
            "Test accuracy:  87.5\n",
            "Training:  Iteration No:  397 Worker Num:  0 \n",
            " Loss:  2.363098621368408\n",
            "Training:  Iteration No:  397 Worker Num:  1 \n",
            " Loss:  3.5539674758911133\n",
            "Training:  Iteration No:  397 Worker Num:  2 \n",
            " Loss:  3.541245698928833\n",
            "Training:  Iteration No:  397 Worker Num:  3 \n",
            " Loss:  2.8921303749084473\n",
            "Training:  Iteration No:  397 Worker Num:  4 \n",
            " Loss:  3.9887795448303223\n",
            "Training:  Iteration No:  397 Worker Num:  5 \n",
            " Loss:  2.1842668056488037\n",
            "Training:  Iteration No:  397 Worker Num:  6 \n",
            " Loss:  3.763554334640503\n",
            "Training:  Iteration No:  397 Worker Num:  7 \n",
            " Loss:  2.419468402862549\n",
            "Test:  Iteration No:  397 \n",
            " Loss:  1.6201901650776471\n",
            "Test accuracy:  86.24\n",
            "Training:  Iteration No:  398 Worker Num:  0 \n",
            " Loss:  2.545454263687134\n",
            "Training:  Iteration No:  398 Worker Num:  1 \n",
            " Loss:  1.3528335094451904\n",
            "Training:  Iteration No:  398 Worker Num:  2 \n",
            " Loss:  4.112668037414551\n",
            "Training:  Iteration No:  398 Worker Num:  3 \n",
            " Loss:  2.1995091438293457\n",
            "Training:  Iteration No:  398 Worker Num:  4 \n",
            " Loss:  2.952881097793579\n",
            "Training:  Iteration No:  398 Worker Num:  5 \n",
            " Loss:  7.4135026931762695\n",
            "Training:  Iteration No:  398 Worker Num:  6 \n",
            " Loss:  3.029937744140625\n",
            "Training:  Iteration No:  398 Worker Num:  7 \n",
            " Loss:  4.297676086425781\n",
            "Test:  Iteration No:  398 \n",
            " Loss:  1.4742511395602667\n",
            "Test accuracy:  87.06\n",
            "Training:  Iteration No:  399 Worker Num:  0 \n",
            " Loss:  5.191444396972656\n",
            "Training:  Iteration No:  399 Worker Num:  1 \n",
            " Loss:  3.7933647632598877\n",
            "Training:  Iteration No:  399 Worker Num:  2 \n",
            " Loss:  4.100045680999756\n",
            "Training:  Iteration No:  399 Worker Num:  3 \n",
            " Loss:  3.2293882369995117\n",
            "Training:  Iteration No:  399 Worker Num:  4 \n",
            " Loss:  1.5154370069503784\n",
            "Training:  Iteration No:  399 Worker Num:  5 \n",
            " Loss:  3.0910842418670654\n",
            "Training:  Iteration No:  399 Worker Num:  6 \n",
            " Loss:  2.3692879676818848\n",
            "Training:  Iteration No:  399 Worker Num:  7 \n",
            " Loss:  3.3374087810516357\n",
            "Test:  Iteration No:  399 \n",
            " Loss:  1.5334573806384573\n",
            "Test accuracy:  86.89\n",
            "Training:  Iteration No:  400 Worker Num:  0 \n",
            " Loss:  3.8255507946014404\n",
            "Training:  Iteration No:  400 Worker Num:  1 \n",
            " Loss:  3.182501792907715\n",
            "Training:  Iteration No:  400 Worker Num:  2 \n",
            " Loss:  3.097919464111328\n",
            "Training:  Iteration No:  400 Worker Num:  3 \n",
            " Loss:  5.4878716468811035\n",
            "Training:  Iteration No:  400 Worker Num:  4 \n",
            " Loss:  3.448065757751465\n",
            "Training:  Iteration No:  400 Worker Num:  5 \n",
            " Loss:  5.268577575683594\n",
            "Training:  Iteration No:  400 Worker Num:  6 \n",
            " Loss:  3.4091196060180664\n",
            "Training:  Iteration No:  400 Worker Num:  7 \n",
            " Loss:  2.460188388824463\n",
            "Test:  Iteration No:  400 \n",
            " Loss:  1.6331978373161142\n",
            "Test accuracy:  86.34\n",
            "Training:  Iteration No:  401 Worker Num:  0 \n",
            " Loss:  3.4422097206115723\n",
            "Training:  Iteration No:  401 Worker Num:  1 \n",
            " Loss:  1.9426014423370361\n",
            "Training:  Iteration No:  401 Worker Num:  2 \n",
            " Loss:  4.520012855529785\n",
            "Training:  Iteration No:  401 Worker Num:  3 \n",
            " Loss:  1.402401089668274\n",
            "Training:  Iteration No:  401 Worker Num:  4 \n",
            " Loss:  2.515927314758301\n",
            "Training:  Iteration No:  401 Worker Num:  5 \n",
            " Loss:  3.2285382747650146\n",
            "Training:  Iteration No:  401 Worker Num:  6 \n",
            " Loss:  3.7001633644104004\n",
            "Training:  Iteration No:  401 Worker Num:  7 \n",
            " Loss:  4.0938215255737305\n",
            "Test:  Iteration No:  401 \n",
            " Loss:  1.4619034047328119\n",
            "Test accuracy:  87.9\n",
            "Training:  Iteration No:  402 Worker Num:  0 \n",
            " Loss:  2.303680658340454\n",
            "Training:  Iteration No:  402 Worker Num:  1 \n",
            " Loss:  3.455935478210449\n",
            "Training:  Iteration No:  402 Worker Num:  2 \n",
            " Loss:  3.934159517288208\n",
            "Training:  Iteration No:  402 Worker Num:  3 \n",
            " Loss:  2.696179151535034\n",
            "Training:  Iteration No:  402 Worker Num:  4 \n",
            " Loss:  2.9524364471435547\n",
            "Training:  Iteration No:  402 Worker Num:  5 \n",
            " Loss:  6.68897008895874\n",
            "Training:  Iteration No:  402 Worker Num:  6 \n",
            " Loss:  4.745707035064697\n",
            "Training:  Iteration No:  402 Worker Num:  7 \n",
            " Loss:  3.8582234382629395\n",
            "Test:  Iteration No:  402 \n",
            " Loss:  1.45512043129291\n",
            "Test accuracy:  87.84\n",
            "Training:  Iteration No:  403 Worker Num:  0 \n",
            " Loss:  2.332949161529541\n",
            "Training:  Iteration No:  403 Worker Num:  1 \n",
            " Loss:  2.9994394779205322\n",
            "Training:  Iteration No:  403 Worker Num:  2 \n",
            " Loss:  1.9510693550109863\n",
            "Training:  Iteration No:  403 Worker Num:  3 \n",
            " Loss:  2.9334750175476074\n",
            "Training:  Iteration No:  403 Worker Num:  4 \n",
            " Loss:  2.635249137878418\n",
            "Training:  Iteration No:  403 Worker Num:  5 \n",
            " Loss:  2.6817092895507812\n",
            "Training:  Iteration No:  403 Worker Num:  6 \n",
            " Loss:  2.9576127529144287\n",
            "Training:  Iteration No:  403 Worker Num:  7 \n",
            " Loss:  2.727893352508545\n",
            "Test:  Iteration No:  403 \n",
            " Loss:  1.616471164334604\n",
            "Test accuracy:  86.98\n",
            "Training:  Iteration No:  404 Worker Num:  0 \n",
            " Loss:  5.986990451812744\n",
            "Training:  Iteration No:  404 Worker Num:  1 \n",
            " Loss:  2.5574309825897217\n",
            "Training:  Iteration No:  404 Worker Num:  2 \n",
            " Loss:  2.55167293548584\n",
            "Training:  Iteration No:  404 Worker Num:  3 \n",
            " Loss:  3.1694812774658203\n",
            "Training:  Iteration No:  404 Worker Num:  4 \n",
            " Loss:  4.135554790496826\n",
            "Training:  Iteration No:  404 Worker Num:  5 \n",
            " Loss:  1.6272505521774292\n",
            "Training:  Iteration No:  404 Worker Num:  6 \n",
            " Loss:  3.1068661212921143\n",
            "Training:  Iteration No:  404 Worker Num:  7 \n",
            " Loss:  2.7782137393951416\n",
            "Test:  Iteration No:  404 \n",
            " Loss:  1.5256104068830552\n",
            "Test accuracy:  87.56\n",
            "Training:  Iteration No:  405 Worker Num:  0 \n",
            " Loss:  3.694201946258545\n",
            "Training:  Iteration No:  405 Worker Num:  1 \n",
            " Loss:  3.6718857288360596\n",
            "Training:  Iteration No:  405 Worker Num:  2 \n",
            " Loss:  3.971116065979004\n",
            "Training:  Iteration No:  405 Worker Num:  3 \n",
            " Loss:  2.041691780090332\n",
            "Training:  Iteration No:  405 Worker Num:  4 \n",
            " Loss:  3.963301420211792\n",
            "Training:  Iteration No:  405 Worker Num:  5 \n",
            " Loss:  4.8744797706604\n",
            "Training:  Iteration No:  405 Worker Num:  6 \n",
            " Loss:  3.6250181198120117\n",
            "Training:  Iteration No:  405 Worker Num:  7 \n",
            " Loss:  3.3448972702026367\n",
            "Test:  Iteration No:  405 \n",
            " Loss:  1.6309828212347672\n",
            "Test accuracy:  87.05\n",
            "Training:  Iteration No:  406 Worker Num:  0 \n",
            " Loss:  5.146216869354248\n",
            "Training:  Iteration No:  406 Worker Num:  1 \n",
            " Loss:  2.073028326034546\n",
            "Training:  Iteration No:  406 Worker Num:  2 \n",
            " Loss:  3.2039999961853027\n",
            "Training:  Iteration No:  406 Worker Num:  3 \n",
            " Loss:  2.488579511642456\n",
            "Training:  Iteration No:  406 Worker Num:  4 \n",
            " Loss:  4.014706611633301\n",
            "Training:  Iteration No:  406 Worker Num:  5 \n",
            " Loss:  3.412822723388672\n",
            "Training:  Iteration No:  406 Worker Num:  6 \n",
            " Loss:  2.6312925815582275\n",
            "Training:  Iteration No:  406 Worker Num:  7 \n",
            " Loss:  2.2265496253967285\n",
            "Test:  Iteration No:  406 \n",
            " Loss:  1.5025581123919776\n",
            "Test accuracy:  87.79\n",
            "Training:  Iteration No:  407 Worker Num:  0 \n",
            " Loss:  2.1021807193756104\n",
            "Training:  Iteration No:  407 Worker Num:  1 \n",
            " Loss:  1.7776137590408325\n",
            "Training:  Iteration No:  407 Worker Num:  2 \n",
            " Loss:  4.237212181091309\n",
            "Training:  Iteration No:  407 Worker Num:  3 \n",
            " Loss:  3.2595620155334473\n",
            "Training:  Iteration No:  407 Worker Num:  4 \n",
            " Loss:  3.6300671100616455\n",
            "Training:  Iteration No:  407 Worker Num:  5 \n",
            " Loss:  3.6977474689483643\n",
            "Training:  Iteration No:  407 Worker Num:  6 \n",
            " Loss:  3.8869576454162598\n",
            "Training:  Iteration No:  407 Worker Num:  7 \n",
            " Loss:  1.544838309288025\n",
            "Test:  Iteration No:  407 \n",
            " Loss:  1.5775589652770508\n",
            "Test accuracy:  87.57\n",
            "Training:  Iteration No:  408 Worker Num:  0 \n",
            " Loss:  3.0002591609954834\n",
            "Training:  Iteration No:  408 Worker Num:  1 \n",
            " Loss:  2.266801595687866\n",
            "Training:  Iteration No:  408 Worker Num:  2 \n",
            " Loss:  2.3456339836120605\n",
            "Training:  Iteration No:  408 Worker Num:  3 \n",
            " Loss:  3.4146947860717773\n",
            "Training:  Iteration No:  408 Worker Num:  4 \n",
            " Loss:  2.5002477169036865\n",
            "Training:  Iteration No:  408 Worker Num:  5 \n",
            " Loss:  2.1521852016448975\n",
            "Training:  Iteration No:  408 Worker Num:  6 \n",
            " Loss:  3.305497646331787\n",
            "Training:  Iteration No:  408 Worker Num:  7 \n",
            " Loss:  1.937835454940796\n",
            "Test:  Iteration No:  408 \n",
            " Loss:  1.508919098803512\n",
            "Test accuracy:  88.05\n",
            "Training:  Iteration No:  409 Worker Num:  0 \n",
            " Loss:  3.020904064178467\n",
            "Training:  Iteration No:  409 Worker Num:  1 \n",
            " Loss:  2.7808425426483154\n",
            "Training:  Iteration No:  409 Worker Num:  2 \n",
            " Loss:  3.694577932357788\n",
            "Training:  Iteration No:  409 Worker Num:  3 \n",
            " Loss:  4.044624328613281\n",
            "Training:  Iteration No:  409 Worker Num:  4 \n",
            " Loss:  5.474241733551025\n",
            "Training:  Iteration No:  409 Worker Num:  5 \n",
            " Loss:  2.6780850887298584\n",
            "Training:  Iteration No:  409 Worker Num:  6 \n",
            " Loss:  2.0820810794830322\n",
            "Training:  Iteration No:  409 Worker Num:  7 \n",
            " Loss:  2.235434055328369\n",
            "Test:  Iteration No:  409 \n",
            " Loss:  1.692233041860132\n",
            "Test accuracy:  87.27\n",
            "Training:  Iteration No:  410 Worker Num:  0 \n",
            " Loss:  2.538757801055908\n",
            "Training:  Iteration No:  410 Worker Num:  1 \n",
            " Loss:  4.921749591827393\n",
            "Training:  Iteration No:  410 Worker Num:  2 \n",
            " Loss:  2.529468059539795\n",
            "Training:  Iteration No:  410 Worker Num:  3 \n",
            " Loss:  3.9454538822174072\n",
            "Training:  Iteration No:  410 Worker Num:  4 \n",
            " Loss:  1.7267940044403076\n",
            "Training:  Iteration No:  410 Worker Num:  5 \n",
            " Loss:  3.9637176990509033\n",
            "Training:  Iteration No:  410 Worker Num:  6 \n",
            " Loss:  3.4451146125793457\n",
            "Training:  Iteration No:  410 Worker Num:  7 \n",
            " Loss:  2.8731508255004883\n",
            "Test:  Iteration No:  410 \n",
            " Loss:  1.5567996394995502\n",
            "Test accuracy:  87.62\n",
            "Training:  Iteration No:  411 Worker Num:  0 \n",
            " Loss:  2.5646820068359375\n",
            "Training:  Iteration No:  411 Worker Num:  1 \n",
            " Loss:  2.3204095363616943\n",
            "Training:  Iteration No:  411 Worker Num:  2 \n",
            " Loss:  3.7877378463745117\n",
            "Training:  Iteration No:  411 Worker Num:  3 \n",
            " Loss:  3.7925963401794434\n",
            "Training:  Iteration No:  411 Worker Num:  4 \n",
            " Loss:  2.465364694595337\n",
            "Training:  Iteration No:  411 Worker Num:  5 \n",
            " Loss:  2.688866138458252\n",
            "Training:  Iteration No:  411 Worker Num:  6 \n",
            " Loss:  2.592836856842041\n",
            "Training:  Iteration No:  411 Worker Num:  7 \n",
            " Loss:  1.8991838693618774\n",
            "Test:  Iteration No:  411 \n",
            " Loss:  1.5467445389189842\n",
            "Test accuracy:  87.94\n",
            "Training:  Iteration No:  412 Worker Num:  0 \n",
            " Loss:  2.603388786315918\n",
            "Training:  Iteration No:  412 Worker Num:  1 \n",
            " Loss:  2.3597397804260254\n",
            "Training:  Iteration No:  412 Worker Num:  2 \n",
            " Loss:  2.025280475616455\n",
            "Training:  Iteration No:  412 Worker Num:  3 \n",
            " Loss:  3.4866228103637695\n",
            "Training:  Iteration No:  412 Worker Num:  4 \n",
            " Loss:  4.872763633728027\n",
            "Training:  Iteration No:  412 Worker Num:  5 \n",
            " Loss:  5.4978928565979\n",
            "Training:  Iteration No:  412 Worker Num:  6 \n",
            " Loss:  1.928627371788025\n",
            "Training:  Iteration No:  412 Worker Num:  7 \n",
            " Loss:  3.7057857513427734\n",
            "Test:  Iteration No:  412 \n",
            " Loss:  1.4691766356522604\n",
            "Test accuracy:  88.26\n",
            "Training:  Iteration No:  413 Worker Num:  0 \n",
            " Loss:  2.6426563262939453\n",
            "Training:  Iteration No:  413 Worker Num:  1 \n",
            " Loss:  2.5584511756896973\n",
            "Training:  Iteration No:  413 Worker Num:  2 \n",
            " Loss:  4.174078464508057\n",
            "Training:  Iteration No:  413 Worker Num:  3 \n",
            " Loss:  2.595912456512451\n",
            "Training:  Iteration No:  413 Worker Num:  4 \n",
            " Loss:  3.3395578861236572\n",
            "Training:  Iteration No:  413 Worker Num:  5 \n",
            " Loss:  1.9991025924682617\n",
            "Training:  Iteration No:  413 Worker Num:  6 \n",
            " Loss:  2.5140626430511475\n",
            "Training:  Iteration No:  413 Worker Num:  7 \n",
            " Loss:  1.8895987272262573\n",
            "Test:  Iteration No:  413 \n",
            " Loss:  1.5061259833813363\n",
            "Test accuracy:  88.06\n",
            "Training:  Iteration No:  414 Worker Num:  0 \n",
            " Loss:  4.113772392272949\n",
            "Training:  Iteration No:  414 Worker Num:  1 \n",
            " Loss:  2.693666696548462\n",
            "Training:  Iteration No:  414 Worker Num:  2 \n",
            " Loss:  4.521573543548584\n",
            "Training:  Iteration No:  414 Worker Num:  3 \n",
            " Loss:  4.436611652374268\n",
            "Training:  Iteration No:  414 Worker Num:  4 \n",
            " Loss:  6.740321159362793\n",
            "Training:  Iteration No:  414 Worker Num:  5 \n",
            " Loss:  2.430457830429077\n",
            "Training:  Iteration No:  414 Worker Num:  6 \n",
            " Loss:  2.1483359336853027\n",
            "Training:  Iteration No:  414 Worker Num:  7 \n",
            " Loss:  4.151865005493164\n",
            "Test:  Iteration No:  414 \n",
            " Loss:  1.5711134570056062\n",
            "Test accuracy:  87.61\n",
            "Training:  Iteration No:  415 Worker Num:  0 \n",
            " Loss:  5.296847343444824\n",
            "Training:  Iteration No:  415 Worker Num:  1 \n",
            " Loss:  3.089564561843872\n",
            "Training:  Iteration No:  415 Worker Num:  2 \n",
            " Loss:  3.870384931564331\n",
            "Training:  Iteration No:  415 Worker Num:  3 \n",
            " Loss:  3.431257486343384\n",
            "Training:  Iteration No:  415 Worker Num:  4 \n",
            " Loss:  2.9097139835357666\n",
            "Training:  Iteration No:  415 Worker Num:  5 \n",
            " Loss:  3.527360439300537\n",
            "Training:  Iteration No:  415 Worker Num:  6 \n",
            " Loss:  2.4874730110168457\n",
            "Training:  Iteration No:  415 Worker Num:  7 \n",
            " Loss:  4.475207328796387\n",
            "Test:  Iteration No:  415 \n",
            " Loss:  1.5274263099375367\n",
            "Test accuracy:  87.86\n",
            "Training:  Iteration No:  416 Worker Num:  0 \n",
            " Loss:  3.4955389499664307\n",
            "Training:  Iteration No:  416 Worker Num:  1 \n",
            " Loss:  3.5380704402923584\n",
            "Training:  Iteration No:  416 Worker Num:  2 \n",
            " Loss:  3.22961163520813\n",
            "Training:  Iteration No:  416 Worker Num:  3 \n",
            " Loss:  2.6576173305511475\n",
            "Training:  Iteration No:  416 Worker Num:  4 \n",
            " Loss:  3.4438087940216064\n",
            "Training:  Iteration No:  416 Worker Num:  5 \n",
            " Loss:  2.056267738342285\n",
            "Training:  Iteration No:  416 Worker Num:  6 \n",
            " Loss:  2.048859119415283\n",
            "Training:  Iteration No:  416 Worker Num:  7 \n",
            " Loss:  3.325488567352295\n",
            "Test:  Iteration No:  416 \n",
            " Loss:  1.5689163739215242\n",
            "Test accuracy:  87.87\n",
            "Training:  Iteration No:  417 Worker Num:  0 \n",
            " Loss:  2.383110523223877\n",
            "Training:  Iteration No:  417 Worker Num:  1 \n",
            " Loss:  2.082751989364624\n",
            "Training:  Iteration No:  417 Worker Num:  2 \n",
            " Loss:  1.7090237140655518\n",
            "Training:  Iteration No:  417 Worker Num:  3 \n",
            " Loss:  3.7323696613311768\n",
            "Training:  Iteration No:  417 Worker Num:  4 \n",
            " Loss:  4.3557915687561035\n",
            "Training:  Iteration No:  417 Worker Num:  5 \n",
            " Loss:  1.7309614419937134\n",
            "Training:  Iteration No:  417 Worker Num:  6 \n",
            " Loss:  2.299353837966919\n",
            "Training:  Iteration No:  417 Worker Num:  7 \n",
            " Loss:  3.2886366844177246\n",
            "Test:  Iteration No:  417 \n",
            " Loss:  1.617800874572655\n",
            "Test accuracy:  87.54\n",
            "Training:  Iteration No:  418 Worker Num:  0 \n",
            " Loss:  4.110874176025391\n",
            "Training:  Iteration No:  418 Worker Num:  1 \n",
            " Loss:  4.256454944610596\n",
            "Training:  Iteration No:  418 Worker Num:  2 \n",
            " Loss:  2.7055063247680664\n",
            "Training:  Iteration No:  418 Worker Num:  3 \n",
            " Loss:  3.8869941234588623\n",
            "Training:  Iteration No:  418 Worker Num:  4 \n",
            " Loss:  3.7739241123199463\n",
            "Training:  Iteration No:  418 Worker Num:  5 \n",
            " Loss:  2.780557870864868\n",
            "Training:  Iteration No:  418 Worker Num:  6 \n",
            " Loss:  2.8071625232696533\n",
            "Training:  Iteration No:  418 Worker Num:  7 \n",
            " Loss:  2.797687292098999\n",
            "Test:  Iteration No:  418 \n",
            " Loss:  1.5852746655438614\n",
            "Test accuracy:  87.72\n",
            "Training:  Iteration No:  419 Worker Num:  0 \n",
            " Loss:  3.5891358852386475\n",
            "Training:  Iteration No:  419 Worker Num:  1 \n",
            " Loss:  3.3725876808166504\n",
            "Training:  Iteration No:  419 Worker Num:  2 \n",
            " Loss:  3.0551047325134277\n",
            "Training:  Iteration No:  419 Worker Num:  3 \n",
            " Loss:  3.015589714050293\n",
            "Training:  Iteration No:  419 Worker Num:  4 \n",
            " Loss:  4.843616008758545\n",
            "Training:  Iteration No:  419 Worker Num:  5 \n",
            " Loss:  2.2130868434906006\n",
            "Training:  Iteration No:  419 Worker Num:  6 \n",
            " Loss:  2.270339250564575\n",
            "Training:  Iteration No:  419 Worker Num:  7 \n",
            " Loss:  3.1413097381591797\n",
            "Test:  Iteration No:  419 \n",
            " Loss:  1.7055576682397235\n",
            "Test accuracy:  86.91\n",
            "Training:  Iteration No:  420 Worker Num:  0 \n",
            " Loss:  2.6846799850463867\n",
            "Training:  Iteration No:  420 Worker Num:  1 \n",
            " Loss:  2.7342820167541504\n",
            "Training:  Iteration No:  420 Worker Num:  2 \n",
            " Loss:  2.254978656768799\n",
            "Training:  Iteration No:  420 Worker Num:  3 \n",
            " Loss:  2.99147367477417\n",
            "Training:  Iteration No:  420 Worker Num:  4 \n",
            " Loss:  3.214202880859375\n",
            "Training:  Iteration No:  420 Worker Num:  5 \n",
            " Loss:  6.0687408447265625\n",
            "Training:  Iteration No:  420 Worker Num:  6 \n",
            " Loss:  1.4079298973083496\n",
            "Training:  Iteration No:  420 Worker Num:  7 \n",
            " Loss:  1.7621047496795654\n",
            "Test:  Iteration No:  420 \n",
            " Loss:  1.629577721674581\n",
            "Test accuracy:  87.53\n",
            "Training:  Iteration No:  421 Worker Num:  0 \n",
            " Loss:  2.2964675426483154\n",
            "Training:  Iteration No:  421 Worker Num:  1 \n",
            " Loss:  2.3450775146484375\n",
            "Training:  Iteration No:  421 Worker Num:  2 \n",
            " Loss:  3.2783260345458984\n",
            "Training:  Iteration No:  421 Worker Num:  3 \n",
            " Loss:  3.7484142780303955\n",
            "Training:  Iteration No:  421 Worker Num:  4 \n",
            " Loss:  7.783076286315918\n",
            "Training:  Iteration No:  421 Worker Num:  5 \n",
            " Loss:  3.1125733852386475\n",
            "Training:  Iteration No:  421 Worker Num:  6 \n",
            " Loss:  3.762733221054077\n",
            "Training:  Iteration No:  421 Worker Num:  7 \n",
            " Loss:  2.5909485816955566\n",
            "Test:  Iteration No:  421 \n",
            " Loss:  1.5903400152053406\n",
            "Test accuracy:  87.67\n",
            "Training:  Iteration No:  422 Worker Num:  0 \n",
            " Loss:  2.0933589935302734\n",
            "Training:  Iteration No:  422 Worker Num:  1 \n",
            " Loss:  4.005309104919434\n",
            "Training:  Iteration No:  422 Worker Num:  2 \n",
            " Loss:  3.8696582317352295\n",
            "Training:  Iteration No:  422 Worker Num:  3 \n",
            " Loss:  3.175784111022949\n",
            "Training:  Iteration No:  422 Worker Num:  4 \n",
            " Loss:  4.714888572692871\n",
            "Training:  Iteration No:  422 Worker Num:  5 \n",
            " Loss:  3.3697690963745117\n",
            "Training:  Iteration No:  422 Worker Num:  6 \n",
            " Loss:  2.822958469390869\n",
            "Training:  Iteration No:  422 Worker Num:  7 \n",
            " Loss:  3.9131619930267334\n",
            "Test:  Iteration No:  422 \n",
            " Loss:  1.6003710394342066\n",
            "Test accuracy:  87.22\n",
            "Training:  Iteration No:  423 Worker Num:  0 \n",
            " Loss:  2.7461564540863037\n",
            "Training:  Iteration No:  423 Worker Num:  1 \n",
            " Loss:  3.8695645332336426\n",
            "Training:  Iteration No:  423 Worker Num:  2 \n",
            " Loss:  1.574319839477539\n",
            "Training:  Iteration No:  423 Worker Num:  3 \n",
            " Loss:  2.5067052841186523\n",
            "Training:  Iteration No:  423 Worker Num:  4 \n",
            " Loss:  3.7119102478027344\n",
            "Training:  Iteration No:  423 Worker Num:  5 \n",
            " Loss:  3.6656360626220703\n",
            "Training:  Iteration No:  423 Worker Num:  6 \n",
            " Loss:  2.099569320678711\n",
            "Training:  Iteration No:  423 Worker Num:  7 \n",
            " Loss:  2.970989942550659\n",
            "Test:  Iteration No:  423 \n",
            " Loss:  1.5517477399013577\n",
            "Test accuracy:  87.78\n",
            "Training:  Iteration No:  424 Worker Num:  0 \n",
            " Loss:  1.480249047279358\n",
            "Training:  Iteration No:  424 Worker Num:  1 \n",
            " Loss:  3.33109450340271\n",
            "Training:  Iteration No:  424 Worker Num:  2 \n",
            " Loss:  4.993475437164307\n",
            "Training:  Iteration No:  424 Worker Num:  3 \n",
            " Loss:  4.881228923797607\n",
            "Training:  Iteration No:  424 Worker Num:  4 \n",
            " Loss:  3.784390926361084\n",
            "Training:  Iteration No:  424 Worker Num:  5 \n",
            " Loss:  1.5307222604751587\n",
            "Training:  Iteration No:  424 Worker Num:  6 \n",
            " Loss:  2.6762518882751465\n",
            "Training:  Iteration No:  424 Worker Num:  7 \n",
            " Loss:  4.635731220245361\n",
            "Test:  Iteration No:  424 \n",
            " Loss:  1.587835774650868\n",
            "Test accuracy:  87.46\n",
            "Training:  Iteration No:  425 Worker Num:  0 \n",
            " Loss:  3.5277843475341797\n",
            "Training:  Iteration No:  425 Worker Num:  1 \n",
            " Loss:  3.910140037536621\n",
            "Training:  Iteration No:  425 Worker Num:  2 \n",
            " Loss:  2.8327484130859375\n",
            "Training:  Iteration No:  425 Worker Num:  3 \n",
            " Loss:  2.440873146057129\n",
            "Training:  Iteration No:  425 Worker Num:  4 \n",
            " Loss:  4.767539978027344\n",
            "Training:  Iteration No:  425 Worker Num:  5 \n",
            " Loss:  3.0539541244506836\n",
            "Training:  Iteration No:  425 Worker Num:  6 \n",
            " Loss:  2.2682571411132812\n",
            "Training:  Iteration No:  425 Worker Num:  7 \n",
            " Loss:  4.313053607940674\n",
            "Test:  Iteration No:  425 \n",
            " Loss:  1.5358108001468442\n",
            "Test accuracy:  87.96\n",
            "Training:  Iteration No:  426 Worker Num:  0 \n",
            " Loss:  3.6048777103424072\n",
            "Training:  Iteration No:  426 Worker Num:  1 \n",
            " Loss:  3.7191789150238037\n",
            "Training:  Iteration No:  426 Worker Num:  2 \n",
            " Loss:  3.9047515392303467\n",
            "Training:  Iteration No:  426 Worker Num:  3 \n",
            " Loss:  1.8894635438919067\n",
            "Training:  Iteration No:  426 Worker Num:  4 \n",
            " Loss:  3.0260424613952637\n",
            "Training:  Iteration No:  426 Worker Num:  5 \n",
            " Loss:  6.270345211029053\n",
            "Training:  Iteration No:  426 Worker Num:  6 \n",
            " Loss:  3.2666757106781006\n",
            "Training:  Iteration No:  426 Worker Num:  7 \n",
            " Loss:  2.2429566383361816\n",
            "Test:  Iteration No:  426 \n",
            " Loss:  1.5659754468906153\n",
            "Test accuracy:  87.88\n",
            "Training:  Iteration No:  427 Worker Num:  0 \n",
            " Loss:  2.9357120990753174\n",
            "Training:  Iteration No:  427 Worker Num:  1 \n",
            " Loss:  2.8130974769592285\n",
            "Training:  Iteration No:  427 Worker Num:  2 \n",
            " Loss:  3.278377056121826\n",
            "Training:  Iteration No:  427 Worker Num:  3 \n",
            " Loss:  1.7231135368347168\n",
            "Training:  Iteration No:  427 Worker Num:  4 \n",
            " Loss:  2.0851263999938965\n",
            "Training:  Iteration No:  427 Worker Num:  5 \n",
            " Loss:  3.096717596054077\n",
            "Training:  Iteration No:  427 Worker Num:  6 \n",
            " Loss:  2.5880517959594727\n",
            "Training:  Iteration No:  427 Worker Num:  7 \n",
            " Loss:  2.9271082878112793\n",
            "Test:  Iteration No:  427 \n",
            " Loss:  1.563414397213278\n",
            "Test accuracy:  87.73\n",
            "Training:  Iteration No:  428 Worker Num:  0 \n",
            " Loss:  3.0785348415374756\n",
            "Training:  Iteration No:  428 Worker Num:  1 \n",
            " Loss:  1.8911478519439697\n",
            "Training:  Iteration No:  428 Worker Num:  2 \n",
            " Loss:  2.8217358589172363\n",
            "Training:  Iteration No:  428 Worker Num:  3 \n",
            " Loss:  4.807878494262695\n",
            "Training:  Iteration No:  428 Worker Num:  4 \n",
            " Loss:  4.328536510467529\n",
            "Training:  Iteration No:  428 Worker Num:  5 \n",
            " Loss:  3.45216703414917\n",
            "Training:  Iteration No:  428 Worker Num:  6 \n",
            " Loss:  2.9546051025390625\n",
            "Training:  Iteration No:  428 Worker Num:  7 \n",
            " Loss:  2.573873281478882\n",
            "Test:  Iteration No:  428 \n",
            " Loss:  1.4996282098508333\n",
            "Test accuracy:  88.56\n",
            "Training:  Iteration No:  429 Worker Num:  0 \n",
            " Loss:  1.5478324890136719\n",
            "Training:  Iteration No:  429 Worker Num:  1 \n",
            " Loss:  2.890284776687622\n",
            "Training:  Iteration No:  429 Worker Num:  2 \n",
            " Loss:  2.604891300201416\n",
            "Training:  Iteration No:  429 Worker Num:  3 \n",
            " Loss:  2.459502935409546\n",
            "Training:  Iteration No:  429 Worker Num:  4 \n",
            " Loss:  2.8288564682006836\n",
            "Training:  Iteration No:  429 Worker Num:  5 \n",
            " Loss:  2.867739200592041\n",
            "Training:  Iteration No:  429 Worker Num:  6 \n",
            " Loss:  1.9425641298294067\n",
            "Training:  Iteration No:  429 Worker Num:  7 \n",
            " Loss:  3.7637126445770264\n",
            "Test:  Iteration No:  429 \n",
            " Loss:  1.648029439387065\n",
            "Test accuracy:  86.93\n",
            "Training:  Iteration No:  430 Worker Num:  0 \n",
            " Loss:  2.8312079906463623\n",
            "Training:  Iteration No:  430 Worker Num:  1 \n",
            " Loss:  1.6196906566619873\n",
            "Training:  Iteration No:  430 Worker Num:  2 \n",
            " Loss:  3.555555582046509\n",
            "Training:  Iteration No:  430 Worker Num:  3 \n",
            " Loss:  3.396761178970337\n",
            "Training:  Iteration No:  430 Worker Num:  4 \n",
            " Loss:  2.4597983360290527\n",
            "Training:  Iteration No:  430 Worker Num:  5 \n",
            " Loss:  3.046968460083008\n",
            "Training:  Iteration No:  430 Worker Num:  6 \n",
            " Loss:  3.2789478302001953\n",
            "Training:  Iteration No:  430 Worker Num:  7 \n",
            " Loss:  3.8016157150268555\n",
            "Test:  Iteration No:  430 \n",
            " Loss:  1.543270224634605\n",
            "Test accuracy:  88.02\n",
            "Training:  Iteration No:  431 Worker Num:  0 \n",
            " Loss:  3.0634894371032715\n",
            "Training:  Iteration No:  431 Worker Num:  1 \n",
            " Loss:  2.874457597732544\n",
            "Training:  Iteration No:  431 Worker Num:  2 \n",
            " Loss:  2.9243645668029785\n",
            "Training:  Iteration No:  431 Worker Num:  3 \n",
            " Loss:  3.487490177154541\n",
            "Training:  Iteration No:  431 Worker Num:  4 \n",
            " Loss:  3.4195716381073\n",
            "Training:  Iteration No:  431 Worker Num:  5 \n",
            " Loss:  2.7953810691833496\n",
            "Training:  Iteration No:  431 Worker Num:  6 \n",
            " Loss:  1.6603238582611084\n",
            "Training:  Iteration No:  431 Worker Num:  7 \n",
            " Loss:  4.213576793670654\n",
            "Test:  Iteration No:  431 \n",
            " Loss:  1.5934478239527656\n",
            "Test accuracy:  87.64\n",
            "Training:  Iteration No:  432 Worker Num:  0 \n",
            " Loss:  4.764521598815918\n",
            "Training:  Iteration No:  432 Worker Num:  1 \n",
            " Loss:  3.536943197250366\n",
            "Training:  Iteration No:  432 Worker Num:  2 \n",
            " Loss:  3.6090595722198486\n",
            "Training:  Iteration No:  432 Worker Num:  3 \n",
            " Loss:  2.837249755859375\n",
            "Training:  Iteration No:  432 Worker Num:  4 \n",
            " Loss:  4.853028297424316\n",
            "Training:  Iteration No:  432 Worker Num:  5 \n",
            " Loss:  3.178414821624756\n",
            "Training:  Iteration No:  432 Worker Num:  6 \n",
            " Loss:  2.7820680141448975\n",
            "Training:  Iteration No:  432 Worker Num:  7 \n",
            " Loss:  5.644747734069824\n",
            "Test:  Iteration No:  432 \n",
            " Loss:  1.5689571618656568\n",
            "Test accuracy:  87.6\n",
            "Training:  Iteration No:  433 Worker Num:  0 \n",
            " Loss:  2.9580295085906982\n",
            "Training:  Iteration No:  433 Worker Num:  1 \n",
            " Loss:  3.2041406631469727\n",
            "Training:  Iteration No:  433 Worker Num:  2 \n",
            " Loss:  4.401098251342773\n",
            "Training:  Iteration No:  433 Worker Num:  3 \n",
            " Loss:  3.2369041442871094\n",
            "Training:  Iteration No:  433 Worker Num:  4 \n",
            " Loss:  2.093035936355591\n",
            "Training:  Iteration No:  433 Worker Num:  5 \n",
            " Loss:  1.94009268283844\n",
            "Training:  Iteration No:  433 Worker Num:  6 \n",
            " Loss:  7.710228443145752\n",
            "Training:  Iteration No:  433 Worker Num:  7 \n",
            " Loss:  3.3219847679138184\n",
            "Test:  Iteration No:  433 \n",
            " Loss:  1.6930881370849247\n",
            "Test accuracy:  87.02\n",
            "Training:  Iteration No:  434 Worker Num:  0 \n",
            " Loss:  5.581029415130615\n",
            "Training:  Iteration No:  434 Worker Num:  1 \n",
            " Loss:  2.2878286838531494\n",
            "Training:  Iteration No:  434 Worker Num:  2 \n",
            " Loss:  4.388537883758545\n",
            "Training:  Iteration No:  434 Worker Num:  3 \n",
            " Loss:  2.9732348918914795\n",
            "Training:  Iteration No:  434 Worker Num:  4 \n",
            " Loss:  5.66886043548584\n",
            "Training:  Iteration No:  434 Worker Num:  5 \n",
            " Loss:  3.17063570022583\n",
            "Training:  Iteration No:  434 Worker Num:  6 \n",
            " Loss:  4.762455463409424\n",
            "Training:  Iteration No:  434 Worker Num:  7 \n",
            " Loss:  2.0782525539398193\n",
            "Test:  Iteration No:  434 \n",
            " Loss:  1.5692337428844427\n",
            "Test accuracy:  87.69\n",
            "Training:  Iteration No:  435 Worker Num:  0 \n",
            " Loss:  1.8024256229400635\n",
            "Training:  Iteration No:  435 Worker Num:  1 \n",
            " Loss:  4.598822593688965\n",
            "Training:  Iteration No:  435 Worker Num:  2 \n",
            " Loss:  3.7910492420196533\n",
            "Training:  Iteration No:  435 Worker Num:  3 \n",
            " Loss:  2.5343098640441895\n",
            "Training:  Iteration No:  435 Worker Num:  4 \n",
            " Loss:  3.8655483722686768\n",
            "Training:  Iteration No:  435 Worker Num:  5 \n",
            " Loss:  5.029942512512207\n",
            "Training:  Iteration No:  435 Worker Num:  6 \n",
            " Loss:  5.076823711395264\n",
            "Training:  Iteration No:  435 Worker Num:  7 \n",
            " Loss:  1.5656516551971436\n",
            "Test:  Iteration No:  435 \n",
            " Loss:  1.6363173830735533\n",
            "Test accuracy:  87.19\n",
            "Training:  Iteration No:  436 Worker Num:  0 \n",
            " Loss:  2.836247205734253\n",
            "Training:  Iteration No:  436 Worker Num:  1 \n",
            " Loss:  3.572136640548706\n",
            "Training:  Iteration No:  436 Worker Num:  2 \n",
            " Loss:  3.448077440261841\n",
            "Training:  Iteration No:  436 Worker Num:  3 \n",
            " Loss:  3.2228903770446777\n",
            "Training:  Iteration No:  436 Worker Num:  4 \n",
            " Loss:  3.723116159439087\n",
            "Training:  Iteration No:  436 Worker Num:  5 \n",
            " Loss:  4.346755504608154\n",
            "Training:  Iteration No:  436 Worker Num:  6 \n",
            " Loss:  1.4292452335357666\n",
            "Training:  Iteration No:  436 Worker Num:  7 \n",
            " Loss:  2.7388436794281006\n",
            "Test:  Iteration No:  436 \n",
            " Loss:  1.5837493304964863\n",
            "Test accuracy:  87.54\n",
            "Training:  Iteration No:  437 Worker Num:  0 \n",
            " Loss:  2.9998273849487305\n",
            "Training:  Iteration No:  437 Worker Num:  1 \n",
            " Loss:  4.705219745635986\n",
            "Training:  Iteration No:  437 Worker Num:  2 \n",
            " Loss:  3.215202808380127\n",
            "Training:  Iteration No:  437 Worker Num:  3 \n",
            " Loss:  4.381403923034668\n",
            "Training:  Iteration No:  437 Worker Num:  4 \n",
            " Loss:  3.114851951599121\n",
            "Training:  Iteration No:  437 Worker Num:  5 \n",
            " Loss:  3.496870756149292\n",
            "Training:  Iteration No:  437 Worker Num:  6 \n",
            " Loss:  5.103911399841309\n",
            "Training:  Iteration No:  437 Worker Num:  7 \n",
            " Loss:  3.700178384780884\n",
            "Test:  Iteration No:  437 \n",
            " Loss:  1.683233069373837\n",
            "Test accuracy:  87.09\n",
            "Training:  Iteration No:  438 Worker Num:  0 \n",
            " Loss:  4.419873237609863\n",
            "Training:  Iteration No:  438 Worker Num:  1 \n",
            " Loss:  2.460808038711548\n",
            "Training:  Iteration No:  438 Worker Num:  2 \n",
            " Loss:  2.6783294677734375\n",
            "Training:  Iteration No:  438 Worker Num:  3 \n",
            " Loss:  4.766294956207275\n",
            "Training:  Iteration No:  438 Worker Num:  4 \n",
            " Loss:  3.651271343231201\n",
            "Training:  Iteration No:  438 Worker Num:  5 \n",
            " Loss:  3.507077932357788\n",
            "Training:  Iteration No:  438 Worker Num:  6 \n",
            " Loss:  1.8384668827056885\n",
            "Training:  Iteration No:  438 Worker Num:  7 \n",
            " Loss:  3.0231378078460693\n",
            "Test:  Iteration No:  438 \n",
            " Loss:  1.6366637291598924\n",
            "Test accuracy:  87.33\n",
            "Training:  Iteration No:  439 Worker Num:  0 \n",
            " Loss:  3.1741299629211426\n",
            "Training:  Iteration No:  439 Worker Num:  1 \n",
            " Loss:  2.9978952407836914\n",
            "Training:  Iteration No:  439 Worker Num:  2 \n",
            " Loss:  4.720302581787109\n",
            "Training:  Iteration No:  439 Worker Num:  3 \n",
            " Loss:  4.30133056640625\n",
            "Training:  Iteration No:  439 Worker Num:  4 \n",
            " Loss:  4.588191986083984\n",
            "Training:  Iteration No:  439 Worker Num:  5 \n",
            " Loss:  3.3317794799804688\n",
            "Training:  Iteration No:  439 Worker Num:  6 \n",
            " Loss:  3.470717191696167\n",
            "Training:  Iteration No:  439 Worker Num:  7 \n",
            " Loss:  3.375075340270996\n",
            "Test:  Iteration No:  439 \n",
            " Loss:  1.6627241571115543\n",
            "Test accuracy:  87.37\n",
            "Training:  Iteration No:  440 Worker Num:  0 \n",
            " Loss:  2.887988328933716\n",
            "Training:  Iteration No:  440 Worker Num:  1 \n",
            " Loss:  2.859637975692749\n",
            "Training:  Iteration No:  440 Worker Num:  2 \n",
            " Loss:  3.9928174018859863\n",
            "Training:  Iteration No:  440 Worker Num:  3 \n",
            " Loss:  3.431145668029785\n",
            "Training:  Iteration No:  440 Worker Num:  4 \n",
            " Loss:  1.794175148010254\n",
            "Training:  Iteration No:  440 Worker Num:  5 \n",
            " Loss:  4.497203350067139\n",
            "Training:  Iteration No:  440 Worker Num:  6 \n",
            " Loss:  2.1708056926727295\n",
            "Training:  Iteration No:  440 Worker Num:  7 \n",
            " Loss:  2.401779890060425\n",
            "Test:  Iteration No:  440 \n",
            " Loss:  1.5976924420912055\n",
            "Test accuracy:  87.71\n",
            "Training:  Iteration No:  441 Worker Num:  0 \n",
            " Loss:  3.9758946895599365\n",
            "Training:  Iteration No:  441 Worker Num:  1 \n",
            " Loss:  3.0803475379943848\n",
            "Training:  Iteration No:  441 Worker Num:  2 \n",
            " Loss:  2.7443654537200928\n",
            "Training:  Iteration No:  441 Worker Num:  3 \n",
            " Loss:  3.1677141189575195\n",
            "Training:  Iteration No:  441 Worker Num:  4 \n",
            " Loss:  4.3312153816223145\n",
            "Training:  Iteration No:  441 Worker Num:  5 \n",
            " Loss:  3.134160280227661\n",
            "Training:  Iteration No:  441 Worker Num:  6 \n",
            " Loss:  4.177097797393799\n",
            "Training:  Iteration No:  441 Worker Num:  7 \n",
            " Loss:  5.429337978363037\n",
            "Test:  Iteration No:  441 \n",
            " Loss:  1.63120033780608\n",
            "Test accuracy:  87.47\n",
            "Training:  Iteration No:  442 Worker Num:  0 \n",
            " Loss:  5.835819721221924\n",
            "Training:  Iteration No:  442 Worker Num:  1 \n",
            " Loss:  3.811736583709717\n",
            "Training:  Iteration No:  442 Worker Num:  2 \n",
            " Loss:  2.826639175415039\n",
            "Training:  Iteration No:  442 Worker Num:  3 \n",
            " Loss:  2.317591428756714\n",
            "Training:  Iteration No:  442 Worker Num:  4 \n",
            " Loss:  3.73128342628479\n",
            "Training:  Iteration No:  442 Worker Num:  5 \n",
            " Loss:  2.952089309692383\n",
            "Training:  Iteration No:  442 Worker Num:  6 \n",
            " Loss:  2.595343589782715\n",
            "Training:  Iteration No:  442 Worker Num:  7 \n",
            " Loss:  4.360576152801514\n",
            "Test:  Iteration No:  442 \n",
            " Loss:  1.6968272449095039\n",
            "Test accuracy:  86.88\n",
            "Training:  Iteration No:  443 Worker Num:  0 \n",
            " Loss:  3.8522157669067383\n",
            "Training:  Iteration No:  443 Worker Num:  1 \n",
            " Loss:  4.932802677154541\n",
            "Training:  Iteration No:  443 Worker Num:  2 \n",
            " Loss:  3.7222533226013184\n",
            "Training:  Iteration No:  443 Worker Num:  3 \n",
            " Loss:  2.923841714859009\n",
            "Training:  Iteration No:  443 Worker Num:  4 \n",
            " Loss:  3.075314998626709\n",
            "Training:  Iteration No:  443 Worker Num:  5 \n",
            " Loss:  3.769076108932495\n",
            "Training:  Iteration No:  443 Worker Num:  6 \n",
            " Loss:  4.477982521057129\n",
            "Training:  Iteration No:  443 Worker Num:  7 \n",
            " Loss:  2.9056787490844727\n",
            "Test:  Iteration No:  443 \n",
            " Loss:  1.5707648442704467\n",
            "Test accuracy:  87.59\n",
            "Training:  Iteration No:  444 Worker Num:  0 \n",
            " Loss:  3.6892294883728027\n",
            "Training:  Iteration No:  444 Worker Num:  1 \n",
            " Loss:  2.4272568225860596\n",
            "Training:  Iteration No:  444 Worker Num:  2 \n",
            " Loss:  3.3239617347717285\n",
            "Training:  Iteration No:  444 Worker Num:  3 \n",
            " Loss:  2.703754425048828\n",
            "Training:  Iteration No:  444 Worker Num:  4 \n",
            " Loss:  2.2454123497009277\n",
            "Training:  Iteration No:  444 Worker Num:  5 \n",
            " Loss:  2.9442074298858643\n",
            "Training:  Iteration No:  444 Worker Num:  6 \n",
            " Loss:  2.472703456878662\n",
            "Training:  Iteration No:  444 Worker Num:  7 \n",
            " Loss:  1.411586880683899\n",
            "Test:  Iteration No:  444 \n",
            " Loss:  1.56993697715711\n",
            "Test accuracy:  87.81\n",
            "Training:  Iteration No:  445 Worker Num:  0 \n",
            " Loss:  6.600340843200684\n",
            "Training:  Iteration No:  445 Worker Num:  1 \n",
            " Loss:  5.1037373542785645\n",
            "Training:  Iteration No:  445 Worker Num:  2 \n",
            " Loss:  3.471280336380005\n",
            "Training:  Iteration No:  445 Worker Num:  3 \n",
            " Loss:  3.7175803184509277\n",
            "Training:  Iteration No:  445 Worker Num:  4 \n",
            " Loss:  2.463461399078369\n",
            "Training:  Iteration No:  445 Worker Num:  5 \n",
            " Loss:  2.2694249153137207\n",
            "Training:  Iteration No:  445 Worker Num:  6 \n",
            " Loss:  3.4156370162963867\n",
            "Training:  Iteration No:  445 Worker Num:  7 \n",
            " Loss:  2.5721542835235596\n",
            "Test:  Iteration No:  445 \n",
            " Loss:  1.6917050973146777\n",
            "Test accuracy:  87.19\n",
            "Training:  Iteration No:  446 Worker Num:  0 \n",
            " Loss:  2.6650867462158203\n",
            "Training:  Iteration No:  446 Worker Num:  1 \n",
            " Loss:  2.497143268585205\n",
            "Training:  Iteration No:  446 Worker Num:  2 \n",
            " Loss:  3.883899450302124\n",
            "Training:  Iteration No:  446 Worker Num:  3 \n",
            " Loss:  1.919590950012207\n",
            "Training:  Iteration No:  446 Worker Num:  4 \n",
            " Loss:  2.3554458618164062\n",
            "Training:  Iteration No:  446 Worker Num:  5 \n",
            " Loss:  3.6020712852478027\n",
            "Training:  Iteration No:  446 Worker Num:  6 \n",
            " Loss:  3.7065069675445557\n",
            "Training:  Iteration No:  446 Worker Num:  7 \n",
            " Loss:  3.5333704948425293\n",
            "Test:  Iteration No:  446 \n",
            " Loss:  1.6555858812754667\n",
            "Test accuracy:  87.25\n",
            "Training:  Iteration No:  447 Worker Num:  0 \n",
            " Loss:  3.145974636077881\n",
            "Training:  Iteration No:  447 Worker Num:  1 \n",
            " Loss:  1.930128574371338\n",
            "Training:  Iteration No:  447 Worker Num:  2 \n",
            " Loss:  2.3846383094787598\n",
            "Training:  Iteration No:  447 Worker Num:  3 \n",
            " Loss:  2.8776049613952637\n",
            "Training:  Iteration No:  447 Worker Num:  4 \n",
            " Loss:  3.3249518871307373\n",
            "Training:  Iteration No:  447 Worker Num:  5 \n",
            " Loss:  3.0351741313934326\n",
            "Training:  Iteration No:  447 Worker Num:  6 \n",
            " Loss:  2.6321959495544434\n",
            "Training:  Iteration No:  447 Worker Num:  7 \n",
            " Loss:  2.017820358276367\n",
            "Test:  Iteration No:  447 \n",
            " Loss:  1.6666384978762157\n",
            "Test accuracy:  87.14\n",
            "Training:  Iteration No:  448 Worker Num:  0 \n",
            " Loss:  4.434655666351318\n",
            "Training:  Iteration No:  448 Worker Num:  1 \n",
            " Loss:  1.8939522504806519\n",
            "Training:  Iteration No:  448 Worker Num:  2 \n",
            " Loss:  3.8985960483551025\n",
            "Training:  Iteration No:  448 Worker Num:  3 \n",
            " Loss:  2.753999948501587\n",
            "Training:  Iteration No:  448 Worker Num:  4 \n",
            " Loss:  3.3954806327819824\n",
            "Training:  Iteration No:  448 Worker Num:  5 \n",
            " Loss:  4.153928279876709\n",
            "Training:  Iteration No:  448 Worker Num:  6 \n",
            " Loss:  3.11085844039917\n",
            "Training:  Iteration No:  448 Worker Num:  7 \n",
            " Loss:  3.706594228744507\n",
            "Test:  Iteration No:  448 \n",
            " Loss:  1.7348464751922632\n",
            "Test accuracy:  86.57\n",
            "Training:  Iteration No:  449 Worker Num:  0 \n",
            " Loss:  3.507568597793579\n",
            "Training:  Iteration No:  449 Worker Num:  1 \n",
            " Loss:  2.84025239944458\n",
            "Training:  Iteration No:  449 Worker Num:  2 \n",
            " Loss:  2.6622729301452637\n",
            "Training:  Iteration No:  449 Worker Num:  3 \n",
            " Loss:  3.22946834564209\n",
            "Training:  Iteration No:  449 Worker Num:  4 \n",
            " Loss:  1.777061939239502\n",
            "Training:  Iteration No:  449 Worker Num:  5 \n",
            " Loss:  3.1033310890197754\n",
            "Training:  Iteration No:  449 Worker Num:  6 \n",
            " Loss:  2.0381736755371094\n",
            "Training:  Iteration No:  449 Worker Num:  7 \n",
            " Loss:  4.165646553039551\n",
            "Test:  Iteration No:  449 \n",
            " Loss:  1.5588137797916992\n",
            "Test accuracy:  87.54\n",
            "Training:  Iteration No:  450 Worker Num:  0 \n",
            " Loss:  2.9808349609375\n",
            "Training:  Iteration No:  450 Worker Num:  1 \n",
            " Loss:  4.220305919647217\n",
            "Training:  Iteration No:  450 Worker Num:  2 \n",
            " Loss:  1.7073003053665161\n",
            "Training:  Iteration No:  450 Worker Num:  3 \n",
            " Loss:  4.873999118804932\n",
            "Training:  Iteration No:  450 Worker Num:  4 \n",
            " Loss:  4.997864723205566\n",
            "Training:  Iteration No:  450 Worker Num:  5 \n",
            " Loss:  2.399352550506592\n",
            "Training:  Iteration No:  450 Worker Num:  6 \n",
            " Loss:  4.614074230194092\n",
            "Training:  Iteration No:  450 Worker Num:  7 \n",
            " Loss:  3.49202036857605\n",
            "Test:  Iteration No:  450 \n",
            " Loss:  1.643307781672176\n",
            "Test accuracy:  87.05\n",
            "Training:  Iteration No:  451 Worker Num:  0 \n",
            " Loss:  5.3562211990356445\n",
            "Training:  Iteration No:  451 Worker Num:  1 \n",
            " Loss:  6.073096752166748\n",
            "Training:  Iteration No:  451 Worker Num:  2 \n",
            " Loss:  3.9479098320007324\n",
            "Training:  Iteration No:  451 Worker Num:  3 \n",
            " Loss:  2.5784099102020264\n",
            "Training:  Iteration No:  451 Worker Num:  4 \n",
            " Loss:  5.477455139160156\n",
            "Training:  Iteration No:  451 Worker Num:  5 \n",
            " Loss:  5.756062984466553\n",
            "Training:  Iteration No:  451 Worker Num:  6 \n",
            " Loss:  1.8282240629196167\n",
            "Training:  Iteration No:  451 Worker Num:  7 \n",
            " Loss:  3.1660895347595215\n",
            "Test:  Iteration No:  451 \n",
            " Loss:  1.7828322013344946\n",
            "Test accuracy:  86.32\n",
            "Training:  Iteration No:  452 Worker Num:  0 \n",
            " Loss:  3.3214893341064453\n",
            "Training:  Iteration No:  452 Worker Num:  1 \n",
            " Loss:  3.7055599689483643\n",
            "Training:  Iteration No:  452 Worker Num:  2 \n",
            " Loss:  3.661591053009033\n",
            "Training:  Iteration No:  452 Worker Num:  3 \n",
            " Loss:  3.5317800045013428\n",
            "Training:  Iteration No:  452 Worker Num:  4 \n",
            " Loss:  3.602924346923828\n",
            "Training:  Iteration No:  452 Worker Num:  5 \n",
            " Loss:  3.1656672954559326\n",
            "Training:  Iteration No:  452 Worker Num:  6 \n",
            " Loss:  3.912285327911377\n",
            "Training:  Iteration No:  452 Worker Num:  7 \n",
            " Loss:  2.275568962097168\n",
            "Test:  Iteration No:  452 \n",
            " Loss:  1.652207162561296\n",
            "Test accuracy:  86.88\n",
            "Training:  Iteration No:  453 Worker Num:  0 \n",
            " Loss:  4.866767406463623\n",
            "Training:  Iteration No:  453 Worker Num:  1 \n",
            " Loss:  2.4603986740112305\n",
            "Training:  Iteration No:  453 Worker Num:  2 \n",
            " Loss:  3.29514217376709\n",
            "Training:  Iteration No:  453 Worker Num:  3 \n",
            " Loss:  4.608318328857422\n",
            "Training:  Iteration No:  453 Worker Num:  4 \n",
            " Loss:  3.8529253005981445\n",
            "Training:  Iteration No:  453 Worker Num:  5 \n",
            " Loss:  3.366352081298828\n",
            "Training:  Iteration No:  453 Worker Num:  6 \n",
            " Loss:  4.553946018218994\n",
            "Training:  Iteration No:  453 Worker Num:  7 \n",
            " Loss:  4.328856468200684\n",
            "Test:  Iteration No:  453 \n",
            " Loss:  1.8101549852120726\n",
            "Test accuracy:  86.14\n",
            "Training:  Iteration No:  454 Worker Num:  0 \n",
            " Loss:  5.024308681488037\n",
            "Training:  Iteration No:  454 Worker Num:  1 \n",
            " Loss:  4.614739418029785\n",
            "Training:  Iteration No:  454 Worker Num:  2 \n",
            " Loss:  2.885526180267334\n",
            "Training:  Iteration No:  454 Worker Num:  3 \n",
            " Loss:  3.7580695152282715\n",
            "Training:  Iteration No:  454 Worker Num:  4 \n",
            " Loss:  4.029046058654785\n",
            "Training:  Iteration No:  454 Worker Num:  5 \n",
            " Loss:  4.554802417755127\n",
            "Training:  Iteration No:  454 Worker Num:  6 \n",
            " Loss:  5.150627613067627\n",
            "Training:  Iteration No:  454 Worker Num:  7 \n",
            " Loss:  3.735769271850586\n",
            "Test:  Iteration No:  454 \n",
            " Loss:  1.77040256536271\n",
            "Test accuracy:  85.88\n",
            "Training:  Iteration No:  455 Worker Num:  0 \n",
            " Loss:  3.029956340789795\n",
            "Training:  Iteration No:  455 Worker Num:  1 \n",
            " Loss:  3.5068957805633545\n",
            "Training:  Iteration No:  455 Worker Num:  2 \n",
            " Loss:  2.9850027561187744\n",
            "Training:  Iteration No:  455 Worker Num:  3 \n",
            " Loss:  3.7887325286865234\n",
            "Training:  Iteration No:  455 Worker Num:  4 \n",
            " Loss:  3.582169771194458\n",
            "Training:  Iteration No:  455 Worker Num:  5 \n",
            " Loss:  2.5123844146728516\n",
            "Training:  Iteration No:  455 Worker Num:  6 \n",
            " Loss:  4.568382740020752\n",
            "Training:  Iteration No:  455 Worker Num:  7 \n",
            " Loss:  3.728273391723633\n",
            "Test:  Iteration No:  455 \n",
            " Loss:  1.7532041679737689\n",
            "Test accuracy:  86.56\n",
            "Training:  Iteration No:  456 Worker Num:  0 \n",
            " Loss:  3.662670373916626\n",
            "Training:  Iteration No:  456 Worker Num:  1 \n",
            " Loss:  5.602487087249756\n",
            "Training:  Iteration No:  456 Worker Num:  2 \n",
            " Loss:  5.01509428024292\n",
            "Training:  Iteration No:  456 Worker Num:  3 \n",
            " Loss:  3.2667272090911865\n",
            "Training:  Iteration No:  456 Worker Num:  4 \n",
            " Loss:  2.936903953552246\n",
            "Training:  Iteration No:  456 Worker Num:  5 \n",
            " Loss:  3.2625064849853516\n",
            "Training:  Iteration No:  456 Worker Num:  6 \n",
            " Loss:  3.405953884124756\n",
            "Training:  Iteration No:  456 Worker Num:  7 \n",
            " Loss:  1.952881932258606\n",
            "Test:  Iteration No:  456 \n",
            " Loss:  1.8424204004857736\n",
            "Test accuracy:  86.16\n",
            "Training:  Iteration No:  457 Worker Num:  0 \n",
            " Loss:  4.336755752563477\n",
            "Training:  Iteration No:  457 Worker Num:  1 \n",
            " Loss:  4.203396797180176\n",
            "Training:  Iteration No:  457 Worker Num:  2 \n",
            " Loss:  3.9944674968719482\n",
            "Training:  Iteration No:  457 Worker Num:  3 \n",
            " Loss:  2.304356813430786\n",
            "Training:  Iteration No:  457 Worker Num:  4 \n",
            " Loss:  3.5541234016418457\n",
            "Training:  Iteration No:  457 Worker Num:  5 \n",
            " Loss:  4.576258182525635\n",
            "Training:  Iteration No:  457 Worker Num:  6 \n",
            " Loss:  5.46259069442749\n",
            "Training:  Iteration No:  457 Worker Num:  7 \n",
            " Loss:  4.602827072143555\n",
            "Test:  Iteration No:  457 \n",
            " Loss:  1.8871208927299403\n",
            "Test accuracy:  85.95\n",
            "Training:  Iteration No:  458 Worker Num:  0 \n",
            " Loss:  2.7189674377441406\n",
            "Training:  Iteration No:  458 Worker Num:  1 \n",
            " Loss:  2.0372655391693115\n",
            "Training:  Iteration No:  458 Worker Num:  2 \n",
            " Loss:  2.185840129852295\n",
            "Training:  Iteration No:  458 Worker Num:  3 \n",
            " Loss:  5.680161952972412\n",
            "Training:  Iteration No:  458 Worker Num:  4 \n",
            " Loss:  3.080923080444336\n",
            "Training:  Iteration No:  458 Worker Num:  5 \n",
            " Loss:  3.302971124649048\n",
            "Training:  Iteration No:  458 Worker Num:  6 \n",
            " Loss:  8.226791381835938\n",
            "Training:  Iteration No:  458 Worker Num:  7 \n",
            " Loss:  3.3182616233825684\n",
            "Test:  Iteration No:  458 \n",
            " Loss:  1.8147084899718249\n",
            "Test accuracy:  86.52\n",
            "Training:  Iteration No:  459 Worker Num:  0 \n",
            " Loss:  7.3945136070251465\n",
            "Training:  Iteration No:  459 Worker Num:  1 \n",
            " Loss:  5.221388339996338\n",
            "Training:  Iteration No:  459 Worker Num:  2 \n",
            " Loss:  3.451781988143921\n",
            "Training:  Iteration No:  459 Worker Num:  3 \n",
            " Loss:  5.097992897033691\n",
            "Training:  Iteration No:  459 Worker Num:  4 \n",
            " Loss:  4.062242031097412\n",
            "Training:  Iteration No:  459 Worker Num:  5 \n",
            " Loss:  2.756580114364624\n",
            "Training:  Iteration No:  459 Worker Num:  6 \n",
            " Loss:  2.9424517154693604\n",
            "Training:  Iteration No:  459 Worker Num:  7 \n",
            " Loss:  3.8533287048339844\n",
            "Test:  Iteration No:  459 \n",
            " Loss:  1.831076919372323\n",
            "Test accuracy:  86.32\n",
            "Training:  Iteration No:  460 Worker Num:  0 \n",
            " Loss:  3.6967408657073975\n",
            "Training:  Iteration No:  460 Worker Num:  1 \n",
            " Loss:  3.5653741359710693\n",
            "Training:  Iteration No:  460 Worker Num:  2 \n",
            " Loss:  2.5055887699127197\n",
            "Training:  Iteration No:  460 Worker Num:  3 \n",
            " Loss:  4.4746551513671875\n",
            "Training:  Iteration No:  460 Worker Num:  4 \n",
            " Loss:  3.144859552383423\n",
            "Training:  Iteration No:  460 Worker Num:  5 \n",
            " Loss:  5.105520725250244\n",
            "Training:  Iteration No:  460 Worker Num:  6 \n",
            " Loss:  4.415149211883545\n",
            "Training:  Iteration No:  460 Worker Num:  7 \n",
            " Loss:  4.6281352043151855\n",
            "Test:  Iteration No:  460 \n",
            " Loss:  1.799970863552033\n",
            "Test accuracy:  86.58\n",
            "Training:  Iteration No:  461 Worker Num:  0 \n",
            " Loss:  4.9573259353637695\n",
            "Training:  Iteration No:  461 Worker Num:  1 \n",
            " Loss:  3.3801074028015137\n",
            "Training:  Iteration No:  461 Worker Num:  2 \n",
            " Loss:  4.423788070678711\n",
            "Training:  Iteration No:  461 Worker Num:  3 \n",
            " Loss:  4.369301795959473\n",
            "Training:  Iteration No:  461 Worker Num:  4 \n",
            " Loss:  3.7588024139404297\n",
            "Training:  Iteration No:  461 Worker Num:  5 \n",
            " Loss:  3.4411675930023193\n",
            "Training:  Iteration No:  461 Worker Num:  6 \n",
            " Loss:  3.2636098861694336\n",
            "Training:  Iteration No:  461 Worker Num:  7 \n",
            " Loss:  3.416307210922241\n",
            "Test:  Iteration No:  461 \n",
            " Loss:  1.864266693309138\n",
            "Test accuracy:  86.09\n",
            "Training:  Iteration No:  462 Worker Num:  0 \n",
            " Loss:  3.27474045753479\n",
            "Training:  Iteration No:  462 Worker Num:  1 \n",
            " Loss:  4.37785005569458\n",
            "Training:  Iteration No:  462 Worker Num:  2 \n",
            " Loss:  1.8424001932144165\n",
            "Training:  Iteration No:  462 Worker Num:  3 \n",
            " Loss:  3.8977506160736084\n",
            "Training:  Iteration No:  462 Worker Num:  4 \n",
            " Loss:  4.385805130004883\n",
            "Training:  Iteration No:  462 Worker Num:  5 \n",
            " Loss:  4.809031009674072\n",
            "Training:  Iteration No:  462 Worker Num:  6 \n",
            " Loss:  5.713143825531006\n",
            "Training:  Iteration No:  462 Worker Num:  7 \n",
            " Loss:  2.021530866622925\n",
            "Test:  Iteration No:  462 \n",
            " Loss:  2.166455820580072\n",
            "Test accuracy:  84.07\n",
            "Training:  Iteration No:  463 Worker Num:  0 \n",
            " Loss:  2.6107943058013916\n",
            "Training:  Iteration No:  463 Worker Num:  1 \n",
            " Loss:  6.267161846160889\n",
            "Training:  Iteration No:  463 Worker Num:  2 \n",
            " Loss:  3.298691511154175\n",
            "Training:  Iteration No:  463 Worker Num:  3 \n",
            " Loss:  4.4102702140808105\n",
            "Training:  Iteration No:  463 Worker Num:  4 \n",
            " Loss:  4.776796340942383\n",
            "Training:  Iteration No:  463 Worker Num:  5 \n",
            " Loss:  3.864506959915161\n",
            "Training:  Iteration No:  463 Worker Num:  6 \n",
            " Loss:  5.0961737632751465\n",
            "Training:  Iteration No:  463 Worker Num:  7 \n",
            " Loss:  4.388518333435059\n",
            "Test:  Iteration No:  463 \n",
            " Loss:  1.9945597597692586\n",
            "Test accuracy:  85.42\n",
            "Training:  Iteration No:  464 Worker Num:  0 \n",
            " Loss:  2.3947885036468506\n",
            "Training:  Iteration No:  464 Worker Num:  1 \n",
            " Loss:  3.5343990325927734\n",
            "Training:  Iteration No:  464 Worker Num:  2 \n",
            " Loss:  2.951329469680786\n",
            "Training:  Iteration No:  464 Worker Num:  3 \n",
            " Loss:  3.515047311782837\n",
            "Training:  Iteration No:  464 Worker Num:  4 \n",
            " Loss:  4.491528511047363\n",
            "Training:  Iteration No:  464 Worker Num:  5 \n",
            " Loss:  3.546905994415283\n",
            "Training:  Iteration No:  464 Worker Num:  6 \n",
            " Loss:  2.6373817920684814\n",
            "Training:  Iteration No:  464 Worker Num:  7 \n",
            " Loss:  4.2158002853393555\n",
            "Test:  Iteration No:  464 \n",
            " Loss:  1.8013259202339156\n",
            "Test accuracy:  86.65\n",
            "Training:  Iteration No:  465 Worker Num:  0 \n",
            " Loss:  2.9584169387817383\n",
            "Training:  Iteration No:  465 Worker Num:  1 \n",
            " Loss:  5.0002570152282715\n",
            "Training:  Iteration No:  465 Worker Num:  2 \n",
            " Loss:  4.118531227111816\n",
            "Training:  Iteration No:  465 Worker Num:  3 \n",
            " Loss:  6.521970748901367\n",
            "Training:  Iteration No:  465 Worker Num:  4 \n",
            " Loss:  3.5158448219299316\n",
            "Training:  Iteration No:  465 Worker Num:  5 \n",
            " Loss:  1.6464831829071045\n",
            "Training:  Iteration No:  465 Worker Num:  6 \n",
            " Loss:  4.4535064697265625\n",
            "Training:  Iteration No:  465 Worker Num:  7 \n",
            " Loss:  3.7691245079040527\n",
            "Test:  Iteration No:  465 \n",
            " Loss:  1.8424347466494464\n",
            "Test accuracy:  86.47\n",
            "Training:  Iteration No:  466 Worker Num:  0 \n",
            " Loss:  2.0677173137664795\n",
            "Training:  Iteration No:  466 Worker Num:  1 \n",
            " Loss:  4.585697174072266\n",
            "Training:  Iteration No:  466 Worker Num:  2 \n",
            " Loss:  3.1860158443450928\n",
            "Training:  Iteration No:  466 Worker Num:  3 \n",
            " Loss:  4.564436435699463\n",
            "Training:  Iteration No:  466 Worker Num:  4 \n",
            " Loss:  3.6369903087615967\n",
            "Training:  Iteration No:  466 Worker Num:  5 \n",
            " Loss:  3.6150128841400146\n",
            "Training:  Iteration No:  466 Worker Num:  6 \n",
            " Loss:  4.6933722496032715\n",
            "Training:  Iteration No:  466 Worker Num:  7 \n",
            " Loss:  2.9727654457092285\n",
            "Test:  Iteration No:  466 \n",
            " Loss:  1.6959554535112804\n",
            "Test accuracy:  87.38\n",
            "Training:  Iteration No:  467 Worker Num:  0 \n",
            " Loss:  6.539677143096924\n",
            "Training:  Iteration No:  467 Worker Num:  1 \n",
            " Loss:  3.6856272220611572\n",
            "Training:  Iteration No:  467 Worker Num:  2 \n",
            " Loss:  3.3713369369506836\n",
            "Training:  Iteration No:  467 Worker Num:  3 \n",
            " Loss:  2.1812503337860107\n",
            "Training:  Iteration No:  467 Worker Num:  4 \n",
            " Loss:  2.538623571395874\n",
            "Training:  Iteration No:  467 Worker Num:  5 \n",
            " Loss:  3.934699773788452\n",
            "Training:  Iteration No:  467 Worker Num:  6 \n",
            " Loss:  2.962357521057129\n",
            "Training:  Iteration No:  467 Worker Num:  7 \n",
            " Loss:  4.442883014678955\n",
            "Test:  Iteration No:  467 \n",
            " Loss:  1.780854827523986\n",
            "Test accuracy:  86.81\n",
            "Training:  Iteration No:  468 Worker Num:  0 \n",
            " Loss:  4.044206619262695\n",
            "Training:  Iteration No:  468 Worker Num:  1 \n",
            " Loss:  3.183722734451294\n",
            "Training:  Iteration No:  468 Worker Num:  2 \n",
            " Loss:  2.0643250942230225\n",
            "Training:  Iteration No:  468 Worker Num:  3 \n",
            " Loss:  3.9733870029449463\n",
            "Training:  Iteration No:  468 Worker Num:  4 \n",
            " Loss:  3.878274440765381\n",
            "Training:  Iteration No:  468 Worker Num:  5 \n",
            " Loss:  2.8273277282714844\n",
            "Training:  Iteration No:  468 Worker Num:  6 \n",
            " Loss:  2.9146294593811035\n",
            "Training:  Iteration No:  468 Worker Num:  7 \n",
            " Loss:  4.0496296882629395\n",
            "Test:  Iteration No:  468 \n",
            " Loss:  1.769714479487908\n",
            "Test accuracy:  86.95\n",
            "Training:  Iteration No:  469 Worker Num:  0 \n",
            " Loss:  4.5485520362854\n",
            "Training:  Iteration No:  469 Worker Num:  1 \n",
            " Loss:  3.20137619972229\n",
            "Training:  Iteration No:  469 Worker Num:  2 \n",
            " Loss:  5.2007551193237305\n",
            "Training:  Iteration No:  469 Worker Num:  3 \n",
            " Loss:  6.0421833992004395\n",
            "Training:  Iteration No:  469 Worker Num:  4 \n",
            " Loss:  3.284137487411499\n",
            "Training:  Iteration No:  469 Worker Num:  5 \n",
            " Loss:  2.8906195163726807\n",
            "Training:  Iteration No:  469 Worker Num:  6 \n",
            " Loss:  4.754027843475342\n",
            "Training:  Iteration No:  469 Worker Num:  7 \n",
            " Loss:  6.138664722442627\n",
            "Test:  Iteration No:  469 \n",
            " Loss:  1.789088027009481\n",
            "Test accuracy:  86.78\n",
            "Training:  Iteration No:  470 Worker Num:  0 \n",
            " Loss:  2.6262946128845215\n",
            "Training:  Iteration No:  470 Worker Num:  1 \n",
            " Loss:  4.468794822692871\n",
            "Training:  Iteration No:  470 Worker Num:  2 \n",
            " Loss:  3.288349151611328\n",
            "Training:  Iteration No:  470 Worker Num:  3 \n",
            " Loss:  2.5685994625091553\n",
            "Training:  Iteration No:  470 Worker Num:  4 \n",
            " Loss:  3.629042148590088\n",
            "Training:  Iteration No:  470 Worker Num:  5 \n",
            " Loss:  2.31992769241333\n",
            "Training:  Iteration No:  470 Worker Num:  6 \n",
            " Loss:  2.712148427963257\n",
            "Training:  Iteration No:  470 Worker Num:  7 \n",
            " Loss:  2.820878028869629\n",
            "Test:  Iteration No:  470 \n",
            " Loss:  1.7027464534710102\n",
            "Test accuracy:  87.35\n",
            "Training:  Iteration No:  471 Worker Num:  0 \n",
            " Loss:  5.343763828277588\n",
            "Training:  Iteration No:  471 Worker Num:  1 \n",
            " Loss:  3.990638017654419\n",
            "Training:  Iteration No:  471 Worker Num:  2 \n",
            " Loss:  7.714413166046143\n",
            "Training:  Iteration No:  471 Worker Num:  3 \n",
            " Loss:  1.7016394138336182\n",
            "Training:  Iteration No:  471 Worker Num:  4 \n",
            " Loss:  4.9461798667907715\n",
            "Training:  Iteration No:  471 Worker Num:  5 \n",
            " Loss:  5.505838871002197\n",
            "Training:  Iteration No:  471 Worker Num:  6 \n",
            " Loss:  5.448261737823486\n",
            "Training:  Iteration No:  471 Worker Num:  7 \n",
            " Loss:  3.5988476276397705\n",
            "Test:  Iteration No:  471 \n",
            " Loss:  1.809923524532137\n",
            "Test accuracy:  86.79\n",
            "Training:  Iteration No:  472 Worker Num:  0 \n",
            " Loss:  2.5909805297851562\n",
            "Training:  Iteration No:  472 Worker Num:  1 \n",
            " Loss:  4.414784908294678\n",
            "Training:  Iteration No:  472 Worker Num:  2 \n",
            " Loss:  3.098273277282715\n",
            "Training:  Iteration No:  472 Worker Num:  3 \n",
            " Loss:  4.8459930419921875\n",
            "Training:  Iteration No:  472 Worker Num:  4 \n",
            " Loss:  7.181923866271973\n",
            "Training:  Iteration No:  472 Worker Num:  5 \n",
            " Loss:  2.9685375690460205\n",
            "Training:  Iteration No:  472 Worker Num:  6 \n",
            " Loss:  1.7205708026885986\n",
            "Training:  Iteration No:  472 Worker Num:  7 \n",
            " Loss:  4.880232334136963\n",
            "Test:  Iteration No:  472 \n",
            " Loss:  1.7835590054923551\n",
            "Test accuracy:  86.69\n",
            "Training:  Iteration No:  473 Worker Num:  0 \n",
            " Loss:  3.9469709396362305\n",
            "Training:  Iteration No:  473 Worker Num:  1 \n",
            " Loss:  2.9264776706695557\n",
            "Training:  Iteration No:  473 Worker Num:  2 \n",
            " Loss:  2.5399186611175537\n",
            "Training:  Iteration No:  473 Worker Num:  3 \n",
            " Loss:  3.5597028732299805\n",
            "Training:  Iteration No:  473 Worker Num:  4 \n",
            " Loss:  2.4258744716644287\n",
            "Training:  Iteration No:  473 Worker Num:  5 \n",
            " Loss:  3.2812914848327637\n",
            "Training:  Iteration No:  473 Worker Num:  6 \n",
            " Loss:  4.6717529296875\n",
            "Training:  Iteration No:  473 Worker Num:  7 \n",
            " Loss:  2.8339293003082275\n",
            "Test:  Iteration No:  473 \n",
            " Loss:  1.920140014041828\n",
            "Test accuracy:  86.28\n",
            "Training:  Iteration No:  474 Worker Num:  0 \n",
            " Loss:  4.574640274047852\n",
            "Training:  Iteration No:  474 Worker Num:  1 \n",
            " Loss:  3.7505462169647217\n",
            "Training:  Iteration No:  474 Worker Num:  2 \n",
            " Loss:  3.1033143997192383\n",
            "Training:  Iteration No:  474 Worker Num:  3 \n",
            " Loss:  4.39281702041626\n",
            "Training:  Iteration No:  474 Worker Num:  4 \n",
            " Loss:  5.263643264770508\n",
            "Training:  Iteration No:  474 Worker Num:  5 \n",
            " Loss:  2.905193567276001\n",
            "Training:  Iteration No:  474 Worker Num:  6 \n",
            " Loss:  2.224027156829834\n",
            "Training:  Iteration No:  474 Worker Num:  7 \n",
            " Loss:  4.438860893249512\n",
            "Test:  Iteration No:  474 \n",
            " Loss:  1.871458979158462\n",
            "Test accuracy:  86.68\n",
            "Training:  Iteration No:  475 Worker Num:  0 \n",
            " Loss:  4.277985095977783\n",
            "Training:  Iteration No:  475 Worker Num:  1 \n",
            " Loss:  3.1613194942474365\n",
            "Training:  Iteration No:  475 Worker Num:  2 \n",
            " Loss:  2.3690121173858643\n",
            "Training:  Iteration No:  475 Worker Num:  3 \n",
            " Loss:  2.014164924621582\n",
            "Training:  Iteration No:  475 Worker Num:  4 \n",
            " Loss:  3.652618885040283\n",
            "Training:  Iteration No:  475 Worker Num:  5 \n",
            " Loss:  4.156790256500244\n",
            "Training:  Iteration No:  475 Worker Num:  6 \n",
            " Loss:  3.988330602645874\n",
            "Training:  Iteration No:  475 Worker Num:  7 \n",
            " Loss:  3.185248851776123\n",
            "Test:  Iteration No:  475 \n",
            " Loss:  1.7469239078662535\n",
            "Test accuracy:  87.26\n",
            "Training:  Iteration No:  476 Worker Num:  0 \n",
            " Loss:  4.297298908233643\n",
            "Training:  Iteration No:  476 Worker Num:  1 \n",
            " Loss:  3.720524549484253\n",
            "Training:  Iteration No:  476 Worker Num:  2 \n",
            " Loss:  3.790741443634033\n",
            "Training:  Iteration No:  476 Worker Num:  3 \n",
            " Loss:  2.9774603843688965\n",
            "Training:  Iteration No:  476 Worker Num:  4 \n",
            " Loss:  5.598819732666016\n",
            "Training:  Iteration No:  476 Worker Num:  5 \n",
            " Loss:  2.9033212661743164\n",
            "Training:  Iteration No:  476 Worker Num:  6 \n",
            " Loss:  4.610741138458252\n",
            "Training:  Iteration No:  476 Worker Num:  7 \n",
            " Loss:  4.407544136047363\n",
            "Test:  Iteration No:  476 \n",
            " Loss:  1.8102926052635229\n",
            "Test accuracy:  87.47\n",
            "Training:  Iteration No:  477 Worker Num:  0 \n",
            " Loss:  2.0941901206970215\n",
            "Training:  Iteration No:  477 Worker Num:  1 \n",
            " Loss:  3.7136335372924805\n",
            "Training:  Iteration No:  477 Worker Num:  2 \n",
            " Loss:  2.8272149562835693\n",
            "Training:  Iteration No:  477 Worker Num:  3 \n",
            " Loss:  3.0862443447113037\n",
            "Training:  Iteration No:  477 Worker Num:  4 \n",
            " Loss:  2.873654842376709\n",
            "Training:  Iteration No:  477 Worker Num:  5 \n",
            " Loss:  2.720094680786133\n",
            "Training:  Iteration No:  477 Worker Num:  6 \n",
            " Loss:  5.1224188804626465\n",
            "Training:  Iteration No:  477 Worker Num:  7 \n",
            " Loss:  3.13568115234375\n",
            "Test:  Iteration No:  477 \n",
            " Loss:  1.8213663032968068\n",
            "Test accuracy:  87.2\n",
            "Training:  Iteration No:  478 Worker Num:  0 \n",
            " Loss:  3.6970033645629883\n",
            "Training:  Iteration No:  478 Worker Num:  1 \n",
            " Loss:  5.703306674957275\n",
            "Training:  Iteration No:  478 Worker Num:  2 \n",
            " Loss:  2.2771756649017334\n",
            "Training:  Iteration No:  478 Worker Num:  3 \n",
            " Loss:  5.238298416137695\n",
            "Training:  Iteration No:  478 Worker Num:  4 \n",
            " Loss:  3.358311653137207\n",
            "Training:  Iteration No:  478 Worker Num:  5 \n",
            " Loss:  4.628702640533447\n",
            "Training:  Iteration No:  478 Worker Num:  6 \n",
            " Loss:  4.527448654174805\n",
            "Training:  Iteration No:  478 Worker Num:  7 \n",
            " Loss:  4.489709377288818\n",
            "Test:  Iteration No:  478 \n",
            " Loss:  1.8204146814591522\n",
            "Test accuracy:  87.16\n",
            "Training:  Iteration No:  479 Worker Num:  0 \n",
            " Loss:  5.943328380584717\n",
            "Training:  Iteration No:  479 Worker Num:  1 \n",
            " Loss:  3.0367214679718018\n",
            "Training:  Iteration No:  479 Worker Num:  2 \n",
            " Loss:  4.228109359741211\n",
            "Training:  Iteration No:  479 Worker Num:  3 \n",
            " Loss:  3.3377206325531006\n",
            "Training:  Iteration No:  479 Worker Num:  4 \n",
            " Loss:  4.5945258140563965\n",
            "Training:  Iteration No:  479 Worker Num:  5 \n",
            " Loss:  3.3636436462402344\n",
            "Training:  Iteration No:  479 Worker Num:  6 \n",
            " Loss:  3.6065120697021484\n",
            "Training:  Iteration No:  479 Worker Num:  7 \n",
            " Loss:  3.357180595397949\n",
            "Test:  Iteration No:  479 \n",
            " Loss:  1.9452860703385328\n",
            "Test accuracy:  86.68\n",
            "Training:  Iteration No:  480 Worker Num:  0 \n",
            " Loss:  4.066003799438477\n",
            "Training:  Iteration No:  480 Worker Num:  1 \n",
            " Loss:  2.58524489402771\n",
            "Training:  Iteration No:  480 Worker Num:  2 \n",
            " Loss:  3.334937334060669\n",
            "Training:  Iteration No:  480 Worker Num:  3 \n",
            " Loss:  5.178319454193115\n",
            "Training:  Iteration No:  480 Worker Num:  4 \n",
            " Loss:  4.316836357116699\n",
            "Training:  Iteration No:  480 Worker Num:  5 \n",
            " Loss:  3.2428174018859863\n",
            "Training:  Iteration No:  480 Worker Num:  6 \n",
            " Loss:  4.324260711669922\n",
            "Training:  Iteration No:  480 Worker Num:  7 \n",
            " Loss:  4.202295780181885\n",
            "Test:  Iteration No:  480 \n",
            " Loss:  1.800311034023644\n",
            "Test accuracy:  87.13\n",
            "Training:  Iteration No:  481 Worker Num:  0 \n",
            " Loss:  2.3824737071990967\n",
            "Training:  Iteration No:  481 Worker Num:  1 \n",
            " Loss:  5.319869041442871\n",
            "Training:  Iteration No:  481 Worker Num:  2 \n",
            " Loss:  2.924924612045288\n",
            "Training:  Iteration No:  481 Worker Num:  3 \n",
            " Loss:  3.868239164352417\n",
            "Training:  Iteration No:  481 Worker Num:  4 \n",
            " Loss:  4.939663410186768\n",
            "Training:  Iteration No:  481 Worker Num:  5 \n",
            " Loss:  3.9839725494384766\n",
            "Training:  Iteration No:  481 Worker Num:  6 \n",
            " Loss:  3.097341775894165\n",
            "Training:  Iteration No:  481 Worker Num:  7 \n",
            " Loss:  5.8335981369018555\n",
            "Test:  Iteration No:  481 \n",
            " Loss:  1.8511074939860572\n",
            "Test accuracy:  87.05\n",
            "Training:  Iteration No:  482 Worker Num:  0 \n",
            " Loss:  4.608477592468262\n",
            "Training:  Iteration No:  482 Worker Num:  1 \n",
            " Loss:  4.561977386474609\n",
            "Training:  Iteration No:  482 Worker Num:  2 \n",
            " Loss:  3.8799023628234863\n",
            "Training:  Iteration No:  482 Worker Num:  3 \n",
            " Loss:  5.044846057891846\n",
            "Training:  Iteration No:  482 Worker Num:  4 \n",
            " Loss:  4.97841215133667\n",
            "Training:  Iteration No:  482 Worker Num:  5 \n",
            " Loss:  4.709417819976807\n",
            "Training:  Iteration No:  482 Worker Num:  6 \n",
            " Loss:  5.486176013946533\n",
            "Training:  Iteration No:  482 Worker Num:  7 \n",
            " Loss:  3.358220100402832\n",
            "Test:  Iteration No:  482 \n",
            " Loss:  1.8543097959194756\n",
            "Test accuracy:  87.08\n",
            "Training:  Iteration No:  483 Worker Num:  0 \n",
            " Loss:  4.388213634490967\n",
            "Training:  Iteration No:  483 Worker Num:  1 \n",
            " Loss:  3.8909969329833984\n",
            "Training:  Iteration No:  483 Worker Num:  2 \n",
            " Loss:  2.504241466522217\n",
            "Training:  Iteration No:  483 Worker Num:  3 \n",
            " Loss:  3.3192124366760254\n",
            "Training:  Iteration No:  483 Worker Num:  4 \n",
            " Loss:  4.145505428314209\n",
            "Training:  Iteration No:  483 Worker Num:  5 \n",
            " Loss:  3.3587281703948975\n",
            "Training:  Iteration No:  483 Worker Num:  6 \n",
            " Loss:  2.3048856258392334\n",
            "Training:  Iteration No:  483 Worker Num:  7 \n",
            " Loss:  4.072076797485352\n",
            "Test:  Iteration No:  483 \n",
            " Loss:  1.8102648099059169\n",
            "Test accuracy:  87.46\n",
            "Training:  Iteration No:  484 Worker Num:  0 \n",
            " Loss:  3.081453561782837\n",
            "Training:  Iteration No:  484 Worker Num:  1 \n",
            " Loss:  3.9062280654907227\n",
            "Training:  Iteration No:  484 Worker Num:  2 \n",
            " Loss:  5.090394973754883\n",
            "Training:  Iteration No:  484 Worker Num:  3 \n",
            " Loss:  2.2994542121887207\n",
            "Training:  Iteration No:  484 Worker Num:  4 \n",
            " Loss:  3.4956815242767334\n",
            "Training:  Iteration No:  484 Worker Num:  5 \n",
            " Loss:  5.354445934295654\n",
            "Training:  Iteration No:  484 Worker Num:  6 \n",
            " Loss:  3.214606523513794\n",
            "Training:  Iteration No:  484 Worker Num:  7 \n",
            " Loss:  4.154650688171387\n",
            "Test:  Iteration No:  484 \n",
            " Loss:  1.8108930873295552\n",
            "Test accuracy:  87.2\n",
            "Training:  Iteration No:  485 Worker Num:  0 \n",
            " Loss:  3.7997677326202393\n",
            "Training:  Iteration No:  485 Worker Num:  1 \n",
            " Loss:  5.181006908416748\n",
            "Training:  Iteration No:  485 Worker Num:  2 \n",
            " Loss:  4.700575351715088\n",
            "Training:  Iteration No:  485 Worker Num:  3 \n",
            " Loss:  2.052689552307129\n",
            "Training:  Iteration No:  485 Worker Num:  4 \n",
            " Loss:  4.2087082862854\n",
            "Training:  Iteration No:  485 Worker Num:  5 \n",
            " Loss:  3.6599490642547607\n",
            "Training:  Iteration No:  485 Worker Num:  6 \n",
            " Loss:  4.422973155975342\n",
            "Training:  Iteration No:  485 Worker Num:  7 \n",
            " Loss:  4.16871452331543\n",
            "Test:  Iteration No:  485 \n",
            " Loss:  1.9380774742015932\n",
            "Test accuracy:  86.58\n",
            "Training:  Iteration No:  486 Worker Num:  0 \n",
            " Loss:  3.6532788276672363\n",
            "Training:  Iteration No:  486 Worker Num:  1 \n",
            " Loss:  4.0052666664123535\n",
            "Training:  Iteration No:  486 Worker Num:  2 \n",
            " Loss:  2.5172805786132812\n",
            "Training:  Iteration No:  486 Worker Num:  3 \n",
            " Loss:  2.5970966815948486\n",
            "Training:  Iteration No:  486 Worker Num:  4 \n",
            " Loss:  4.514004707336426\n",
            "Training:  Iteration No:  486 Worker Num:  5 \n",
            " Loss:  3.995908260345459\n",
            "Training:  Iteration No:  486 Worker Num:  6 \n",
            " Loss:  3.9692888259887695\n",
            "Training:  Iteration No:  486 Worker Num:  7 \n",
            " Loss:  4.756579875946045\n",
            "Test:  Iteration No:  486 \n",
            " Loss:  1.7958066606116068\n",
            "Test accuracy:  87.42\n",
            "Training:  Iteration No:  487 Worker Num:  0 \n",
            " Loss:  4.235833168029785\n",
            "Training:  Iteration No:  487 Worker Num:  1 \n",
            " Loss:  5.865511417388916\n",
            "Training:  Iteration No:  487 Worker Num:  2 \n",
            " Loss:  4.2235918045043945\n",
            "Training:  Iteration No:  487 Worker Num:  3 \n",
            " Loss:  1.3267358541488647\n",
            "Training:  Iteration No:  487 Worker Num:  4 \n",
            " Loss:  2.265096664428711\n",
            "Training:  Iteration No:  487 Worker Num:  5 \n",
            " Loss:  3.4257421493530273\n",
            "Training:  Iteration No:  487 Worker Num:  6 \n",
            " Loss:  3.0851261615753174\n",
            "Training:  Iteration No:  487 Worker Num:  7 \n",
            " Loss:  5.243996620178223\n",
            "Test:  Iteration No:  487 \n",
            " Loss:  2.0087917048154\n",
            "Test accuracy:  86.46\n",
            "Training:  Iteration No:  488 Worker Num:  0 \n",
            " Loss:  4.737170219421387\n",
            "Training:  Iteration No:  488 Worker Num:  1 \n",
            " Loss:  3.7149744033813477\n",
            "Training:  Iteration No:  488 Worker Num:  2 \n",
            " Loss:  3.9721179008483887\n",
            "Training:  Iteration No:  488 Worker Num:  3 \n",
            " Loss:  3.213003396987915\n",
            "Training:  Iteration No:  488 Worker Num:  4 \n",
            " Loss:  4.0373148918151855\n",
            "Training:  Iteration No:  488 Worker Num:  5 \n",
            " Loss:  4.472892761230469\n",
            "Training:  Iteration No:  488 Worker Num:  6 \n",
            " Loss:  4.109659194946289\n",
            "Training:  Iteration No:  488 Worker Num:  7 \n",
            " Loss:  3.865694046020508\n",
            "Test:  Iteration No:  488 \n",
            " Loss:  1.9199507198756254\n",
            "Test accuracy:  86.84\n",
            "Training:  Iteration No:  489 Worker Num:  0 \n",
            " Loss:  3.2688486576080322\n",
            "Training:  Iteration No:  489 Worker Num:  1 \n",
            " Loss:  2.792435884475708\n",
            "Training:  Iteration No:  489 Worker Num:  2 \n",
            " Loss:  4.205260276794434\n",
            "Training:  Iteration No:  489 Worker Num:  3 \n",
            " Loss:  2.619424343109131\n",
            "Training:  Iteration No:  489 Worker Num:  4 \n",
            " Loss:  5.308784484863281\n",
            "Training:  Iteration No:  489 Worker Num:  5 \n",
            " Loss:  3.129500389099121\n",
            "Training:  Iteration No:  489 Worker Num:  6 \n",
            " Loss:  4.434128761291504\n",
            "Training:  Iteration No:  489 Worker Num:  7 \n",
            " Loss:  3.488826036453247\n",
            "Test:  Iteration No:  489 \n",
            " Loss:  1.8548349181685266\n",
            "Test accuracy:  87.16\n",
            "Training:  Iteration No:  490 Worker Num:  0 \n",
            " Loss:  6.253291130065918\n",
            "Training:  Iteration No:  490 Worker Num:  1 \n",
            " Loss:  2.586179733276367\n",
            "Training:  Iteration No:  490 Worker Num:  2 \n",
            " Loss:  3.8476810455322266\n",
            "Training:  Iteration No:  490 Worker Num:  3 \n",
            " Loss:  7.702085971832275\n",
            "Training:  Iteration No:  490 Worker Num:  4 \n",
            " Loss:  2.8694968223571777\n",
            "Training:  Iteration No:  490 Worker Num:  5 \n",
            " Loss:  4.856067657470703\n",
            "Training:  Iteration No:  490 Worker Num:  6 \n",
            " Loss:  4.086645126342773\n",
            "Training:  Iteration No:  490 Worker Num:  7 \n",
            " Loss:  4.788663387298584\n",
            "Test:  Iteration No:  490 \n",
            " Loss:  1.9656293589999028\n",
            "Test accuracy:  86.68\n",
            "Training:  Iteration No:  491 Worker Num:  0 \n",
            " Loss:  4.77379035949707\n",
            "Training:  Iteration No:  491 Worker Num:  1 \n",
            " Loss:  3.63372540473938\n",
            "Training:  Iteration No:  491 Worker Num:  2 \n",
            " Loss:  3.4073500633239746\n",
            "Training:  Iteration No:  491 Worker Num:  3 \n",
            " Loss:  2.779031753540039\n",
            "Training:  Iteration No:  491 Worker Num:  4 \n",
            " Loss:  3.354581594467163\n",
            "Training:  Iteration No:  491 Worker Num:  5 \n",
            " Loss:  4.056259632110596\n",
            "Training:  Iteration No:  491 Worker Num:  6 \n",
            " Loss:  7.338421821594238\n",
            "Training:  Iteration No:  491 Worker Num:  7 \n",
            " Loss:  3.697786808013916\n",
            "Test:  Iteration No:  491 \n",
            " Loss:  1.9744251836327058\n",
            "Test accuracy:  86.85\n",
            "Training:  Iteration No:  492 Worker Num:  0 \n",
            " Loss:  9.127822875976562\n",
            "Training:  Iteration No:  492 Worker Num:  1 \n",
            " Loss:  5.539079666137695\n",
            "Training:  Iteration No:  492 Worker Num:  2 \n",
            " Loss:  5.259228229522705\n",
            "Training:  Iteration No:  492 Worker Num:  3 \n",
            " Loss:  3.4076988697052\n",
            "Training:  Iteration No:  492 Worker Num:  4 \n",
            " Loss:  3.6384599208831787\n",
            "Training:  Iteration No:  492 Worker Num:  5 \n",
            " Loss:  3.0871942043304443\n",
            "Training:  Iteration No:  492 Worker Num:  6 \n",
            " Loss:  4.827220439910889\n",
            "Training:  Iteration No:  492 Worker Num:  7 \n",
            " Loss:  8.665242195129395\n",
            "Test:  Iteration No:  492 \n",
            " Loss:  1.9336009651799746\n",
            "Test accuracy:  87.02\n",
            "Training:  Iteration No:  493 Worker Num:  0 \n",
            " Loss:  3.8155593872070312\n",
            "Training:  Iteration No:  493 Worker Num:  1 \n",
            " Loss:  4.72566556930542\n",
            "Training:  Iteration No:  493 Worker Num:  2 \n",
            " Loss:  4.27791166305542\n",
            "Training:  Iteration No:  493 Worker Num:  3 \n",
            " Loss:  2.980356454849243\n",
            "Training:  Iteration No:  493 Worker Num:  4 \n",
            " Loss:  2.352879047393799\n",
            "Training:  Iteration No:  493 Worker Num:  5 \n",
            " Loss:  4.152174472808838\n",
            "Training:  Iteration No:  493 Worker Num:  6 \n",
            " Loss:  3.274442434310913\n",
            "Training:  Iteration No:  493 Worker Num:  7 \n",
            " Loss:  3.3358237743377686\n",
            "Test:  Iteration No:  493 \n",
            " Loss:  2.0519999524083317\n",
            "Test accuracy:  86.61\n",
            "Training:  Iteration No:  494 Worker Num:  0 \n",
            " Loss:  4.521629333496094\n",
            "Training:  Iteration No:  494 Worker Num:  1 \n",
            " Loss:  4.2817511558532715\n",
            "Training:  Iteration No:  494 Worker Num:  2 \n",
            " Loss:  2.4348349571228027\n",
            "Training:  Iteration No:  494 Worker Num:  3 \n",
            " Loss:  3.4062907695770264\n",
            "Training:  Iteration No:  494 Worker Num:  4 \n",
            " Loss:  4.167670249938965\n",
            "Training:  Iteration No:  494 Worker Num:  5 \n",
            " Loss:  3.2755470275878906\n",
            "Training:  Iteration No:  494 Worker Num:  6 \n",
            " Loss:  3.3673808574676514\n",
            "Training:  Iteration No:  494 Worker Num:  7 \n",
            " Loss:  3.0141797065734863\n",
            "Test:  Iteration No:  494 \n",
            " Loss:  1.8697175010537752\n",
            "Test accuracy:  87.61\n",
            "Training:  Iteration No:  495 Worker Num:  0 \n",
            " Loss:  4.0044097900390625\n",
            "Training:  Iteration No:  495 Worker Num:  1 \n",
            " Loss:  3.2844765186309814\n",
            "Training:  Iteration No:  495 Worker Num:  2 \n",
            " Loss:  3.5614330768585205\n",
            "Training:  Iteration No:  495 Worker Num:  3 \n",
            " Loss:  2.5483853816986084\n",
            "Training:  Iteration No:  495 Worker Num:  4 \n",
            " Loss:  2.5906834602355957\n",
            "Training:  Iteration No:  495 Worker Num:  5 \n",
            " Loss:  3.555419921875\n",
            "Training:  Iteration No:  495 Worker Num:  6 \n",
            " Loss:  3.5470588207244873\n",
            "Training:  Iteration No:  495 Worker Num:  7 \n",
            " Loss:  4.8171234130859375\n",
            "Test:  Iteration No:  495 \n",
            " Loss:  2.0236746957407723\n",
            "Test accuracy:  86.81\n",
            "Training:  Iteration No:  496 Worker Num:  0 \n",
            " Loss:  2.9349684715270996\n",
            "Training:  Iteration No:  496 Worker Num:  1 \n",
            " Loss:  3.7300827503204346\n",
            "Training:  Iteration No:  496 Worker Num:  2 \n",
            " Loss:  2.6220266819000244\n",
            "Training:  Iteration No:  496 Worker Num:  3 \n",
            " Loss:  3.3725509643554688\n",
            "Training:  Iteration No:  496 Worker Num:  4 \n",
            " Loss:  2.8906753063201904\n",
            "Training:  Iteration No:  496 Worker Num:  5 \n",
            " Loss:  3.983198404312134\n",
            "Training:  Iteration No:  496 Worker Num:  6 \n",
            " Loss:  3.392526149749756\n",
            "Training:  Iteration No:  496 Worker Num:  7 \n",
            " Loss:  8.563355445861816\n",
            "Test:  Iteration No:  496 \n",
            " Loss:  1.9764337417775695\n",
            "Test accuracy:  87.27\n",
            "Training:  Iteration No:  497 Worker Num:  0 \n",
            " Loss:  4.841944694519043\n",
            "Training:  Iteration No:  497 Worker Num:  1 \n",
            " Loss:  5.9195942878723145\n",
            "Training:  Iteration No:  497 Worker Num:  2 \n",
            " Loss:  2.6518185138702393\n",
            "Training:  Iteration No:  497 Worker Num:  3 \n",
            " Loss:  4.361865043640137\n",
            "Training:  Iteration No:  497 Worker Num:  4 \n",
            " Loss:  5.839898109436035\n",
            "Training:  Iteration No:  497 Worker Num:  5 \n",
            " Loss:  3.285501003265381\n",
            "Training:  Iteration No:  497 Worker Num:  6 \n",
            " Loss:  7.779057025909424\n",
            "Training:  Iteration No:  497 Worker Num:  7 \n",
            " Loss:  4.547734260559082\n",
            "Test:  Iteration No:  497 \n",
            " Loss:  1.8877877674964065\n",
            "Test accuracy:  87.79\n",
            "Training:  Iteration No:  498 Worker Num:  0 \n",
            " Loss:  2.9446492195129395\n",
            "Training:  Iteration No:  498 Worker Num:  1 \n",
            " Loss:  4.770308494567871\n",
            "Training:  Iteration No:  498 Worker Num:  2 \n",
            " Loss:  3.2513225078582764\n",
            "Training:  Iteration No:  498 Worker Num:  3 \n",
            " Loss:  3.1496622562408447\n",
            "Training:  Iteration No:  498 Worker Num:  4 \n",
            " Loss:  3.139435291290283\n",
            "Training:  Iteration No:  498 Worker Num:  5 \n",
            " Loss:  6.114571571350098\n",
            "Training:  Iteration No:  498 Worker Num:  6 \n",
            " Loss:  3.521042823791504\n",
            "Training:  Iteration No:  498 Worker Num:  7 \n",
            " Loss:  5.590633392333984\n",
            "Test:  Iteration No:  498 \n",
            " Loss:  2.2099438621132057\n",
            "Test accuracy:  85.8\n",
            "Training:  Iteration No:  499 Worker Num:  0 \n",
            " Loss:  3.5638744831085205\n",
            "Training:  Iteration No:  499 Worker Num:  1 \n",
            " Loss:  3.271345376968384\n",
            "Training:  Iteration No:  499 Worker Num:  2 \n",
            " Loss:  2.8445305824279785\n",
            "Training:  Iteration No:  499 Worker Num:  3 \n",
            " Loss:  3.0383479595184326\n",
            "Training:  Iteration No:  499 Worker Num:  4 \n",
            " Loss:  3.987765312194824\n",
            "Training:  Iteration No:  499 Worker Num:  5 \n",
            " Loss:  4.006392478942871\n",
            "Training:  Iteration No:  499 Worker Num:  6 \n",
            " Loss:  5.317827224731445\n",
            "Training:  Iteration No:  499 Worker Num:  7 \n",
            " Loss:  2.6588361263275146\n",
            "Test:  Iteration No:  499 \n",
            " Loss:  1.8545484894363848\n",
            "Test accuracy:  88.14\n",
            "Training:  Iteration No:  500 Worker Num:  0 \n",
            " Loss:  4.597618579864502\n",
            "Training:  Iteration No:  500 Worker Num:  1 \n",
            " Loss:  2.743279218673706\n",
            "Training:  Iteration No:  500 Worker Num:  2 \n",
            " Loss:  3.9928243160247803\n",
            "Training:  Iteration No:  500 Worker Num:  3 \n",
            " Loss:  3.4177565574645996\n",
            "Training:  Iteration No:  500 Worker Num:  4 \n",
            " Loss:  3.2337050437927246\n",
            "Training:  Iteration No:  500 Worker Num:  5 \n",
            " Loss:  3.4635274410247803\n",
            "Training:  Iteration No:  500 Worker Num:  6 \n",
            " Loss:  4.004219055175781\n",
            "Training:  Iteration No:  500 Worker Num:  7 \n",
            " Loss:  4.255102634429932\n",
            "Test:  Iteration No:  500 \n",
            " Loss:  2.0080095838985255\n",
            "Test accuracy:  87.36\n",
            "Training:  Iteration No:  501 Worker Num:  0 \n",
            " Loss:  2.231400728225708\n",
            "Training:  Iteration No:  501 Worker Num:  1 \n",
            " Loss:  3.957615375518799\n",
            "Training:  Iteration No:  501 Worker Num:  2 \n",
            " Loss:  2.2088427543640137\n",
            "Training:  Iteration No:  501 Worker Num:  3 \n",
            " Loss:  2.6737306118011475\n",
            "Training:  Iteration No:  501 Worker Num:  4 \n",
            " Loss:  3.7065269947052\n",
            "Training:  Iteration No:  501 Worker Num:  5 \n",
            " Loss:  3.888427257537842\n",
            "Training:  Iteration No:  501 Worker Num:  6 \n",
            " Loss:  6.45944881439209\n",
            "Training:  Iteration No:  501 Worker Num:  7 \n",
            " Loss:  4.132256507873535\n",
            "Test:  Iteration No:  501 \n",
            " Loss:  2.1017645685167254\n",
            "Test accuracy:  87.21\n",
            "Training:  Iteration No:  502 Worker Num:  0 \n",
            " Loss:  2.5458855628967285\n",
            "Training:  Iteration No:  502 Worker Num:  1 \n",
            " Loss:  5.481961250305176\n",
            "Training:  Iteration No:  502 Worker Num:  2 \n",
            " Loss:  4.175179481506348\n",
            "Training:  Iteration No:  502 Worker Num:  3 \n",
            " Loss:  2.270359754562378\n",
            "Training:  Iteration No:  502 Worker Num:  4 \n",
            " Loss:  3.3158421516418457\n",
            "Training:  Iteration No:  502 Worker Num:  5 \n",
            " Loss:  4.597668647766113\n",
            "Training:  Iteration No:  502 Worker Num:  6 \n",
            " Loss:  4.943743705749512\n",
            "Training:  Iteration No:  502 Worker Num:  7 \n",
            " Loss:  2.425675392150879\n",
            "Test:  Iteration No:  502 \n",
            " Loss:  2.066799706320022\n",
            "Test accuracy:  87.25\n",
            "Training:  Iteration No:  503 Worker Num:  0 \n",
            " Loss:  4.505153179168701\n",
            "Training:  Iteration No:  503 Worker Num:  1 \n",
            " Loss:  2.3490262031555176\n",
            "Training:  Iteration No:  503 Worker Num:  2 \n",
            " Loss:  3.537815570831299\n",
            "Training:  Iteration No:  503 Worker Num:  3 \n",
            " Loss:  4.102417469024658\n",
            "Training:  Iteration No:  503 Worker Num:  4 \n",
            " Loss:  4.932024002075195\n",
            "Training:  Iteration No:  503 Worker Num:  5 \n",
            " Loss:  5.186483860015869\n",
            "Training:  Iteration No:  503 Worker Num:  6 \n",
            " Loss:  4.717529773712158\n",
            "Training:  Iteration No:  503 Worker Num:  7 \n",
            " Loss:  4.323000431060791\n",
            "Test:  Iteration No:  503 \n",
            " Loss:  2.029678155632713\n",
            "Test accuracy:  87.31\n",
            "Training:  Iteration No:  504 Worker Num:  0 \n",
            " Loss:  3.392165184020996\n",
            "Training:  Iteration No:  504 Worker Num:  1 \n",
            " Loss:  3.7487952709198\n",
            "Training:  Iteration No:  504 Worker Num:  2 \n",
            " Loss:  4.2215189933776855\n",
            "Training:  Iteration No:  504 Worker Num:  3 \n",
            " Loss:  4.164435386657715\n",
            "Training:  Iteration No:  504 Worker Num:  4 \n",
            " Loss:  6.612225532531738\n",
            "Training:  Iteration No:  504 Worker Num:  5 \n",
            " Loss:  2.5782244205474854\n",
            "Training:  Iteration No:  504 Worker Num:  6 \n",
            " Loss:  4.256584167480469\n",
            "Training:  Iteration No:  504 Worker Num:  7 \n",
            " Loss:  3.466235399246216\n",
            "Test:  Iteration No:  504 \n",
            " Loss:  2.0025458075626514\n",
            "Test accuracy:  87.38\n",
            "Training:  Iteration No:  505 Worker Num:  0 \n",
            " Loss:  3.49532151222229\n",
            "Training:  Iteration No:  505 Worker Num:  1 \n",
            " Loss:  4.640307903289795\n",
            "Training:  Iteration No:  505 Worker Num:  2 \n",
            " Loss:  3.2195160388946533\n",
            "Training:  Iteration No:  505 Worker Num:  3 \n",
            " Loss:  3.1886093616485596\n",
            "Training:  Iteration No:  505 Worker Num:  4 \n",
            " Loss:  3.9057512283325195\n",
            "Training:  Iteration No:  505 Worker Num:  5 \n",
            " Loss:  3.6441352367401123\n",
            "Training:  Iteration No:  505 Worker Num:  6 \n",
            " Loss:  3.0490856170654297\n",
            "Training:  Iteration No:  505 Worker Num:  7 \n",
            " Loss:  4.061247825622559\n",
            "Test:  Iteration No:  505 \n",
            " Loss:  1.9725630204605906\n",
            "Test accuracy:  87.5\n",
            "Training:  Iteration No:  506 Worker Num:  0 \n",
            " Loss:  4.071601867675781\n",
            "Training:  Iteration No:  506 Worker Num:  1 \n",
            " Loss:  2.8484749794006348\n",
            "Training:  Iteration No:  506 Worker Num:  2 \n",
            " Loss:  4.269375801086426\n",
            "Training:  Iteration No:  506 Worker Num:  3 \n",
            " Loss:  4.529544830322266\n",
            "Training:  Iteration No:  506 Worker Num:  4 \n",
            " Loss:  5.1367268562316895\n",
            "Training:  Iteration No:  506 Worker Num:  5 \n",
            " Loss:  2.7572884559631348\n",
            "Training:  Iteration No:  506 Worker Num:  6 \n",
            " Loss:  3.8474559783935547\n",
            "Training:  Iteration No:  506 Worker Num:  7 \n",
            " Loss:  4.529642581939697\n",
            "Test:  Iteration No:  506 \n",
            " Loss:  2.0341873887196535\n",
            "Test accuracy:  87.32\n",
            "Training:  Iteration No:  507 Worker Num:  0 \n",
            " Loss:  2.572687864303589\n",
            "Training:  Iteration No:  507 Worker Num:  1 \n",
            " Loss:  2.1130502223968506\n",
            "Training:  Iteration No:  507 Worker Num:  2 \n",
            " Loss:  4.052096843719482\n",
            "Training:  Iteration No:  507 Worker Num:  3 \n",
            " Loss:  4.245368957519531\n",
            "Training:  Iteration No:  507 Worker Num:  4 \n",
            " Loss:  2.531285285949707\n",
            "Training:  Iteration No:  507 Worker Num:  5 \n",
            " Loss:  4.168224811553955\n",
            "Training:  Iteration No:  507 Worker Num:  6 \n",
            " Loss:  3.760563611984253\n",
            "Training:  Iteration No:  507 Worker Num:  7 \n",
            " Loss:  4.15617561340332\n",
            "Test:  Iteration No:  507 \n",
            " Loss:  1.9822290514973788\n",
            "Test accuracy:  88.0\n",
            "Training:  Iteration No:  508 Worker Num:  0 \n",
            " Loss:  3.581493854522705\n",
            "Training:  Iteration No:  508 Worker Num:  1 \n",
            " Loss:  4.076797962188721\n",
            "Training:  Iteration No:  508 Worker Num:  2 \n",
            " Loss:  3.2571918964385986\n",
            "Training:  Iteration No:  508 Worker Num:  3 \n",
            " Loss:  3.149538040161133\n",
            "Training:  Iteration No:  508 Worker Num:  4 \n",
            " Loss:  4.7329487800598145\n",
            "Training:  Iteration No:  508 Worker Num:  5 \n",
            " Loss:  2.861999034881592\n",
            "Training:  Iteration No:  508 Worker Num:  6 \n",
            " Loss:  3.3206210136413574\n",
            "Training:  Iteration No:  508 Worker Num:  7 \n",
            " Loss:  8.531785011291504\n",
            "Test:  Iteration No:  508 \n",
            " Loss:  2.052709734727499\n",
            "Test accuracy:  87.81\n",
            "Training:  Iteration No:  509 Worker Num:  0 \n",
            " Loss:  3.0677618980407715\n",
            "Training:  Iteration No:  509 Worker Num:  1 \n",
            " Loss:  2.4467101097106934\n",
            "Training:  Iteration No:  509 Worker Num:  2 \n",
            " Loss:  2.751030921936035\n",
            "Training:  Iteration No:  509 Worker Num:  3 \n",
            " Loss:  2.7181787490844727\n",
            "Training:  Iteration No:  509 Worker Num:  4 \n",
            " Loss:  4.857723236083984\n",
            "Training:  Iteration No:  509 Worker Num:  5 \n",
            " Loss:  3.0931708812713623\n",
            "Training:  Iteration No:  509 Worker Num:  6 \n",
            " Loss:  5.4896769523620605\n",
            "Training:  Iteration No:  509 Worker Num:  7 \n",
            " Loss:  2.987450122833252\n",
            "Test:  Iteration No:  509 \n",
            " Loss:  2.022479578423096\n",
            "Test accuracy:  88.1\n",
            "Training:  Iteration No:  510 Worker Num:  0 \n",
            " Loss:  2.2245068550109863\n",
            "Training:  Iteration No:  510 Worker Num:  1 \n",
            " Loss:  3.539680004119873\n",
            "Training:  Iteration No:  510 Worker Num:  2 \n",
            " Loss:  5.909674644470215\n",
            "Training:  Iteration No:  510 Worker Num:  3 \n",
            " Loss:  2.594248056411743\n",
            "Training:  Iteration No:  510 Worker Num:  4 \n",
            " Loss:  5.814184188842773\n",
            "Training:  Iteration No:  510 Worker Num:  5 \n",
            " Loss:  3.248504161834717\n",
            "Training:  Iteration No:  510 Worker Num:  6 \n",
            " Loss:  1.8110014200210571\n",
            "Training:  Iteration No:  510 Worker Num:  7 \n",
            " Loss:  3.2251405715942383\n",
            "Test:  Iteration No:  510 \n",
            " Loss:  2.018893818110537\n",
            "Test accuracy:  87.75\n",
            "Training:  Iteration No:  511 Worker Num:  0 \n",
            " Loss:  2.1597907543182373\n",
            "Training:  Iteration No:  511 Worker Num:  1 \n",
            " Loss:  3.648080587387085\n",
            "Training:  Iteration No:  511 Worker Num:  2 \n",
            " Loss:  5.847908973693848\n",
            "Training:  Iteration No:  511 Worker Num:  3 \n",
            " Loss:  4.268120765686035\n",
            "Training:  Iteration No:  511 Worker Num:  4 \n",
            " Loss:  2.3118956089019775\n",
            "Training:  Iteration No:  511 Worker Num:  5 \n",
            " Loss:  3.667424440383911\n",
            "Training:  Iteration No:  511 Worker Num:  6 \n",
            " Loss:  4.699644088745117\n",
            "Training:  Iteration No:  511 Worker Num:  7 \n",
            " Loss:  2.9022746086120605\n",
            "Test:  Iteration No:  511 \n",
            " Loss:  2.018435344584172\n",
            "Test accuracy:  87.87\n",
            "Training:  Iteration No:  512 Worker Num:  0 \n",
            " Loss:  4.054223537445068\n",
            "Training:  Iteration No:  512 Worker Num:  1 \n",
            " Loss:  4.618251323699951\n",
            "Training:  Iteration No:  512 Worker Num:  2 \n",
            " Loss:  3.8538646697998047\n",
            "Training:  Iteration No:  512 Worker Num:  3 \n",
            " Loss:  4.750176429748535\n",
            "Training:  Iteration No:  512 Worker Num:  4 \n",
            " Loss:  4.717079162597656\n",
            "Training:  Iteration No:  512 Worker Num:  5 \n",
            " Loss:  3.578141450881958\n",
            "Training:  Iteration No:  512 Worker Num:  6 \n",
            " Loss:  3.9470937252044678\n",
            "Training:  Iteration No:  512 Worker Num:  7 \n",
            " Loss:  3.8665895462036133\n",
            "Test:  Iteration No:  512 \n",
            " Loss:  1.9932204836425431\n",
            "Test accuracy:  87.68\n",
            "Training:  Iteration No:  513 Worker Num:  0 \n",
            " Loss:  3.0185556411743164\n",
            "Training:  Iteration No:  513 Worker Num:  1 \n",
            " Loss:  2.6073551177978516\n",
            "Training:  Iteration No:  513 Worker Num:  2 \n",
            " Loss:  3.4427871704101562\n",
            "Training:  Iteration No:  513 Worker Num:  3 \n",
            " Loss:  3.1293182373046875\n",
            "Training:  Iteration No:  513 Worker Num:  4 \n",
            " Loss:  3.848538398742676\n",
            "Training:  Iteration No:  513 Worker Num:  5 \n",
            " Loss:  3.509403705596924\n",
            "Training:  Iteration No:  513 Worker Num:  6 \n",
            " Loss:  4.007989883422852\n",
            "Training:  Iteration No:  513 Worker Num:  7 \n",
            " Loss:  7.648334980010986\n",
            "Test:  Iteration No:  513 \n",
            " Loss:  1.997478838609178\n",
            "Test accuracy:  88.12\n",
            "Training:  Iteration No:  514 Worker Num:  0 \n",
            " Loss:  3.0804004669189453\n",
            "Training:  Iteration No:  514 Worker Num:  1 \n",
            " Loss:  3.3041324615478516\n",
            "Training:  Iteration No:  514 Worker Num:  2 \n",
            " Loss:  4.406584739685059\n",
            "Training:  Iteration No:  514 Worker Num:  3 \n",
            " Loss:  3.600188970565796\n",
            "Training:  Iteration No:  514 Worker Num:  4 \n",
            " Loss:  3.536367416381836\n",
            "Training:  Iteration No:  514 Worker Num:  5 \n",
            " Loss:  4.268303394317627\n",
            "Training:  Iteration No:  514 Worker Num:  6 \n",
            " Loss:  3.9289710521698\n",
            "Training:  Iteration No:  514 Worker Num:  7 \n",
            " Loss:  4.120796203613281\n",
            "Test:  Iteration No:  514 \n",
            " Loss:  2.054838448071954\n",
            "Test accuracy:  87.68\n",
            "Training:  Iteration No:  515 Worker Num:  0 \n",
            " Loss:  3.7497875690460205\n",
            "Training:  Iteration No:  515 Worker Num:  1 \n",
            " Loss:  3.6622509956359863\n",
            "Training:  Iteration No:  515 Worker Num:  2 \n",
            " Loss:  3.526353597640991\n",
            "Training:  Iteration No:  515 Worker Num:  3 \n",
            " Loss:  3.804643392562866\n",
            "Training:  Iteration No:  515 Worker Num:  4 \n",
            " Loss:  3.4922046661376953\n",
            "Training:  Iteration No:  515 Worker Num:  5 \n",
            " Loss:  4.134983539581299\n",
            "Training:  Iteration No:  515 Worker Num:  6 \n",
            " Loss:  3.185030460357666\n",
            "Training:  Iteration No:  515 Worker Num:  7 \n",
            " Loss:  4.907426834106445\n",
            "Test:  Iteration No:  515 \n",
            " Loss:  2.050878948215955\n",
            "Test accuracy:  87.86\n",
            "Training:  Iteration No:  516 Worker Num:  0 \n",
            " Loss:  2.61612606048584\n",
            "Training:  Iteration No:  516 Worker Num:  1 \n",
            " Loss:  4.759749889373779\n",
            "Training:  Iteration No:  516 Worker Num:  2 \n",
            " Loss:  8.194623947143555\n",
            "Training:  Iteration No:  516 Worker Num:  3 \n",
            " Loss:  4.997458457946777\n",
            "Training:  Iteration No:  516 Worker Num:  4 \n",
            " Loss:  6.503578186035156\n",
            "Training:  Iteration No:  516 Worker Num:  5 \n",
            " Loss:  4.7640790939331055\n",
            "Training:  Iteration No:  516 Worker Num:  6 \n",
            " Loss:  4.958960056304932\n",
            "Training:  Iteration No:  516 Worker Num:  7 \n",
            " Loss:  3.4993181228637695\n",
            "Test:  Iteration No:  516 \n",
            " Loss:  1.990450040456438\n",
            "Test accuracy:  88.29\n",
            "Training:  Iteration No:  517 Worker Num:  0 \n",
            " Loss:  6.125579833984375\n",
            "Training:  Iteration No:  517 Worker Num:  1 \n",
            " Loss:  4.566234588623047\n",
            "Training:  Iteration No:  517 Worker Num:  2 \n",
            " Loss:  4.292098522186279\n",
            "Training:  Iteration No:  517 Worker Num:  3 \n",
            " Loss:  3.8376059532165527\n",
            "Training:  Iteration No:  517 Worker Num:  4 \n",
            " Loss:  2.8941924571990967\n",
            "Training:  Iteration No:  517 Worker Num:  5 \n",
            " Loss:  2.85300350189209\n",
            "Training:  Iteration No:  517 Worker Num:  6 \n",
            " Loss:  4.593801498413086\n",
            "Training:  Iteration No:  517 Worker Num:  7 \n",
            " Loss:  3.731398820877075\n",
            "Test:  Iteration No:  517 \n",
            " Loss:  2.2331918466105303\n",
            "Test accuracy:  86.42\n",
            "Training:  Iteration No:  518 Worker Num:  0 \n",
            " Loss:  4.27151346206665\n",
            "Training:  Iteration No:  518 Worker Num:  1 \n",
            " Loss:  3.661747455596924\n",
            "Training:  Iteration No:  518 Worker Num:  2 \n",
            " Loss:  4.3412957191467285\n",
            "Training:  Iteration No:  518 Worker Num:  3 \n",
            " Loss:  4.018164157867432\n",
            "Training:  Iteration No:  518 Worker Num:  4 \n",
            " Loss:  5.158080577850342\n",
            "Training:  Iteration No:  518 Worker Num:  5 \n",
            " Loss:  3.8421568870544434\n",
            "Training:  Iteration No:  518 Worker Num:  6 \n",
            " Loss:  5.679533958435059\n",
            "Training:  Iteration No:  518 Worker Num:  7 \n",
            " Loss:  3.852717876434326\n",
            "Test:  Iteration No:  518 \n",
            " Loss:  2.0740980967124805\n",
            "Test accuracy:  87.75\n",
            "Training:  Iteration No:  519 Worker Num:  0 \n",
            " Loss:  2.0155465602874756\n",
            "Training:  Iteration No:  519 Worker Num:  1 \n",
            " Loss:  3.1428720951080322\n",
            "Training:  Iteration No:  519 Worker Num:  2 \n",
            " Loss:  4.309001445770264\n",
            "Training:  Iteration No:  519 Worker Num:  3 \n",
            " Loss:  5.167860984802246\n",
            "Training:  Iteration No:  519 Worker Num:  4 \n",
            " Loss:  3.3344151973724365\n",
            "Training:  Iteration No:  519 Worker Num:  5 \n",
            " Loss:  5.042017459869385\n",
            "Training:  Iteration No:  519 Worker Num:  6 \n",
            " Loss:  2.3119993209838867\n",
            "Training:  Iteration No:  519 Worker Num:  7 \n",
            " Loss:  3.005711555480957\n",
            "Test:  Iteration No:  519 \n",
            " Loss:  2.049271681216321\n",
            "Test accuracy:  87.8\n",
            "Training:  Iteration No:  520 Worker Num:  0 \n",
            " Loss:  4.263650417327881\n",
            "Training:  Iteration No:  520 Worker Num:  1 \n",
            " Loss:  1.9776078462600708\n",
            "Training:  Iteration No:  520 Worker Num:  2 \n",
            " Loss:  3.7357046604156494\n",
            "Training:  Iteration No:  520 Worker Num:  3 \n",
            " Loss:  3.5622544288635254\n",
            "Training:  Iteration No:  520 Worker Num:  4 \n",
            " Loss:  4.474187850952148\n",
            "Training:  Iteration No:  520 Worker Num:  5 \n",
            " Loss:  5.514496326446533\n",
            "Training:  Iteration No:  520 Worker Num:  6 \n",
            " Loss:  5.099756717681885\n",
            "Training:  Iteration No:  520 Worker Num:  7 \n",
            " Loss:  4.580110549926758\n",
            "Test:  Iteration No:  520 \n",
            " Loss:  1.9487071819014\n",
            "Test accuracy:  88.11\n",
            "Training:  Iteration No:  521 Worker Num:  0 \n",
            " Loss:  2.507246971130371\n",
            "Training:  Iteration No:  521 Worker Num:  1 \n",
            " Loss:  4.694947242736816\n",
            "Training:  Iteration No:  521 Worker Num:  2 \n",
            " Loss:  2.494744062423706\n",
            "Training:  Iteration No:  521 Worker Num:  3 \n",
            " Loss:  5.350014686584473\n",
            "Training:  Iteration No:  521 Worker Num:  4 \n",
            " Loss:  3.98240327835083\n",
            "Training:  Iteration No:  521 Worker Num:  5 \n",
            " Loss:  3.7063281536102295\n",
            "Training:  Iteration No:  521 Worker Num:  6 \n",
            " Loss:  3.2203948497772217\n",
            "Training:  Iteration No:  521 Worker Num:  7 \n",
            " Loss:  5.005764007568359\n",
            "Test:  Iteration No:  521 \n",
            " Loss:  2.035828074316577\n",
            "Test accuracy:  87.87\n",
            "Training:  Iteration No:  522 Worker Num:  0 \n",
            " Loss:  2.601029872894287\n",
            "Training:  Iteration No:  522 Worker Num:  1 \n",
            " Loss:  4.531676769256592\n",
            "Training:  Iteration No:  522 Worker Num:  2 \n",
            " Loss:  3.961299180984497\n",
            "Training:  Iteration No:  522 Worker Num:  3 \n",
            " Loss:  2.6240627765655518\n",
            "Training:  Iteration No:  522 Worker Num:  4 \n",
            " Loss:  1.332331895828247\n",
            "Training:  Iteration No:  522 Worker Num:  5 \n",
            " Loss:  4.202597141265869\n",
            "Training:  Iteration No:  522 Worker Num:  6 \n",
            " Loss:  4.842931270599365\n",
            "Training:  Iteration No:  522 Worker Num:  7 \n",
            " Loss:  2.9209530353546143\n",
            "Test:  Iteration No:  522 \n",
            " Loss:  2.020323815030529\n",
            "Test accuracy:  87.96\n",
            "Training:  Iteration No:  523 Worker Num:  0 \n",
            " Loss:  4.293274402618408\n",
            "Training:  Iteration No:  523 Worker Num:  1 \n",
            " Loss:  4.8664093017578125\n",
            "Training:  Iteration No:  523 Worker Num:  2 \n",
            " Loss:  2.3556149005889893\n",
            "Training:  Iteration No:  523 Worker Num:  3 \n",
            " Loss:  3.089445114135742\n",
            "Training:  Iteration No:  523 Worker Num:  4 \n",
            " Loss:  3.2254838943481445\n",
            "Training:  Iteration No:  523 Worker Num:  5 \n",
            " Loss:  4.5356621742248535\n",
            "Training:  Iteration No:  523 Worker Num:  6 \n",
            " Loss:  1.8852306604385376\n",
            "Training:  Iteration No:  523 Worker Num:  7 \n",
            " Loss:  2.259920358657837\n",
            "Test:  Iteration No:  523 \n",
            " Loss:  1.9034316244570515\n",
            "Test accuracy:  88.41\n",
            "Training:  Iteration No:  524 Worker Num:  0 \n",
            " Loss:  3.2254064083099365\n",
            "Training:  Iteration No:  524 Worker Num:  1 \n",
            " Loss:  1.7831707000732422\n",
            "Training:  Iteration No:  524 Worker Num:  2 \n",
            " Loss:  4.939441204071045\n",
            "Training:  Iteration No:  524 Worker Num:  3 \n",
            " Loss:  3.4812417030334473\n",
            "Training:  Iteration No:  524 Worker Num:  4 \n",
            " Loss:  3.561493396759033\n",
            "Training:  Iteration No:  524 Worker Num:  5 \n",
            " Loss:  3.972923994064331\n",
            "Training:  Iteration No:  524 Worker Num:  6 \n",
            " Loss:  2.65299129486084\n",
            "Training:  Iteration No:  524 Worker Num:  7 \n",
            " Loss:  3.227203607559204\n",
            "Test:  Iteration No:  524 \n",
            " Loss:  2.009234510503615\n",
            "Test accuracy:  88.05\n",
            "Training:  Iteration No:  525 Worker Num:  0 \n",
            " Loss:  2.915891170501709\n",
            "Training:  Iteration No:  525 Worker Num:  1 \n",
            " Loss:  6.174108505249023\n",
            "Training:  Iteration No:  525 Worker Num:  2 \n",
            " Loss:  2.808115005493164\n",
            "Training:  Iteration No:  525 Worker Num:  3 \n",
            " Loss:  4.560854434967041\n",
            "Training:  Iteration No:  525 Worker Num:  4 \n",
            " Loss:  3.2429707050323486\n",
            "Training:  Iteration No:  525 Worker Num:  5 \n",
            " Loss:  4.437126159667969\n",
            "Training:  Iteration No:  525 Worker Num:  6 \n",
            " Loss:  4.503751277923584\n",
            "Training:  Iteration No:  525 Worker Num:  7 \n",
            " Loss:  4.775327682495117\n",
            "Test:  Iteration No:  525 \n",
            " Loss:  1.884575379894504\n",
            "Test accuracy:  88.29\n",
            "Training:  Iteration No:  526 Worker Num:  0 \n",
            " Loss:  2.828657865524292\n",
            "Training:  Iteration No:  526 Worker Num:  1 \n",
            " Loss:  4.898319244384766\n",
            "Training:  Iteration No:  526 Worker Num:  2 \n",
            " Loss:  3.1832616329193115\n",
            "Training:  Iteration No:  526 Worker Num:  3 \n",
            " Loss:  4.71373176574707\n",
            "Training:  Iteration No:  526 Worker Num:  4 \n",
            " Loss:  2.935175895690918\n",
            "Training:  Iteration No:  526 Worker Num:  5 \n",
            " Loss:  3.129099130630493\n",
            "Training:  Iteration No:  526 Worker Num:  6 \n",
            " Loss:  5.49082612991333\n",
            "Training:  Iteration No:  526 Worker Num:  7 \n",
            " Loss:  3.3972465991973877\n",
            "Test:  Iteration No:  526 \n",
            " Loss:  1.9949456502276743\n",
            "Test accuracy:  87.97\n",
            "Training:  Iteration No:  527 Worker Num:  0 \n",
            " Loss:  4.321166038513184\n",
            "Training:  Iteration No:  527 Worker Num:  1 \n",
            " Loss:  3.014647960662842\n",
            "Training:  Iteration No:  527 Worker Num:  2 \n",
            " Loss:  3.2751755714416504\n",
            "Training:  Iteration No:  527 Worker Num:  3 \n",
            " Loss:  5.118197441101074\n",
            "Training:  Iteration No:  527 Worker Num:  4 \n",
            " Loss:  3.7183785438537598\n",
            "Training:  Iteration No:  527 Worker Num:  5 \n",
            " Loss:  3.769838333129883\n",
            "Training:  Iteration No:  527 Worker Num:  6 \n",
            " Loss:  4.606411457061768\n",
            "Training:  Iteration No:  527 Worker Num:  7 \n",
            " Loss:  2.7859814167022705\n",
            "Test:  Iteration No:  527 \n",
            " Loss:  1.8495097746751146\n",
            "Test accuracy:  88.93\n",
            "Training:  Iteration No:  528 Worker Num:  0 \n",
            " Loss:  3.222031831741333\n",
            "Training:  Iteration No:  528 Worker Num:  1 \n",
            " Loss:  3.163287878036499\n",
            "Training:  Iteration No:  528 Worker Num:  2 \n",
            " Loss:  2.863151788711548\n",
            "Training:  Iteration No:  528 Worker Num:  3 \n",
            " Loss:  6.096170425415039\n",
            "Training:  Iteration No:  528 Worker Num:  4 \n",
            " Loss:  5.600523471832275\n",
            "Training:  Iteration No:  528 Worker Num:  5 \n",
            " Loss:  3.750061511993408\n",
            "Training:  Iteration No:  528 Worker Num:  6 \n",
            " Loss:  2.0237324237823486\n",
            "Training:  Iteration No:  528 Worker Num:  7 \n",
            " Loss:  3.9964122772216797\n",
            "Test:  Iteration No:  528 \n",
            " Loss:  2.1159207572146683\n",
            "Test accuracy:  87.47\n",
            "Training:  Iteration No:  529 Worker Num:  0 \n",
            " Loss:  2.923901081085205\n",
            "Training:  Iteration No:  529 Worker Num:  1 \n",
            " Loss:  3.2797417640686035\n",
            "Training:  Iteration No:  529 Worker Num:  2 \n",
            " Loss:  6.650477409362793\n",
            "Training:  Iteration No:  529 Worker Num:  3 \n",
            " Loss:  2.8619861602783203\n",
            "Training:  Iteration No:  529 Worker Num:  4 \n",
            " Loss:  3.4646589756011963\n",
            "Training:  Iteration No:  529 Worker Num:  5 \n",
            " Loss:  5.38863468170166\n",
            "Training:  Iteration No:  529 Worker Num:  6 \n",
            " Loss:  3.220518112182617\n",
            "Training:  Iteration No:  529 Worker Num:  7 \n",
            " Loss:  1.4193531274795532\n",
            "Test:  Iteration No:  529 \n",
            " Loss:  1.9245839043272823\n",
            "Test accuracy:  88.5\n",
            "Training:  Iteration No:  530 Worker Num:  0 \n",
            " Loss:  3.5728490352630615\n",
            "Training:  Iteration No:  530 Worker Num:  1 \n",
            " Loss:  3.50400710105896\n",
            "Training:  Iteration No:  530 Worker Num:  2 \n",
            " Loss:  5.029367923736572\n",
            "Training:  Iteration No:  530 Worker Num:  3 \n",
            " Loss:  4.773223400115967\n",
            "Training:  Iteration No:  530 Worker Num:  4 \n",
            " Loss:  4.4923415184021\n",
            "Training:  Iteration No:  530 Worker Num:  5 \n",
            " Loss:  2.680657386779785\n",
            "Training:  Iteration No:  530 Worker Num:  6 \n",
            " Loss:  2.41314697265625\n",
            "Training:  Iteration No:  530 Worker Num:  7 \n",
            " Loss:  3.6085739135742188\n",
            "Test:  Iteration No:  530 \n",
            " Loss:  1.9439958357377438\n",
            "Test accuracy:  88.28\n",
            "Training:  Iteration No:  531 Worker Num:  0 \n",
            " Loss:  4.0176825523376465\n",
            "Training:  Iteration No:  531 Worker Num:  1 \n",
            " Loss:  3.0175297260284424\n",
            "Training:  Iteration No:  531 Worker Num:  2 \n",
            " Loss:  2.084925651550293\n",
            "Training:  Iteration No:  531 Worker Num:  3 \n",
            " Loss:  4.202489376068115\n",
            "Training:  Iteration No:  531 Worker Num:  4 \n",
            " Loss:  2.9067118167877197\n",
            "Training:  Iteration No:  531 Worker Num:  5 \n",
            " Loss:  1.9373079538345337\n",
            "Training:  Iteration No:  531 Worker Num:  6 \n",
            " Loss:  5.935756683349609\n",
            "Training:  Iteration No:  531 Worker Num:  7 \n",
            " Loss:  2.84767746925354\n",
            "Test:  Iteration No:  531 \n",
            " Loss:  1.9495724615864927\n",
            "Test accuracy:  88.21\n",
            "Training:  Iteration No:  532 Worker Num:  0 \n",
            " Loss:  3.172593593597412\n",
            "Training:  Iteration No:  532 Worker Num:  1 \n",
            " Loss:  5.0614542961120605\n",
            "Training:  Iteration No:  532 Worker Num:  2 \n",
            " Loss:  6.733162879943848\n",
            "Training:  Iteration No:  532 Worker Num:  3 \n",
            " Loss:  2.2431113719940186\n",
            "Training:  Iteration No:  532 Worker Num:  4 \n",
            " Loss:  3.888066053390503\n",
            "Training:  Iteration No:  532 Worker Num:  5 \n",
            " Loss:  2.8223049640655518\n",
            "Training:  Iteration No:  532 Worker Num:  6 \n",
            " Loss:  3.8624446392059326\n",
            "Training:  Iteration No:  532 Worker Num:  7 \n",
            " Loss:  2.5392751693725586\n",
            "Test:  Iteration No:  532 \n",
            " Loss:  1.9660777440265957\n",
            "Test accuracy:  88.03\n",
            "Training:  Iteration No:  533 Worker Num:  0 \n",
            " Loss:  4.727731227874756\n",
            "Training:  Iteration No:  533 Worker Num:  1 \n",
            " Loss:  4.3542399406433105\n",
            "Training:  Iteration No:  533 Worker Num:  2 \n",
            " Loss:  3.177832841873169\n",
            "Training:  Iteration No:  533 Worker Num:  3 \n",
            " Loss:  4.260382175445557\n",
            "Training:  Iteration No:  533 Worker Num:  4 \n",
            " Loss:  3.0213680267333984\n",
            "Training:  Iteration No:  533 Worker Num:  5 \n",
            " Loss:  4.02965784072876\n",
            "Training:  Iteration No:  533 Worker Num:  6 \n",
            " Loss:  2.768653154373169\n",
            "Training:  Iteration No:  533 Worker Num:  7 \n",
            " Loss:  4.0385966300964355\n",
            "Test:  Iteration No:  533 \n",
            " Loss:  1.962293111299062\n",
            "Test accuracy:  88.27\n",
            "Training:  Iteration No:  534 Worker Num:  0 \n",
            " Loss:  6.127554893493652\n",
            "Training:  Iteration No:  534 Worker Num:  1 \n",
            " Loss:  1.4807014465332031\n",
            "Training:  Iteration No:  534 Worker Num:  2 \n",
            " Loss:  2.063913583755493\n",
            "Training:  Iteration No:  534 Worker Num:  3 \n",
            " Loss:  3.598973512649536\n",
            "Training:  Iteration No:  534 Worker Num:  4 \n",
            " Loss:  2.57676362991333\n",
            "Training:  Iteration No:  534 Worker Num:  5 \n",
            " Loss:  4.476354598999023\n",
            "Training:  Iteration No:  534 Worker Num:  6 \n",
            " Loss:  3.7541286945343018\n",
            "Training:  Iteration No:  534 Worker Num:  7 \n",
            " Loss:  3.2342689037323\n",
            "Test:  Iteration No:  534 \n",
            " Loss:  1.9156771123916194\n",
            "Test accuracy:  88.36\n",
            "Training:  Iteration No:  535 Worker Num:  0 \n",
            " Loss:  3.1952006816864014\n",
            "Training:  Iteration No:  535 Worker Num:  1 \n",
            " Loss:  5.9170823097229\n",
            "Training:  Iteration No:  535 Worker Num:  2 \n",
            " Loss:  2.3332865238189697\n",
            "Training:  Iteration No:  535 Worker Num:  3 \n",
            " Loss:  4.9893879890441895\n",
            "Training:  Iteration No:  535 Worker Num:  4 \n",
            " Loss:  2.3013410568237305\n",
            "Training:  Iteration No:  535 Worker Num:  5 \n",
            " Loss:  3.343334674835205\n",
            "Training:  Iteration No:  535 Worker Num:  6 \n",
            " Loss:  2.397152900695801\n",
            "Training:  Iteration No:  535 Worker Num:  7 \n",
            " Loss:  1.005146861076355\n",
            "Test:  Iteration No:  535 \n",
            " Loss:  1.9844459187929657\n",
            "Test accuracy:  88.0\n",
            "Training:  Iteration No:  536 Worker Num:  0 \n",
            " Loss:  2.7068934440612793\n",
            "Training:  Iteration No:  536 Worker Num:  1 \n",
            " Loss:  3.822584629058838\n",
            "Training:  Iteration No:  536 Worker Num:  2 \n",
            " Loss:  4.1113600730896\n",
            "Training:  Iteration No:  536 Worker Num:  3 \n",
            " Loss:  3.1182782649993896\n",
            "Training:  Iteration No:  536 Worker Num:  4 \n",
            " Loss:  4.2976837158203125\n",
            "Training:  Iteration No:  536 Worker Num:  5 \n",
            " Loss:  5.466212749481201\n",
            "Training:  Iteration No:  536 Worker Num:  6 \n",
            " Loss:  2.4117586612701416\n",
            "Training:  Iteration No:  536 Worker Num:  7 \n",
            " Loss:  1.6594794988632202\n",
            "Test:  Iteration No:  536 \n",
            " Loss:  1.8868323291622755\n",
            "Test accuracy:  88.65\n",
            "Training:  Iteration No:  537 Worker Num:  0 \n",
            " Loss:  4.23628044128418\n",
            "Training:  Iteration No:  537 Worker Num:  1 \n",
            " Loss:  2.9870831966400146\n",
            "Training:  Iteration No:  537 Worker Num:  2 \n",
            " Loss:  4.436954021453857\n",
            "Training:  Iteration No:  537 Worker Num:  3 \n",
            " Loss:  2.976516008377075\n",
            "Training:  Iteration No:  537 Worker Num:  4 \n",
            " Loss:  2.332944631576538\n",
            "Training:  Iteration No:  537 Worker Num:  5 \n",
            " Loss:  2.1363563537597656\n",
            "Training:  Iteration No:  537 Worker Num:  6 \n",
            " Loss:  3.333815336227417\n",
            "Training:  Iteration No:  537 Worker Num:  7 \n",
            " Loss:  3.3480141162872314\n",
            "Test:  Iteration No:  537 \n",
            " Loss:  1.9590049644648158\n",
            "Test accuracy:  88.38\n",
            "Training:  Iteration No:  538 Worker Num:  0 \n",
            " Loss:  3.306793451309204\n",
            "Training:  Iteration No:  538 Worker Num:  1 \n",
            " Loss:  2.8209750652313232\n",
            "Training:  Iteration No:  538 Worker Num:  2 \n",
            " Loss:  3.6746175289154053\n",
            "Training:  Iteration No:  538 Worker Num:  3 \n",
            " Loss:  2.8700907230377197\n",
            "Training:  Iteration No:  538 Worker Num:  4 \n",
            " Loss:  2.9440133571624756\n",
            "Training:  Iteration No:  538 Worker Num:  5 \n",
            " Loss:  2.023057222366333\n",
            "Training:  Iteration No:  538 Worker Num:  6 \n",
            " Loss:  5.270020961761475\n",
            "Training:  Iteration No:  538 Worker Num:  7 \n",
            " Loss:  4.890063285827637\n",
            "Test:  Iteration No:  538 \n",
            " Loss:  1.9432164448275837\n",
            "Test accuracy:  88.21\n",
            "Training:  Iteration No:  539 Worker Num:  0 \n",
            " Loss:  2.7939095497131348\n",
            "Training:  Iteration No:  539 Worker Num:  1 \n",
            " Loss:  2.949963331222534\n",
            "Training:  Iteration No:  539 Worker Num:  2 \n",
            " Loss:  2.8923633098602295\n",
            "Training:  Iteration No:  539 Worker Num:  3 \n",
            " Loss:  4.104757308959961\n",
            "Training:  Iteration No:  539 Worker Num:  4 \n",
            " Loss:  5.248632431030273\n",
            "Training:  Iteration No:  539 Worker Num:  5 \n",
            " Loss:  2.8829591274261475\n",
            "Training:  Iteration No:  539 Worker Num:  6 \n",
            " Loss:  2.0131685733795166\n",
            "Training:  Iteration No:  539 Worker Num:  7 \n",
            " Loss:  3.143115282058716\n",
            "Test:  Iteration No:  539 \n",
            " Loss:  1.9696569911118935\n",
            "Test accuracy:  88.18\n",
            "Training:  Iteration No:  540 Worker Num:  0 \n",
            " Loss:  3.2481272220611572\n",
            "Training:  Iteration No:  540 Worker Num:  1 \n",
            " Loss:  2.6639018058776855\n",
            "Training:  Iteration No:  540 Worker Num:  2 \n",
            " Loss:  3.701690196990967\n",
            "Training:  Iteration No:  540 Worker Num:  3 \n",
            " Loss:  2.3422770500183105\n",
            "Training:  Iteration No:  540 Worker Num:  4 \n",
            " Loss:  2.830845355987549\n",
            "Training:  Iteration No:  540 Worker Num:  5 \n",
            " Loss:  2.458949565887451\n",
            "Training:  Iteration No:  540 Worker Num:  6 \n",
            " Loss:  4.001382350921631\n",
            "Training:  Iteration No:  540 Worker Num:  7 \n",
            " Loss:  5.8438310623168945\n",
            "Test:  Iteration No:  540 \n",
            " Loss:  1.9298694169596318\n",
            "Test accuracy:  88.6\n",
            "Training:  Iteration No:  541 Worker Num:  0 \n",
            " Loss:  4.687952995300293\n",
            "Training:  Iteration No:  541 Worker Num:  1 \n",
            " Loss:  2.7320940494537354\n",
            "Training:  Iteration No:  541 Worker Num:  2 \n",
            " Loss:  3.1325185298919678\n",
            "Training:  Iteration No:  541 Worker Num:  3 \n",
            " Loss:  2.5495798587799072\n",
            "Training:  Iteration No:  541 Worker Num:  4 \n",
            " Loss:  2.1988155841827393\n",
            "Training:  Iteration No:  541 Worker Num:  5 \n",
            " Loss:  3.2599501609802246\n",
            "Training:  Iteration No:  541 Worker Num:  6 \n",
            " Loss:  4.2265706062316895\n",
            "Training:  Iteration No:  541 Worker Num:  7 \n",
            " Loss:  3.375135898590088\n",
            "Test:  Iteration No:  541 \n",
            " Loss:  1.9484549103587676\n",
            "Test accuracy:  88.35\n",
            "Training:  Iteration No:  542 Worker Num:  0 \n",
            " Loss:  3.555009603500366\n",
            "Training:  Iteration No:  542 Worker Num:  1 \n",
            " Loss:  2.151641368865967\n",
            "Training:  Iteration No:  542 Worker Num:  2 \n",
            " Loss:  2.634404182434082\n",
            "Training:  Iteration No:  542 Worker Num:  3 \n",
            " Loss:  3.640040397644043\n",
            "Training:  Iteration No:  542 Worker Num:  4 \n",
            " Loss:  4.0096235275268555\n",
            "Training:  Iteration No:  542 Worker Num:  5 \n",
            " Loss:  4.015897274017334\n",
            "Training:  Iteration No:  542 Worker Num:  6 \n",
            " Loss:  2.599804639816284\n",
            "Training:  Iteration No:  542 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  542 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  543 Worker Num:  0 \n",
            " Loss:  4.17244815826416\n",
            "Training:  Iteration No:  543 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  543 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  543 Worker Num:  3 \n",
            " Loss:  4.8351240158081055\n",
            "Training:  Iteration No:  543 Worker Num:  4 \n",
            " Loss:  2.0715901851654053\n",
            "Training:  Iteration No:  543 Worker Num:  5 \n",
            " Loss:  4.07475471496582\n",
            "Training:  Iteration No:  543 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  543 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  543 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  544 Worker Num:  0 \n",
            " Loss:  3.5092899799346924\n",
            "Training:  Iteration No:  544 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  544 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  544 Worker Num:  3 \n",
            " Loss:  3.3499810695648193\n",
            "Training:  Iteration No:  544 Worker Num:  4 \n",
            " Loss:  3.771904945373535\n",
            "Training:  Iteration No:  544 Worker Num:  5 \n",
            " Loss:  2.823434829711914\n",
            "Training:  Iteration No:  544 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  544 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  544 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  545 Worker Num:  0 \n",
            " Loss:  1.4830855131149292\n",
            "Training:  Iteration No:  545 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  545 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  545 Worker Num:  3 \n",
            " Loss:  2.5234954357147217\n",
            "Training:  Iteration No:  545 Worker Num:  4 \n",
            " Loss:  2.464589834213257\n",
            "Training:  Iteration No:  545 Worker Num:  5 \n",
            " Loss:  3.3775675296783447\n",
            "Training:  Iteration No:  545 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  545 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  545 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  546 Worker Num:  0 \n",
            " Loss:  5.535274982452393\n",
            "Training:  Iteration No:  546 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  546 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  546 Worker Num:  3 \n",
            " Loss:  2.8038957118988037\n",
            "Training:  Iteration No:  546 Worker Num:  4 \n",
            " Loss:  3.00543475151062\n",
            "Training:  Iteration No:  546 Worker Num:  5 \n",
            " Loss:  1.371793508529663\n",
            "Training:  Iteration No:  546 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  546 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  546 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  547 Worker Num:  0 \n",
            " Loss:  2.548367500305176\n",
            "Training:  Iteration No:  547 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  547 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  547 Worker Num:  3 \n",
            " Loss:  4.23623514175415\n",
            "Training:  Iteration No:  547 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  547 Worker Num:  5 \n",
            " Loss:  5.305854797363281\n",
            "Training:  Iteration No:  547 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  547 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  547 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  548 Worker Num:  0 \n",
            " Loss:  4.637595176696777\n",
            "Training:  Iteration No:  548 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  548 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  548 Worker Num:  3 \n",
            " Loss:  2.1416704654693604\n",
            "Training:  Iteration No:  548 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  548 Worker Num:  5 \n",
            " Loss:  3.489929437637329\n",
            "Training:  Iteration No:  548 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  548 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  548 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  549 Worker Num:  0 \n",
            " Loss:  2.9473891258239746\n",
            "Training:  Iteration No:  549 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  549 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  549 Worker Num:  3 \n",
            " Loss:  2.5990777015686035\n",
            "Training:  Iteration No:  549 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  549 Worker Num:  5 \n",
            " Loss:  4.541557788848877\n",
            "Training:  Iteration No:  549 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  549 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  549 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  550 Worker Num:  0 \n",
            " Loss:  3.9895403385162354\n",
            "Training:  Iteration No:  550 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  550 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  550 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  550 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  550 Worker Num:  5 \n",
            " Loss:  3.051254987716675\n",
            "Training:  Iteration No:  550 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  550 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  550 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  551 Worker Num:  0 \n",
            " Loss:  2.0719406604766846\n",
            "Training:  Iteration No:  551 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  551 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  551 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  551 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  551 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  551 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  551 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  551 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  552 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  552 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  552 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  552 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  552 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  552 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  552 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  552 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  552 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  553 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  553 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  553 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  553 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  553 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  553 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  553 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  553 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  553 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  554 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  554 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  554 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  554 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  554 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  554 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  554 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  554 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  554 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  555 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  555 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  555 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  555 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  555 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  555 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  555 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  555 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  555 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  556 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  556 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  556 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  556 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  556 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  556 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  556 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  556 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  556 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  557 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  557 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  557 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  557 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  557 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  557 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  557 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  557 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  557 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  558 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  558 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  558 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  558 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  558 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  558 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  558 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  558 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  558 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  559 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  559 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  559 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  559 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  559 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  559 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  559 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  559 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  559 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  560 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  560 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  560 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  560 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  560 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  560 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  560 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  560 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  560 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  561 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  561 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  561 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  561 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  561 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  561 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  561 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  561 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  561 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  562 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  562 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  562 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  562 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  562 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  562 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  562 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  562 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  562 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  563 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  563 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  563 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  563 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  563 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  563 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  563 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  563 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  563 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  564 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  564 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  564 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  564 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  564 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  564 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  564 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  564 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  564 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  565 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  565 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  565 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  565 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  565 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  565 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  565 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  565 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  565 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  566 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  566 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  566 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  566 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  566 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  566 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  566 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  566 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  566 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  567 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  567 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  567 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  567 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  567 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  567 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  567 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  567 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  567 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  568 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  568 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  568 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  568 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  568 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  568 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  568 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  568 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  568 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  569 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  569 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  569 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  569 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  569 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  569 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  569 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  569 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  569 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  570 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  570 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  570 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  570 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  570 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  570 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  570 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  570 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  570 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  571 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  571 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  571 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  571 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  571 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  571 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  571 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  571 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  571 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  572 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  572 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  572 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  572 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  572 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  572 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  572 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  572 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  572 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  573 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  573 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  573 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  573 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  573 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  573 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  573 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  573 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  573 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  574 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  574 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  574 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  574 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  574 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  574 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  574 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  574 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  574 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  575 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  575 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  575 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  575 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  575 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  575 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  575 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  575 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  575 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  576 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  576 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  576 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  576 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  576 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  576 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  576 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  576 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  576 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  577 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  577 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  577 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  577 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  577 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  577 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  577 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  577 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  577 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  578 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  578 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  578 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  578 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  578 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  578 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  578 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  578 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  578 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  579 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  579 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  579 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  579 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  579 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  579 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  579 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  579 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  579 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  580 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  580 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  580 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  580 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  580 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  580 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  580 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  580 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  580 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  581 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  581 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  581 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  581 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  581 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  581 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  581 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  581 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  581 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  582 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  582 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  582 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  582 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  582 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  582 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  582 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  582 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  582 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  583 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  583 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  583 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  583 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  583 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  583 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  583 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  583 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  583 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  584 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  584 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  584 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  584 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  584 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  584 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  584 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  584 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  584 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  585 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  585 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  585 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  585 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  585 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  585 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  585 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  585 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  585 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  586 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  586 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  586 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  586 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  586 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  586 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  586 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  586 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  586 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  587 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  587 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  587 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  587 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  587 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  587 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  587 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  587 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  587 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  588 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  588 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  588 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  588 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  588 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  588 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  588 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  588 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  588 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  589 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  589 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  589 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  589 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  589 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  589 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  589 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  589 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  589 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  590 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  590 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  590 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  590 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  590 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  590 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  590 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  590 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  590 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  591 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  591 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  591 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  591 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  591 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  591 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  591 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  591 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  591 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  592 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  592 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  592 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  592 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  592 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  592 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  592 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  592 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  592 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  593 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  593 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  593 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  593 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  593 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  593 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  593 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  593 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  593 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  594 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  594 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  594 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  594 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  594 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  594 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  594 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  594 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  594 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  595 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  595 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  595 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  595 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  595 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  595 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  595 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  595 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  595 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  596 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  596 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  596 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  596 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  596 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  596 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  596 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  596 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  596 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  597 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  597 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  597 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  597 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  597 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  597 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  597 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  597 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  597 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  598 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  598 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  598 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  598 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  598 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  598 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  598 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  598 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  598 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  599 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  599 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  599 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  599 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  599 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  599 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  599 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  599 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  599 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  600 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  600 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  600 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  600 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  600 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  600 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  600 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  600 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  600 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  601 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  601 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  601 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  601 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  601 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  601 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  601 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  601 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  601 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  602 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  602 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  602 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  602 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  602 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  602 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  602 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  602 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  602 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  603 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  603 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  603 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  603 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  603 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  603 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  603 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  603 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  603 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  604 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  604 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  604 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  604 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  604 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  604 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  604 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  604 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  604 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  605 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  605 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  605 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  605 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  605 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  605 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  605 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  605 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  605 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  606 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  606 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  606 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  606 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  606 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  606 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  606 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  606 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  606 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  607 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  607 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  607 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  607 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  607 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  607 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  607 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  607 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  607 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  608 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  608 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  608 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  608 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  608 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  608 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  608 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  608 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  608 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  609 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  609 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  609 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  609 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  609 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  609 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  609 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  609 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  609 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  610 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  610 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  610 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  610 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  610 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  610 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  610 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  610 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  610 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  611 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  611 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  611 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  611 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  611 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  611 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  611 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  611 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  611 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  612 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  612 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  612 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  612 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  612 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  612 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  612 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  612 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  612 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  613 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  613 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  613 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  613 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  613 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  613 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  613 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  613 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  613 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  614 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  614 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  614 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  614 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  614 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  614 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  614 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  614 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  614 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  615 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  615 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  615 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  615 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  615 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  615 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  615 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  615 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  615 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  616 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  616 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  616 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  616 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  616 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  616 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  616 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  616 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  616 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  617 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  617 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  617 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  617 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  617 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  617 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  617 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  617 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  617 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  618 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  618 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  618 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  618 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  618 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  618 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  618 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  618 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  618 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  619 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  619 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  619 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  619 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  619 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  619 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  619 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  619 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  619 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  620 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  620 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  620 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  620 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  620 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  620 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  620 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  620 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  620 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  621 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  621 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  621 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  621 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  621 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  621 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  621 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  621 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  621 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  622 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  622 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  622 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  622 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  622 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  622 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  622 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  622 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  622 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  623 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  623 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  623 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  623 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  623 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  623 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  623 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  623 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  623 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  624 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  624 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  624 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  624 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  624 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  624 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  624 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  624 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  624 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  625 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  625 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  625 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  625 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  625 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  625 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  625 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  625 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  625 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  626 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  626 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  626 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  626 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  626 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  626 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  626 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  626 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  626 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  627 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  627 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  627 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  627 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  627 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  627 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  627 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  627 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  627 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  628 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  628 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  628 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  628 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  628 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  628 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  628 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  628 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  628 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  629 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  629 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  629 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  629 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  629 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  629 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  629 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  629 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  629 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  630 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  630 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  630 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  630 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  630 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  630 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  630 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  630 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  630 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  631 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  631 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  631 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  631 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  631 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  631 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  631 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  631 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  631 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  632 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  632 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  632 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  632 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  632 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  632 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  632 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  632 Worker Num:  7 \n",
            " Loss:  nan\n",
            "Test:  Iteration No:  632 \n",
            " Loss:  nan\n",
            "Test accuracy:  9.8\n",
            "Training:  Iteration No:  633 Worker Num:  0 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  633 Worker Num:  1 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  633 Worker Num:  2 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  633 Worker Num:  3 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  633 Worker Num:  4 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  633 Worker Num:  5 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  633 Worker Num:  6 \n",
            " Loss:  nan\n",
            "Training:  Iteration No:  633 Worker Num:  7 \n",
            " Loss:  nan\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-010c152d8c37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mworker_num\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter_no\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworker_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_noise\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter_no\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnets_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m96\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-42705bb4e40f>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(iter_no, networks)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;31m# Transfer to GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;31m# doing this so that it is consistent with all other datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# to return a PIL Image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'L'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   2733\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtostring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2735\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrombuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"raw\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrawmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2736\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfrombuffer\u001b[0;34m(mode, size, data, decoder_name, *args)\u001b[0m\n\u001b[1;32m   2669\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_MAPMODES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2670\u001b[0m             \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2671\u001b[0;31m             \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2672\u001b[0m             \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadonly\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "avg_train_losses = []\n",
        "for i in range(len(train_losses[0])):\n",
        "    avg_loss = 0\n",
        "    for j in range(num_workers):\n",
        "        avg_loss += train_losses[j][i]\n",
        "\n",
        "    avg_train_losses.append(avg_loss / num_workers)    \n"
      ],
      "metadata": {
        "id": "u0KDmdL5gWgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_losses)"
      ],
      "metadata": {
        "id": "ZSQEvIVugWgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(avg_train_losses[0:-20], label='train')\n",
        "plt.plot(test_losses[0:-20], label='test')\n",
        "plt.xlabel('Iteration Number')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Loss function');"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "f610c05d-2d3f-4180-f632-0ef35a43a647",
        "id": "982qj-TNgWgl"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Glv_wuT_mBKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Doubly Stochastic Matrix"
      ],
      "metadata": {
        "id": "t_FMd4dbz8Gm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_workers = 8\n",
        "all_workers = [i for i in range (num_workers)]"
      ],
      "metadata": {
        "id": "ara3iHmJ1DxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nets_list = [CNN().to(device)]\n",
        "optimizers = [optim.SGD(nets_list[0].parameters(), lr = 0.1)]\n",
        "# schedulers = [optim.lr_scheduler.MultiStepLR(optimizers[0], milestones=[210], gamma=0.1)]\n",
        "for i in range(1,num_workers):\n",
        "    new_net = CNN().to(device)\n",
        "    new_net.load_state_dict(nets_list[0].state_dict())\n",
        "    nets_list.append(new_net)\n",
        "    optimizer = optim.SGD(new_net.parameters(), lr = 0.1) \n",
        "    optimizers.append(optimizer)\n",
        "    # scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[210], gamma=0.1)\n",
        "    # schedulers.append(scheduler)"
      ],
      "metadata": {
        "id": "WZnTkE6N1DxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_network = CNN().to(device)"
      ],
      "metadata": {
        "id": "DHZ8nWvW1DxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "queues_list = [[] for i in range (num_workers)]\n",
        "alpha_list = [1 / num_workers for i in range (num_workers)]"
      ],
      "metadata": {
        "id": "DrL-xKIu1DxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_iters = 1000\n",
        "p = 0.5\n",
        "batch_size = 128"
      ],
      "metadata": {
        "id": "Pq-j9jPq1DxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_iters = []\n",
        "for i in range(num_workers):\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "                 dataset=mnist_trainset,\n",
        "                 batch_size=batch_size,\n",
        "                 shuffle=True\n",
        "                 )\n",
        "    \n",
        "    data_iters.append(iter(train_loader))"
      ],
      "metadata": {
        "id": "E-oxRGPg1DxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader = torch.utils.data.DataLoader(\n",
        "                dataset=mnist_testset,\n",
        "                batch_size=batch_size,\n",
        "                shuffle=False\n",
        "                )"
      ],
      "metadata": {
        "id": "gu4mSQfP1DxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "train_losses = [[] for i in range (num_workers)]\n",
        "test_losses = []\n",
        "test_accuracy = []"
      ],
      "metadata": {
        "id": "JTvv0UHE1DxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comm_matrix = []\n",
        "for i in range (num_workers):\n",
        "    adj_nodes = list(range(num_workers))\n",
        "    del adj_nodes[i]\n",
        "    comm_matrix.append(adj_nodes)"
      ],
      "metadata": {
        "id": "sHv8LKmM1DxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for iter_no in range(max_iters):\n",
        "    for worker_num in range(num_workers):\n",
        "        train(iter_no, worker_num, with_noise=False, alpha_update=False)\n",
        "    acc = test(iter_no, nets_list)\n",
        "    if acc >= 96:\n",
        "        break\n",
        "\n",
        "    # for scheduler in schedulers:\n",
        "    #     scheduler.step()    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d4d8ba4-ddd5-4d9f-8570-54d952c6b657",
        "id": "a3DiMbq-1DxU"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training:  Iteration No:  1 Worker Num:  0 \n",
            " Loss:  2.3060126304626465\n",
            "Training:  Iteration No:  1 Worker Num:  1 \n",
            " Loss:  2.311095952987671\n",
            "Training:  Iteration No:  1 Worker Num:  2 \n",
            " Loss:  2.306824207305908\n",
            "Training:  Iteration No:  1 Worker Num:  3 \n",
            " Loss:  2.3151047229766846\n",
            "Training:  Iteration No:  1 Worker Num:  4 \n",
            " Loss:  2.306621789932251\n",
            "Training:  Iteration No:  1 Worker Num:  5 \n",
            " Loss:  2.3119373321533203\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            " Loss:  0.43747336389143254\n",
            "Test accuracy:  88.2\n",
            "Training:  Iteration No:  73 Worker Num:  0 \n",
            " Loss:  0.4474974572658539\n",
            "Training:  Iteration No:  73 Worker Num:  1 \n",
            " Loss:  0.4420727789402008\n",
            "Training:  Iteration No:  73 Worker Num:  2 \n",
            " Loss:  0.5361692309379578\n",
            "Training:  Iteration No:  73 Worker Num:  3 \n",
            " Loss:  0.469364732503891\n",
            "Training:  Iteration No:  73 Worker Num:  4 \n",
            " Loss:  0.3839412331581116\n",
            "Training:  Iteration No:  73 Worker Num:  5 \n",
            " Loss:  0.4154209494590759\n",
            "Training:  Iteration No:  73 Worker Num:  6 \n",
            " Loss:  0.5448828935623169\n",
            "Training:  Iteration No:  73 Worker Num:  7 \n",
            " Loss:  0.4542374014854431\n",
            "Test:  Iteration No:  73 \n",
            " Loss:  0.42674975578166263\n",
            "Test accuracy:  88.62\n",
            "Training:  Iteration No:  74 Worker Num:  0 \n",
            " Loss:  0.6254955530166626\n",
            "Training:  Iteration No:  74 Worker Num:  1 \n",
            " Loss:  0.7226269245147705\n",
            "Training:  Iteration No:  74 Worker Num:  2 \n",
            " Loss:  0.5301005840301514\n",
            "Training:  Iteration No:  74 Worker Num:  3 \n",
            " Loss:  0.5322631001472473\n",
            "Training:  Iteration No:  74 Worker Num:  4 \n",
            " Loss:  0.5057770013809204\n",
            "Training:  Iteration No:  74 Worker Num:  5 \n",
            " Loss:  0.4663987159729004\n",
            "Training:  Iteration No:  74 Worker Num:  6 \n",
            " Loss:  0.49641650915145874\n",
            "Training:  Iteration No:  74 Worker Num:  7 \n",
            " Loss:  0.44274476170539856\n",
            "Test:  Iteration No:  74 \n",
            " Loss:  0.4245749581840974\n",
            "Test accuracy:  88.52\n",
            "Training:  Iteration No:  75 Worker Num:  0 \n",
            " Loss:  0.4118691682815552\n",
            "Training:  Iteration No:  75 Worker Num:  1 \n",
            " Loss:  0.5369216203689575\n",
            "Training:  Iteration No:  75 Worker Num:  2 \n",
            " Loss:  0.47949111461639404\n",
            "Training:  Iteration No:  75 Worker Num:  3 \n",
            " Loss:  0.6182002425193787\n",
            "Training:  Iteration No:  75 Worker Num:  4 \n",
            " Loss:  0.5585569739341736\n",
            "Training:  Iteration No:  75 Worker Num:  5 \n",
            " Loss:  0.5936570167541504\n",
            "Training:  Iteration No:  75 Worker Num:  6 \n",
            " Loss:  0.39799922704696655\n",
            "Training:  Iteration No:  75 Worker Num:  7 \n",
            " Loss:  0.6667909622192383\n",
            "Test:  Iteration No:  75 \n",
            " Loss:  0.4321796961977512\n",
            "Test accuracy:  88.3\n",
            "Training:  Iteration No:  76 Worker Num:  0 \n",
            " Loss:  0.5754069089889526\n",
            "Training:  Iteration No:  76 Worker Num:  1 \n",
            " Loss:  0.45662304759025574\n",
            "Training:  Iteration No:  76 Worker Num:  2 \n",
            " Loss:  0.4781879782676697\n",
            "Training:  Iteration No:  76 Worker Num:  3 \n",
            " Loss:  0.5531743168830872\n",
            "Training:  Iteration No:  76 Worker Num:  4 \n",
            " Loss:  0.5450670123100281\n",
            "Training:  Iteration No:  76 Worker Num:  5 \n",
            " Loss:  0.38544461131095886\n",
            "Training:  Iteration No:  76 Worker Num:  6 \n",
            " Loss:  0.445533812046051\n",
            "Training:  Iteration No:  76 Worker Num:  7 \n",
            " Loss:  0.4460485577583313\n",
            "Test:  Iteration No:  76 \n",
            " Loss:  0.4199326202461991\n",
            "Test accuracy:  88.66\n",
            "Training:  Iteration No:  77 Worker Num:  0 \n",
            " Loss:  0.523821234703064\n",
            "Training:  Iteration No:  77 Worker Num:  1 \n",
            " Loss:  0.39601126313209534\n",
            "Training:  Iteration No:  77 Worker Num:  2 \n",
            " Loss:  0.5934885740280151\n",
            "Training:  Iteration No:  77 Worker Num:  3 \n",
            " Loss:  0.49854549765586853\n",
            "Training:  Iteration No:  77 Worker Num:  4 \n",
            " Loss:  0.48909834027290344\n",
            "Training:  Iteration No:  77 Worker Num:  5 \n",
            " Loss:  0.4852072298526764\n",
            "Training:  Iteration No:  77 Worker Num:  6 \n",
            " Loss:  0.5676432847976685\n",
            "Training:  Iteration No:  77 Worker Num:  7 \n",
            " Loss:  0.5111278295516968\n",
            "Test:  Iteration No:  77 \n",
            " Loss:  0.4167816787958145\n",
            "Test accuracy:  88.76\n",
            "Training:  Iteration No:  78 Worker Num:  0 \n",
            " Loss:  0.5496574640274048\n",
            "Training:  Iteration No:  78 Worker Num:  1 \n",
            " Loss:  0.5661752820014954\n",
            "Training:  Iteration No:  78 Worker Num:  2 \n",
            " Loss:  0.4028926491737366\n",
            "Training:  Iteration No:  78 Worker Num:  3 \n",
            " Loss:  0.5537092685699463\n",
            "Training:  Iteration No:  78 Worker Num:  4 \n",
            " Loss:  0.5900773406028748\n",
            "Training:  Iteration No:  78 Worker Num:  5 \n",
            " Loss:  0.6731957793235779\n",
            "Training:  Iteration No:  78 Worker Num:  6 \n",
            " Loss:  0.6028388142585754\n",
            "Training:  Iteration No:  78 Worker Num:  7 \n",
            " Loss:  0.41505298018455505\n",
            "Test:  Iteration No:  78 \n",
            " Loss:  0.4130974005483374\n",
            "Test accuracy:  88.9\n",
            "Training:  Iteration No:  79 Worker Num:  0 \n",
            " Loss:  0.4995676875114441\n",
            "Training:  Iteration No:  79 Worker Num:  1 \n",
            " Loss:  0.5638786554336548\n",
            "Training:  Iteration No:  79 Worker Num:  2 \n",
            " Loss:  0.6531082391738892\n",
            "Training:  Iteration No:  79 Worker Num:  3 \n",
            " Loss:  0.5925293564796448\n",
            "Training:  Iteration No:  79 Worker Num:  4 \n",
            " Loss:  0.43765342235565186\n",
            "Training:  Iteration No:  79 Worker Num:  5 \n",
            " Loss:  0.397836297750473\n",
            "Training:  Iteration No:  79 Worker Num:  6 \n",
            " Loss:  0.43674516677856445\n",
            "Training:  Iteration No:  79 Worker Num:  7 \n",
            " Loss:  0.5910208225250244\n",
            "Test:  Iteration No:  79 \n",
            " Loss:  0.4111707699072512\n",
            "Test accuracy:  88.9\n",
            "Training:  Iteration No:  80 Worker Num:  0 \n",
            " Loss:  0.5651131272315979\n",
            "Training:  Iteration No:  80 Worker Num:  1 \n",
            " Loss:  0.5483720302581787\n",
            "Training:  Iteration No:  80 Worker Num:  2 \n",
            " Loss:  0.47638624906539917\n",
            "Training:  Iteration No:  80 Worker Num:  3 \n",
            " Loss:  0.38467586040496826\n",
            "Training:  Iteration No:  80 Worker Num:  4 \n",
            " Loss:  0.4265756905078888\n",
            "Training:  Iteration No:  80 Worker Num:  5 \n",
            " Loss:  0.4676920771598816\n",
            "Training:  Iteration No:  80 Worker Num:  6 \n",
            " Loss:  0.4566913843154907\n",
            "Training:  Iteration No:  80 Worker Num:  7 \n",
            " Loss:  0.5416685342788696\n",
            "Test:  Iteration No:  80 \n",
            " Loss:  0.40291919785587094\n",
            "Test accuracy:  89.09\n",
            "Training:  Iteration No:  81 Worker Num:  0 \n",
            " Loss:  0.4842659533023834\n",
            "Training:  Iteration No:  81 Worker Num:  1 \n",
            " Loss:  0.4342458248138428\n",
            "Training:  Iteration No:  81 Worker Num:  2 \n",
            " Loss:  0.5004569888114929\n",
            "Training:  Iteration No:  81 Worker Num:  3 \n",
            " Loss:  0.30527815222740173\n",
            "Training:  Iteration No:  81 Worker Num:  4 \n",
            " Loss:  0.5048918724060059\n",
            "Training:  Iteration No:  81 Worker Num:  5 \n",
            " Loss:  0.5063489675521851\n",
            "Training:  Iteration No:  81 Worker Num:  6 \n",
            " Loss:  0.5212236642837524\n",
            "Training:  Iteration No:  81 Worker Num:  7 \n",
            " Loss:  0.4155438244342804\n",
            "Test:  Iteration No:  81 \n",
            " Loss:  0.4061238021601605\n",
            "Test accuracy:  88.89\n",
            "Training:  Iteration No:  82 Worker Num:  0 \n",
            " Loss:  0.4761897027492523\n",
            "Training:  Iteration No:  82 Worker Num:  1 \n",
            " Loss:  0.49810948967933655\n",
            "Training:  Iteration No:  82 Worker Num:  2 \n",
            " Loss:  0.3764210045337677\n",
            "Training:  Iteration No:  82 Worker Num:  3 \n",
            " Loss:  0.5786880254745483\n",
            "Training:  Iteration No:  82 Worker Num:  4 \n",
            " Loss:  0.4458538293838501\n",
            "Training:  Iteration No:  82 Worker Num:  5 \n",
            " Loss:  0.616958498954773\n",
            "Training:  Iteration No:  82 Worker Num:  6 \n",
            " Loss:  0.4689991772174835\n",
            "Training:  Iteration No:  82 Worker Num:  7 \n",
            " Loss:  0.4944726526737213\n",
            "Test:  Iteration No:  82 \n",
            " Loss:  0.3971369295746465\n",
            "Test accuracy:  89.35\n",
            "Training:  Iteration No:  83 Worker Num:  0 \n",
            " Loss:  0.41669198870658875\n",
            "Training:  Iteration No:  83 Worker Num:  1 \n",
            " Loss:  0.5676426291465759\n",
            "Training:  Iteration No:  83 Worker Num:  2 \n",
            " Loss:  0.5145389437675476\n",
            "Training:  Iteration No:  83 Worker Num:  3 \n",
            " Loss:  0.5014685988426208\n",
            "Training:  Iteration No:  83 Worker Num:  4 \n",
            " Loss:  0.5736615061759949\n",
            "Training:  Iteration No:  83 Worker Num:  5 \n",
            " Loss:  0.4774278700351715\n",
            "Training:  Iteration No:  83 Worker Num:  6 \n",
            " Loss:  0.5863285660743713\n",
            "Training:  Iteration No:  83 Worker Num:  7 \n",
            " Loss:  0.5421983003616333\n",
            "Test:  Iteration No:  83 \n",
            " Loss:  0.398730470315565\n",
            "Test accuracy:  89.13\n",
            "Training:  Iteration No:  84 Worker Num:  0 \n",
            " Loss:  0.5367094874382019\n",
            "Training:  Iteration No:  84 Worker Num:  1 \n",
            " Loss:  0.653898298740387\n",
            "Training:  Iteration No:  84 Worker Num:  2 \n",
            " Loss:  0.455747127532959\n",
            "Training:  Iteration No:  84 Worker Num:  3 \n",
            " Loss:  0.4870016872882843\n",
            "Training:  Iteration No:  84 Worker Num:  4 \n",
            " Loss:  0.6107865571975708\n",
            "Training:  Iteration No:  84 Worker Num:  5 \n",
            " Loss:  0.4500575363636017\n",
            "Training:  Iteration No:  84 Worker Num:  6 \n",
            " Loss:  0.3179284930229187\n",
            "Training:  Iteration No:  84 Worker Num:  7 \n",
            " Loss:  0.3873255252838135\n",
            "Test:  Iteration No:  84 \n",
            " Loss:  0.40083626601137695\n",
            "Test accuracy:  88.93\n",
            "Training:  Iteration No:  85 Worker Num:  0 \n",
            " Loss:  0.482429563999176\n",
            "Training:  Iteration No:  85 Worker Num:  1 \n",
            " Loss:  0.3479476869106293\n",
            "Training:  Iteration No:  85 Worker Num:  2 \n",
            " Loss:  0.5139821171760559\n",
            "Training:  Iteration No:  85 Worker Num:  3 \n",
            " Loss:  0.41238582134246826\n",
            "Training:  Iteration No:  85 Worker Num:  4 \n",
            " Loss:  0.5074554681777954\n",
            "Training:  Iteration No:  85 Worker Num:  5 \n",
            " Loss:  0.4213823676109314\n",
            "Training:  Iteration No:  85 Worker Num:  6 \n",
            " Loss:  0.4753078520298004\n",
            "Training:  Iteration No:  85 Worker Num:  7 \n",
            " Loss:  0.46550896763801575\n",
            "Test:  Iteration No:  85 \n",
            " Loss:  0.39408548902484436\n",
            "Test accuracy:  89.08\n",
            "Training:  Iteration No:  86 Worker Num:  0 \n",
            " Loss:  0.3187966048717499\n",
            "Training:  Iteration No:  86 Worker Num:  1 \n",
            " Loss:  0.4683815836906433\n",
            "Training:  Iteration No:  86 Worker Num:  2 \n",
            " Loss:  0.42816126346588135\n",
            "Training:  Iteration No:  86 Worker Num:  3 \n",
            " Loss:  0.5465810894966125\n",
            "Training:  Iteration No:  86 Worker Num:  4 \n",
            " Loss:  0.5780634880065918\n",
            "Training:  Iteration No:  86 Worker Num:  5 \n",
            " Loss:  0.4431544542312622\n",
            "Training:  Iteration No:  86 Worker Num:  6 \n",
            " Loss:  0.3319024443626404\n",
            "Training:  Iteration No:  86 Worker Num:  7 \n",
            " Loss:  0.5191981196403503\n",
            "Test:  Iteration No:  86 \n",
            " Loss:  0.39374141989252237\n",
            "Test accuracy:  89.34\n",
            "Training:  Iteration No:  87 Worker Num:  0 \n",
            " Loss:  0.5331010222434998\n",
            "Training:  Iteration No:  87 Worker Num:  1 \n",
            " Loss:  0.4796299338340759\n",
            "Training:  Iteration No:  87 Worker Num:  2 \n",
            " Loss:  0.3393450379371643\n",
            "Training:  Iteration No:  87 Worker Num:  3 \n",
            " Loss:  0.5001703500747681\n",
            "Training:  Iteration No:  87 Worker Num:  4 \n",
            " Loss:  0.466682493686676\n",
            "Training:  Iteration No:  87 Worker Num:  5 \n",
            " Loss:  0.3980700969696045\n",
            "Training:  Iteration No:  87 Worker Num:  6 \n",
            " Loss:  0.4020940661430359\n",
            "Training:  Iteration No:  87 Worker Num:  7 \n",
            " Loss:  0.44013139605522156\n",
            "Test:  Iteration No:  87 \n",
            " Loss:  0.3861843043867546\n",
            "Test accuracy:  89.54\n",
            "Training:  Iteration No:  88 Worker Num:  0 \n",
            " Loss:  0.5025558471679688\n",
            "Training:  Iteration No:  88 Worker Num:  1 \n",
            " Loss:  0.4033254384994507\n",
            "Training:  Iteration No:  88 Worker Num:  2 \n",
            " Loss:  0.5655684471130371\n",
            "Training:  Iteration No:  88 Worker Num:  3 \n",
            " Loss:  0.38332128524780273\n",
            "Training:  Iteration No:  88 Worker Num:  4 \n",
            " Loss:  0.5423022508621216\n",
            "Training:  Iteration No:  88 Worker Num:  5 \n",
            " Loss:  0.5127626657485962\n",
            "Training:  Iteration No:  88 Worker Num:  6 \n",
            " Loss:  0.476972758769989\n",
            "Training:  Iteration No:  88 Worker Num:  7 \n",
            " Loss:  0.5612745881080627\n",
            "Test:  Iteration No:  88 \n",
            " Loss:  0.38678894314584855\n",
            "Test accuracy:  89.27\n",
            "Training:  Iteration No:  89 Worker Num:  0 \n",
            " Loss:  0.44799432158470154\n",
            "Training:  Iteration No:  89 Worker Num:  1 \n",
            " Loss:  0.3816051781177521\n",
            "Training:  Iteration No:  89 Worker Num:  2 \n",
            " Loss:  0.4265907406806946\n",
            "Training:  Iteration No:  89 Worker Num:  3 \n",
            " Loss:  0.5019102096557617\n",
            "Training:  Iteration No:  89 Worker Num:  4 \n",
            " Loss:  0.41142234206199646\n",
            "Training:  Iteration No:  89 Worker Num:  5 \n",
            " Loss:  0.42934754490852356\n",
            "Training:  Iteration No:  89 Worker Num:  6 \n",
            " Loss:  0.6029109358787537\n",
            "Training:  Iteration No:  89 Worker Num:  7 \n",
            " Loss:  0.39799410104751587\n",
            "Test:  Iteration No:  89 \n",
            " Loss:  0.3817209308094616\n",
            "Test accuracy:  89.56\n",
            "Training:  Iteration No:  90 Worker Num:  0 \n",
            " Loss:  0.5303055644035339\n",
            "Training:  Iteration No:  90 Worker Num:  1 \n",
            " Loss:  0.4673958420753479\n",
            "Training:  Iteration No:  90 Worker Num:  2 \n",
            " Loss:  0.5460878610610962\n",
            "Training:  Iteration No:  90 Worker Num:  3 \n",
            " Loss:  0.5361872911453247\n",
            "Training:  Iteration No:  90 Worker Num:  4 \n",
            " Loss:  0.4000842273235321\n",
            "Training:  Iteration No:  90 Worker Num:  5 \n",
            " Loss:  0.5140734314918518\n",
            "Training:  Iteration No:  90 Worker Num:  6 \n",
            " Loss:  0.3744182586669922\n",
            "Training:  Iteration No:  90 Worker Num:  7 \n",
            " Loss:  0.4445253908634186\n",
            "Test:  Iteration No:  90 \n",
            " Loss:  0.3793339344519603\n",
            "Test accuracy:  89.71\n",
            "Training:  Iteration No:  91 Worker Num:  0 \n",
            " Loss:  0.33506932854652405\n",
            "Training:  Iteration No:  91 Worker Num:  1 \n",
            " Loss:  0.5221495032310486\n",
            "Training:  Iteration No:  91 Worker Num:  2 \n",
            " Loss:  0.5860040783882141\n",
            "Training:  Iteration No:  91 Worker Num:  3 \n",
            " Loss:  0.39651399850845337\n",
            "Training:  Iteration No:  91 Worker Num:  4 \n",
            " Loss:  0.4711924195289612\n",
            "Training:  Iteration No:  91 Worker Num:  5 \n",
            " Loss:  0.4096505343914032\n",
            "Training:  Iteration No:  91 Worker Num:  6 \n",
            " Loss:  0.4210362136363983\n",
            "Training:  Iteration No:  91 Worker Num:  7 \n",
            " Loss:  0.4244877099990845\n",
            "Test:  Iteration No:  91 \n",
            " Loss:  0.38015732178582423\n",
            "Test accuracy:  89.38\n",
            "Training:  Iteration No:  92 Worker Num:  0 \n",
            " Loss:  0.46034982800483704\n",
            "Training:  Iteration No:  92 Worker Num:  1 \n",
            " Loss:  0.41667214035987854\n",
            "Training:  Iteration No:  92 Worker Num:  2 \n",
            " Loss:  0.3853757381439209\n",
            "Training:  Iteration No:  92 Worker Num:  3 \n",
            " Loss:  0.3639056086540222\n",
            "Training:  Iteration No:  92 Worker Num:  4 \n",
            " Loss:  0.37937530875205994\n",
            "Training:  Iteration No:  92 Worker Num:  5 \n",
            " Loss:  0.4977559447288513\n",
            "Training:  Iteration No:  92 Worker Num:  6 \n",
            " Loss:  0.477551132440567\n",
            "Training:  Iteration No:  92 Worker Num:  7 \n",
            " Loss:  0.4607952833175659\n",
            "Test:  Iteration No:  92 \n",
            " Loss:  0.3899298500788363\n",
            "Test accuracy:  88.96\n",
            "Training:  Iteration No:  93 Worker Num:  0 \n",
            " Loss:  0.3274441957473755\n",
            "Training:  Iteration No:  93 Worker Num:  1 \n",
            " Loss:  0.6567114591598511\n",
            "Training:  Iteration No:  93 Worker Num:  2 \n",
            " Loss:  0.4522428810596466\n",
            "Training:  Iteration No:  93 Worker Num:  3 \n",
            " Loss:  0.3399415612220764\n",
            "Training:  Iteration No:  93 Worker Num:  4 \n",
            " Loss:  0.6070432662963867\n",
            "Training:  Iteration No:  93 Worker Num:  5 \n",
            " Loss:  0.5467815399169922\n",
            "Training:  Iteration No:  93 Worker Num:  6 \n",
            " Loss:  0.6041161417961121\n",
            "Training:  Iteration No:  93 Worker Num:  7 \n",
            " Loss:  0.5054259300231934\n",
            "Test:  Iteration No:  93 \n",
            " Loss:  0.3742779881893834\n",
            "Test accuracy:  89.48\n",
            "Training:  Iteration No:  94 Worker Num:  0 \n",
            " Loss:  0.5159415602684021\n",
            "Training:  Iteration No:  94 Worker Num:  1 \n",
            " Loss:  0.4113498032093048\n",
            "Training:  Iteration No:  94 Worker Num:  2 \n",
            " Loss:  0.3848215639591217\n",
            "Training:  Iteration No:  94 Worker Num:  3 \n",
            " Loss:  0.432056725025177\n",
            "Training:  Iteration No:  94 Worker Num:  4 \n",
            " Loss:  0.61295485496521\n",
            "Training:  Iteration No:  94 Worker Num:  5 \n",
            " Loss:  0.339299738407135\n",
            "Training:  Iteration No:  94 Worker Num:  6 \n",
            " Loss:  0.321056991815567\n",
            "Training:  Iteration No:  94 Worker Num:  7 \n",
            " Loss:  0.4628843069076538\n",
            "Test:  Iteration No:  94 \n",
            " Loss:  0.37838440463890005\n",
            "Test accuracy:  89.36\n",
            "Training:  Iteration No:  95 Worker Num:  0 \n",
            " Loss:  0.3991909325122833\n",
            "Training:  Iteration No:  95 Worker Num:  1 \n",
            " Loss:  0.5345273017883301\n",
            "Training:  Iteration No:  95 Worker Num:  2 \n",
            " Loss:  0.5931915044784546\n",
            "Training:  Iteration No:  95 Worker Num:  3 \n",
            " Loss:  0.41544032096862793\n",
            "Training:  Iteration No:  95 Worker Num:  4 \n",
            " Loss:  0.43586665391921997\n",
            "Training:  Iteration No:  95 Worker Num:  5 \n",
            " Loss:  0.5575566291809082\n",
            "Training:  Iteration No:  95 Worker Num:  6 \n",
            " Loss:  0.42586207389831543\n",
            "Training:  Iteration No:  95 Worker Num:  7 \n",
            " Loss:  0.4870622158050537\n",
            "Test:  Iteration No:  95 \n",
            " Loss:  0.3972268837350833\n",
            "Test accuracy:  88.5\n",
            "Training:  Iteration No:  96 Worker Num:  0 \n",
            " Loss:  0.29775893688201904\n",
            "Training:  Iteration No:  96 Worker Num:  1 \n",
            " Loss:  0.41171446442604065\n",
            "Training:  Iteration No:  96 Worker Num:  2 \n",
            " Loss:  0.40377137064933777\n",
            "Training:  Iteration No:  96 Worker Num:  3 \n",
            " Loss:  0.6325880885124207\n",
            "Training:  Iteration No:  96 Worker Num:  4 \n",
            " Loss:  0.5186496376991272\n",
            "Training:  Iteration No:  96 Worker Num:  5 \n",
            " Loss:  0.41890859603881836\n",
            "Training:  Iteration No:  96 Worker Num:  6 \n",
            " Loss:  0.3941279351711273\n",
            "Training:  Iteration No:  96 Worker Num:  7 \n",
            " Loss:  0.5093508362770081\n",
            "Test:  Iteration No:  96 \n",
            " Loss:  0.39276367397625234\n",
            "Test accuracy:  88.41\n",
            "Training:  Iteration No:  97 Worker Num:  0 \n",
            " Loss:  0.3433692455291748\n",
            "Training:  Iteration No:  97 Worker Num:  1 \n",
            " Loss:  0.2814357876777649\n",
            "Training:  Iteration No:  97 Worker Num:  2 \n",
            " Loss:  0.4666527211666107\n",
            "Training:  Iteration No:  97 Worker Num:  3 \n",
            " Loss:  0.4012303054332733\n",
            "Training:  Iteration No:  97 Worker Num:  4 \n",
            " Loss:  0.43744802474975586\n",
            "Training:  Iteration No:  97 Worker Num:  5 \n",
            " Loss:  0.4551205039024353\n",
            "Training:  Iteration No:  97 Worker Num:  6 \n",
            " Loss:  0.4663878381252289\n",
            "Training:  Iteration No:  97 Worker Num:  7 \n",
            " Loss:  0.3807661235332489\n",
            "Test:  Iteration No:  97 \n",
            " Loss:  0.3845529631723332\n",
            "Test accuracy:  89.13\n",
            "Training:  Iteration No:  98 Worker Num:  0 \n",
            " Loss:  0.380248099565506\n",
            "Training:  Iteration No:  98 Worker Num:  1 \n",
            " Loss:  0.4565812349319458\n",
            "Training:  Iteration No:  98 Worker Num:  2 \n",
            " Loss:  0.4219694435596466\n",
            "Training:  Iteration No:  98 Worker Num:  3 \n",
            " Loss:  0.3230847716331482\n",
            "Training:  Iteration No:  98 Worker Num:  4 \n",
            " Loss:  0.5947937965393066\n",
            "Training:  Iteration No:  98 Worker Num:  5 \n",
            " Loss:  0.4174550175666809\n",
            "Training:  Iteration No:  98 Worker Num:  6 \n",
            " Loss:  0.4023210406303406\n",
            "Training:  Iteration No:  98 Worker Num:  7 \n",
            " Loss:  0.4869023561477661\n",
            "Test:  Iteration No:  98 \n",
            " Loss:  0.3696699893172783\n",
            "Test accuracy:  89.52\n",
            "Training:  Iteration No:  99 Worker Num:  0 \n",
            " Loss:  0.6005852818489075\n",
            "Training:  Iteration No:  99 Worker Num:  1 \n",
            " Loss:  0.34186458587646484\n",
            "Training:  Iteration No:  99 Worker Num:  2 \n",
            " Loss:  0.35127773880958557\n",
            "Training:  Iteration No:  99 Worker Num:  3 \n",
            " Loss:  0.4422743022441864\n",
            "Training:  Iteration No:  99 Worker Num:  4 \n",
            " Loss:  0.3906565308570862\n",
            "Training:  Iteration No:  99 Worker Num:  5 \n",
            " Loss:  0.35841211676597595\n",
            "Training:  Iteration No:  99 Worker Num:  6 \n",
            " Loss:  0.3475881814956665\n",
            "Training:  Iteration No:  99 Worker Num:  7 \n",
            " Loss:  0.4632628262042999\n",
            "Test:  Iteration No:  99 \n",
            " Loss:  0.35940635968235474\n",
            "Test accuracy:  90.15\n",
            "Training:  Iteration No:  100 Worker Num:  0 \n",
            " Loss:  0.43710219860076904\n",
            "Training:  Iteration No:  100 Worker Num:  1 \n",
            " Loss:  0.42389172315597534\n",
            "Training:  Iteration No:  100 Worker Num:  2 \n",
            " Loss:  0.46551254391670227\n",
            "Training:  Iteration No:  100 Worker Num:  3 \n",
            " Loss:  0.2424311488866806\n",
            "Training:  Iteration No:  100 Worker Num:  4 \n",
            " Loss:  0.46229562163352966\n",
            "Training:  Iteration No:  100 Worker Num:  5 \n",
            " Loss:  0.35188060998916626\n",
            "Training:  Iteration No:  100 Worker Num:  6 \n",
            " Loss:  0.32585954666137695\n",
            "Training:  Iteration No:  100 Worker Num:  7 \n",
            " Loss:  0.367306113243103\n",
            "Test:  Iteration No:  100 \n",
            " Loss:  0.3599310273045226\n",
            "Test accuracy:  89.88\n",
            "Training:  Iteration No:  101 Worker Num:  0 \n",
            " Loss:  0.4807351529598236\n",
            "Training:  Iteration No:  101 Worker Num:  1 \n",
            " Loss:  0.4858797788619995\n",
            "Training:  Iteration No:  101 Worker Num:  2 \n",
            " Loss:  0.5392636656761169\n",
            "Training:  Iteration No:  101 Worker Num:  3 \n",
            " Loss:  0.45384731888771057\n",
            "Training:  Iteration No:  101 Worker Num:  4 \n",
            " Loss:  0.46913671493530273\n",
            "Training:  Iteration No:  101 Worker Num:  5 \n",
            " Loss:  0.5776814818382263\n",
            "Training:  Iteration No:  101 Worker Num:  6 \n",
            " Loss:  0.4229952096939087\n",
            "Training:  Iteration No:  101 Worker Num:  7 \n",
            " Loss:  0.3737753927707672\n",
            "Test:  Iteration No:  101 \n",
            " Loss:  0.3536880581250674\n",
            "Test accuracy:  90.2\n",
            "Training:  Iteration No:  102 Worker Num:  0 \n",
            " Loss:  0.49523797631263733\n",
            "Training:  Iteration No:  102 Worker Num:  1 \n",
            " Loss:  0.4979928731918335\n",
            "Training:  Iteration No:  102 Worker Num:  2 \n",
            " Loss:  0.29004767537117004\n",
            "Training:  Iteration No:  102 Worker Num:  3 \n",
            " Loss:  0.49523958563804626\n",
            "Training:  Iteration No:  102 Worker Num:  4 \n",
            " Loss:  0.29649344086647034\n",
            "Training:  Iteration No:  102 Worker Num:  5 \n",
            " Loss:  0.4631184935569763\n",
            "Training:  Iteration No:  102 Worker Num:  6 \n",
            " Loss:  0.4719296395778656\n",
            "Training:  Iteration No:  102 Worker Num:  7 \n",
            " Loss:  0.46389153599739075\n",
            "Test:  Iteration No:  102 \n",
            " Loss:  0.3500624396661414\n",
            "Test accuracy:  90.32\n",
            "Training:  Iteration No:  103 Worker Num:  0 \n",
            " Loss:  0.36679160594940186\n",
            "Training:  Iteration No:  103 Worker Num:  1 \n",
            " Loss:  0.4636175036430359\n",
            "Training:  Iteration No:  103 Worker Num:  2 \n",
            " Loss:  0.3935817778110504\n",
            "Training:  Iteration No:  103 Worker Num:  3 \n",
            " Loss:  0.345490038394928\n",
            "Training:  Iteration No:  103 Worker Num:  4 \n",
            " Loss:  0.585249125957489\n",
            "Training:  Iteration No:  103 Worker Num:  5 \n",
            " Loss:  0.5737175941467285\n",
            "Training:  Iteration No:  103 Worker Num:  6 \n",
            " Loss:  0.48915809392929077\n",
            "Training:  Iteration No:  103 Worker Num:  7 \n",
            " Loss:  0.25021177530288696\n",
            "Test:  Iteration No:  103 \n",
            " Loss:  0.3552500564086286\n",
            "Test accuracy:  89.62\n",
            "Training:  Iteration No:  104 Worker Num:  0 \n",
            " Loss:  0.4872874915599823\n",
            "Training:  Iteration No:  104 Worker Num:  1 \n",
            " Loss:  0.487528532743454\n",
            "Training:  Iteration No:  104 Worker Num:  2 \n",
            " Loss:  0.34721872210502625\n",
            "Training:  Iteration No:  104 Worker Num:  3 \n",
            " Loss:  0.4641692340373993\n",
            "Training:  Iteration No:  104 Worker Num:  4 \n",
            " Loss:  0.3817302882671356\n",
            "Training:  Iteration No:  104 Worker Num:  5 \n",
            " Loss:  0.5710813999176025\n",
            "Training:  Iteration No:  104 Worker Num:  6 \n",
            " Loss:  0.396173894405365\n",
            "Training:  Iteration No:  104 Worker Num:  7 \n",
            " Loss:  0.354586124420166\n",
            "Test:  Iteration No:  104 \n",
            " Loss:  0.35140090992179096\n",
            "Test accuracy:  89.97\n",
            "Training:  Iteration No:  105 Worker Num:  0 \n",
            " Loss:  0.33771857619285583\n",
            "Training:  Iteration No:  105 Worker Num:  1 \n",
            " Loss:  0.506747841835022\n",
            "Training:  Iteration No:  105 Worker Num:  2 \n",
            " Loss:  0.3320683538913727\n",
            "Training:  Iteration No:  105 Worker Num:  3 \n",
            " Loss:  0.5800204277038574\n",
            "Training:  Iteration No:  105 Worker Num:  4 \n",
            " Loss:  0.4742956757545471\n",
            "Training:  Iteration No:  105 Worker Num:  5 \n",
            " Loss:  0.3977203071117401\n",
            "Training:  Iteration No:  105 Worker Num:  6 \n",
            " Loss:  0.3931259214878082\n",
            "Training:  Iteration No:  105 Worker Num:  7 \n",
            " Loss:  0.43986976146698\n",
            "Test:  Iteration No:  105 \n",
            " Loss:  0.3481573983083797\n",
            "Test accuracy:  90.36\n",
            "Training:  Iteration No:  106 Worker Num:  0 \n",
            " Loss:  0.2967257499694824\n",
            "Training:  Iteration No:  106 Worker Num:  1 \n",
            " Loss:  0.5558749437332153\n",
            "Training:  Iteration No:  106 Worker Num:  2 \n",
            " Loss:  0.5400373935699463\n",
            "Training:  Iteration No:  106 Worker Num:  3 \n",
            " Loss:  0.3561222553253174\n",
            "Training:  Iteration No:  106 Worker Num:  4 \n",
            " Loss:  0.5614292025566101\n",
            "Training:  Iteration No:  106 Worker Num:  5 \n",
            " Loss:  0.4520101547241211\n",
            "Training:  Iteration No:  106 Worker Num:  6 \n",
            " Loss:  0.3557153344154358\n",
            "Training:  Iteration No:  106 Worker Num:  7 \n",
            " Loss:  0.2356823980808258\n",
            "Test:  Iteration No:  106 \n",
            " Loss:  0.3602870646156842\n",
            "Test accuracy:  89.84\n",
            "Training:  Iteration No:  107 Worker Num:  0 \n",
            " Loss:  0.510942816734314\n",
            "Training:  Iteration No:  107 Worker Num:  1 \n",
            " Loss:  0.49558210372924805\n",
            "Training:  Iteration No:  107 Worker Num:  2 \n",
            " Loss:  0.5051431655883789\n",
            "Training:  Iteration No:  107 Worker Num:  3 \n",
            " Loss:  0.3189556300640106\n",
            "Training:  Iteration No:  107 Worker Num:  4 \n",
            " Loss:  0.45085787773132324\n",
            "Training:  Iteration No:  107 Worker Num:  5 \n",
            " Loss:  0.38221433758735657\n",
            "Training:  Iteration No:  107 Worker Num:  6 \n",
            " Loss:  0.3142261207103729\n",
            "Training:  Iteration No:  107 Worker Num:  7 \n",
            " Loss:  0.5645726323127747\n",
            "Test:  Iteration No:  107 \n",
            " Loss:  0.3490794458532635\n",
            "Test accuracy:  90.05\n",
            "Training:  Iteration No:  108 Worker Num:  0 \n",
            " Loss:  0.5357903242111206\n",
            "Training:  Iteration No:  108 Worker Num:  1 \n",
            " Loss:  0.3885994851589203\n",
            "Training:  Iteration No:  108 Worker Num:  2 \n",
            " Loss:  0.3603602945804596\n",
            "Training:  Iteration No:  108 Worker Num:  3 \n",
            " Loss:  0.40342313051223755\n",
            "Training:  Iteration No:  108 Worker Num:  4 \n",
            " Loss:  0.413797527551651\n",
            "Training:  Iteration No:  108 Worker Num:  5 \n",
            " Loss:  0.3788079023361206\n",
            "Training:  Iteration No:  108 Worker Num:  6 \n",
            " Loss:  0.4807044565677643\n",
            "Training:  Iteration No:  108 Worker Num:  7 \n",
            " Loss:  0.4653015732765198\n",
            "Test:  Iteration No:  108 \n",
            " Loss:  0.3442484059288532\n",
            "Test accuracy:  90.3\n",
            "Training:  Iteration No:  109 Worker Num:  0 \n",
            " Loss:  0.4100079834461212\n",
            "Training:  Iteration No:  109 Worker Num:  1 \n",
            " Loss:  0.3918771743774414\n",
            "Training:  Iteration No:  109 Worker Num:  2 \n",
            " Loss:  0.42664361000061035\n",
            "Training:  Iteration No:  109 Worker Num:  3 \n",
            " Loss:  0.4138124883174896\n",
            "Training:  Iteration No:  109 Worker Num:  4 \n",
            " Loss:  0.40894970297813416\n",
            "Training:  Iteration No:  109 Worker Num:  5 \n",
            " Loss:  0.30205732583999634\n",
            "Training:  Iteration No:  109 Worker Num:  6 \n",
            " Loss:  0.4181164503097534\n",
            "Training:  Iteration No:  109 Worker Num:  7 \n",
            " Loss:  0.3378062844276428\n",
            "Test:  Iteration No:  109 \n",
            " Loss:  0.33808351774947554\n",
            "Test accuracy:  90.49\n",
            "Training:  Iteration No:  110 Worker Num:  0 \n",
            " Loss:  0.45603251457214355\n",
            "Training:  Iteration No:  110 Worker Num:  1 \n",
            " Loss:  0.4610465466976166\n",
            "Training:  Iteration No:  110 Worker Num:  2 \n",
            " Loss:  0.3593350648880005\n",
            "Training:  Iteration No:  110 Worker Num:  3 \n",
            " Loss:  0.5162127614021301\n",
            "Training:  Iteration No:  110 Worker Num:  4 \n",
            " Loss:  0.37251293659210205\n",
            "Training:  Iteration No:  110 Worker Num:  5 \n",
            " Loss:  0.5167384147644043\n",
            "Training:  Iteration No:  110 Worker Num:  6 \n",
            " Loss:  0.5091805458068848\n",
            "Training:  Iteration No:  110 Worker Num:  7 \n",
            " Loss:  0.40828225016593933\n",
            "Test:  Iteration No:  110 \n",
            " Loss:  0.33703440790878064\n",
            "Test accuracy:  90.59\n",
            "Training:  Iteration No:  111 Worker Num:  0 \n",
            " Loss:  0.3973231911659241\n",
            "Training:  Iteration No:  111 Worker Num:  1 \n",
            " Loss:  0.5641504526138306\n",
            "Training:  Iteration No:  111 Worker Num:  2 \n",
            " Loss:  0.47135505080223083\n",
            "Training:  Iteration No:  111 Worker Num:  3 \n",
            " Loss:  0.4191093444824219\n",
            "Training:  Iteration No:  111 Worker Num:  4 \n",
            " Loss:  0.37586864829063416\n",
            "Training:  Iteration No:  111 Worker Num:  5 \n",
            " Loss:  0.35042473673820496\n",
            "Training:  Iteration No:  111 Worker Num:  6 \n",
            " Loss:  0.39018866419792175\n",
            "Training:  Iteration No:  111 Worker Num:  7 \n",
            " Loss:  0.48375803232192993\n",
            "Test:  Iteration No:  111 \n",
            " Loss:  0.3388584424517577\n",
            "Test accuracy:  90.59\n",
            "Training:  Iteration No:  112 Worker Num:  0 \n",
            " Loss:  0.5111627578735352\n",
            "Training:  Iteration No:  112 Worker Num:  1 \n",
            " Loss:  0.2737775444984436\n",
            "Training:  Iteration No:  112 Worker Num:  2 \n",
            " Loss:  0.5254716873168945\n",
            "Training:  Iteration No:  112 Worker Num:  3 \n",
            " Loss:  0.37139207124710083\n",
            "Training:  Iteration No:  112 Worker Num:  4 \n",
            " Loss:  0.4746001064777374\n",
            "Training:  Iteration No:  112 Worker Num:  5 \n",
            " Loss:  0.34929513931274414\n",
            "Training:  Iteration No:  112 Worker Num:  6 \n",
            " Loss:  0.47111955285072327\n",
            "Training:  Iteration No:  112 Worker Num:  7 \n",
            " Loss:  0.4098191261291504\n",
            "Test:  Iteration No:  112 \n",
            " Loss:  0.33553481314189826\n",
            "Test accuracy:  90.47\n",
            "Training:  Iteration No:  113 Worker Num:  0 \n",
            " Loss:  0.3644488453865051\n",
            "Training:  Iteration No:  113 Worker Num:  1 \n",
            " Loss:  0.3182936906814575\n",
            "Training:  Iteration No:  113 Worker Num:  2 \n",
            " Loss:  0.3489067554473877\n",
            "Training:  Iteration No:  113 Worker Num:  3 \n",
            " Loss:  0.3576146960258484\n",
            "Training:  Iteration No:  113 Worker Num:  4 \n",
            " Loss:  0.3381209969520569\n",
            "Training:  Iteration No:  113 Worker Num:  5 \n",
            " Loss:  0.2646218538284302\n",
            "Training:  Iteration No:  113 Worker Num:  6 \n",
            " Loss:  0.39323651790618896\n",
            "Training:  Iteration No:  113 Worker Num:  7 \n",
            " Loss:  0.4551069438457489\n",
            "Test:  Iteration No:  113 \n",
            " Loss:  0.34510346860448016\n",
            "Test accuracy:  90.02\n",
            "Training:  Iteration No:  114 Worker Num:  0 \n",
            " Loss:  0.41507020592689514\n",
            "Training:  Iteration No:  114 Worker Num:  1 \n",
            " Loss:  0.33775344491004944\n",
            "Training:  Iteration No:  114 Worker Num:  2 \n",
            " Loss:  0.3900129795074463\n",
            "Training:  Iteration No:  114 Worker Num:  3 \n",
            " Loss:  0.3665609657764435\n",
            "Training:  Iteration No:  114 Worker Num:  4 \n",
            " Loss:  0.2228139340877533\n",
            "Training:  Iteration No:  114 Worker Num:  5 \n",
            " Loss:  0.3290974497795105\n",
            "Training:  Iteration No:  114 Worker Num:  6 \n",
            " Loss:  0.3972606658935547\n",
            "Training:  Iteration No:  114 Worker Num:  7 \n",
            " Loss:  0.3780052959918976\n",
            "Test:  Iteration No:  114 \n",
            " Loss:  0.33165696013388757\n",
            "Test accuracy:  90.51\n",
            "Training:  Iteration No:  115 Worker Num:  0 \n",
            " Loss:  0.40320372581481934\n",
            "Training:  Iteration No:  115 Worker Num:  1 \n",
            " Loss:  0.3985654413700104\n",
            "Training:  Iteration No:  115 Worker Num:  2 \n",
            " Loss:  0.46483275294303894\n",
            "Training:  Iteration No:  115 Worker Num:  3 \n",
            " Loss:  0.5155987739562988\n",
            "Training:  Iteration No:  115 Worker Num:  4 \n",
            " Loss:  0.29105138778686523\n",
            "Training:  Iteration No:  115 Worker Num:  5 \n",
            " Loss:  0.31047943234443665\n",
            "Training:  Iteration No:  115 Worker Num:  6 \n",
            " Loss:  0.325107604265213\n",
            "Training:  Iteration No:  115 Worker Num:  7 \n",
            " Loss:  0.40030670166015625\n",
            "Test:  Iteration No:  115 \n",
            " Loss:  0.3317304714288138\n",
            "Test accuracy:  90.68\n",
            "Training:  Iteration No:  116 Worker Num:  0 \n",
            " Loss:  0.39743196964263916\n",
            "Training:  Iteration No:  116 Worker Num:  1 \n",
            " Loss:  0.38833504915237427\n",
            "Training:  Iteration No:  116 Worker Num:  2 \n",
            " Loss:  0.29382944107055664\n",
            "Training:  Iteration No:  116 Worker Num:  3 \n",
            " Loss:  0.5031667947769165\n",
            "Training:  Iteration No:  116 Worker Num:  4 \n",
            " Loss:  0.3156641125679016\n",
            "Training:  Iteration No:  116 Worker Num:  5 \n",
            " Loss:  0.34704160690307617\n",
            "Training:  Iteration No:  116 Worker Num:  6 \n",
            " Loss:  0.5544688105583191\n",
            "Training:  Iteration No:  116 Worker Num:  7 \n",
            " Loss:  0.4416261315345764\n",
            "Test:  Iteration No:  116 \n",
            " Loss:  0.3308186731949637\n",
            "Test accuracy:  90.42\n",
            "Training:  Iteration No:  117 Worker Num:  0 \n",
            " Loss:  0.3218676745891571\n",
            "Training:  Iteration No:  117 Worker Num:  1 \n",
            " Loss:  0.38738638162612915\n",
            "Training:  Iteration No:  117 Worker Num:  2 \n",
            " Loss:  0.4522516131401062\n",
            "Training:  Iteration No:  117 Worker Num:  3 \n",
            " Loss:  0.40953323245048523\n",
            "Training:  Iteration No:  117 Worker Num:  4 \n",
            " Loss:  0.4438669681549072\n",
            "Training:  Iteration No:  117 Worker Num:  5 \n",
            " Loss:  0.2857668697834015\n",
            "Training:  Iteration No:  117 Worker Num:  6 \n",
            " Loss:  0.28477391600608826\n",
            "Training:  Iteration No:  117 Worker Num:  7 \n",
            " Loss:  0.4376460313796997\n",
            "Test:  Iteration No:  117 \n",
            " Loss:  0.3354768286683137\n",
            "Test accuracy:  90.47\n",
            "Training:  Iteration No:  118 Worker Num:  0 \n",
            " Loss:  0.41546162962913513\n",
            "Training:  Iteration No:  118 Worker Num:  1 \n",
            " Loss:  0.45050710439682007\n",
            "Training:  Iteration No:  118 Worker Num:  2 \n",
            " Loss:  0.34679296612739563\n",
            "Training:  Iteration No:  118 Worker Num:  3 \n",
            " Loss:  0.34192728996276855\n",
            "Training:  Iteration No:  118 Worker Num:  4 \n",
            " Loss:  0.4232073426246643\n",
            "Training:  Iteration No:  118 Worker Num:  5 \n",
            " Loss:  0.45470067858695984\n",
            "Training:  Iteration No:  118 Worker Num:  6 \n",
            " Loss:  0.4079112112522125\n",
            "Training:  Iteration No:  118 Worker Num:  7 \n",
            " Loss:  0.41879698634147644\n",
            "Test:  Iteration No:  118 \n",
            " Loss:  0.3281348798471161\n",
            "Test accuracy:  90.45\n",
            "Training:  Iteration No:  119 Worker Num:  0 \n",
            " Loss:  0.43932560086250305\n",
            "Training:  Iteration No:  119 Worker Num:  1 \n",
            " Loss:  0.3531530797481537\n",
            "Training:  Iteration No:  119 Worker Num:  2 \n",
            " Loss:  0.5078219175338745\n",
            "Training:  Iteration No:  119 Worker Num:  3 \n",
            " Loss:  0.47666242718696594\n",
            "Training:  Iteration No:  119 Worker Num:  4 \n",
            " Loss:  0.35020318627357483\n",
            "Training:  Iteration No:  119 Worker Num:  5 \n",
            " Loss:  0.48159098625183105\n",
            "Training:  Iteration No:  119 Worker Num:  6 \n",
            " Loss:  0.3928879499435425\n",
            "Training:  Iteration No:  119 Worker Num:  7 \n",
            " Loss:  0.5145877003669739\n",
            "Test:  Iteration No:  119 \n",
            " Loss:  0.3256977165612993\n",
            "Test accuracy:  90.79\n",
            "Training:  Iteration No:  120 Worker Num:  0 \n",
            " Loss:  0.37343984842300415\n",
            "Training:  Iteration No:  120 Worker Num:  1 \n",
            " Loss:  0.44007211923599243\n",
            "Training:  Iteration No:  120 Worker Num:  2 \n",
            " Loss:  0.36657676100730896\n",
            "Training:  Iteration No:  120 Worker Num:  3 \n",
            " Loss:  0.3229396641254425\n",
            "Training:  Iteration No:  120 Worker Num:  4 \n",
            " Loss:  0.4572637379169464\n",
            "Training:  Iteration No:  120 Worker Num:  5 \n",
            " Loss:  0.49061691761016846\n",
            "Training:  Iteration No:  120 Worker Num:  6 \n",
            " Loss:  0.24722474813461304\n",
            "Training:  Iteration No:  120 Worker Num:  7 \n",
            " Loss:  0.48595646023750305\n",
            "Test:  Iteration No:  120 \n",
            " Loss:  0.32546881454277643\n",
            "Test accuracy:  90.9\n",
            "Training:  Iteration No:  121 Worker Num:  0 \n",
            " Loss:  0.36125627160072327\n",
            "Training:  Iteration No:  121 Worker Num:  1 \n",
            " Loss:  0.37195008993148804\n",
            "Training:  Iteration No:  121 Worker Num:  2 \n",
            " Loss:  0.491176575422287\n",
            "Training:  Iteration No:  121 Worker Num:  3 \n",
            " Loss:  0.4570395350456238\n",
            "Training:  Iteration No:  121 Worker Num:  4 \n",
            " Loss:  0.3541856110095978\n",
            "Training:  Iteration No:  121 Worker Num:  5 \n",
            " Loss:  0.3199983537197113\n",
            "Training:  Iteration No:  121 Worker Num:  6 \n",
            " Loss:  0.5355093479156494\n",
            "Training:  Iteration No:  121 Worker Num:  7 \n",
            " Loss:  0.3512040078639984\n",
            "Test:  Iteration No:  121 \n",
            " Loss:  0.32184336546667014\n",
            "Test accuracy:  90.63\n",
            "Training:  Iteration No:  122 Worker Num:  0 \n",
            " Loss:  0.3090033531188965\n",
            "Training:  Iteration No:  122 Worker Num:  1 \n",
            " Loss:  0.2869197130203247\n",
            "Training:  Iteration No:  122 Worker Num:  2 \n",
            " Loss:  0.34470435976982117\n",
            "Training:  Iteration No:  122 Worker Num:  3 \n",
            " Loss:  0.3177368640899658\n",
            "Training:  Iteration No:  122 Worker Num:  4 \n",
            " Loss:  0.4655904173851013\n",
            "Training:  Iteration No:  122 Worker Num:  5 \n",
            " Loss:  0.5963019132614136\n",
            "Training:  Iteration No:  122 Worker Num:  6 \n",
            " Loss:  0.336374968290329\n",
            "Training:  Iteration No:  122 Worker Num:  7 \n",
            " Loss:  0.42715778946876526\n",
            "Test:  Iteration No:  122 \n",
            " Loss:  0.31952676615571673\n",
            "Test accuracy:  90.86\n",
            "Training:  Iteration No:  123 Worker Num:  0 \n",
            " Loss:  0.3180813193321228\n",
            "Training:  Iteration No:  123 Worker Num:  1 \n",
            " Loss:  0.42787814140319824\n",
            "Training:  Iteration No:  123 Worker Num:  2 \n",
            " Loss:  0.32757142186164856\n",
            "Training:  Iteration No:  123 Worker Num:  3 \n",
            " Loss:  0.5047370195388794\n",
            "Training:  Iteration No:  123 Worker Num:  4 \n",
            " Loss:  0.41168588399887085\n",
            "Training:  Iteration No:  123 Worker Num:  5 \n",
            " Loss:  0.3887522518634796\n",
            "Training:  Iteration No:  123 Worker Num:  6 \n",
            " Loss:  0.3271031677722931\n",
            "Training:  Iteration No:  123 Worker Num:  7 \n",
            " Loss:  0.35066863894462585\n",
            "Test:  Iteration No:  123 \n",
            " Loss:  0.3187698778943925\n",
            "Test accuracy:  90.88\n",
            "Training:  Iteration No:  124 Worker Num:  0 \n",
            " Loss:  0.31562045216560364\n",
            "Training:  Iteration No:  124 Worker Num:  1 \n",
            " Loss:  0.3513067364692688\n",
            "Training:  Iteration No:  124 Worker Num:  2 \n",
            " Loss:  0.43760809302330017\n",
            "Training:  Iteration No:  124 Worker Num:  3 \n",
            " Loss:  0.3429550528526306\n",
            "Training:  Iteration No:  124 Worker Num:  4 \n",
            " Loss:  0.21338380873203278\n",
            "Training:  Iteration No:  124 Worker Num:  5 \n",
            " Loss:  0.36128681898117065\n",
            "Training:  Iteration No:  124 Worker Num:  6 \n",
            " Loss:  0.4384658634662628\n",
            "Training:  Iteration No:  124 Worker Num:  7 \n",
            " Loss:  0.31757593154907227\n",
            "Test:  Iteration No:  124 \n",
            " Loss:  0.31524741668489914\n",
            "Test accuracy:  91.03\n",
            "Training:  Iteration No:  125 Worker Num:  0 \n",
            " Loss:  0.3919815123081207\n",
            "Training:  Iteration No:  125 Worker Num:  1 \n",
            " Loss:  0.4024336338043213\n",
            "Training:  Iteration No:  125 Worker Num:  2 \n",
            " Loss:  0.359338641166687\n",
            "Training:  Iteration No:  125 Worker Num:  3 \n",
            " Loss:  0.3803572952747345\n",
            "Training:  Iteration No:  125 Worker Num:  4 \n",
            " Loss:  0.24503342807292938\n",
            "Training:  Iteration No:  125 Worker Num:  5 \n",
            " Loss:  0.28370118141174316\n",
            "Training:  Iteration No:  125 Worker Num:  6 \n",
            " Loss:  0.442129909992218\n",
            "Training:  Iteration No:  125 Worker Num:  7 \n",
            " Loss:  0.49402961134910583\n",
            "Test:  Iteration No:  125 \n",
            " Loss:  0.3152171321024623\n",
            "Test accuracy:  90.99\n",
            "Training:  Iteration No:  126 Worker Num:  0 \n",
            " Loss:  0.35450324416160583\n",
            "Training:  Iteration No:  126 Worker Num:  1 \n",
            " Loss:  0.4390980005264282\n",
            "Training:  Iteration No:  126 Worker Num:  2 \n",
            " Loss:  0.32848480343818665\n",
            "Training:  Iteration No:  126 Worker Num:  3 \n",
            " Loss:  0.3523911237716675\n",
            "Training:  Iteration No:  126 Worker Num:  4 \n",
            " Loss:  0.32118576765060425\n",
            "Training:  Iteration No:  126 Worker Num:  5 \n",
            " Loss:  0.47573599219322205\n",
            "Training:  Iteration No:  126 Worker Num:  6 \n",
            " Loss:  0.3597211241722107\n",
            "Training:  Iteration No:  126 Worker Num:  7 \n",
            " Loss:  0.3399801254272461\n",
            "Test:  Iteration No:  126 \n",
            " Loss:  0.31725279119195815\n",
            "Test accuracy:  90.9\n",
            "Training:  Iteration No:  127 Worker Num:  0 \n",
            " Loss:  0.29500532150268555\n",
            "Training:  Iteration No:  127 Worker Num:  1 \n",
            " Loss:  0.39150720834732056\n",
            "Training:  Iteration No:  127 Worker Num:  2 \n",
            " Loss:  0.32127276062965393\n",
            "Training:  Iteration No:  127 Worker Num:  3 \n",
            " Loss:  0.2880348265171051\n",
            "Training:  Iteration No:  127 Worker Num:  4 \n",
            " Loss:  0.3307272791862488\n",
            "Training:  Iteration No:  127 Worker Num:  5 \n",
            " Loss:  0.3378138840198517\n",
            "Training:  Iteration No:  127 Worker Num:  6 \n",
            " Loss:  0.5136086940765381\n",
            "Training:  Iteration No:  127 Worker Num:  7 \n",
            " Loss:  0.4497653543949127\n",
            "Test:  Iteration No:  127 \n",
            " Loss:  0.31016140558485744\n",
            "Test accuracy:  91.02\n",
            "Training:  Iteration No:  128 Worker Num:  0 \n",
            " Loss:  0.4672155976295471\n",
            "Training:  Iteration No:  128 Worker Num:  1 \n",
            " Loss:  0.43599122762680054\n",
            "Training:  Iteration No:  128 Worker Num:  2 \n",
            " Loss:  0.288422554731369\n",
            "Training:  Iteration No:  128 Worker Num:  3 \n",
            " Loss:  0.4847278594970703\n",
            "Training:  Iteration No:  128 Worker Num:  4 \n",
            " Loss:  0.36847928166389465\n",
            "Training:  Iteration No:  128 Worker Num:  5 \n",
            " Loss:  0.49452483654022217\n",
            "Training:  Iteration No:  128 Worker Num:  6 \n",
            " Loss:  0.2573970556259155\n",
            "Training:  Iteration No:  128 Worker Num:  7 \n",
            " Loss:  0.29530736804008484\n",
            "Test:  Iteration No:  128 \n",
            " Loss:  0.31026488801912416\n",
            "Test accuracy:  91.14\n",
            "Training:  Iteration No:  129 Worker Num:  0 \n",
            " Loss:  0.4311608076095581\n",
            "Training:  Iteration No:  129 Worker Num:  1 \n",
            " Loss:  0.3600485920906067\n",
            "Training:  Iteration No:  129 Worker Num:  2 \n",
            " Loss:  0.2593441605567932\n",
            "Training:  Iteration No:  129 Worker Num:  3 \n",
            " Loss:  0.33326882123947144\n",
            "Training:  Iteration No:  129 Worker Num:  4 \n",
            " Loss:  0.26802733540534973\n",
            "Training:  Iteration No:  129 Worker Num:  5 \n",
            " Loss:  0.4814738631248474\n",
            "Training:  Iteration No:  129 Worker Num:  6 \n",
            " Loss:  0.32444530725479126\n",
            "Training:  Iteration No:  129 Worker Num:  7 \n",
            " Loss:  0.2846275866031647\n",
            "Test:  Iteration No:  129 \n",
            " Loss:  0.3410114051797722\n",
            "Test accuracy:  89.44\n",
            "Training:  Iteration No:  130 Worker Num:  0 \n",
            " Loss:  0.4292534589767456\n",
            "Training:  Iteration No:  130 Worker Num:  1 \n",
            " Loss:  0.3340336084365845\n",
            "Training:  Iteration No:  130 Worker Num:  2 \n",
            " Loss:  0.30410274863243103\n",
            "Training:  Iteration No:  130 Worker Num:  3 \n",
            " Loss:  0.507385790348053\n",
            "Training:  Iteration No:  130 Worker Num:  4 \n",
            " Loss:  0.3978399336338043\n",
            "Training:  Iteration No:  130 Worker Num:  5 \n",
            " Loss:  0.4161931574344635\n",
            "Training:  Iteration No:  130 Worker Num:  6 \n",
            " Loss:  0.3218359053134918\n",
            "Training:  Iteration No:  130 Worker Num:  7 \n",
            " Loss:  0.44043925404548645\n",
            "Test:  Iteration No:  130 \n",
            " Loss:  0.3209827091497711\n",
            "Test accuracy:  90.42\n",
            "Training:  Iteration No:  131 Worker Num:  0 \n",
            " Loss:  0.46362751722335815\n",
            "Training:  Iteration No:  131 Worker Num:  1 \n",
            " Loss:  0.3744872510433197\n",
            "Training:  Iteration No:  131 Worker Num:  2 \n",
            " Loss:  0.5110505223274231\n",
            "Training:  Iteration No:  131 Worker Num:  3 \n",
            " Loss:  0.34357163310050964\n",
            "Training:  Iteration No:  131 Worker Num:  4 \n",
            " Loss:  0.39877715706825256\n",
            "Training:  Iteration No:  131 Worker Num:  5 \n",
            " Loss:  0.36223742365837097\n",
            "Training:  Iteration No:  131 Worker Num:  6 \n",
            " Loss:  0.29566219449043274\n",
            "Training:  Iteration No:  131 Worker Num:  7 \n",
            " Loss:  0.46559399366378784\n",
            "Test:  Iteration No:  131 \n",
            " Loss:  0.32349016300485106\n",
            "Test accuracy:  90.47\n",
            "Training:  Iteration No:  132 Worker Num:  0 \n",
            " Loss:  0.27511897683143616\n",
            "Training:  Iteration No:  132 Worker Num:  1 \n",
            " Loss:  0.27097225189208984\n",
            "Training:  Iteration No:  132 Worker Num:  2 \n",
            " Loss:  0.30146536231040955\n",
            "Training:  Iteration No:  132 Worker Num:  3 \n",
            " Loss:  0.36312615871429443\n",
            "Training:  Iteration No:  132 Worker Num:  4 \n",
            " Loss:  0.41676342487335205\n",
            "Training:  Iteration No:  132 Worker Num:  5 \n",
            " Loss:  0.32690703868865967\n",
            "Training:  Iteration No:  132 Worker Num:  6 \n",
            " Loss:  0.21507298946380615\n",
            "Training:  Iteration No:  132 Worker Num:  7 \n",
            " Loss:  0.2573308050632477\n",
            "Test:  Iteration No:  132 \n",
            " Loss:  0.3107015902200077\n",
            "Test accuracy:  91.27\n",
            "Training:  Iteration No:  133 Worker Num:  0 \n",
            " Loss:  0.2841947674751282\n",
            "Training:  Iteration No:  133 Worker Num:  1 \n",
            " Loss:  0.40723925828933716\n",
            "Training:  Iteration No:  133 Worker Num:  2 \n",
            " Loss:  0.2367805391550064\n",
            "Training:  Iteration No:  133 Worker Num:  3 \n",
            " Loss:  0.49978357553482056\n",
            "Training:  Iteration No:  133 Worker Num:  4 \n",
            " Loss:  0.5187202095985413\n",
            "Training:  Iteration No:  133 Worker Num:  5 \n",
            " Loss:  0.5089580416679382\n",
            "Training:  Iteration No:  133 Worker Num:  6 \n",
            " Loss:  0.26500290632247925\n",
            "Training:  Iteration No:  133 Worker Num:  7 \n",
            " Loss:  0.29267433285713196\n",
            "Test:  Iteration No:  133 \n",
            " Loss:  0.30248932450821125\n",
            "Test accuracy:  91.2\n",
            "Training:  Iteration No:  134 Worker Num:  0 \n",
            " Loss:  0.3593515157699585\n",
            "Training:  Iteration No:  134 Worker Num:  1 \n",
            " Loss:  0.1815306842327118\n",
            "Training:  Iteration No:  134 Worker Num:  2 \n",
            " Loss:  0.3032605051994324\n",
            "Training:  Iteration No:  134 Worker Num:  3 \n",
            " Loss:  0.3057129383087158\n",
            "Training:  Iteration No:  134 Worker Num:  4 \n",
            " Loss:  0.3417266607284546\n",
            "Training:  Iteration No:  134 Worker Num:  5 \n",
            " Loss:  0.33163338899612427\n",
            "Training:  Iteration No:  134 Worker Num:  6 \n",
            " Loss:  0.34194907546043396\n",
            "Training:  Iteration No:  134 Worker Num:  7 \n",
            " Loss:  0.5473560094833374\n",
            "Test:  Iteration No:  134 \n",
            " Loss:  0.29840881462338603\n",
            "Test accuracy:  91.47\n",
            "Training:  Iteration No:  135 Worker Num:  0 \n",
            " Loss:  0.23005323112010956\n",
            "Training:  Iteration No:  135 Worker Num:  1 \n",
            " Loss:  0.4669821858406067\n",
            "Training:  Iteration No:  135 Worker Num:  2 \n",
            " Loss:  0.28913021087646484\n",
            "Training:  Iteration No:  135 Worker Num:  3 \n",
            " Loss:  0.411348432302475\n",
            "Training:  Iteration No:  135 Worker Num:  4 \n",
            " Loss:  0.3615913689136505\n",
            "Training:  Iteration No:  135 Worker Num:  5 \n",
            " Loss:  0.2871920168399811\n",
            "Training:  Iteration No:  135 Worker Num:  6 \n",
            " Loss:  0.31051889061927795\n",
            "Training:  Iteration No:  135 Worker Num:  7 \n",
            " Loss:  0.40209653973579407\n",
            "Test:  Iteration No:  135 \n",
            " Loss:  0.3063045612807515\n",
            "Test accuracy:  91.19\n",
            "Training:  Iteration No:  136 Worker Num:  0 \n",
            " Loss:  0.37035879492759705\n",
            "Training:  Iteration No:  136 Worker Num:  1 \n",
            " Loss:  0.30551180243492126\n",
            "Training:  Iteration No:  136 Worker Num:  2 \n",
            " Loss:  0.4109373986721039\n",
            "Training:  Iteration No:  136 Worker Num:  3 \n",
            " Loss:  0.37242740392684937\n",
            "Training:  Iteration No:  136 Worker Num:  4 \n",
            " Loss:  0.48170575499534607\n",
            "Training:  Iteration No:  136 Worker Num:  5 \n",
            " Loss:  0.5516720414161682\n",
            "Training:  Iteration No:  136 Worker Num:  6 \n",
            " Loss:  0.3256981074810028\n",
            "Training:  Iteration No:  136 Worker Num:  7 \n",
            " Loss:  0.31625476479530334\n",
            "Test:  Iteration No:  136 \n",
            " Loss:  0.31539266778132585\n",
            "Test accuracy:  90.83\n",
            "Training:  Iteration No:  137 Worker Num:  0 \n",
            " Loss:  0.39432844519615173\n",
            "Training:  Iteration No:  137 Worker Num:  1 \n",
            " Loss:  0.2700318694114685\n",
            "Training:  Iteration No:  137 Worker Num:  2 \n",
            " Loss:  0.341011643409729\n",
            "Training:  Iteration No:  137 Worker Num:  3 \n",
            " Loss:  0.3562004268169403\n",
            "Training:  Iteration No:  137 Worker Num:  4 \n",
            " Loss:  0.35510537028312683\n",
            "Training:  Iteration No:  137 Worker Num:  5 \n",
            " Loss:  0.44641929864883423\n",
            "Training:  Iteration No:  137 Worker Num:  6 \n",
            " Loss:  0.2693871855735779\n",
            "Training:  Iteration No:  137 Worker Num:  7 \n",
            " Loss:  0.5379587411880493\n",
            "Test:  Iteration No:  137 \n",
            " Loss:  0.30208100557704515\n",
            "Test accuracy:  91.02\n",
            "Training:  Iteration No:  138 Worker Num:  0 \n",
            " Loss:  0.3379620909690857\n",
            "Training:  Iteration No:  138 Worker Num:  1 \n",
            " Loss:  0.3530881404876709\n",
            "Training:  Iteration No:  138 Worker Num:  2 \n",
            " Loss:  0.35114580392837524\n",
            "Training:  Iteration No:  138 Worker Num:  3 \n",
            " Loss:  0.24700355529785156\n",
            "Training:  Iteration No:  138 Worker Num:  4 \n",
            " Loss:  0.48015502095222473\n",
            "Training:  Iteration No:  138 Worker Num:  5 \n",
            " Loss:  0.3308144509792328\n",
            "Training:  Iteration No:  138 Worker Num:  6 \n",
            " Loss:  0.32301241159439087\n",
            "Training:  Iteration No:  138 Worker Num:  7 \n",
            " Loss:  0.6474335193634033\n",
            "Test:  Iteration No:  138 \n",
            " Loss:  0.3033152631472183\n",
            "Test accuracy:  91.23\n",
            "Training:  Iteration No:  139 Worker Num:  0 \n",
            " Loss:  0.4436948299407959\n",
            "Training:  Iteration No:  139 Worker Num:  1 \n",
            " Loss:  0.7026746869087219\n",
            "Training:  Iteration No:  139 Worker Num:  2 \n",
            " Loss:  0.46654295921325684\n",
            "Training:  Iteration No:  139 Worker Num:  3 \n",
            " Loss:  0.3983106017112732\n",
            "Training:  Iteration No:  139 Worker Num:  4 \n",
            " Loss:  0.33374011516571045\n",
            "Training:  Iteration No:  139 Worker Num:  5 \n",
            " Loss:  0.4170385003089905\n",
            "Training:  Iteration No:  139 Worker Num:  6 \n",
            " Loss:  0.3975191116333008\n",
            "Training:  Iteration No:  139 Worker Num:  7 \n",
            " Loss:  0.3342350721359253\n",
            "Test:  Iteration No:  139 \n",
            " Loss:  0.2947570195397999\n",
            "Test accuracy:  91.49\n",
            "Training:  Iteration No:  140 Worker Num:  0 \n",
            " Loss:  0.4520948827266693\n",
            "Training:  Iteration No:  140 Worker Num:  1 \n",
            " Loss:  0.29347455501556396\n",
            "Training:  Iteration No:  140 Worker Num:  2 \n",
            " Loss:  0.27567994594573975\n",
            "Training:  Iteration No:  140 Worker Num:  3 \n",
            " Loss:  0.5261476039886475\n",
            "Training:  Iteration No:  140 Worker Num:  4 \n",
            " Loss:  0.42232730984687805\n",
            "Training:  Iteration No:  140 Worker Num:  5 \n",
            " Loss:  0.3156195878982544\n",
            "Training:  Iteration No:  140 Worker Num:  6 \n",
            " Loss:  0.28345391154289246\n",
            "Training:  Iteration No:  140 Worker Num:  7 \n",
            " Loss:  0.38829174637794495\n",
            "Test:  Iteration No:  140 \n",
            " Loss:  0.29110451792425746\n",
            "Test accuracy:  91.68\n",
            "Training:  Iteration No:  141 Worker Num:  0 \n",
            " Loss:  0.3812798857688904\n",
            "Training:  Iteration No:  141 Worker Num:  1 \n",
            " Loss:  0.2705036401748657\n",
            "Training:  Iteration No:  141 Worker Num:  2 \n",
            " Loss:  0.37840110063552856\n",
            "Training:  Iteration No:  141 Worker Num:  3 \n",
            " Loss:  0.37058478593826294\n",
            "Training:  Iteration No:  141 Worker Num:  4 \n",
            " Loss:  0.4199368357658386\n",
            "Training:  Iteration No:  141 Worker Num:  5 \n",
            " Loss:  0.43758878111839294\n",
            "Training:  Iteration No:  141 Worker Num:  6 \n",
            " Loss:  0.3760969042778015\n",
            "Training:  Iteration No:  141 Worker Num:  7 \n",
            " Loss:  0.28790152072906494\n",
            "Test:  Iteration No:  141 \n",
            " Loss:  0.29188069116465654\n",
            "Test accuracy:  91.73\n",
            "Training:  Iteration No:  142 Worker Num:  0 \n",
            " Loss:  0.3721243143081665\n",
            "Training:  Iteration No:  142 Worker Num:  1 \n",
            " Loss:  0.47485262155532837\n",
            "Training:  Iteration No:  142 Worker Num:  2 \n",
            " Loss:  0.30106785893440247\n",
            "Training:  Iteration No:  142 Worker Num:  3 \n",
            " Loss:  0.48326295614242554\n",
            "Training:  Iteration No:  142 Worker Num:  4 \n",
            " Loss:  0.4272787272930145\n",
            "Training:  Iteration No:  142 Worker Num:  5 \n",
            " Loss:  0.33698755502700806\n",
            "Training:  Iteration No:  142 Worker Num:  6 \n",
            " Loss:  0.5868709087371826\n",
            "Training:  Iteration No:  142 Worker Num:  7 \n",
            " Loss:  0.3265452980995178\n",
            "Test:  Iteration No:  142 \n",
            " Loss:  0.2924791087832632\n",
            "Test accuracy:  91.48\n",
            "Training:  Iteration No:  143 Worker Num:  0 \n",
            " Loss:  0.5319353342056274\n",
            "Training:  Iteration No:  143 Worker Num:  1 \n",
            " Loss:  0.32657304406166077\n",
            "Training:  Iteration No:  143 Worker Num:  2 \n",
            " Loss:  0.37576764822006226\n",
            "Training:  Iteration No:  143 Worker Num:  3 \n",
            " Loss:  0.2819508910179138\n",
            "Training:  Iteration No:  143 Worker Num:  4 \n",
            " Loss:  0.3586133122444153\n",
            "Training:  Iteration No:  143 Worker Num:  5 \n",
            " Loss:  0.3882633149623871\n",
            "Training:  Iteration No:  143 Worker Num:  6 \n",
            " Loss:  0.34164029359817505\n",
            "Training:  Iteration No:  143 Worker Num:  7 \n",
            " Loss:  0.35791102051734924\n",
            "Test:  Iteration No:  143 \n",
            " Loss:  0.28959217724166336\n",
            "Test accuracy:  91.73\n",
            "Training:  Iteration No:  144 Worker Num:  0 \n",
            " Loss:  0.29201605916023254\n",
            "Training:  Iteration No:  144 Worker Num:  1 \n",
            " Loss:  0.46342772245407104\n",
            "Training:  Iteration No:  144 Worker Num:  2 \n",
            " Loss:  0.37095150351524353\n",
            "Training:  Iteration No:  144 Worker Num:  3 \n",
            " Loss:  0.5430043935775757\n",
            "Training:  Iteration No:  144 Worker Num:  4 \n",
            " Loss:  0.5252305269241333\n",
            "Training:  Iteration No:  144 Worker Num:  5 \n",
            " Loss:  0.3718925714492798\n",
            "Training:  Iteration No:  144 Worker Num:  6 \n",
            " Loss:  0.19841113686561584\n",
            "Training:  Iteration No:  144 Worker Num:  7 \n",
            " Loss:  0.4056953191757202\n",
            "Test:  Iteration No:  144 \n",
            " Loss:  0.28938542259267613\n",
            "Test accuracy:  91.73\n",
            "Training:  Iteration No:  145 Worker Num:  0 \n",
            " Loss:  0.5071237683296204\n",
            "Training:  Iteration No:  145 Worker Num:  1 \n",
            " Loss:  0.3371465504169464\n",
            "Training:  Iteration No:  145 Worker Num:  2 \n",
            " Loss:  0.4201463758945465\n",
            "Training:  Iteration No:  145 Worker Num:  3 \n",
            " Loss:  0.2751263380050659\n",
            "Training:  Iteration No:  145 Worker Num:  4 \n",
            " Loss:  0.3040822148323059\n",
            "Training:  Iteration No:  145 Worker Num:  5 \n",
            " Loss:  0.35763344168663025\n",
            "Training:  Iteration No:  145 Worker Num:  6 \n",
            " Loss:  0.42644190788269043\n",
            "Training:  Iteration No:  145 Worker Num:  7 \n",
            " Loss:  0.3995162546634674\n",
            "Test:  Iteration No:  145 \n",
            " Loss:  0.2852759896011292\n",
            "Test accuracy:  91.85\n",
            "Training:  Iteration No:  146 Worker Num:  0 \n",
            " Loss:  0.35045909881591797\n",
            "Training:  Iteration No:  146 Worker Num:  1 \n",
            " Loss:  0.35589656233787537\n",
            "Training:  Iteration No:  146 Worker Num:  2 \n",
            " Loss:  0.2574865221977234\n",
            "Training:  Iteration No:  146 Worker Num:  3 \n",
            " Loss:  0.33922940492630005\n",
            "Training:  Iteration No:  146 Worker Num:  4 \n",
            " Loss:  0.5231117606163025\n",
            "Training:  Iteration No:  146 Worker Num:  5 \n",
            " Loss:  0.3341582417488098\n",
            "Training:  Iteration No:  146 Worker Num:  6 \n",
            " Loss:  0.32396236062049866\n",
            "Training:  Iteration No:  146 Worker Num:  7 \n",
            " Loss:  0.34509116411209106\n",
            "Test:  Iteration No:  146 \n",
            " Loss:  0.28489638732958444\n",
            "Test accuracy:  91.83\n",
            "Training:  Iteration No:  147 Worker Num:  0 \n",
            " Loss:  0.4676791727542877\n",
            "Training:  Iteration No:  147 Worker Num:  1 \n",
            " Loss:  0.31141042709350586\n",
            "Training:  Iteration No:  147 Worker Num:  2 \n",
            " Loss:  0.3002942204475403\n",
            "Training:  Iteration No:  147 Worker Num:  3 \n",
            " Loss:  0.28905442357063293\n",
            "Training:  Iteration No:  147 Worker Num:  4 \n",
            " Loss:  0.23643741011619568\n",
            "Training:  Iteration No:  147 Worker Num:  5 \n",
            " Loss:  0.31442058086395264\n",
            "Training:  Iteration No:  147 Worker Num:  6 \n",
            " Loss:  0.31317639350891113\n",
            "Training:  Iteration No:  147 Worker Num:  7 \n",
            " Loss:  0.37251824140548706\n",
            "Test:  Iteration No:  147 \n",
            " Loss:  0.282743422055169\n",
            "Test accuracy:  91.85\n",
            "Training:  Iteration No:  148 Worker Num:  0 \n",
            " Loss:  0.44631141424179077\n",
            "Training:  Iteration No:  148 Worker Num:  1 \n",
            " Loss:  0.28521963953971863\n",
            "Training:  Iteration No:  148 Worker Num:  2 \n",
            " Loss:  0.3297884464263916\n",
            "Training:  Iteration No:  148 Worker Num:  3 \n",
            " Loss:  0.29508742690086365\n",
            "Training:  Iteration No:  148 Worker Num:  4 \n",
            " Loss:  0.39222806692123413\n",
            "Training:  Iteration No:  148 Worker Num:  5 \n",
            " Loss:  0.36433666944503784\n",
            "Training:  Iteration No:  148 Worker Num:  6 \n",
            " Loss:  0.30531084537506104\n",
            "Training:  Iteration No:  148 Worker Num:  7 \n",
            " Loss:  0.35261353850364685\n",
            "Test:  Iteration No:  148 \n",
            " Loss:  0.2796077578552539\n",
            "Test accuracy:  92.14\n",
            "Training:  Iteration No:  149 Worker Num:  0 \n",
            " Loss:  0.3852471709251404\n",
            "Training:  Iteration No:  149 Worker Num:  1 \n",
            " Loss:  0.43370023369789124\n",
            "Training:  Iteration No:  149 Worker Num:  2 \n",
            " Loss:  0.29382041096687317\n",
            "Training:  Iteration No:  149 Worker Num:  3 \n",
            " Loss:  0.3232131004333496\n",
            "Training:  Iteration No:  149 Worker Num:  4 \n",
            " Loss:  0.24545520544052124\n",
            "Training:  Iteration No:  149 Worker Num:  5 \n",
            " Loss:  0.29527267813682556\n",
            "Training:  Iteration No:  149 Worker Num:  6 \n",
            " Loss:  0.24902769923210144\n",
            "Training:  Iteration No:  149 Worker Num:  7 \n",
            " Loss:  0.373422771692276\n",
            "Test:  Iteration No:  149 \n",
            " Loss:  0.292617186856798\n",
            "Test accuracy:  91.65\n",
            "Training:  Iteration No:  150 Worker Num:  0 \n",
            " Loss:  0.28590038418769836\n",
            "Training:  Iteration No:  150 Worker Num:  1 \n",
            " Loss:  0.20293071866035461\n",
            "Training:  Iteration No:  150 Worker Num:  2 \n",
            " Loss:  0.3244672119617462\n",
            "Training:  Iteration No:  150 Worker Num:  3 \n",
            " Loss:  0.24971507489681244\n",
            "Training:  Iteration No:  150 Worker Num:  4 \n",
            " Loss:  0.3147437572479248\n",
            "Training:  Iteration No:  150 Worker Num:  5 \n",
            " Loss:  0.4133659899234772\n",
            "Training:  Iteration No:  150 Worker Num:  6 \n",
            " Loss:  0.27437257766723633\n",
            "Training:  Iteration No:  150 Worker Num:  7 \n",
            " Loss:  0.4720950424671173\n",
            "Test:  Iteration No:  150 \n",
            " Loss:  0.28062801657220987\n",
            "Test accuracy:  91.87\n",
            "Training:  Iteration No:  151 Worker Num:  0 \n",
            " Loss:  0.3120437264442444\n",
            "Training:  Iteration No:  151 Worker Num:  1 \n",
            " Loss:  0.35460612177848816\n",
            "Training:  Iteration No:  151 Worker Num:  2 \n",
            " Loss:  0.3066256642341614\n",
            "Training:  Iteration No:  151 Worker Num:  3 \n",
            " Loss:  0.4052393138408661\n",
            "Training:  Iteration No:  151 Worker Num:  4 \n",
            " Loss:  0.41909152269363403\n",
            "Training:  Iteration No:  151 Worker Num:  5 \n",
            " Loss:  0.3308873772621155\n",
            "Training:  Iteration No:  151 Worker Num:  6 \n",
            " Loss:  0.389415442943573\n",
            "Training:  Iteration No:  151 Worker Num:  7 \n",
            " Loss:  0.3961978852748871\n",
            "Test:  Iteration No:  151 \n",
            " Loss:  0.28219406500081473\n",
            "Test accuracy:  91.86\n",
            "Training:  Iteration No:  152 Worker Num:  0 \n",
            " Loss:  0.43909889459609985\n",
            "Training:  Iteration No:  152 Worker Num:  1 \n",
            " Loss:  0.31855446100234985\n",
            "Training:  Iteration No:  152 Worker Num:  2 \n",
            " Loss:  0.3618275821208954\n",
            "Training:  Iteration No:  152 Worker Num:  3 \n",
            " Loss:  0.25469282269477844\n",
            "Training:  Iteration No:  152 Worker Num:  4 \n",
            " Loss:  0.3000740706920624\n",
            "Training:  Iteration No:  152 Worker Num:  5 \n",
            " Loss:  0.21360443532466888\n",
            "Training:  Iteration No:  152 Worker Num:  6 \n",
            " Loss:  0.33632853627204895\n",
            "Training:  Iteration No:  152 Worker Num:  7 \n",
            " Loss:  0.31462448835372925\n",
            "Test:  Iteration No:  152 \n",
            " Loss:  0.275324945161237\n",
            "Test accuracy:  92.11\n",
            "Training:  Iteration No:  153 Worker Num:  0 \n",
            " Loss:  0.36836355924606323\n",
            "Training:  Iteration No:  153 Worker Num:  1 \n",
            " Loss:  0.3337422013282776\n",
            "Training:  Iteration No:  153 Worker Num:  2 \n",
            " Loss:  0.3192882835865021\n",
            "Training:  Iteration No:  153 Worker Num:  3 \n",
            " Loss:  0.35677099227905273\n",
            "Training:  Iteration No:  153 Worker Num:  4 \n",
            " Loss:  0.5579039454460144\n",
            "Training:  Iteration No:  153 Worker Num:  5 \n",
            " Loss:  0.3677189350128174\n",
            "Training:  Iteration No:  153 Worker Num:  6 \n",
            " Loss:  0.2107597142457962\n",
            "Training:  Iteration No:  153 Worker Num:  7 \n",
            " Loss:  0.3299349546432495\n",
            "Test:  Iteration No:  153 \n",
            " Loss:  0.27422455568573895\n",
            "Test accuracy:  92.18\n",
            "Training:  Iteration No:  154 Worker Num:  0 \n",
            " Loss:  0.2426108419895172\n",
            "Training:  Iteration No:  154 Worker Num:  1 \n",
            " Loss:  0.29177552461624146\n",
            "Training:  Iteration No:  154 Worker Num:  2 \n",
            " Loss:  0.3084012567996979\n",
            "Training:  Iteration No:  154 Worker Num:  3 \n",
            " Loss:  0.2709487974643707\n",
            "Training:  Iteration No:  154 Worker Num:  4 \n",
            " Loss:  0.2998109757900238\n",
            "Training:  Iteration No:  154 Worker Num:  5 \n",
            " Loss:  0.3217911422252655\n",
            "Training:  Iteration No:  154 Worker Num:  6 \n",
            " Loss:  0.478476881980896\n",
            "Training:  Iteration No:  154 Worker Num:  7 \n",
            " Loss:  0.3690534234046936\n",
            "Test:  Iteration No:  154 \n",
            " Loss:  0.27348704654959183\n",
            "Test accuracy:  92.14\n",
            "Training:  Iteration No:  155 Worker Num:  0 \n",
            " Loss:  0.27625247836112976\n",
            "Training:  Iteration No:  155 Worker Num:  1 \n",
            " Loss:  0.3326626420021057\n",
            "Training:  Iteration No:  155 Worker Num:  2 \n",
            " Loss:  0.38618698716163635\n",
            "Training:  Iteration No:  155 Worker Num:  3 \n",
            " Loss:  0.48969218134880066\n",
            "Training:  Iteration No:  155 Worker Num:  4 \n",
            " Loss:  0.29747432470321655\n",
            "Training:  Iteration No:  155 Worker Num:  5 \n",
            " Loss:  0.3945275545120239\n",
            "Training:  Iteration No:  155 Worker Num:  6 \n",
            " Loss:  0.3646368086338043\n",
            "Training:  Iteration No:  155 Worker Num:  7 \n",
            " Loss:  0.35795164108276367\n",
            "Test:  Iteration No:  155 \n",
            " Loss:  0.2767008461671162\n",
            "Test accuracy:  91.98\n",
            "Training:  Iteration No:  156 Worker Num:  0 \n",
            " Loss:  0.2903944253921509\n",
            "Training:  Iteration No:  156 Worker Num:  1 \n",
            " Loss:  0.5369287729263306\n",
            "Training:  Iteration No:  156 Worker Num:  2 \n",
            " Loss:  0.23586848378181458\n",
            "Training:  Iteration No:  156 Worker Num:  3 \n",
            " Loss:  0.22790992259979248\n",
            "Training:  Iteration No:  156 Worker Num:  4 \n",
            " Loss:  0.33579736948013306\n",
            "Training:  Iteration No:  156 Worker Num:  5 \n",
            " Loss:  0.4230913519859314\n",
            "Training:  Iteration No:  156 Worker Num:  6 \n",
            " Loss:  0.34068673849105835\n",
            "Training:  Iteration No:  156 Worker Num:  7 \n",
            " Loss:  0.31075212359428406\n",
            "Test:  Iteration No:  156 \n",
            " Loss:  0.27101002598204943\n",
            "Test accuracy:  92.35\n",
            "Training:  Iteration No:  157 Worker Num:  0 \n",
            " Loss:  0.19007118046283722\n",
            "Training:  Iteration No:  157 Worker Num:  1 \n",
            " Loss:  0.33056679368019104\n",
            "Training:  Iteration No:  157 Worker Num:  2 \n",
            " Loss:  0.3231523036956787\n",
            "Training:  Iteration No:  157 Worker Num:  3 \n",
            " Loss:  0.373361736536026\n",
            "Training:  Iteration No:  157 Worker Num:  4 \n",
            " Loss:  0.3347577452659607\n",
            "Training:  Iteration No:  157 Worker Num:  5 \n",
            " Loss:  0.32889384031295776\n",
            "Training:  Iteration No:  157 Worker Num:  6 \n",
            " Loss:  0.352144330739975\n",
            "Training:  Iteration No:  157 Worker Num:  7 \n",
            " Loss:  0.3705926537513733\n",
            "Test:  Iteration No:  157 \n",
            " Loss:  0.2688338175604615\n",
            "Test accuracy:  92.41\n",
            "Training:  Iteration No:  158 Worker Num:  0 \n",
            " Loss:  0.37524500489234924\n",
            "Training:  Iteration No:  158 Worker Num:  1 \n",
            " Loss:  0.3035643398761749\n",
            "Training:  Iteration No:  158 Worker Num:  2 \n",
            " Loss:  0.49088162183761597\n",
            "Training:  Iteration No:  158 Worker Num:  3 \n",
            " Loss:  0.46264296770095825\n",
            "Training:  Iteration No:  158 Worker Num:  4 \n",
            " Loss:  0.2880372703075409\n",
            "Training:  Iteration No:  158 Worker Num:  5 \n",
            " Loss:  0.2565499246120453\n",
            "Training:  Iteration No:  158 Worker Num:  6 \n",
            " Loss:  0.3243851363658905\n",
            "Training:  Iteration No:  158 Worker Num:  7 \n",
            " Loss:  0.32919079065322876\n",
            "Test:  Iteration No:  158 \n",
            " Loss:  0.27279831742561317\n",
            "Test accuracy:  92.18\n",
            "Training:  Iteration No:  159 Worker Num:  0 \n",
            " Loss:  0.2397640347480774\n",
            "Training:  Iteration No:  159 Worker Num:  1 \n",
            " Loss:  0.4084662199020386\n",
            "Training:  Iteration No:  159 Worker Num:  2 \n",
            " Loss:  0.3349866271018982\n",
            "Training:  Iteration No:  159 Worker Num:  3 \n",
            " Loss:  0.3280286490917206\n",
            "Training:  Iteration No:  159 Worker Num:  4 \n",
            " Loss:  0.4404146373271942\n",
            "Training:  Iteration No:  159 Worker Num:  5 \n",
            " Loss:  0.38665297627449036\n",
            "Training:  Iteration No:  159 Worker Num:  6 \n",
            " Loss:  0.3048134744167328\n",
            "Training:  Iteration No:  159 Worker Num:  7 \n",
            " Loss:  0.4104001820087433\n",
            "Test:  Iteration No:  159 \n",
            " Loss:  0.28186116598640815\n",
            "Test accuracy:  91.7\n",
            "Training:  Iteration No:  160 Worker Num:  0 \n",
            " Loss:  0.40500956773757935\n",
            "Training:  Iteration No:  160 Worker Num:  1 \n",
            " Loss:  0.3244039714336395\n",
            "Training:  Iteration No:  160 Worker Num:  2 \n",
            " Loss:  0.2420772910118103\n",
            "Training:  Iteration No:  160 Worker Num:  3 \n",
            " Loss:  0.3503740429878235\n",
            "Training:  Iteration No:  160 Worker Num:  4 \n",
            " Loss:  0.250585675239563\n",
            "Training:  Iteration No:  160 Worker Num:  5 \n",
            " Loss:  0.2798503637313843\n",
            "Training:  Iteration No:  160 Worker Num:  6 \n",
            " Loss:  0.2741892635822296\n",
            "Training:  Iteration No:  160 Worker Num:  7 \n",
            " Loss:  0.25624892115592957\n",
            "Test:  Iteration No:  160 \n",
            " Loss:  0.2748909736452978\n",
            "Test accuracy:  92.0\n",
            "Training:  Iteration No:  161 Worker Num:  0 \n",
            " Loss:  0.3558797836303711\n",
            "Training:  Iteration No:  161 Worker Num:  1 \n",
            " Loss:  0.2972811162471771\n",
            "Training:  Iteration No:  161 Worker Num:  2 \n",
            " Loss:  0.2670368552207947\n",
            "Training:  Iteration No:  161 Worker Num:  3 \n",
            " Loss:  0.3739658296108246\n",
            "Training:  Iteration No:  161 Worker Num:  4 \n",
            " Loss:  0.2752680778503418\n",
            "Training:  Iteration No:  161 Worker Num:  5 \n",
            " Loss:  0.3270413279533386\n",
            "Training:  Iteration No:  161 Worker Num:  6 \n",
            " Loss:  0.3011223375797272\n",
            "Training:  Iteration No:  161 Worker Num:  7 \n",
            " Loss:  0.3699193000793457\n",
            "Test:  Iteration No:  161 \n",
            " Loss:  0.2692408574438548\n",
            "Test accuracy:  92.11\n",
            "Training:  Iteration No:  162 Worker Num:  0 \n",
            " Loss:  0.39743733406066895\n",
            "Training:  Iteration No:  162 Worker Num:  1 \n",
            " Loss:  0.2477760910987854\n",
            "Training:  Iteration No:  162 Worker Num:  2 \n",
            " Loss:  0.36249083280563354\n",
            "Training:  Iteration No:  162 Worker Num:  3 \n",
            " Loss:  0.19204524159431458\n",
            "Training:  Iteration No:  162 Worker Num:  4 \n",
            " Loss:  0.3747865855693817\n",
            "Training:  Iteration No:  162 Worker Num:  5 \n",
            " Loss:  0.2997138798236847\n",
            "Training:  Iteration No:  162 Worker Num:  6 \n",
            " Loss:  0.3732004463672638\n",
            "Training:  Iteration No:  162 Worker Num:  7 \n",
            " Loss:  0.4298804998397827\n",
            "Test:  Iteration No:  162 \n",
            " Loss:  0.26826736345132696\n",
            "Test accuracy:  92.35\n",
            "Training:  Iteration No:  163 Worker Num:  0 \n",
            " Loss:  0.3126184940338135\n",
            "Training:  Iteration No:  163 Worker Num:  1 \n",
            " Loss:  0.2718202769756317\n",
            "Training:  Iteration No:  163 Worker Num:  2 \n",
            " Loss:  0.4193233251571655\n",
            "Training:  Iteration No:  163 Worker Num:  3 \n",
            " Loss:  0.2644040584564209\n",
            "Training:  Iteration No:  163 Worker Num:  4 \n",
            " Loss:  0.32153695821762085\n",
            "Training:  Iteration No:  163 Worker Num:  5 \n",
            " Loss:  0.37814241647720337\n",
            "Training:  Iteration No:  163 Worker Num:  6 \n",
            " Loss:  0.27366188168525696\n",
            "Training:  Iteration No:  163 Worker Num:  7 \n",
            " Loss:  0.29750576615333557\n",
            "Test:  Iteration No:  163 \n",
            " Loss:  0.26550918595888945\n",
            "Test accuracy:  92.32\n",
            "Training:  Iteration No:  164 Worker Num:  0 \n",
            " Loss:  0.38762015104293823\n",
            "Training:  Iteration No:  164 Worker Num:  1 \n",
            " Loss:  0.28792911767959595\n",
            "Training:  Iteration No:  164 Worker Num:  2 \n",
            " Loss:  0.46137773990631104\n",
            "Training:  Iteration No:  164 Worker Num:  3 \n",
            " Loss:  0.2466285526752472\n",
            "Training:  Iteration No:  164 Worker Num:  4 \n",
            " Loss:  0.23786969482898712\n",
            "Training:  Iteration No:  164 Worker Num:  5 \n",
            " Loss:  0.29932159185409546\n",
            "Training:  Iteration No:  164 Worker Num:  6 \n",
            " Loss:  0.3072511851787567\n",
            "Training:  Iteration No:  164 Worker Num:  7 \n",
            " Loss:  0.3833540380001068\n",
            "Test:  Iteration No:  164 \n",
            " Loss:  0.26232358617590196\n",
            "Test accuracy:  92.33\n",
            "Training:  Iteration No:  165 Worker Num:  0 \n",
            " Loss:  0.4111367166042328\n",
            "Training:  Iteration No:  165 Worker Num:  1 \n",
            " Loss:  0.42708197236061096\n",
            "Training:  Iteration No:  165 Worker Num:  2 \n",
            " Loss:  0.267010897397995\n",
            "Training:  Iteration No:  165 Worker Num:  3 \n",
            " Loss:  0.372830867767334\n",
            "Training:  Iteration No:  165 Worker Num:  4 \n",
            " Loss:  0.21535195410251617\n",
            "Training:  Iteration No:  165 Worker Num:  5 \n",
            " Loss:  0.2865673005580902\n",
            "Training:  Iteration No:  165 Worker Num:  6 \n",
            " Loss:  0.3111414313316345\n",
            "Training:  Iteration No:  165 Worker Num:  7 \n",
            " Loss:  0.4448017179965973\n",
            "Test:  Iteration No:  165 \n",
            " Loss:  0.27488408341437953\n",
            "Test accuracy:  92.06\n",
            "Training:  Iteration No:  166 Worker Num:  0 \n",
            " Loss:  0.448478102684021\n",
            "Training:  Iteration No:  166 Worker Num:  1 \n",
            " Loss:  0.44145044684410095\n",
            "Training:  Iteration No:  166 Worker Num:  2 \n",
            " Loss:  0.3772943913936615\n",
            "Training:  Iteration No:  166 Worker Num:  3 \n",
            " Loss:  0.22332821786403656\n",
            "Training:  Iteration No:  166 Worker Num:  4 \n",
            " Loss:  0.2745344340801239\n",
            "Training:  Iteration No:  166 Worker Num:  5 \n",
            " Loss:  0.38589924573898315\n",
            "Training:  Iteration No:  166 Worker Num:  6 \n",
            " Loss:  0.31887006759643555\n",
            "Training:  Iteration No:  166 Worker Num:  7 \n",
            " Loss:  0.4186168909072876\n",
            "Test:  Iteration No:  166 \n",
            " Loss:  0.2669900299250325\n",
            "Test accuracy:  92.18\n",
            "Training:  Iteration No:  167 Worker Num:  0 \n",
            " Loss:  0.2700882852077484\n",
            "Training:  Iteration No:  167 Worker Num:  1 \n",
            " Loss:  0.2059253305196762\n",
            "Training:  Iteration No:  167 Worker Num:  2 \n",
            " Loss:  0.2762843072414398\n",
            "Training:  Iteration No:  167 Worker Num:  3 \n",
            " Loss:  0.30404072999954224\n",
            "Training:  Iteration No:  167 Worker Num:  4 \n",
            " Loss:  0.39840686321258545\n",
            "Training:  Iteration No:  167 Worker Num:  5 \n",
            " Loss:  0.3513856530189514\n",
            "Training:  Iteration No:  167 Worker Num:  6 \n",
            " Loss:  0.25064727663993835\n",
            "Training:  Iteration No:  167 Worker Num:  7 \n",
            " Loss:  0.41285669803619385\n",
            "Test:  Iteration No:  167 \n",
            " Loss:  0.2653586784071183\n",
            "Test accuracy:  92.33\n",
            "Training:  Iteration No:  168 Worker Num:  0 \n",
            " Loss:  0.2937733232975006\n",
            "Training:  Iteration No:  168 Worker Num:  1 \n",
            " Loss:  0.30822843313217163\n",
            "Training:  Iteration No:  168 Worker Num:  2 \n",
            " Loss:  0.2608999013900757\n",
            "Training:  Iteration No:  168 Worker Num:  3 \n",
            " Loss:  0.318625807762146\n",
            "Training:  Iteration No:  168 Worker Num:  4 \n",
            " Loss:  0.34032946825027466\n",
            "Training:  Iteration No:  168 Worker Num:  5 \n",
            " Loss:  0.2842883765697479\n",
            "Training:  Iteration No:  168 Worker Num:  6 \n",
            " Loss:  0.3778938353061676\n",
            "Training:  Iteration No:  168 Worker Num:  7 \n",
            " Loss:  0.38005009293556213\n",
            "Test:  Iteration No:  168 \n",
            " Loss:  0.2582310826718053\n",
            "Test accuracy:  92.61\n",
            "Training:  Iteration No:  169 Worker Num:  0 \n",
            " Loss:  0.22350463271141052\n",
            "Training:  Iteration No:  169 Worker Num:  1 \n",
            " Loss:  0.20629167556762695\n",
            "Training:  Iteration No:  169 Worker Num:  2 \n",
            " Loss:  0.3306230902671814\n",
            "Training:  Iteration No:  169 Worker Num:  3 \n",
            " Loss:  0.19339631497859955\n",
            "Training:  Iteration No:  169 Worker Num:  4 \n",
            " Loss:  0.3562917113304138\n",
            "Training:  Iteration No:  169 Worker Num:  5 \n",
            " Loss:  0.3913279175758362\n",
            "Training:  Iteration No:  169 Worker Num:  6 \n",
            " Loss:  0.2891213297843933\n",
            "Training:  Iteration No:  169 Worker Num:  7 \n",
            " Loss:  0.3360942006111145\n",
            "Test:  Iteration No:  169 \n",
            " Loss:  0.25602291701243646\n",
            "Test accuracy:  92.7\n",
            "Training:  Iteration No:  170 Worker Num:  0 \n",
            " Loss:  0.28046658635139465\n",
            "Training:  Iteration No:  170 Worker Num:  1 \n",
            " Loss:  0.3525466024875641\n",
            "Training:  Iteration No:  170 Worker Num:  2 \n",
            " Loss:  0.329219251871109\n",
            "Training:  Iteration No:  170 Worker Num:  3 \n",
            " Loss:  0.3929619789123535\n",
            "Training:  Iteration No:  170 Worker Num:  4 \n",
            " Loss:  0.2901371419429779\n",
            "Training:  Iteration No:  170 Worker Num:  5 \n",
            " Loss:  0.3428633511066437\n",
            "Training:  Iteration No:  170 Worker Num:  6 \n",
            " Loss:  0.40151286125183105\n",
            "Training:  Iteration No:  170 Worker Num:  7 \n",
            " Loss:  0.2815992832183838\n",
            "Test:  Iteration No:  170 \n",
            " Loss:  0.2580365764236526\n",
            "Test accuracy:  92.47\n",
            "Training:  Iteration No:  171 Worker Num:  0 \n",
            " Loss:  0.18418395519256592\n",
            "Training:  Iteration No:  171 Worker Num:  1 \n",
            " Loss:  0.3190648555755615\n",
            "Training:  Iteration No:  171 Worker Num:  2 \n",
            " Loss:  0.2928260862827301\n",
            "Training:  Iteration No:  171 Worker Num:  3 \n",
            " Loss:  0.3198640048503876\n",
            "Training:  Iteration No:  171 Worker Num:  4 \n",
            " Loss:  0.35670384764671326\n",
            "Training:  Iteration No:  171 Worker Num:  5 \n",
            " Loss:  0.2984228730201721\n",
            "Training:  Iteration No:  171 Worker Num:  6 \n",
            " Loss:  0.22528688609600067\n",
            "Training:  Iteration No:  171 Worker Num:  7 \n",
            " Loss:  0.2759709060192108\n",
            "Test:  Iteration No:  171 \n",
            " Loss:  0.2568258136699471\n",
            "Test accuracy:  92.45\n",
            "Training:  Iteration No:  172 Worker Num:  0 \n",
            " Loss:  0.31396758556365967\n",
            "Training:  Iteration No:  172 Worker Num:  1 \n",
            " Loss:  0.2329331785440445\n",
            "Training:  Iteration No:  172 Worker Num:  2 \n",
            " Loss:  0.32159048318862915\n",
            "Training:  Iteration No:  172 Worker Num:  3 \n",
            " Loss:  0.28574442863464355\n",
            "Training:  Iteration No:  172 Worker Num:  4 \n",
            " Loss:  0.3607242703437805\n",
            "Training:  Iteration No:  172 Worker Num:  5 \n",
            " Loss:  0.30891141295433044\n",
            "Training:  Iteration No:  172 Worker Num:  6 \n",
            " Loss:  0.2396828532218933\n",
            "Training:  Iteration No:  172 Worker Num:  7 \n",
            " Loss:  0.28848057985305786\n",
            "Test:  Iteration No:  172 \n",
            " Loss:  0.25741642810215676\n",
            "Test accuracy:  92.48\n",
            "Training:  Iteration No:  173 Worker Num:  0 \n",
            " Loss:  0.22669681906700134\n",
            "Training:  Iteration No:  173 Worker Num:  1 \n",
            " Loss:  0.28162580728530884\n",
            "Training:  Iteration No:  173 Worker Num:  2 \n",
            " Loss:  0.2378082275390625\n",
            "Training:  Iteration No:  173 Worker Num:  3 \n",
            " Loss:  0.44892415404319763\n",
            "Training:  Iteration No:  173 Worker Num:  4 \n",
            " Loss:  0.3919227123260498\n",
            "Training:  Iteration No:  173 Worker Num:  5 \n",
            " Loss:  0.26903748512268066\n",
            "Training:  Iteration No:  173 Worker Num:  6 \n",
            " Loss:  0.3083493709564209\n",
            "Training:  Iteration No:  173 Worker Num:  7 \n",
            " Loss:  0.4491874873638153\n",
            "Test:  Iteration No:  173 \n",
            " Loss:  0.25823632957814613\n",
            "Test accuracy:  92.47\n",
            "Training:  Iteration No:  174 Worker Num:  0 \n",
            " Loss:  0.19694970548152924\n",
            "Training:  Iteration No:  174 Worker Num:  1 \n",
            " Loss:  0.35251525044441223\n",
            "Training:  Iteration No:  174 Worker Num:  2 \n",
            " Loss:  0.3434952199459076\n",
            "Training:  Iteration No:  174 Worker Num:  3 \n",
            " Loss:  0.3434416651725769\n",
            "Training:  Iteration No:  174 Worker Num:  4 \n",
            " Loss:  0.3036499321460724\n",
            "Training:  Iteration No:  174 Worker Num:  5 \n",
            " Loss:  0.3749150037765503\n",
            "Training:  Iteration No:  174 Worker Num:  6 \n",
            " Loss:  0.3270834982395172\n",
            "Training:  Iteration No:  174 Worker Num:  7 \n",
            " Loss:  0.2519816756248474\n",
            "Test:  Iteration No:  174 \n",
            " Loss:  0.2630033112024959\n",
            "Test accuracy:  92.36\n",
            "Training:  Iteration No:  175 Worker Num:  0 \n",
            " Loss:  0.3120619058609009\n",
            "Training:  Iteration No:  175 Worker Num:  1 \n",
            " Loss:  0.39926961064338684\n",
            "Training:  Iteration No:  175 Worker Num:  2 \n",
            " Loss:  0.4378175735473633\n",
            "Training:  Iteration No:  175 Worker Num:  3 \n",
            " Loss:  0.24508678913116455\n",
            "Training:  Iteration No:  175 Worker Num:  4 \n",
            " Loss:  0.3782952129840851\n",
            "Training:  Iteration No:  175 Worker Num:  5 \n",
            " Loss:  0.37623167037963867\n",
            "Training:  Iteration No:  175 Worker Num:  6 \n",
            " Loss:  0.33047202229499817\n",
            "Training:  Iteration No:  175 Worker Num:  7 \n",
            " Loss:  0.35908055305480957\n",
            "Test:  Iteration No:  175 \n",
            " Loss:  0.259440996908123\n",
            "Test accuracy:  92.28\n",
            "Training:  Iteration No:  176 Worker Num:  0 \n",
            " Loss:  0.28826308250427246\n",
            "Training:  Iteration No:  176 Worker Num:  1 \n",
            " Loss:  0.3808249235153198\n",
            "Training:  Iteration No:  176 Worker Num:  2 \n",
            " Loss:  0.32920902967453003\n",
            "Training:  Iteration No:  176 Worker Num:  3 \n",
            " Loss:  0.4035760760307312\n",
            "Training:  Iteration No:  176 Worker Num:  4 \n",
            " Loss:  0.2894894480705261\n",
            "Training:  Iteration No:  176 Worker Num:  5 \n",
            " Loss:  0.24704237282276154\n",
            "Training:  Iteration No:  176 Worker Num:  6 \n",
            " Loss:  0.30064937472343445\n",
            "Training:  Iteration No:  176 Worker Num:  7 \n",
            " Loss:  0.40310540795326233\n",
            "Test:  Iteration No:  176 \n",
            " Loss:  0.2550855094826297\n",
            "Test accuracy:  92.64\n",
            "Training:  Iteration No:  177 Worker Num:  0 \n",
            " Loss:  0.3172784447669983\n",
            "Training:  Iteration No:  177 Worker Num:  1 \n",
            " Loss:  0.2807713747024536\n",
            "Training:  Iteration No:  177 Worker Num:  2 \n",
            " Loss:  0.21392515301704407\n",
            "Training:  Iteration No:  177 Worker Num:  3 \n",
            " Loss:  0.26795658469200134\n",
            "Training:  Iteration No:  177 Worker Num:  4 \n",
            " Loss:  0.3693656623363495\n",
            "Training:  Iteration No:  177 Worker Num:  5 \n",
            " Loss:  0.2558645009994507\n",
            "Training:  Iteration No:  177 Worker Num:  6 \n",
            " Loss:  0.291650652885437\n",
            "Training:  Iteration No:  177 Worker Num:  7 \n",
            " Loss:  0.32371029257774353\n",
            "Test:  Iteration No:  177 \n",
            " Loss:  0.2517791415883016\n",
            "Test accuracy:  92.48\n",
            "Training:  Iteration No:  178 Worker Num:  0 \n",
            " Loss:  0.24416904151439667\n",
            "Training:  Iteration No:  178 Worker Num:  1 \n",
            " Loss:  0.27188369631767273\n",
            "Training:  Iteration No:  178 Worker Num:  2 \n",
            " Loss:  0.1309201717376709\n",
            "Training:  Iteration No:  178 Worker Num:  3 \n",
            " Loss:  0.30543053150177\n",
            "Training:  Iteration No:  178 Worker Num:  4 \n",
            " Loss:  0.39996084570884705\n",
            "Training:  Iteration No:  178 Worker Num:  5 \n",
            " Loss:  0.2073722779750824\n",
            "Training:  Iteration No:  178 Worker Num:  6 \n",
            " Loss:  0.17088815569877625\n",
            "Training:  Iteration No:  178 Worker Num:  7 \n",
            " Loss:  0.265752375125885\n",
            "Test:  Iteration No:  178 \n",
            " Loss:  0.2494005706114105\n",
            "Test accuracy:  92.68\n",
            "Training:  Iteration No:  179 Worker Num:  0 \n",
            " Loss:  0.3181763291358948\n",
            "Training:  Iteration No:  179 Worker Num:  1 \n",
            " Loss:  0.215750053524971\n",
            "Training:  Iteration No:  179 Worker Num:  2 \n",
            " Loss:  0.17658735811710358\n",
            "Training:  Iteration No:  179 Worker Num:  3 \n",
            " Loss:  0.1806824654340744\n",
            "Training:  Iteration No:  179 Worker Num:  4 \n",
            " Loss:  0.3625071346759796\n",
            "Training:  Iteration No:  179 Worker Num:  5 \n",
            " Loss:  0.16808687150478363\n",
            "Training:  Iteration No:  179 Worker Num:  6 \n",
            " Loss:  0.2011982798576355\n",
            "Training:  Iteration No:  179 Worker Num:  7 \n",
            " Loss:  0.2898139953613281\n",
            "Test:  Iteration No:  179 \n",
            " Loss:  0.24676424413447892\n",
            "Test accuracy:  92.73\n",
            "Training:  Iteration No:  180 Worker Num:  0 \n",
            " Loss:  0.40641504526138306\n",
            "Training:  Iteration No:  180 Worker Num:  1 \n",
            " Loss:  0.3032612204551697\n",
            "Training:  Iteration No:  180 Worker Num:  2 \n",
            " Loss:  0.3011387586593628\n",
            "Training:  Iteration No:  180 Worker Num:  3 \n",
            " Loss:  0.3507094979286194\n",
            "Training:  Iteration No:  180 Worker Num:  4 \n",
            " Loss:  0.23006883263587952\n",
            "Training:  Iteration No:  180 Worker Num:  5 \n",
            " Loss:  0.30848202109336853\n",
            "Training:  Iteration No:  180 Worker Num:  6 \n",
            " Loss:  0.31748852133750916\n",
            "Training:  Iteration No:  180 Worker Num:  7 \n",
            " Loss:  0.36414068937301636\n",
            "Test:  Iteration No:  180 \n",
            " Loss:  0.2474942722746843\n",
            "Test accuracy:  92.9\n",
            "Training:  Iteration No:  181 Worker Num:  0 \n",
            " Loss:  0.25193771719932556\n",
            "Training:  Iteration No:  181 Worker Num:  1 \n",
            " Loss:  0.3299008309841156\n",
            "Training:  Iteration No:  181 Worker Num:  2 \n",
            " Loss:  0.2684542238712311\n",
            "Training:  Iteration No:  181 Worker Num:  3 \n",
            " Loss:  0.35414859652519226\n",
            "Training:  Iteration No:  181 Worker Num:  4 \n",
            " Loss:  0.4265846610069275\n",
            "Training:  Iteration No:  181 Worker Num:  5 \n",
            " Loss:  0.4031749963760376\n",
            "Training:  Iteration No:  181 Worker Num:  6 \n",
            " Loss:  0.3109991252422333\n",
            "Training:  Iteration No:  181 Worker Num:  7 \n",
            " Loss:  0.348222553730011\n",
            "Test:  Iteration No:  181 \n",
            " Loss:  0.2435834564834456\n",
            "Test accuracy:  92.78\n",
            "Training:  Iteration No:  182 Worker Num:  0 \n",
            " Loss:  0.31651070713996887\n",
            "Training:  Iteration No:  182 Worker Num:  1 \n",
            " Loss:  0.21743465960025787\n",
            "Training:  Iteration No:  182 Worker Num:  2 \n",
            " Loss:  0.3471551239490509\n",
            "Training:  Iteration No:  182 Worker Num:  3 \n",
            " Loss:  0.28857138752937317\n",
            "Training:  Iteration No:  182 Worker Num:  4 \n",
            " Loss:  0.2994174659252167\n",
            "Training:  Iteration No:  182 Worker Num:  5 \n",
            " Loss:  0.17730894684791565\n",
            "Training:  Iteration No:  182 Worker Num:  6 \n",
            " Loss:  0.3077864348888397\n",
            "Training:  Iteration No:  182 Worker Num:  7 \n",
            " Loss:  0.4567978084087372\n",
            "Test:  Iteration No:  182 \n",
            " Loss:  0.24538364336837695\n",
            "Test accuracy:  92.83\n",
            "Training:  Iteration No:  183 Worker Num:  0 \n",
            " Loss:  0.38204485177993774\n",
            "Training:  Iteration No:  183 Worker Num:  1 \n",
            " Loss:  0.3209383189678192\n",
            "Training:  Iteration No:  183 Worker Num:  2 \n",
            " Loss:  0.31147733330726624\n",
            "Training:  Iteration No:  183 Worker Num:  3 \n",
            " Loss:  0.2705516815185547\n",
            "Training:  Iteration No:  183 Worker Num:  4 \n",
            " Loss:  0.2478991448879242\n",
            "Training:  Iteration No:  183 Worker Num:  5 \n",
            " Loss:  0.3668723404407501\n",
            "Training:  Iteration No:  183 Worker Num:  6 \n",
            " Loss:  0.35956624150276184\n",
            "Training:  Iteration No:  183 Worker Num:  7 \n",
            " Loss:  0.1860460788011551\n",
            "Test:  Iteration No:  183 \n",
            " Loss:  0.24078000329812116\n",
            "Test accuracy:  93.03\n",
            "Training:  Iteration No:  184 Worker Num:  0 \n",
            " Loss:  0.3302837312221527\n",
            "Training:  Iteration No:  184 Worker Num:  1 \n",
            " Loss:  0.3516005873680115\n",
            "Training:  Iteration No:  184 Worker Num:  2 \n",
            " Loss:  0.3087179958820343\n",
            "Training:  Iteration No:  184 Worker Num:  3 \n",
            " Loss:  0.19411906599998474\n",
            "Training:  Iteration No:  184 Worker Num:  4 \n",
            " Loss:  0.325920045375824\n",
            "Training:  Iteration No:  184 Worker Num:  5 \n",
            " Loss:  0.2884562015533447\n",
            "Training:  Iteration No:  184 Worker Num:  6 \n",
            " Loss:  0.32745712995529175\n",
            "Training:  Iteration No:  184 Worker Num:  7 \n",
            " Loss:  0.30844631791114807\n",
            "Test:  Iteration No:  184 \n",
            " Loss:  0.24052936897341964\n",
            "Test accuracy:  92.8\n",
            "Training:  Iteration No:  185 Worker Num:  0 \n",
            " Loss:  0.2693198025226593\n",
            "Training:  Iteration No:  185 Worker Num:  1 \n",
            " Loss:  0.3635980188846588\n",
            "Training:  Iteration No:  185 Worker Num:  2 \n",
            " Loss:  0.27723124623298645\n",
            "Training:  Iteration No:  185 Worker Num:  3 \n",
            " Loss:  0.37111446261405945\n",
            "Training:  Iteration No:  185 Worker Num:  4 \n",
            " Loss:  0.20916040241718292\n",
            "Training:  Iteration No:  185 Worker Num:  5 \n",
            " Loss:  0.5197706818580627\n",
            "Training:  Iteration No:  185 Worker Num:  6 \n",
            " Loss:  0.24807532131671906\n",
            "Training:  Iteration No:  185 Worker Num:  7 \n",
            " Loss:  0.2656063437461853\n",
            "Test:  Iteration No:  185 \n",
            " Loss:  0.23968884351227102\n",
            "Test accuracy:  92.99\n",
            "Training:  Iteration No:  186 Worker Num:  0 \n",
            " Loss:  0.28058502078056335\n",
            "Training:  Iteration No:  186 Worker Num:  1 \n",
            " Loss:  0.33651530742645264\n",
            "Training:  Iteration No:  186 Worker Num:  2 \n",
            " Loss:  0.2773515284061432\n",
            "Training:  Iteration No:  186 Worker Num:  3 \n",
            " Loss:  0.14776962995529175\n",
            "Training:  Iteration No:  186 Worker Num:  4 \n",
            " Loss:  0.22779418528079987\n",
            "Training:  Iteration No:  186 Worker Num:  5 \n",
            " Loss:  0.44828641414642334\n",
            "Training:  Iteration No:  186 Worker Num:  6 \n",
            " Loss:  0.30838117003440857\n",
            "Training:  Iteration No:  186 Worker Num:  7 \n",
            " Loss:  0.3934212327003479\n",
            "Test:  Iteration No:  186 \n",
            " Loss:  0.23979887618577178\n",
            "Test accuracy:  93.08\n",
            "Training:  Iteration No:  187 Worker Num:  0 \n",
            " Loss:  0.26178574562072754\n",
            "Training:  Iteration No:  187 Worker Num:  1 \n",
            " Loss:  0.29173368215560913\n",
            "Training:  Iteration No:  187 Worker Num:  2 \n",
            " Loss:  0.42546549439430237\n",
            "Training:  Iteration No:  187 Worker Num:  3 \n",
            " Loss:  0.1967039406299591\n",
            "Training:  Iteration No:  187 Worker Num:  4 \n",
            " Loss:  0.17207525670528412\n",
            "Training:  Iteration No:  187 Worker Num:  5 \n",
            " Loss:  0.2477002590894699\n",
            "Training:  Iteration No:  187 Worker Num:  6 \n",
            " Loss:  0.40285590291023254\n",
            "Training:  Iteration No:  187 Worker Num:  7 \n",
            " Loss:  0.2823497951030731\n",
            "Test:  Iteration No:  187 \n",
            " Loss:  0.23942217900405957\n",
            "Test accuracy:  93.1\n",
            "Training:  Iteration No:  188 Worker Num:  0 \n",
            " Loss:  0.27358055114746094\n",
            "Training:  Iteration No:  188 Worker Num:  1 \n",
            " Loss:  0.26964813470840454\n",
            "Training:  Iteration No:  188 Worker Num:  2 \n",
            " Loss:  0.21766674518585205\n",
            "Training:  Iteration No:  188 Worker Num:  3 \n",
            " Loss:  0.25546738505363464\n",
            "Training:  Iteration No:  188 Worker Num:  4 \n",
            " Loss:  0.3189162313938141\n",
            "Training:  Iteration No:  188 Worker Num:  5 \n",
            " Loss:  0.42983391880989075\n",
            "Training:  Iteration No:  188 Worker Num:  6 \n",
            " Loss:  0.2785916328430176\n",
            "Training:  Iteration No:  188 Worker Num:  7 \n",
            " Loss:  0.36226364970207214\n",
            "Test:  Iteration No:  188 \n",
            " Loss:  0.23731839692196513\n",
            "Test accuracy:  93.1\n",
            "Training:  Iteration No:  189 Worker Num:  0 \n",
            " Loss:  0.18511585891246796\n",
            "Training:  Iteration No:  189 Worker Num:  1 \n",
            " Loss:  0.3302145004272461\n",
            "Training:  Iteration No:  189 Worker Num:  2 \n",
            " Loss:  0.148651584982872\n",
            "Training:  Iteration No:  189 Worker Num:  3 \n",
            " Loss:  0.3067346513271332\n",
            "Training:  Iteration No:  189 Worker Num:  4 \n",
            " Loss:  0.31270378828048706\n",
            "Training:  Iteration No:  189 Worker Num:  5 \n",
            " Loss:  0.39290598034858704\n",
            "Training:  Iteration No:  189 Worker Num:  6 \n",
            " Loss:  0.294766366481781\n",
            "Training:  Iteration No:  189 Worker Num:  7 \n",
            " Loss:  0.2672777771949768\n",
            "Test:  Iteration No:  189 \n",
            " Loss:  0.2346897729307036\n",
            "Test accuracy:  93.19\n",
            "Training:  Iteration No:  190 Worker Num:  0 \n",
            " Loss:  0.24601595103740692\n",
            "Training:  Iteration No:  190 Worker Num:  1 \n",
            " Loss:  0.2254907637834549\n",
            "Training:  Iteration No:  190 Worker Num:  2 \n",
            " Loss:  0.3034569025039673\n",
            "Training:  Iteration No:  190 Worker Num:  3 \n",
            " Loss:  0.25075578689575195\n",
            "Training:  Iteration No:  190 Worker Num:  4 \n",
            " Loss:  0.2555612623691559\n",
            "Training:  Iteration No:  190 Worker Num:  5 \n",
            " Loss:  0.2308492660522461\n",
            "Training:  Iteration No:  190 Worker Num:  6 \n",
            " Loss:  0.18220002949237823\n",
            "Training:  Iteration No:  190 Worker Num:  7 \n",
            " Loss:  0.2923075258731842\n",
            "Test:  Iteration No:  190 \n",
            " Loss:  0.23847348492922663\n",
            "Test accuracy:  92.97\n",
            "Training:  Iteration No:  191 Worker Num:  0 \n",
            " Loss:  0.17282775044441223\n",
            "Training:  Iteration No:  191 Worker Num:  1 \n",
            " Loss:  0.242758646607399\n",
            "Training:  Iteration No:  191 Worker Num:  2 \n",
            " Loss:  0.30139490962028503\n",
            "Training:  Iteration No:  191 Worker Num:  3 \n",
            " Loss:  0.2052992284297943\n",
            "Training:  Iteration No:  191 Worker Num:  4 \n",
            " Loss:  0.26788127422332764\n",
            "Training:  Iteration No:  191 Worker Num:  5 \n",
            " Loss:  0.37404024600982666\n",
            "Training:  Iteration No:  191 Worker Num:  6 \n",
            " Loss:  0.3132293224334717\n",
            "Training:  Iteration No:  191 Worker Num:  7 \n",
            " Loss:  0.1932859718799591\n",
            "Test:  Iteration No:  191 \n",
            " Loss:  0.23628306134214885\n",
            "Test accuracy:  93.1\n",
            "Training:  Iteration No:  192 Worker Num:  0 \n",
            " Loss:  0.42539435625076294\n",
            "Training:  Iteration No:  192 Worker Num:  1 \n",
            " Loss:  0.2877567410469055\n",
            "Training:  Iteration No:  192 Worker Num:  2 \n",
            " Loss:  0.35888001322746277\n",
            "Training:  Iteration No:  192 Worker Num:  3 \n",
            " Loss:  0.3131581246852875\n",
            "Training:  Iteration No:  192 Worker Num:  4 \n",
            " Loss:  0.26673078536987305\n",
            "Training:  Iteration No:  192 Worker Num:  5 \n",
            " Loss:  0.28064635396003723\n",
            "Training:  Iteration No:  192 Worker Num:  6 \n",
            " Loss:  0.37946373224258423\n",
            "Training:  Iteration No:  192 Worker Num:  7 \n",
            " Loss:  0.23161032795906067\n",
            "Test:  Iteration No:  192 \n",
            " Loss:  0.2338768380305058\n",
            "Test accuracy:  93.18\n",
            "Training:  Iteration No:  193 Worker Num:  0 \n",
            " Loss:  0.22931237518787384\n",
            "Training:  Iteration No:  193 Worker Num:  1 \n",
            " Loss:  0.2575906813144684\n",
            "Training:  Iteration No:  193 Worker Num:  2 \n",
            " Loss:  0.18850192427635193\n",
            "Training:  Iteration No:  193 Worker Num:  3 \n",
            " Loss:  0.31235355138778687\n",
            "Training:  Iteration No:  193 Worker Num:  4 \n",
            " Loss:  0.29188719391822815\n",
            "Training:  Iteration No:  193 Worker Num:  5 \n",
            " Loss:  0.2356196492910385\n",
            "Training:  Iteration No:  193 Worker Num:  6 \n",
            " Loss:  0.3281078636646271\n",
            "Training:  Iteration No:  193 Worker Num:  7 \n",
            " Loss:  0.17550085484981537\n",
            "Test:  Iteration No:  193 \n",
            " Loss:  0.23690321353040164\n",
            "Test accuracy:  92.85\n",
            "Training:  Iteration No:  194 Worker Num:  0 \n",
            " Loss:  0.23726949095726013\n",
            "Training:  Iteration No:  194 Worker Num:  1 \n",
            " Loss:  0.23192530870437622\n",
            "Training:  Iteration No:  194 Worker Num:  2 \n",
            " Loss:  0.26722094416618347\n",
            "Training:  Iteration No:  194 Worker Num:  3 \n",
            " Loss:  0.12141103297472\n",
            "Training:  Iteration No:  194 Worker Num:  4 \n",
            " Loss:  0.4450146555900574\n",
            "Training:  Iteration No:  194 Worker Num:  5 \n",
            " Loss:  0.36665427684783936\n",
            "Training:  Iteration No:  194 Worker Num:  6 \n",
            " Loss:  0.29519587755203247\n",
            "Training:  Iteration No:  194 Worker Num:  7 \n",
            " Loss:  0.2163379192352295\n",
            "Test:  Iteration No:  194 \n",
            " Loss:  0.23440799810275248\n",
            "Test accuracy:  93.13\n",
            "Training:  Iteration No:  195 Worker Num:  0 \n",
            " Loss:  0.30563825368881226\n",
            "Training:  Iteration No:  195 Worker Num:  1 \n",
            " Loss:  0.32600143551826477\n",
            "Training:  Iteration No:  195 Worker Num:  2 \n",
            " Loss:  0.3151891827583313\n",
            "Training:  Iteration No:  195 Worker Num:  3 \n",
            " Loss:  0.2921636402606964\n",
            "Training:  Iteration No:  195 Worker Num:  4 \n",
            " Loss:  0.30895063281059265\n",
            "Training:  Iteration No:  195 Worker Num:  5 \n",
            " Loss:  0.27537333965301514\n",
            "Training:  Iteration No:  195 Worker Num:  6 \n",
            " Loss:  0.280685693025589\n",
            "Training:  Iteration No:  195 Worker Num:  7 \n",
            " Loss:  0.2952653765678406\n",
            "Test:  Iteration No:  195 \n",
            " Loss:  0.23120187146329804\n",
            "Test accuracy:  93.25\n",
            "Training:  Iteration No:  196 Worker Num:  0 \n",
            " Loss:  0.2669741213321686\n",
            "Training:  Iteration No:  196 Worker Num:  1 \n",
            " Loss:  0.18667611479759216\n",
            "Training:  Iteration No:  196 Worker Num:  2 \n",
            " Loss:  0.24835821986198425\n",
            "Training:  Iteration No:  196 Worker Num:  3 \n",
            " Loss:  0.24528998136520386\n",
            "Training:  Iteration No:  196 Worker Num:  4 \n",
            " Loss:  0.38817185163497925\n",
            "Training:  Iteration No:  196 Worker Num:  5 \n",
            " Loss:  0.34926077723503113\n",
            "Training:  Iteration No:  196 Worker Num:  6 \n",
            " Loss:  0.30403828620910645\n",
            "Training:  Iteration No:  196 Worker Num:  7 \n",
            " Loss:  0.31665924191474915\n",
            "Test:  Iteration No:  196 \n",
            " Loss:  0.2285019149650124\n",
            "Test accuracy:  93.24\n",
            "Training:  Iteration No:  197 Worker Num:  0 \n",
            " Loss:  0.1786622405052185\n",
            "Training:  Iteration No:  197 Worker Num:  1 \n",
            " Loss:  0.22470270097255707\n",
            "Training:  Iteration No:  197 Worker Num:  2 \n",
            " Loss:  0.27429378032684326\n",
            "Training:  Iteration No:  197 Worker Num:  3 \n",
            " Loss:  0.34628719091415405\n",
            "Training:  Iteration No:  197 Worker Num:  4 \n",
            " Loss:  0.2901186943054199\n",
            "Training:  Iteration No:  197 Worker Num:  5 \n",
            " Loss:  0.19991764426231384\n",
            "Training:  Iteration No:  197 Worker Num:  6 \n",
            " Loss:  0.2684650421142578\n",
            "Training:  Iteration No:  197 Worker Num:  7 \n",
            " Loss:  0.18015612661838531\n",
            "Test:  Iteration No:  197 \n",
            " Loss:  0.23025081351493734\n",
            "Test accuracy:  93.2\n",
            "Training:  Iteration No:  198 Worker Num:  0 \n",
            " Loss:  0.1790890395641327\n",
            "Training:  Iteration No:  198 Worker Num:  1 \n",
            " Loss:  0.27105361223220825\n",
            "Training:  Iteration No:  198 Worker Num:  2 \n",
            " Loss:  0.2959175109863281\n",
            "Training:  Iteration No:  198 Worker Num:  3 \n",
            " Loss:  0.27723565697669983\n",
            "Training:  Iteration No:  198 Worker Num:  4 \n",
            " Loss:  0.3059355318546295\n",
            "Training:  Iteration No:  198 Worker Num:  5 \n",
            " Loss:  0.3701465427875519\n",
            "Training:  Iteration No:  198 Worker Num:  6 \n",
            " Loss:  0.41709670424461365\n",
            "Training:  Iteration No:  198 Worker Num:  7 \n",
            " Loss:  0.2671203315258026\n",
            "Test:  Iteration No:  198 \n",
            " Loss:  0.22872237215125107\n",
            "Test accuracy:  93.16\n",
            "Training:  Iteration No:  199 Worker Num:  0 \n",
            " Loss:  0.3954675495624542\n",
            "Training:  Iteration No:  199 Worker Num:  1 \n",
            " Loss:  0.20169053971767426\n",
            "Training:  Iteration No:  199 Worker Num:  2 \n",
            " Loss:  0.26664265990257263\n",
            "Training:  Iteration No:  199 Worker Num:  3 \n",
            " Loss:  0.3328189253807068\n",
            "Training:  Iteration No:  199 Worker Num:  4 \n",
            " Loss:  0.3044431805610657\n",
            "Training:  Iteration No:  199 Worker Num:  5 \n",
            " Loss:  0.25665226578712463\n",
            "Training:  Iteration No:  199 Worker Num:  6 \n",
            " Loss:  0.3253873884677887\n",
            "Training:  Iteration No:  199 Worker Num:  7 \n",
            " Loss:  0.25683462619781494\n",
            "Test:  Iteration No:  199 \n",
            " Loss:  0.2258834111020912\n",
            "Test accuracy:  93.27\n",
            "Training:  Iteration No:  200 Worker Num:  0 \n",
            " Loss:  0.32836973667144775\n",
            "Training:  Iteration No:  200 Worker Num:  1 \n",
            " Loss:  0.39714300632476807\n",
            "Training:  Iteration No:  200 Worker Num:  2 \n",
            " Loss:  0.22418756783008575\n",
            "Training:  Iteration No:  200 Worker Num:  3 \n",
            " Loss:  0.2401052564382553\n",
            "Training:  Iteration No:  200 Worker Num:  4 \n",
            " Loss:  0.24515829980373383\n",
            "Training:  Iteration No:  200 Worker Num:  5 \n",
            " Loss:  0.17176656424999237\n",
            "Training:  Iteration No:  200 Worker Num:  6 \n",
            " Loss:  0.16481874883174896\n",
            "Training:  Iteration No:  200 Worker Num:  7 \n",
            " Loss:  0.13201561570167542\n",
            "Test:  Iteration No:  200 \n",
            " Loss:  0.22651380760288692\n",
            "Test accuracy:  93.44\n",
            "Training:  Iteration No:  201 Worker Num:  0 \n",
            " Loss:  0.24418111145496368\n",
            "Training:  Iteration No:  201 Worker Num:  1 \n",
            " Loss:  0.28299224376678467\n",
            "Training:  Iteration No:  201 Worker Num:  2 \n",
            " Loss:  0.2784847319126129\n",
            "Training:  Iteration No:  201 Worker Num:  3 \n",
            " Loss:  0.3204959034919739\n",
            "Training:  Iteration No:  201 Worker Num:  4 \n",
            " Loss:  0.3225954473018646\n",
            "Training:  Iteration No:  201 Worker Num:  5 \n",
            " Loss:  0.22903940081596375\n",
            "Training:  Iteration No:  201 Worker Num:  6 \n",
            " Loss:  0.2050544023513794\n",
            "Training:  Iteration No:  201 Worker Num:  7 \n",
            " Loss:  0.1630808413028717\n",
            "Test:  Iteration No:  201 \n",
            " Loss:  0.22668626253740698\n",
            "Test accuracy:  93.41\n",
            "Training:  Iteration No:  202 Worker Num:  0 \n",
            " Loss:  0.3397231101989746\n",
            "Training:  Iteration No:  202 Worker Num:  1 \n",
            " Loss:  0.19441957771778107\n",
            "Training:  Iteration No:  202 Worker Num:  2 \n",
            " Loss:  0.2500416040420532\n",
            "Training:  Iteration No:  202 Worker Num:  3 \n",
            " Loss:  0.33449694514274597\n",
            "Training:  Iteration No:  202 Worker Num:  4 \n",
            " Loss:  0.22427783906459808\n",
            "Training:  Iteration No:  202 Worker Num:  5 \n",
            " Loss:  0.3180082142353058\n",
            "Training:  Iteration No:  202 Worker Num:  6 \n",
            " Loss:  0.18497510254383087\n",
            "Training:  Iteration No:  202 Worker Num:  7 \n",
            " Loss:  0.20968501269817352\n",
            "Test:  Iteration No:  202 \n",
            " Loss:  0.22473819049287447\n",
            "Test accuracy:  93.33\n",
            "Training:  Iteration No:  203 Worker Num:  0 \n",
            " Loss:  0.211518332362175\n",
            "Training:  Iteration No:  203 Worker Num:  1 \n",
            " Loss:  0.2709958851337433\n",
            "Training:  Iteration No:  203 Worker Num:  2 \n",
            " Loss:  0.292490690946579\n",
            "Training:  Iteration No:  203 Worker Num:  3 \n",
            " Loss:  0.2315845787525177\n",
            "Training:  Iteration No:  203 Worker Num:  4 \n",
            " Loss:  0.31975868344306946\n",
            "Training:  Iteration No:  203 Worker Num:  5 \n",
            " Loss:  0.2618263363838196\n",
            "Training:  Iteration No:  203 Worker Num:  6 \n",
            " Loss:  0.1933516561985016\n",
            "Training:  Iteration No:  203 Worker Num:  7 \n",
            " Loss:  0.2666150629520416\n",
            "Test:  Iteration No:  203 \n",
            " Loss:  0.22540050839321524\n",
            "Test accuracy:  93.34\n",
            "Training:  Iteration No:  204 Worker Num:  0 \n",
            " Loss:  0.3522825539112091\n",
            "Training:  Iteration No:  204 Worker Num:  1 \n",
            " Loss:  0.1836586594581604\n",
            "Training:  Iteration No:  204 Worker Num:  2 \n",
            " Loss:  0.3047899305820465\n",
            "Training:  Iteration No:  204 Worker Num:  3 \n",
            " Loss:  0.19468620419502258\n",
            "Training:  Iteration No:  204 Worker Num:  4 \n",
            " Loss:  0.2259497046470642\n",
            "Training:  Iteration No:  204 Worker Num:  5 \n",
            " Loss:  0.308936208486557\n",
            "Training:  Iteration No:  204 Worker Num:  6 \n",
            " Loss:  0.40933874249458313\n",
            "Training:  Iteration No:  204 Worker Num:  7 \n",
            " Loss:  0.3612188994884491\n",
            "Test:  Iteration No:  204 \n",
            " Loss:  0.2263696518596969\n",
            "Test accuracy:  93.21\n",
            "Training:  Iteration No:  205 Worker Num:  0 \n",
            " Loss:  0.3308557868003845\n",
            "Training:  Iteration No:  205 Worker Num:  1 \n",
            " Loss:  0.35366883873939514\n",
            "Training:  Iteration No:  205 Worker Num:  2 \n",
            " Loss:  0.29884910583496094\n",
            "Training:  Iteration No:  205 Worker Num:  3 \n",
            " Loss:  0.18086647987365723\n",
            "Training:  Iteration No:  205 Worker Num:  4 \n",
            " Loss:  0.27142632007598877\n",
            "Training:  Iteration No:  205 Worker Num:  5 \n",
            " Loss:  0.36008065938949585\n",
            "Training:  Iteration No:  205 Worker Num:  6 \n",
            " Loss:  0.33234429359436035\n",
            "Training:  Iteration No:  205 Worker Num:  7 \n",
            " Loss:  0.4670059084892273\n",
            "Test:  Iteration No:  205 \n",
            " Loss:  0.22231822717887692\n",
            "Test accuracy:  93.42\n",
            "Training:  Iteration No:  206 Worker Num:  0 \n",
            " Loss:  0.2835971415042877\n",
            "Training:  Iteration No:  206 Worker Num:  1 \n",
            " Loss:  0.3691927492618561\n",
            "Training:  Iteration No:  206 Worker Num:  2 \n",
            " Loss:  0.26295793056488037\n",
            "Training:  Iteration No:  206 Worker Num:  3 \n",
            " Loss:  0.3033052980899811\n",
            "Training:  Iteration No:  206 Worker Num:  4 \n",
            " Loss:  0.21772190928459167\n",
            "Training:  Iteration No:  206 Worker Num:  5 \n",
            " Loss:  0.32492056488990784\n",
            "Training:  Iteration No:  206 Worker Num:  6 \n",
            " Loss:  0.18411211669445038\n",
            "Training:  Iteration No:  206 Worker Num:  7 \n",
            " Loss:  0.28553691506385803\n",
            "Test:  Iteration No:  206 \n",
            " Loss:  0.22925130044451997\n",
            "Test accuracy:  93.15\n",
            "Training:  Iteration No:  207 Worker Num:  0 \n",
            " Loss:  0.24727678298950195\n",
            "Training:  Iteration No:  207 Worker Num:  1 \n",
            " Loss:  0.20157015323638916\n",
            "Training:  Iteration No:  207 Worker Num:  2 \n",
            " Loss:  0.20269989967346191\n",
            "Training:  Iteration No:  207 Worker Num:  3 \n",
            " Loss:  0.246896430850029\n",
            "Training:  Iteration No:  207 Worker Num:  4 \n",
            " Loss:  0.38598790764808655\n",
            "Training:  Iteration No:  207 Worker Num:  5 \n",
            " Loss:  0.39008229970932007\n",
            "Training:  Iteration No:  207 Worker Num:  6 \n",
            " Loss:  0.4724835157394409\n",
            "Training:  Iteration No:  207 Worker Num:  7 \n",
            " Loss:  0.2040221095085144\n",
            "Test:  Iteration No:  207 \n",
            " Loss:  0.22141687588506861\n",
            "Test accuracy:  93.48\n",
            "Training:  Iteration No:  208 Worker Num:  0 \n",
            " Loss:  0.3362830579280853\n",
            "Training:  Iteration No:  208 Worker Num:  1 \n",
            " Loss:  0.3275986313819885\n",
            "Training:  Iteration No:  208 Worker Num:  2 \n",
            " Loss:  0.15444616973400116\n",
            "Training:  Iteration No:  208 Worker Num:  3 \n",
            " Loss:  0.24778208136558533\n",
            "Training:  Iteration No:  208 Worker Num:  4 \n",
            " Loss:  0.23588845133781433\n",
            "Training:  Iteration No:  208 Worker Num:  5 \n",
            " Loss:  0.2292630970478058\n",
            "Training:  Iteration No:  208 Worker Num:  6 \n",
            " Loss:  0.23720622062683105\n",
            "Training:  Iteration No:  208 Worker Num:  7 \n",
            " Loss:  0.24072274565696716\n",
            "Test:  Iteration No:  208 \n",
            " Loss:  0.22387431125757815\n",
            "Test accuracy:  93.51\n",
            "Training:  Iteration No:  209 Worker Num:  0 \n",
            " Loss:  0.5440201163291931\n",
            "Training:  Iteration No:  209 Worker Num:  1 \n",
            " Loss:  0.322197288274765\n",
            "Training:  Iteration No:  209 Worker Num:  2 \n",
            " Loss:  0.4318721294403076\n",
            "Training:  Iteration No:  209 Worker Num:  3 \n",
            " Loss:  0.40607815980911255\n",
            "Training:  Iteration No:  209 Worker Num:  4 \n",
            " Loss:  0.2542082369327545\n",
            "Training:  Iteration No:  209 Worker Num:  5 \n",
            " Loss:  0.35661718249320984\n",
            "Training:  Iteration No:  209 Worker Num:  6 \n",
            " Loss:  0.32401224970817566\n",
            "Training:  Iteration No:  209 Worker Num:  7 \n",
            " Loss:  0.3706875145435333\n",
            "Test:  Iteration No:  209 \n",
            " Loss:  0.2185275110215703\n",
            "Test accuracy:  93.72\n",
            "Training:  Iteration No:  210 Worker Num:  0 \n",
            " Loss:  0.23346666991710663\n",
            "Training:  Iteration No:  210 Worker Num:  1 \n",
            " Loss:  0.27265995740890503\n",
            "Training:  Iteration No:  210 Worker Num:  2 \n",
            " Loss:  0.30386021733283997\n",
            "Training:  Iteration No:  210 Worker Num:  3 \n",
            " Loss:  0.23037557303905487\n",
            "Training:  Iteration No:  210 Worker Num:  4 \n",
            " Loss:  0.2462128847837448\n",
            "Training:  Iteration No:  210 Worker Num:  5 \n",
            " Loss:  0.2738296091556549\n",
            "Training:  Iteration No:  210 Worker Num:  6 \n",
            " Loss:  0.3547308146953583\n",
            "Training:  Iteration No:  210 Worker Num:  7 \n",
            " Loss:  0.37403053045272827\n",
            "Test:  Iteration No:  210 \n",
            " Loss:  0.22419354863040433\n",
            "Test accuracy:  93.44\n",
            "Training:  Iteration No:  211 Worker Num:  0 \n",
            " Loss:  0.37856534123420715\n",
            "Training:  Iteration No:  211 Worker Num:  1 \n",
            " Loss:  0.2019965946674347\n",
            "Training:  Iteration No:  211 Worker Num:  2 \n",
            " Loss:  0.2273518443107605\n",
            "Training:  Iteration No:  211 Worker Num:  3 \n",
            " Loss:  0.2813364267349243\n",
            "Training:  Iteration No:  211 Worker Num:  4 \n",
            " Loss:  0.2541787624359131\n",
            "Training:  Iteration No:  211 Worker Num:  5 \n",
            " Loss:  0.22268830239772797\n",
            "Training:  Iteration No:  211 Worker Num:  6 \n",
            " Loss:  0.18917962908744812\n",
            "Training:  Iteration No:  211 Worker Num:  7 \n",
            " Loss:  0.25344642996788025\n",
            "Test:  Iteration No:  211 \n",
            " Loss:  0.2217993260467354\n",
            "Test accuracy:  93.76\n",
            "Training:  Iteration No:  212 Worker Num:  0 \n",
            " Loss:  0.2737254798412323\n",
            "Training:  Iteration No:  212 Worker Num:  1 \n",
            " Loss:  0.2753046154975891\n",
            "Training:  Iteration No:  212 Worker Num:  2 \n",
            " Loss:  0.27939265966415405\n",
            "Training:  Iteration No:  212 Worker Num:  3 \n",
            " Loss:  0.11427929252386093\n",
            "Training:  Iteration No:  212 Worker Num:  4 \n",
            " Loss:  0.2686348855495453\n",
            "Training:  Iteration No:  212 Worker Num:  5 \n",
            " Loss:  0.38251402974128723\n",
            "Training:  Iteration No:  212 Worker Num:  6 \n",
            " Loss:  0.2840001881122589\n",
            "Training:  Iteration No:  212 Worker Num:  7 \n",
            " Loss:  0.22399520874023438\n",
            "Test:  Iteration No:  212 \n",
            " Loss:  0.21812520610003533\n",
            "Test accuracy:  93.54\n",
            "Training:  Iteration No:  213 Worker Num:  0 \n",
            " Loss:  0.20604941248893738\n",
            "Training:  Iteration No:  213 Worker Num:  1 \n",
            " Loss:  0.47326794266700745\n",
            "Training:  Iteration No:  213 Worker Num:  2 \n",
            " Loss:  0.22503194212913513\n",
            "Training:  Iteration No:  213 Worker Num:  3 \n",
            " Loss:  0.26468625664711\n",
            "Training:  Iteration No:  213 Worker Num:  4 \n",
            " Loss:  0.2598537504673004\n",
            "Training:  Iteration No:  213 Worker Num:  5 \n",
            " Loss:  0.2598012387752533\n",
            "Training:  Iteration No:  213 Worker Num:  6 \n",
            " Loss:  0.15870794653892517\n",
            "Training:  Iteration No:  213 Worker Num:  7 \n",
            " Loss:  0.2341458797454834\n",
            "Test:  Iteration No:  213 \n",
            " Loss:  0.2163889488747603\n",
            "Test accuracy:  93.72\n",
            "Training:  Iteration No:  214 Worker Num:  0 \n",
            " Loss:  0.3655511736869812\n",
            "Training:  Iteration No:  214 Worker Num:  1 \n",
            " Loss:  0.17888106405735016\n",
            "Training:  Iteration No:  214 Worker Num:  2 \n",
            " Loss:  0.19866470992565155\n",
            "Training:  Iteration No:  214 Worker Num:  3 \n",
            " Loss:  0.3595905303955078\n",
            "Training:  Iteration No:  214 Worker Num:  4 \n",
            " Loss:  0.209869384765625\n",
            "Training:  Iteration No:  214 Worker Num:  5 \n",
            " Loss:  0.2875218093395233\n",
            "Training:  Iteration No:  214 Worker Num:  6 \n",
            " Loss:  0.4085550010204315\n",
            "Training:  Iteration No:  214 Worker Num:  7 \n",
            " Loss:  0.2744007706642151\n",
            "Test:  Iteration No:  214 \n",
            " Loss:  0.2179298529024177\n",
            "Test accuracy:  93.56\n",
            "Training:  Iteration No:  215 Worker Num:  0 \n",
            " Loss:  0.20074895024299622\n",
            "Training:  Iteration No:  215 Worker Num:  1 \n",
            " Loss:  0.2739599943161011\n",
            "Training:  Iteration No:  215 Worker Num:  2 \n",
            " Loss:  0.1955929398536682\n",
            "Training:  Iteration No:  215 Worker Num:  3 \n",
            " Loss:  0.20528225600719452\n",
            "Training:  Iteration No:  215 Worker Num:  4 \n",
            " Loss:  0.27234336733818054\n",
            "Training:  Iteration No:  215 Worker Num:  5 \n",
            " Loss:  0.2460685819387436\n",
            "Training:  Iteration No:  215 Worker Num:  6 \n",
            " Loss:  0.30895423889160156\n",
            "Training:  Iteration No:  215 Worker Num:  7 \n",
            " Loss:  0.26139897108078003\n",
            "Test:  Iteration No:  215 \n",
            " Loss:  0.21708273189731792\n",
            "Test accuracy:  93.58\n",
            "Training:  Iteration No:  216 Worker Num:  0 \n",
            " Loss:  0.27433449029922485\n",
            "Training:  Iteration No:  216 Worker Num:  1 \n",
            " Loss:  0.2562702000141144\n",
            "Training:  Iteration No:  216 Worker Num:  2 \n",
            " Loss:  0.2639825642108917\n",
            "Training:  Iteration No:  216 Worker Num:  3 \n",
            " Loss:  0.19141867756843567\n",
            "Training:  Iteration No:  216 Worker Num:  4 \n",
            " Loss:  0.16355755925178528\n",
            "Training:  Iteration No:  216 Worker Num:  5 \n",
            " Loss:  0.255439430475235\n",
            "Training:  Iteration No:  216 Worker Num:  6 \n",
            " Loss:  0.4034140110015869\n",
            "Training:  Iteration No:  216 Worker Num:  7 \n",
            " Loss:  0.23295678198337555\n",
            "Test:  Iteration No:  216 \n",
            " Loss:  0.21494351267295925\n",
            "Test accuracy:  93.64\n",
            "Training:  Iteration No:  217 Worker Num:  0 \n",
            " Loss:  0.2122507244348526\n",
            "Training:  Iteration No:  217 Worker Num:  1 \n",
            " Loss:  0.263759046792984\n",
            "Training:  Iteration No:  217 Worker Num:  2 \n",
            " Loss:  0.20727761089801788\n",
            "Training:  Iteration No:  217 Worker Num:  3 \n",
            " Loss:  0.16474446654319763\n",
            "Training:  Iteration No:  217 Worker Num:  4 \n",
            " Loss:  0.19872042536735535\n",
            "Training:  Iteration No:  217 Worker Num:  5 \n",
            " Loss:  0.2046162635087967\n",
            "Training:  Iteration No:  217 Worker Num:  6 \n",
            " Loss:  0.22047901153564453\n",
            "Training:  Iteration No:  217 Worker Num:  7 \n",
            " Loss:  0.23503950238227844\n",
            "Test:  Iteration No:  217 \n",
            " Loss:  0.2134503876035915\n",
            "Test accuracy:  93.8\n",
            "Training:  Iteration No:  218 Worker Num:  0 \n",
            " Loss:  0.25118985772132874\n",
            "Training:  Iteration No:  218 Worker Num:  1 \n",
            " Loss:  0.20606504380702972\n",
            "Training:  Iteration No:  218 Worker Num:  2 \n",
            " Loss:  0.24433240294456482\n",
            "Training:  Iteration No:  218 Worker Num:  3 \n",
            " Loss:  0.3339950740337372\n",
            "Training:  Iteration No:  218 Worker Num:  4 \n",
            " Loss:  0.2759346067905426\n",
            "Training:  Iteration No:  218 Worker Num:  5 \n",
            " Loss:  0.20648424327373505\n",
            "Training:  Iteration No:  218 Worker Num:  6 \n",
            " Loss:  0.20819935202598572\n",
            "Training:  Iteration No:  218 Worker Num:  7 \n",
            " Loss:  0.29306039214134216\n",
            "Test:  Iteration No:  218 \n",
            " Loss:  0.2146277732416233\n",
            "Test accuracy:  93.55\n",
            "Training:  Iteration No:  219 Worker Num:  0 \n",
            " Loss:  0.1543179452419281\n",
            "Training:  Iteration No:  219 Worker Num:  1 \n",
            " Loss:  0.27125978469848633\n",
            "Training:  Iteration No:  219 Worker Num:  2 \n",
            " Loss:  0.19403037428855896\n",
            "Training:  Iteration No:  219 Worker Num:  3 \n",
            " Loss:  0.1569308042526245\n",
            "Training:  Iteration No:  219 Worker Num:  4 \n",
            " Loss:  0.3923024535179138\n",
            "Training:  Iteration No:  219 Worker Num:  5 \n",
            " Loss:  0.34208235144615173\n",
            "Training:  Iteration No:  219 Worker Num:  6 \n",
            " Loss:  0.3456788957118988\n",
            "Training:  Iteration No:  219 Worker Num:  7 \n",
            " Loss:  0.2865389883518219\n",
            "Test:  Iteration No:  219 \n",
            " Loss:  0.21432967525258473\n",
            "Test accuracy:  93.74\n",
            "Training:  Iteration No:  220 Worker Num:  0 \n",
            " Loss:  0.39654138684272766\n",
            "Training:  Iteration No:  220 Worker Num:  1 \n",
            " Loss:  0.30167925357818604\n",
            "Training:  Iteration No:  220 Worker Num:  2 \n",
            " Loss:  0.290603905916214\n",
            "Training:  Iteration No:  220 Worker Num:  3 \n",
            " Loss:  0.26184794306755066\n",
            "Training:  Iteration No:  220 Worker Num:  4 \n",
            " Loss:  0.19112572073936462\n",
            "Training:  Iteration No:  220 Worker Num:  5 \n",
            " Loss:  0.1561645120382309\n",
            "Training:  Iteration No:  220 Worker Num:  6 \n",
            " Loss:  0.1926117241382599\n",
            "Training:  Iteration No:  220 Worker Num:  7 \n",
            " Loss:  0.24062946438789368\n",
            "Test:  Iteration No:  220 \n",
            " Loss:  0.2116250414165515\n",
            "Test accuracy:  93.63\n",
            "Training:  Iteration No:  221 Worker Num:  0 \n",
            " Loss:  0.17426416277885437\n",
            "Training:  Iteration No:  221 Worker Num:  1 \n",
            " Loss:  0.277246356010437\n",
            "Training:  Iteration No:  221 Worker Num:  2 \n",
            " Loss:  0.2572418451309204\n",
            "Training:  Iteration No:  221 Worker Num:  3 \n",
            " Loss:  0.3040049374103546\n",
            "Training:  Iteration No:  221 Worker Num:  4 \n",
            " Loss:  0.2898601293563843\n",
            "Training:  Iteration No:  221 Worker Num:  5 \n",
            " Loss:  0.3405201733112335\n",
            "Training:  Iteration No:  221 Worker Num:  6 \n",
            " Loss:  0.183131605386734\n",
            "Training:  Iteration No:  221 Worker Num:  7 \n",
            " Loss:  0.2646386921405792\n",
            "Test:  Iteration No:  221 \n",
            " Loss:  0.2084139590799997\n",
            "Test accuracy:  93.95\n",
            "Training:  Iteration No:  222 Worker Num:  0 \n",
            " Loss:  0.20861375331878662\n",
            "Training:  Iteration No:  222 Worker Num:  1 \n",
            " Loss:  0.1667136400938034\n",
            "Training:  Iteration No:  222 Worker Num:  2 \n",
            " Loss:  0.2983393967151642\n",
            "Training:  Iteration No:  222 Worker Num:  3 \n",
            " Loss:  0.23756080865859985\n",
            "Training:  Iteration No:  222 Worker Num:  4 \n",
            " Loss:  0.13167057931423187\n",
            "Training:  Iteration No:  222 Worker Num:  5 \n",
            " Loss:  0.23587818443775177\n",
            "Training:  Iteration No:  222 Worker Num:  6 \n",
            " Loss:  0.18854059278964996\n",
            "Training:  Iteration No:  222 Worker Num:  7 \n",
            " Loss:  0.2828008532524109\n",
            "Test:  Iteration No:  222 \n",
            " Loss:  0.20947077240842052\n",
            "Test accuracy:  93.77\n",
            "Training:  Iteration No:  223 Worker Num:  0 \n",
            " Loss:  0.2666233777999878\n",
            "Training:  Iteration No:  223 Worker Num:  1 \n",
            " Loss:  0.29857754707336426\n",
            "Training:  Iteration No:  223 Worker Num:  2 \n",
            " Loss:  0.2344537377357483\n",
            "Training:  Iteration No:  223 Worker Num:  3 \n",
            " Loss:  0.34847548604011536\n",
            "Training:  Iteration No:  223 Worker Num:  4 \n",
            " Loss:  0.3610115051269531\n",
            "Training:  Iteration No:  223 Worker Num:  5 \n",
            " Loss:  0.276240736246109\n",
            "Training:  Iteration No:  223 Worker Num:  6 \n",
            " Loss:  0.2450205534696579\n",
            "Training:  Iteration No:  223 Worker Num:  7 \n",
            " Loss:  0.15711313486099243\n",
            "Test:  Iteration No:  223 \n",
            " Loss:  0.2092737073573885\n",
            "Test accuracy:  93.95\n",
            "Training:  Iteration No:  224 Worker Num:  0 \n",
            " Loss:  0.2740952968597412\n",
            "Training:  Iteration No:  224 Worker Num:  1 \n",
            " Loss:  0.1839573085308075\n",
            "Training:  Iteration No:  224 Worker Num:  2 \n",
            " Loss:  0.25413474440574646\n",
            "Training:  Iteration No:  224 Worker Num:  3 \n",
            " Loss:  0.1357489675283432\n",
            "Training:  Iteration No:  224 Worker Num:  4 \n",
            " Loss:  0.14859125018119812\n",
            "Training:  Iteration No:  224 Worker Num:  5 \n",
            " Loss:  0.26808735728263855\n",
            "Training:  Iteration No:  224 Worker Num:  6 \n",
            " Loss:  0.19353525340557098\n",
            "Training:  Iteration No:  224 Worker Num:  7 \n",
            " Loss:  0.29873278737068176\n",
            "Test:  Iteration No:  224 \n",
            " Loss:  0.2087446743198022\n",
            "Test accuracy:  93.82\n",
            "Training:  Iteration No:  225 Worker Num:  0 \n",
            " Loss:  0.20739887654781342\n",
            "Training:  Iteration No:  225 Worker Num:  1 \n",
            " Loss:  0.2566792964935303\n",
            "Training:  Iteration No:  225 Worker Num:  2 \n",
            " Loss:  0.20601588487625122\n",
            "Training:  Iteration No:  225 Worker Num:  3 \n",
            " Loss:  0.14251305162906647\n",
            "Training:  Iteration No:  225 Worker Num:  4 \n",
            " Loss:  0.3466421067714691\n",
            "Training:  Iteration No:  225 Worker Num:  5 \n",
            " Loss:  0.20045673847198486\n",
            "Training:  Iteration No:  225 Worker Num:  6 \n",
            " Loss:  0.25668835639953613\n",
            "Training:  Iteration No:  225 Worker Num:  7 \n",
            " Loss:  0.2625257670879364\n",
            "Test:  Iteration No:  225 \n",
            " Loss:  0.20835442211526103\n",
            "Test accuracy:  93.86\n",
            "Training:  Iteration No:  226 Worker Num:  0 \n",
            " Loss:  0.3687608540058136\n",
            "Training:  Iteration No:  226 Worker Num:  1 \n",
            " Loss:  0.214870885014534\n",
            "Training:  Iteration No:  226 Worker Num:  2 \n",
            " Loss:  0.32089006900787354\n",
            "Training:  Iteration No:  226 Worker Num:  3 \n",
            " Loss:  0.2193090319633484\n",
            "Training:  Iteration No:  226 Worker Num:  4 \n",
            " Loss:  0.33779072761535645\n",
            "Training:  Iteration No:  226 Worker Num:  5 \n",
            " Loss:  0.20539884269237518\n",
            "Training:  Iteration No:  226 Worker Num:  6 \n",
            " Loss:  0.38691508769989014\n",
            "Training:  Iteration No:  226 Worker Num:  7 \n",
            " Loss:  0.20542919635772705\n",
            "Test:  Iteration No:  226 \n",
            " Loss:  0.2100472952698982\n",
            "Test accuracy:  93.8\n",
            "Training:  Iteration No:  227 Worker Num:  0 \n",
            " Loss:  0.1741483509540558\n",
            "Training:  Iteration No:  227 Worker Num:  1 \n",
            " Loss:  0.2162860929965973\n",
            "Training:  Iteration No:  227 Worker Num:  2 \n",
            " Loss:  0.2558053135871887\n",
            "Training:  Iteration No:  227 Worker Num:  3 \n",
            " Loss:  0.19572247564792633\n",
            "Training:  Iteration No:  227 Worker Num:  4 \n",
            " Loss:  0.27470970153808594\n",
            "Training:  Iteration No:  227 Worker Num:  5 \n",
            " Loss:  0.2108132541179657\n",
            "Training:  Iteration No:  227 Worker Num:  6 \n",
            " Loss:  0.27900034189224243\n",
            "Training:  Iteration No:  227 Worker Num:  7 \n",
            " Loss:  0.28122571110725403\n",
            "Test:  Iteration No:  227 \n",
            " Loss:  0.2062555375167086\n",
            "Test accuracy:  93.87\n",
            "Training:  Iteration No:  228 Worker Num:  0 \n",
            " Loss:  0.16483376920223236\n",
            "Training:  Iteration No:  228 Worker Num:  1 \n",
            " Loss:  0.24392123520374298\n",
            "Training:  Iteration No:  228 Worker Num:  2 \n",
            " Loss:  0.177564337849617\n",
            "Training:  Iteration No:  228 Worker Num:  3 \n",
            " Loss:  0.2688421308994293\n",
            "Training:  Iteration No:  228 Worker Num:  4 \n",
            " Loss:  0.1926848590373993\n",
            "Training:  Iteration No:  228 Worker Num:  5 \n",
            " Loss:  0.18363846838474274\n",
            "Training:  Iteration No:  228 Worker Num:  6 \n",
            " Loss:  0.1587226241827011\n",
            "Training:  Iteration No:  228 Worker Num:  7 \n",
            " Loss:  0.23065085709095\n",
            "Test:  Iteration No:  228 \n",
            " Loss:  0.21917387625037493\n",
            "Test accuracy:  93.43\n",
            "Training:  Iteration No:  229 Worker Num:  0 \n",
            " Loss:  0.1831870973110199\n",
            "Training:  Iteration No:  229 Worker Num:  1 \n",
            " Loss:  0.1995875984430313\n",
            "Training:  Iteration No:  229 Worker Num:  2 \n",
            " Loss:  0.35820940136909485\n",
            "Training:  Iteration No:  229 Worker Num:  3 \n",
            " Loss:  0.20985755324363708\n",
            "Training:  Iteration No:  229 Worker Num:  4 \n",
            " Loss:  0.1371685266494751\n",
            "Training:  Iteration No:  229 Worker Num:  5 \n",
            " Loss:  0.18757778406143188\n",
            "Training:  Iteration No:  229 Worker Num:  6 \n",
            " Loss:  0.3080087900161743\n",
            "Training:  Iteration No:  229 Worker Num:  7 \n",
            " Loss:  0.22644487023353577\n",
            "Test:  Iteration No:  229 \n",
            " Loss:  0.20813693765173608\n",
            "Test accuracy:  93.88\n",
            "Training:  Iteration No:  230 Worker Num:  0 \n",
            " Loss:  0.27712810039520264\n",
            "Training:  Iteration No:  230 Worker Num:  1 \n",
            " Loss:  0.14320695400238037\n",
            "Training:  Iteration No:  230 Worker Num:  2 \n",
            " Loss:  0.18151673674583435\n",
            "Training:  Iteration No:  230 Worker Num:  3 \n",
            " Loss:  0.27810734510421753\n",
            "Training:  Iteration No:  230 Worker Num:  4 \n",
            " Loss:  0.2593609690666199\n",
            "Training:  Iteration No:  230 Worker Num:  5 \n",
            " Loss:  0.2481003701686859\n",
            "Training:  Iteration No:  230 Worker Num:  6 \n",
            " Loss:  0.2501910924911499\n",
            "Training:  Iteration No:  230 Worker Num:  7 \n",
            " Loss:  0.16647791862487793\n",
            "Test:  Iteration No:  230 \n",
            " Loss:  0.2057792467029789\n",
            "Test accuracy:  93.95\n",
            "Training:  Iteration No:  231 Worker Num:  0 \n",
            " Loss:  0.25042763352394104\n",
            "Training:  Iteration No:  231 Worker Num:  1 \n",
            " Loss:  0.21991074085235596\n",
            "Training:  Iteration No:  231 Worker Num:  2 \n",
            " Loss:  0.20835895836353302\n",
            "Training:  Iteration No:  231 Worker Num:  3 \n",
            " Loss:  0.26106327772140503\n",
            "Training:  Iteration No:  231 Worker Num:  4 \n",
            " Loss:  0.21486152708530426\n",
            "Training:  Iteration No:  231 Worker Num:  5 \n",
            " Loss:  0.386534720659256\n",
            "Training:  Iteration No:  231 Worker Num:  6 \n",
            " Loss:  0.3128829598426819\n",
            "Training:  Iteration No:  231 Worker Num:  7 \n",
            " Loss:  0.21415095031261444\n",
            "Test:  Iteration No:  231 \n",
            " Loss:  0.2032179642775202\n",
            "Test accuracy:  94.11\n",
            "Training:  Iteration No:  232 Worker Num:  0 \n",
            " Loss:  0.2244822382926941\n",
            "Training:  Iteration No:  232 Worker Num:  1 \n",
            " Loss:  0.21779406070709229\n",
            "Training:  Iteration No:  232 Worker Num:  2 \n",
            " Loss:  0.23336254060268402\n",
            "Training:  Iteration No:  232 Worker Num:  3 \n",
            " Loss:  0.21046580374240875\n",
            "Training:  Iteration No:  232 Worker Num:  4 \n",
            " Loss:  0.20025670528411865\n",
            "Training:  Iteration No:  232 Worker Num:  5 \n",
            " Loss:  0.15101735293865204\n",
            "Training:  Iteration No:  232 Worker Num:  6 \n",
            " Loss:  0.27277544140815735\n",
            "Training:  Iteration No:  232 Worker Num:  7 \n",
            " Loss:  0.18193760514259338\n",
            "Test:  Iteration No:  232 \n",
            " Loss:  0.20087612237592664\n",
            "Test accuracy:  94.07\n",
            "Training:  Iteration No:  233 Worker Num:  0 \n",
            " Loss:  0.20249062776565552\n",
            "Training:  Iteration No:  233 Worker Num:  1 \n",
            " Loss:  0.24856530129909515\n",
            "Training:  Iteration No:  233 Worker Num:  2 \n",
            " Loss:  0.26498347520828247\n",
            "Training:  Iteration No:  233 Worker Num:  3 \n",
            " Loss:  0.25262951850891113\n",
            "Training:  Iteration No:  233 Worker Num:  4 \n",
            " Loss:  0.2971923351287842\n",
            "Training:  Iteration No:  233 Worker Num:  5 \n",
            " Loss:  0.3238961398601532\n",
            "Training:  Iteration No:  233 Worker Num:  6 \n",
            " Loss:  0.29920703172683716\n",
            "Training:  Iteration No:  233 Worker Num:  7 \n",
            " Loss:  0.2291109412908554\n",
            "Test:  Iteration No:  233 \n",
            " Loss:  0.20528774741518346\n",
            "Test accuracy:  93.8\n",
            "Training:  Iteration No:  234 Worker Num:  0 \n",
            " Loss:  0.26365095376968384\n",
            "Training:  Iteration No:  234 Worker Num:  1 \n",
            " Loss:  0.1306125372648239\n",
            "Training:  Iteration No:  234 Worker Num:  2 \n",
            " Loss:  0.23889270424842834\n",
            "Training:  Iteration No:  234 Worker Num:  3 \n",
            " Loss:  0.2518880069255829\n",
            "Training:  Iteration No:  234 Worker Num:  4 \n",
            " Loss:  0.20611658692359924\n",
            "Training:  Iteration No:  234 Worker Num:  5 \n",
            " Loss:  0.2884983718395233\n",
            "Training:  Iteration No:  234 Worker Num:  6 \n",
            " Loss:  0.27446499466896057\n",
            "Training:  Iteration No:  234 Worker Num:  7 \n",
            " Loss:  0.2947697341442108\n",
            "Test:  Iteration No:  234 \n",
            " Loss:  0.20053876269184337\n",
            "Test accuracy:  94.11\n",
            "Training:  Iteration No:  235 Worker Num:  0 \n",
            " Loss:  0.2748337984085083\n",
            "Training:  Iteration No:  235 Worker Num:  1 \n",
            " Loss:  0.17346438765525818\n",
            "Training:  Iteration No:  235 Worker Num:  2 \n",
            " Loss:  0.25368475914001465\n",
            "Training:  Iteration No:  235 Worker Num:  3 \n",
            " Loss:  0.18817253410816193\n",
            "Training:  Iteration No:  235 Worker Num:  4 \n",
            " Loss:  0.20446597039699554\n",
            "Training:  Iteration No:  235 Worker Num:  5 \n",
            " Loss:  0.2804594039916992\n",
            "Training:  Iteration No:  235 Worker Num:  6 \n",
            " Loss:  0.2735443115234375\n",
            "Training:  Iteration No:  235 Worker Num:  7 \n",
            " Loss:  0.27892404794692993\n",
            "Test:  Iteration No:  235 \n",
            " Loss:  0.20892172474178333\n",
            "Test accuracy:  93.75\n",
            "Training:  Iteration No:  236 Worker Num:  0 \n",
            " Loss:  0.18025469779968262\n",
            "Training:  Iteration No:  236 Worker Num:  1 \n",
            " Loss:  0.3682957887649536\n",
            "Training:  Iteration No:  236 Worker Num:  2 \n",
            " Loss:  0.20874622464179993\n",
            "Training:  Iteration No:  236 Worker Num:  3 \n",
            " Loss:  0.23883230984210968\n",
            "Training:  Iteration No:  236 Worker Num:  4 \n",
            " Loss:  0.17511993646621704\n",
            "Training:  Iteration No:  236 Worker Num:  5 \n",
            " Loss:  0.3172767460346222\n",
            "Training:  Iteration No:  236 Worker Num:  6 \n",
            " Loss:  0.27499818801879883\n",
            "Training:  Iteration No:  236 Worker Num:  7 \n",
            " Loss:  0.24905429780483246\n",
            "Test:  Iteration No:  236 \n",
            " Loss:  0.20219388242245098\n",
            "Test accuracy:  93.88\n",
            "Training:  Iteration No:  237 Worker Num:  0 \n",
            " Loss:  0.2978111505508423\n",
            "Training:  Iteration No:  237 Worker Num:  1 \n",
            " Loss:  0.21095803380012512\n",
            "Training:  Iteration No:  237 Worker Num:  2 \n",
            " Loss:  0.17925971746444702\n",
            "Training:  Iteration No:  237 Worker Num:  3 \n",
            " Loss:  0.31214264035224915\n",
            "Training:  Iteration No:  237 Worker Num:  4 \n",
            " Loss:  0.32179325819015503\n",
            "Training:  Iteration No:  237 Worker Num:  5 \n",
            " Loss:  0.25694969296455383\n",
            "Training:  Iteration No:  237 Worker Num:  6 \n",
            " Loss:  0.19123490154743195\n",
            "Training:  Iteration No:  237 Worker Num:  7 \n",
            " Loss:  0.1848069727420807\n",
            "Test:  Iteration No:  237 \n",
            " Loss:  0.19815290914966335\n",
            "Test accuracy:  94.21\n",
            "Training:  Iteration No:  238 Worker Num:  0 \n",
            " Loss:  0.39897215366363525\n",
            "Training:  Iteration No:  238 Worker Num:  1 \n",
            " Loss:  0.23934559524059296\n",
            "Training:  Iteration No:  238 Worker Num:  2 \n",
            " Loss:  0.24518486857414246\n",
            "Training:  Iteration No:  238 Worker Num:  3 \n",
            " Loss:  0.26966601610183716\n",
            "Training:  Iteration No:  238 Worker Num:  4 \n",
            " Loss:  0.22417214512825012\n",
            "Training:  Iteration No:  238 Worker Num:  5 \n",
            " Loss:  0.22483766078948975\n",
            "Training:  Iteration No:  238 Worker Num:  6 \n",
            " Loss:  0.26134786009788513\n",
            "Training:  Iteration No:  238 Worker Num:  7 \n",
            " Loss:  0.21482138335704803\n",
            "Test:  Iteration No:  238 \n",
            " Loss:  0.19769843602906675\n",
            "Test accuracy:  94.15\n",
            "Training:  Iteration No:  239 Worker Num:  0 \n",
            " Loss:  0.18134167790412903\n",
            "Training:  Iteration No:  239 Worker Num:  1 \n",
            " Loss:  0.2862105667591095\n",
            "Training:  Iteration No:  239 Worker Num:  2 \n",
            " Loss:  0.3048965036869049\n",
            "Training:  Iteration No:  239 Worker Num:  3 \n",
            " Loss:  0.2141614407300949\n",
            "Training:  Iteration No:  239 Worker Num:  4 \n",
            " Loss:  0.20478315651416779\n",
            "Training:  Iteration No:  239 Worker Num:  5 \n",
            " Loss:  0.3420863151550293\n",
            "Training:  Iteration No:  239 Worker Num:  6 \n",
            " Loss:  0.17180609703063965\n",
            "Training:  Iteration No:  239 Worker Num:  7 \n",
            " Loss:  0.2639904320240021\n",
            "Test:  Iteration No:  239 \n",
            " Loss:  0.19769932844829333\n",
            "Test accuracy:  94.11\n",
            "Training:  Iteration No:  240 Worker Num:  0 \n",
            " Loss:  0.21512657403945923\n",
            "Training:  Iteration No:  240 Worker Num:  1 \n",
            " Loss:  0.3023967146873474\n",
            "Training:  Iteration No:  240 Worker Num:  2 \n",
            " Loss:  0.22552558779716492\n",
            "Training:  Iteration No:  240 Worker Num:  3 \n",
            " Loss:  0.21047694981098175\n",
            "Training:  Iteration No:  240 Worker Num:  4 \n",
            " Loss:  0.21365101635456085\n",
            "Training:  Iteration No:  240 Worker Num:  5 \n",
            " Loss:  0.3130919933319092\n",
            "Training:  Iteration No:  240 Worker Num:  6 \n",
            " Loss:  0.17719534039497375\n",
            "Training:  Iteration No:  240 Worker Num:  7 \n",
            " Loss:  0.1841149479150772\n",
            "Test:  Iteration No:  240 \n",
            " Loss:  0.1965141898917058\n",
            "Test accuracy:  94.22\n",
            "Training:  Iteration No:  241 Worker Num:  0 \n",
            " Loss:  0.3229314982891083\n",
            "Training:  Iteration No:  241 Worker Num:  1 \n",
            " Loss:  0.39004719257354736\n",
            "Training:  Iteration No:  241 Worker Num:  2 \n",
            " Loss:  0.3105444610118866\n",
            "Training:  Iteration No:  241 Worker Num:  3 \n",
            " Loss:  0.19735707342624664\n",
            "Training:  Iteration No:  241 Worker Num:  4 \n",
            " Loss:  0.28152987360954285\n",
            "Training:  Iteration No:  241 Worker Num:  5 \n",
            " Loss:  0.2714713215827942\n",
            "Training:  Iteration No:  241 Worker Num:  6 \n",
            " Loss:  0.3753502666950226\n",
            "Training:  Iteration No:  241 Worker Num:  7 \n",
            " Loss:  0.19327731430530548\n",
            "Test:  Iteration No:  241 \n",
            " Loss:  0.20027647582413274\n",
            "Test accuracy:  94.1\n",
            "Training:  Iteration No:  242 Worker Num:  0 \n",
            " Loss:  0.19935259222984314\n",
            "Training:  Iteration No:  242 Worker Num:  1 \n",
            " Loss:  0.28613582253456116\n",
            "Training:  Iteration No:  242 Worker Num:  2 \n",
            " Loss:  0.11831464618444443\n",
            "Training:  Iteration No:  242 Worker Num:  3 \n",
            " Loss:  0.19015073776245117\n",
            "Training:  Iteration No:  242 Worker Num:  4 \n",
            " Loss:  0.20201492309570312\n",
            "Training:  Iteration No:  242 Worker Num:  5 \n",
            " Loss:  0.3592877686023712\n",
            "Training:  Iteration No:  242 Worker Num:  6 \n",
            " Loss:  0.17845311760902405\n",
            "Training:  Iteration No:  242 Worker Num:  7 \n",
            " Loss:  0.24364261329174042\n",
            "Test:  Iteration No:  242 \n",
            " Loss:  0.19981263522530282\n",
            "Test accuracy:  93.97\n",
            "Training:  Iteration No:  243 Worker Num:  0 \n",
            " Loss:  0.22386321425437927\n",
            "Training:  Iteration No:  243 Worker Num:  1 \n",
            " Loss:  0.24636919796466827\n",
            "Training:  Iteration No:  243 Worker Num:  2 \n",
            " Loss:  0.19849777221679688\n",
            "Training:  Iteration No:  243 Worker Num:  3 \n",
            " Loss:  0.22155125439167023\n",
            "Training:  Iteration No:  243 Worker Num:  4 \n",
            " Loss:  0.20471705496311188\n",
            "Training:  Iteration No:  243 Worker Num:  5 \n",
            " Loss:  0.25674256682395935\n",
            "Training:  Iteration No:  243 Worker Num:  6 \n",
            " Loss:  0.3163202702999115\n",
            "Training:  Iteration No:  243 Worker Num:  7 \n",
            " Loss:  0.24761660397052765\n",
            "Test:  Iteration No:  243 \n",
            " Loss:  0.1958091389269957\n",
            "Test accuracy:  94.3\n",
            "Training:  Iteration No:  244 Worker Num:  0 \n",
            " Loss:  0.2925330698490143\n",
            "Training:  Iteration No:  244 Worker Num:  1 \n",
            " Loss:  0.3482675850391388\n",
            "Training:  Iteration No:  244 Worker Num:  2 \n",
            " Loss:  0.21198244392871857\n",
            "Training:  Iteration No:  244 Worker Num:  3 \n",
            " Loss:  0.28108182549476624\n",
            "Training:  Iteration No:  244 Worker Num:  4 \n",
            " Loss:  0.20572605729103088\n",
            "Training:  Iteration No:  244 Worker Num:  5 \n",
            " Loss:  0.15133310854434967\n",
            "Training:  Iteration No:  244 Worker Num:  6 \n",
            " Loss:  0.0946219339966774\n",
            "Training:  Iteration No:  244 Worker Num:  7 \n",
            " Loss:  0.196221262216568\n",
            "Test:  Iteration No:  244 \n",
            " Loss:  0.19756458935481083\n",
            "Test accuracy:  94.05\n",
            "Training:  Iteration No:  245 Worker Num:  0 \n",
            " Loss:  0.2876785099506378\n",
            "Training:  Iteration No:  245 Worker Num:  1 \n",
            " Loss:  0.2609623670578003\n",
            "Training:  Iteration No:  245 Worker Num:  2 \n",
            " Loss:  0.28777047991752625\n",
            "Training:  Iteration No:  245 Worker Num:  3 \n",
            " Loss:  0.24459250271320343\n",
            "Training:  Iteration No:  245 Worker Num:  4 \n",
            " Loss:  0.2716478705406189\n",
            "Training:  Iteration No:  245 Worker Num:  5 \n",
            " Loss:  0.22891245782375336\n",
            "Training:  Iteration No:  245 Worker Num:  6 \n",
            " Loss:  0.22333532571792603\n",
            "Training:  Iteration No:  245 Worker Num:  7 \n",
            " Loss:  0.21435414254665375\n",
            "Test:  Iteration No:  245 \n",
            " Loss:  0.19657087239990884\n",
            "Test accuracy:  94.2\n",
            "Training:  Iteration No:  246 Worker Num:  0 \n",
            " Loss:  0.2365003228187561\n",
            "Training:  Iteration No:  246 Worker Num:  1 \n",
            " Loss:  0.15653096139431\n",
            "Training:  Iteration No:  246 Worker Num:  2 \n",
            " Loss:  0.25302040576934814\n",
            "Training:  Iteration No:  246 Worker Num:  3 \n",
            " Loss:  0.2244735062122345\n",
            "Training:  Iteration No:  246 Worker Num:  4 \n",
            " Loss:  0.1932908296585083\n",
            "Training:  Iteration No:  246 Worker Num:  5 \n",
            " Loss:  0.19807298481464386\n",
            "Training:  Iteration No:  246 Worker Num:  6 \n",
            " Loss:  0.2011374831199646\n",
            "Training:  Iteration No:  246 Worker Num:  7 \n",
            " Loss:  0.23564811050891876\n",
            "Test:  Iteration No:  246 \n",
            " Loss:  0.19205985621466667\n",
            "Test accuracy:  94.3\n",
            "Training:  Iteration No:  247 Worker Num:  0 \n",
            " Loss:  0.22062180936336517\n",
            "Training:  Iteration No:  247 Worker Num:  1 \n",
            " Loss:  0.3366931974887848\n",
            "Training:  Iteration No:  247 Worker Num:  2 \n",
            " Loss:  0.4123547673225403\n",
            "Training:  Iteration No:  247 Worker Num:  3 \n",
            " Loss:  0.2139636129140854\n",
            "Training:  Iteration No:  247 Worker Num:  4 \n",
            " Loss:  0.17259636521339417\n",
            "Training:  Iteration No:  247 Worker Num:  5 \n",
            " Loss:  0.22026467323303223\n",
            "Training:  Iteration No:  247 Worker Num:  6 \n",
            " Loss:  0.26948389410972595\n",
            "Training:  Iteration No:  247 Worker Num:  7 \n",
            " Loss:  0.19232133030891418\n",
            "Test:  Iteration No:  247 \n",
            " Loss:  0.19736302007414117\n",
            "Test accuracy:  94.23\n",
            "Training:  Iteration No:  248 Worker Num:  0 \n",
            " Loss:  0.33446815609931946\n",
            "Training:  Iteration No:  248 Worker Num:  1 \n",
            " Loss:  0.24664048850536346\n",
            "Training:  Iteration No:  248 Worker Num:  2 \n",
            " Loss:  0.20005850493907928\n",
            "Training:  Iteration No:  248 Worker Num:  3 \n",
            " Loss:  0.19910740852355957\n",
            "Training:  Iteration No:  248 Worker Num:  4 \n",
            " Loss:  0.3828241527080536\n",
            "Training:  Iteration No:  248 Worker Num:  5 \n",
            " Loss:  0.16814880073070526\n",
            "Training:  Iteration No:  248 Worker Num:  6 \n",
            " Loss:  0.1374419778585434\n",
            "Training:  Iteration No:  248 Worker Num:  7 \n",
            " Loss:  0.3691025376319885\n",
            "Test:  Iteration No:  248 \n",
            " Loss:  0.19205437022956867\n",
            "Test accuracy:  94.26\n",
            "Training:  Iteration No:  249 Worker Num:  0 \n",
            " Loss:  0.2349797785282135\n",
            "Training:  Iteration No:  249 Worker Num:  1 \n",
            " Loss:  0.18641525506973267\n",
            "Training:  Iteration No:  249 Worker Num:  2 \n",
            " Loss:  0.2835380434989929\n",
            "Training:  Iteration No:  249 Worker Num:  3 \n",
            " Loss:  0.1834726780653\n",
            "Training:  Iteration No:  249 Worker Num:  4 \n",
            " Loss:  0.15177109837532043\n",
            "Training:  Iteration No:  249 Worker Num:  5 \n",
            " Loss:  0.202392578125\n",
            "Training:  Iteration No:  249 Worker Num:  6 \n",
            " Loss:  0.16087210178375244\n",
            "Training:  Iteration No:  249 Worker Num:  7 \n",
            " Loss:  0.21694263815879822\n",
            "Test:  Iteration No:  249 \n",
            " Loss:  0.19468992076153996\n",
            "Test accuracy:  94.32\n",
            "Training:  Iteration No:  250 Worker Num:  0 \n",
            " Loss:  0.21055158972740173\n",
            "Training:  Iteration No:  250 Worker Num:  1 \n",
            " Loss:  0.22792930901050568\n",
            "Training:  Iteration No:  250 Worker Num:  2 \n",
            " Loss:  0.14929303526878357\n",
            "Training:  Iteration No:  250 Worker Num:  3 \n",
            " Loss:  0.23506076633930206\n",
            "Training:  Iteration No:  250 Worker Num:  4 \n",
            " Loss:  0.2114705741405487\n",
            "Training:  Iteration No:  250 Worker Num:  5 \n",
            " Loss:  0.2470674216747284\n",
            "Training:  Iteration No:  250 Worker Num:  6 \n",
            " Loss:  0.19362366199493408\n",
            "Training:  Iteration No:  250 Worker Num:  7 \n",
            " Loss:  0.1782364547252655\n",
            "Test:  Iteration No:  250 \n",
            " Loss:  0.19075711398043588\n",
            "Test accuracy:  94.26\n",
            "Training:  Iteration No:  251 Worker Num:  0 \n",
            " Loss:  0.2176501452922821\n",
            "Training:  Iteration No:  251 Worker Num:  1 \n",
            " Loss:  0.36741408705711365\n",
            "Training:  Iteration No:  251 Worker Num:  2 \n",
            " Loss:  0.23594987392425537\n",
            "Training:  Iteration No:  251 Worker Num:  3 \n",
            " Loss:  0.3079748749732971\n",
            "Training:  Iteration No:  251 Worker Num:  4 \n",
            " Loss:  0.1422068327665329\n",
            "Training:  Iteration No:  251 Worker Num:  5 \n",
            " Loss:  0.25797757506370544\n",
            "Training:  Iteration No:  251 Worker Num:  6 \n",
            " Loss:  0.22063378989696503\n",
            "Training:  Iteration No:  251 Worker Num:  7 \n",
            " Loss:  0.2979066073894501\n",
            "Test:  Iteration No:  251 \n",
            " Loss:  0.18834629423822027\n",
            "Test accuracy:  94.43\n",
            "Training:  Iteration No:  252 Worker Num:  0 \n",
            " Loss:  0.22287216782569885\n",
            "Training:  Iteration No:  252 Worker Num:  1 \n",
            " Loss:  0.24828560650348663\n",
            "Training:  Iteration No:  252 Worker Num:  2 \n",
            " Loss:  0.20619556307792664\n",
            "Training:  Iteration No:  252 Worker Num:  3 \n",
            " Loss:  0.21709443628787994\n",
            "Training:  Iteration No:  252 Worker Num:  4 \n",
            " Loss:  0.35955533385276794\n",
            "Training:  Iteration No:  252 Worker Num:  5 \n",
            " Loss:  0.3771524429321289\n",
            "Training:  Iteration No:  252 Worker Num:  6 \n",
            " Loss:  0.2414482831954956\n",
            "Training:  Iteration No:  252 Worker Num:  7 \n",
            " Loss:  0.2679181396961212\n",
            "Test:  Iteration No:  252 \n",
            " Loss:  0.19010208016615124\n",
            "Test accuracy:  94.47\n",
            "Training:  Iteration No:  253 Worker Num:  0 \n",
            " Loss:  0.3592384159564972\n",
            "Training:  Iteration No:  253 Worker Num:  1 \n",
            " Loss:  0.18802866339683533\n",
            "Training:  Iteration No:  253 Worker Num:  2 \n",
            " Loss:  0.1915060579776764\n",
            "Training:  Iteration No:  253 Worker Num:  3 \n",
            " Loss:  0.2260466068983078\n",
            "Training:  Iteration No:  253 Worker Num:  4 \n",
            " Loss:  0.2905629873275757\n",
            "Training:  Iteration No:  253 Worker Num:  5 \n",
            " Loss:  0.1980743259191513\n",
            "Training:  Iteration No:  253 Worker Num:  6 \n",
            " Loss:  0.2170858234167099\n",
            "Training:  Iteration No:  253 Worker Num:  7 \n",
            " Loss:  0.20758014917373657\n",
            "Test:  Iteration No:  253 \n",
            " Loss:  0.1872807298256438\n",
            "Test accuracy:  94.61\n",
            "Training:  Iteration No:  254 Worker Num:  0 \n",
            " Loss:  0.1296437680721283\n",
            "Training:  Iteration No:  254 Worker Num:  1 \n",
            " Loss:  0.29358091950416565\n",
            "Training:  Iteration No:  254 Worker Num:  2 \n",
            " Loss:  0.21678800880908966\n",
            "Training:  Iteration No:  254 Worker Num:  3 \n",
            " Loss:  0.14064821600914001\n",
            "Training:  Iteration No:  254 Worker Num:  4 \n",
            " Loss:  0.2501669228076935\n",
            "Training:  Iteration No:  254 Worker Num:  5 \n",
            " Loss:  0.18555399775505066\n",
            "Training:  Iteration No:  254 Worker Num:  6 \n",
            " Loss:  0.18036751449108124\n",
            "Training:  Iteration No:  254 Worker Num:  7 \n",
            " Loss:  0.42527300119400024\n",
            "Test:  Iteration No:  254 \n",
            " Loss:  0.18488759428404178\n",
            "Test accuracy:  94.67\n",
            "Training:  Iteration No:  255 Worker Num:  0 \n",
            " Loss:  0.19275812804698944\n",
            "Training:  Iteration No:  255 Worker Num:  1 \n",
            " Loss:  0.20522713661193848\n",
            "Training:  Iteration No:  255 Worker Num:  2 \n",
            " Loss:  0.22000761330127716\n",
            "Training:  Iteration No:  255 Worker Num:  3 \n",
            " Loss:  0.15929773449897766\n",
            "Training:  Iteration No:  255 Worker Num:  4 \n",
            " Loss:  0.1834852397441864\n",
            "Training:  Iteration No:  255 Worker Num:  5 \n",
            " Loss:  0.29183512926101685\n",
            "Training:  Iteration No:  255 Worker Num:  6 \n",
            " Loss:  0.18678176403045654\n",
            "Training:  Iteration No:  255 Worker Num:  7 \n",
            " Loss:  0.3244595527648926\n",
            "Test:  Iteration No:  255 \n",
            " Loss:  0.19478113601526506\n",
            "Test accuracy:  94.17\n",
            "Training:  Iteration No:  256 Worker Num:  0 \n",
            " Loss:  0.17815080285072327\n",
            "Training:  Iteration No:  256 Worker Num:  1 \n",
            " Loss:  0.31636831164360046\n",
            "Training:  Iteration No:  256 Worker Num:  2 \n",
            " Loss:  0.28529664874076843\n",
            "Training:  Iteration No:  256 Worker Num:  3 \n",
            " Loss:  0.16322290897369385\n",
            "Training:  Iteration No:  256 Worker Num:  4 \n",
            " Loss:  0.20181487500667572\n",
            "Training:  Iteration No:  256 Worker Num:  5 \n",
            " Loss:  0.20154085755348206\n",
            "Training:  Iteration No:  256 Worker Num:  6 \n",
            " Loss:  0.22853228449821472\n",
            "Training:  Iteration No:  256 Worker Num:  7 \n",
            " Loss:  0.17590656876564026\n",
            "Test:  Iteration No:  256 \n",
            " Loss:  0.18951767877545916\n",
            "Test accuracy:  94.44\n",
            "Training:  Iteration No:  257 Worker Num:  0 \n",
            " Loss:  0.39839401841163635\n",
            "Training:  Iteration No:  257 Worker Num:  1 \n",
            " Loss:  0.20059126615524292\n",
            "Training:  Iteration No:  257 Worker Num:  2 \n",
            " Loss:  0.31463029980659485\n",
            "Training:  Iteration No:  257 Worker Num:  3 \n",
            " Loss:  0.18072651326656342\n",
            "Training:  Iteration No:  257 Worker Num:  4 \n",
            " Loss:  0.16517005860805511\n",
            "Training:  Iteration No:  257 Worker Num:  5 \n",
            " Loss:  0.24574029445648193\n",
            "Training:  Iteration No:  257 Worker Num:  6 \n",
            " Loss:  0.21084538102149963\n",
            "Training:  Iteration No:  257 Worker Num:  7 \n",
            " Loss:  0.13009104132652283\n",
            "Test:  Iteration No:  257 \n",
            " Loss:  0.18276520061624957\n",
            "Test accuracy:  94.77\n",
            "Training:  Iteration No:  258 Worker Num:  0 \n",
            " Loss:  0.1837414652109146\n",
            "Training:  Iteration No:  258 Worker Num:  1 \n",
            " Loss:  0.26559412479400635\n",
            "Training:  Iteration No:  258 Worker Num:  2 \n",
            " Loss:  0.1726599633693695\n",
            "Training:  Iteration No:  258 Worker Num:  3 \n",
            " Loss:  0.2244778275489807\n",
            "Training:  Iteration No:  258 Worker Num:  4 \n",
            " Loss:  0.17161551117897034\n",
            "Training:  Iteration No:  258 Worker Num:  5 \n",
            " Loss:  0.32597437500953674\n",
            "Training:  Iteration No:  258 Worker Num:  6 \n",
            " Loss:  0.20283837616443634\n",
            "Training:  Iteration No:  258 Worker Num:  7 \n",
            " Loss:  0.23081594705581665\n",
            "Test:  Iteration No:  258 \n",
            " Loss:  0.18548680900867226\n",
            "Test accuracy:  94.53\n",
            "Training:  Iteration No:  259 Worker Num:  0 \n",
            " Loss:  0.16612589359283447\n",
            "Training:  Iteration No:  259 Worker Num:  1 \n",
            " Loss:  0.25986388325691223\n",
            "Training:  Iteration No:  259 Worker Num:  2 \n",
            " Loss:  0.25974735617637634\n",
            "Training:  Iteration No:  259 Worker Num:  3 \n",
            " Loss:  0.20138047635555267\n",
            "Training:  Iteration No:  259 Worker Num:  4 \n",
            " Loss:  0.2323315590620041\n",
            "Training:  Iteration No:  259 Worker Num:  5 \n",
            " Loss:  0.2710045576095581\n",
            "Training:  Iteration No:  259 Worker Num:  6 \n",
            " Loss:  0.2070787400007248\n",
            "Training:  Iteration No:  259 Worker Num:  7 \n",
            " Loss:  0.28378060460090637\n",
            "Test:  Iteration No:  259 \n",
            " Loss:  0.1828010926211748\n",
            "Test accuracy:  94.74\n",
            "Training:  Iteration No:  260 Worker Num:  0 \n",
            " Loss:  0.3290844261646271\n",
            "Training:  Iteration No:  260 Worker Num:  1 \n",
            " Loss:  0.13136117160320282\n",
            "Training:  Iteration No:  260 Worker Num:  2 \n",
            " Loss:  0.21247628331184387\n",
            "Training:  Iteration No:  260 Worker Num:  3 \n",
            " Loss:  0.17947249114513397\n",
            "Training:  Iteration No:  260 Worker Num:  4 \n",
            " Loss:  0.22684615850448608\n",
            "Training:  Iteration No:  260 Worker Num:  5 \n",
            " Loss:  0.13494868576526642\n",
            "Training:  Iteration No:  260 Worker Num:  6 \n",
            " Loss:  0.19469016790390015\n",
            "Training:  Iteration No:  260 Worker Num:  7 \n",
            " Loss:  0.21983930468559265\n",
            "Test:  Iteration No:  260 \n",
            " Loss:  0.18344033220664988\n",
            "Test accuracy:  94.58\n",
            "Training:  Iteration No:  261 Worker Num:  0 \n",
            " Loss:  0.23749534785747528\n",
            "Training:  Iteration No:  261 Worker Num:  1 \n",
            " Loss:  0.30396515130996704\n",
            "Training:  Iteration No:  261 Worker Num:  2 \n",
            " Loss:  0.23312261700630188\n",
            "Training:  Iteration No:  261 Worker Num:  3 \n",
            " Loss:  0.3044378459453583\n",
            "Training:  Iteration No:  261 Worker Num:  4 \n",
            " Loss:  0.2106623649597168\n",
            "Training:  Iteration No:  261 Worker Num:  5 \n",
            " Loss:  0.12832830846309662\n",
            "Training:  Iteration No:  261 Worker Num:  6 \n",
            " Loss:  0.2354370504617691\n",
            "Training:  Iteration No:  261 Worker Num:  7 \n",
            " Loss:  0.1358422189950943\n",
            "Test:  Iteration No:  261 \n",
            " Loss:  0.18171481665553926\n",
            "Test accuracy:  94.57\n",
            "Training:  Iteration No:  262 Worker Num:  0 \n",
            " Loss:  0.2504728436470032\n",
            "Training:  Iteration No:  262 Worker Num:  1 \n",
            " Loss:  0.1456102430820465\n",
            "Training:  Iteration No:  262 Worker Num:  2 \n",
            " Loss:  0.14725472033023834\n",
            "Training:  Iteration No:  262 Worker Num:  3 \n",
            " Loss:  0.18994639813899994\n",
            "Training:  Iteration No:  262 Worker Num:  4 \n",
            " Loss:  0.1968459188938141\n",
            "Training:  Iteration No:  262 Worker Num:  5 \n",
            " Loss:  0.172537699341774\n",
            "Training:  Iteration No:  262 Worker Num:  6 \n",
            " Loss:  0.257590115070343\n",
            "Training:  Iteration No:  262 Worker Num:  7 \n",
            " Loss:  0.20139016211032867\n",
            "Test:  Iteration No:  262 \n",
            " Loss:  0.18077112595208839\n",
            "Test accuracy:  94.71\n",
            "Training:  Iteration No:  263 Worker Num:  0 \n",
            " Loss:  0.11624255776405334\n",
            "Training:  Iteration No:  263 Worker Num:  1 \n",
            " Loss:  0.19462916254997253\n",
            "Training:  Iteration No:  263 Worker Num:  2 \n",
            " Loss:  0.21799558401107788\n",
            "Training:  Iteration No:  263 Worker Num:  3 \n",
            " Loss:  0.25012797117233276\n",
            "Training:  Iteration No:  263 Worker Num:  4 \n",
            " Loss:  0.13780730962753296\n",
            "Training:  Iteration No:  263 Worker Num:  5 \n",
            " Loss:  0.13724707067012787\n",
            "Training:  Iteration No:  263 Worker Num:  6 \n",
            " Loss:  0.2448417842388153\n",
            "Training:  Iteration No:  263 Worker Num:  7 \n",
            " Loss:  0.2064770609140396\n",
            "Test:  Iteration No:  263 \n",
            " Loss:  0.18328855497927604\n",
            "Test accuracy:  94.67\n",
            "Training:  Iteration No:  264 Worker Num:  0 \n",
            " Loss:  0.2256501466035843\n",
            "Training:  Iteration No:  264 Worker Num:  1 \n",
            " Loss:  0.22339268028736115\n",
            "Training:  Iteration No:  264 Worker Num:  2 \n",
            " Loss:  0.19743314385414124\n",
            "Training:  Iteration No:  264 Worker Num:  3 \n",
            " Loss:  0.17973695695400238\n",
            "Training:  Iteration No:  264 Worker Num:  4 \n",
            " Loss:  0.2901468276977539\n",
            "Training:  Iteration No:  264 Worker Num:  5 \n",
            " Loss:  0.18325169384479523\n",
            "Training:  Iteration No:  264 Worker Num:  6 \n",
            " Loss:  0.24706928431987762\n",
            "Training:  Iteration No:  264 Worker Num:  7 \n",
            " Loss:  0.13312186300754547\n",
            "Test:  Iteration No:  264 \n",
            " Loss:  0.18167350755980874\n",
            "Test accuracy:  94.7\n",
            "Training:  Iteration No:  265 Worker Num:  0 \n",
            " Loss:  0.14902079105377197\n",
            "Training:  Iteration No:  265 Worker Num:  1 \n",
            " Loss:  0.27657651901245117\n",
            "Training:  Iteration No:  265 Worker Num:  2 \n",
            " Loss:  0.20844560861587524\n",
            "Training:  Iteration No:  265 Worker Num:  3 \n",
            " Loss:  0.09471944719552994\n",
            "Training:  Iteration No:  265 Worker Num:  4 \n",
            " Loss:  0.3147321045398712\n",
            "Training:  Iteration No:  265 Worker Num:  5 \n",
            " Loss:  0.24403683841228485\n",
            "Training:  Iteration No:  265 Worker Num:  6 \n",
            " Loss:  0.2395927906036377\n",
            "Training:  Iteration No:  265 Worker Num:  7 \n",
            " Loss:  0.20595254004001617\n",
            "Test:  Iteration No:  265 \n",
            " Loss:  0.18160568587953532\n",
            "Test accuracy:  94.82\n",
            "Training:  Iteration No:  266 Worker Num:  0 \n",
            " Loss:  0.18345379829406738\n",
            "Training:  Iteration No:  266 Worker Num:  1 \n",
            " Loss:  0.2400781214237213\n",
            "Training:  Iteration No:  266 Worker Num:  2 \n",
            " Loss:  0.20480236411094666\n",
            "Training:  Iteration No:  266 Worker Num:  3 \n",
            " Loss:  0.13778135180473328\n",
            "Training:  Iteration No:  266 Worker Num:  4 \n",
            " Loss:  0.2165694534778595\n",
            "Training:  Iteration No:  266 Worker Num:  5 \n",
            " Loss:  0.3204374611377716\n",
            "Training:  Iteration No:  266 Worker Num:  6 \n",
            " Loss:  0.30726540088653564\n",
            "Training:  Iteration No:  266 Worker Num:  7 \n",
            " Loss:  0.21502439677715302\n",
            "Test:  Iteration No:  266 \n",
            " Loss:  0.1830530908383146\n",
            "Test accuracy:  94.48\n",
            "Training:  Iteration No:  267 Worker Num:  0 \n",
            " Loss:  0.16305536031723022\n",
            "Training:  Iteration No:  267 Worker Num:  1 \n",
            " Loss:  0.293751984834671\n",
            "Training:  Iteration No:  267 Worker Num:  2 \n",
            " Loss:  0.2085646688938141\n",
            "Training:  Iteration No:  267 Worker Num:  3 \n",
            " Loss:  0.1767341047525406\n",
            "Training:  Iteration No:  267 Worker Num:  4 \n",
            " Loss:  0.19384939968585968\n",
            "Training:  Iteration No:  267 Worker Num:  5 \n",
            " Loss:  0.18186995387077332\n",
            "Training:  Iteration No:  267 Worker Num:  6 \n",
            " Loss:  0.14233243465423584\n",
            "Training:  Iteration No:  267 Worker Num:  7 \n",
            " Loss:  0.12130207568407059\n",
            "Test:  Iteration No:  267 \n",
            " Loss:  0.1758145013753372\n",
            "Test accuracy:  94.74\n",
            "Training:  Iteration No:  268 Worker Num:  0 \n",
            " Loss:  0.19960148632526398\n",
            "Training:  Iteration No:  268 Worker Num:  1 \n",
            " Loss:  0.17983713746070862\n",
            "Training:  Iteration No:  268 Worker Num:  2 \n",
            " Loss:  0.24871957302093506\n",
            "Training:  Iteration No:  268 Worker Num:  3 \n",
            " Loss:  0.27574291825294495\n",
            "Training:  Iteration No:  268 Worker Num:  4 \n",
            " Loss:  0.10556670278310776\n",
            "Training:  Iteration No:  268 Worker Num:  5 \n",
            " Loss:  0.18788272142410278\n",
            "Training:  Iteration No:  268 Worker Num:  6 \n",
            " Loss:  0.19806316494941711\n",
            "Training:  Iteration No:  268 Worker Num:  7 \n",
            " Loss:  0.24297325313091278\n",
            "Test:  Iteration No:  268 \n",
            " Loss:  0.17848284511909454\n",
            "Test accuracy:  94.64\n",
            "Training:  Iteration No:  269 Worker Num:  0 \n",
            " Loss:  0.28040748834609985\n",
            "Training:  Iteration No:  269 Worker Num:  1 \n",
            " Loss:  0.28625980019569397\n",
            "Training:  Iteration No:  269 Worker Num:  2 \n",
            " Loss:  0.19556225836277008\n",
            "Training:  Iteration No:  269 Worker Num:  3 \n",
            " Loss:  0.2520265579223633\n",
            "Training:  Iteration No:  269 Worker Num:  4 \n",
            " Loss:  0.24606822431087494\n",
            "Training:  Iteration No:  269 Worker Num:  5 \n",
            " Loss:  0.34234336018562317\n",
            "Training:  Iteration No:  269 Worker Num:  6 \n",
            " Loss:  0.20060133934020996\n",
            "Training:  Iteration No:  269 Worker Num:  7 \n",
            " Loss:  0.18561303615570068\n",
            "Test:  Iteration No:  269 \n",
            " Loss:  0.17502970471270854\n",
            "Test accuracy:  94.75\n",
            "Training:  Iteration No:  270 Worker Num:  0 \n",
            " Loss:  0.13793843984603882\n",
            "Training:  Iteration No:  270 Worker Num:  1 \n",
            " Loss:  0.1792326271533966\n",
            "Training:  Iteration No:  270 Worker Num:  2 \n",
            " Loss:  0.22056926786899567\n",
            "Training:  Iteration No:  270 Worker Num:  3 \n",
            " Loss:  0.2893231511116028\n",
            "Training:  Iteration No:  270 Worker Num:  4 \n",
            " Loss:  0.2663838565349579\n",
            "Training:  Iteration No:  270 Worker Num:  5 \n",
            " Loss:  0.2707116901874542\n",
            "Training:  Iteration No:  270 Worker Num:  6 \n",
            " Loss:  0.2542459964752197\n",
            "Training:  Iteration No:  270 Worker Num:  7 \n",
            " Loss:  0.20774859189987183\n",
            "Test:  Iteration No:  270 \n",
            " Loss:  0.17352566424804397\n",
            "Test accuracy:  94.97\n",
            "Training:  Iteration No:  271 Worker Num:  0 \n",
            " Loss:  0.24538692831993103\n",
            "Training:  Iteration No:  271 Worker Num:  1 \n",
            " Loss:  0.2700880467891693\n",
            "Training:  Iteration No:  271 Worker Num:  2 \n",
            " Loss:  0.12150730192661285\n",
            "Training:  Iteration No:  271 Worker Num:  3 \n",
            " Loss:  0.18482248485088348\n",
            "Training:  Iteration No:  271 Worker Num:  4 \n",
            " Loss:  0.1851622611284256\n",
            "Training:  Iteration No:  271 Worker Num:  5 \n",
            " Loss:  0.23243586719036102\n",
            "Training:  Iteration No:  271 Worker Num:  6 \n",
            " Loss:  0.18047647178173065\n",
            "Training:  Iteration No:  271 Worker Num:  7 \n",
            " Loss:  0.11386159062385559\n",
            "Test:  Iteration No:  271 \n",
            " Loss:  0.17552285346709476\n",
            "Test accuracy:  94.71\n",
            "Training:  Iteration No:  272 Worker Num:  0 \n",
            " Loss:  0.17027592658996582\n",
            "Training:  Iteration No:  272 Worker Num:  1 \n",
            " Loss:  0.21034866571426392\n",
            "Training:  Iteration No:  272 Worker Num:  2 \n",
            " Loss:  0.17609328031539917\n",
            "Training:  Iteration No:  272 Worker Num:  3 \n",
            " Loss:  0.26190435886383057\n",
            "Training:  Iteration No:  272 Worker Num:  4 \n",
            " Loss:  0.222181037068367\n",
            "Training:  Iteration No:  272 Worker Num:  5 \n",
            " Loss:  0.28975075483322144\n",
            "Training:  Iteration No:  272 Worker Num:  6 \n",
            " Loss:  0.09759530425071716\n",
            "Training:  Iteration No:  272 Worker Num:  7 \n",
            " Loss:  0.15242667496204376\n",
            "Test:  Iteration No:  272 \n",
            " Loss:  0.1755840523121289\n",
            "Test accuracy:  94.88\n",
            "Training:  Iteration No:  273 Worker Num:  0 \n",
            " Loss:  0.10493965446949005\n",
            "Training:  Iteration No:  273 Worker Num:  1 \n",
            " Loss:  0.23565784096717834\n",
            "Training:  Iteration No:  273 Worker Num:  2 \n",
            " Loss:  0.2988232374191284\n",
            "Training:  Iteration No:  273 Worker Num:  3 \n",
            " Loss:  0.27233389019966125\n",
            "Training:  Iteration No:  273 Worker Num:  4 \n",
            " Loss:  0.12692613899707794\n",
            "Training:  Iteration No:  273 Worker Num:  5 \n",
            " Loss:  0.25935086607933044\n",
            "Training:  Iteration No:  273 Worker Num:  6 \n",
            " Loss:  0.22949093580245972\n",
            "Training:  Iteration No:  273 Worker Num:  7 \n",
            " Loss:  0.15240491926670074\n",
            "Test:  Iteration No:  273 \n",
            " Loss:  0.17256956785634348\n",
            "Test accuracy:  94.96\n",
            "Training:  Iteration No:  274 Worker Num:  0 \n",
            " Loss:  0.23493695259094238\n",
            "Training:  Iteration No:  274 Worker Num:  1 \n",
            " Loss:  0.15692460536956787\n",
            "Training:  Iteration No:  274 Worker Num:  2 \n",
            " Loss:  0.12643632292747498\n",
            "Training:  Iteration No:  274 Worker Num:  3 \n",
            " Loss:  0.24802014231681824\n",
            "Training:  Iteration No:  274 Worker Num:  4 \n",
            " Loss:  0.13806094229221344\n",
            "Training:  Iteration No:  274 Worker Num:  5 \n",
            " Loss:  0.21435680985450745\n",
            "Training:  Iteration No:  274 Worker Num:  6 \n",
            " Loss:  0.24121354520320892\n",
            "Training:  Iteration No:  274 Worker Num:  7 \n",
            " Loss:  0.17833617329597473\n",
            "Test:  Iteration No:  274 \n",
            " Loss:  0.1743921092711389\n",
            "Test accuracy:  94.95\n",
            "Training:  Iteration No:  275 Worker Num:  0 \n",
            " Loss:  0.17532090842723846\n",
            "Training:  Iteration No:  275 Worker Num:  1 \n",
            " Loss:  0.22160707414150238\n",
            "Training:  Iteration No:  275 Worker Num:  2 \n",
            " Loss:  0.18783806264400482\n",
            "Training:  Iteration No:  275 Worker Num:  3 \n",
            " Loss:  0.2842622995376587\n",
            "Training:  Iteration No:  275 Worker Num:  4 \n",
            " Loss:  0.17524297535419464\n",
            "Training:  Iteration No:  275 Worker Num:  5 \n",
            " Loss:  0.254133939743042\n",
            "Training:  Iteration No:  275 Worker Num:  6 \n",
            " Loss:  0.21478663384914398\n",
            "Training:  Iteration No:  275 Worker Num:  7 \n",
            " Loss:  0.2742155194282532\n",
            "Test:  Iteration No:  275 \n",
            " Loss:  0.17428252566605806\n",
            "Test accuracy:  94.82\n",
            "Training:  Iteration No:  276 Worker Num:  0 \n",
            " Loss:  0.13554301857948303\n",
            "Training:  Iteration No:  276 Worker Num:  1 \n",
            " Loss:  0.12199369817972183\n",
            "Training:  Iteration No:  276 Worker Num:  2 \n",
            " Loss:  0.18548505008220673\n",
            "Training:  Iteration No:  276 Worker Num:  3 \n",
            " Loss:  0.28259021043777466\n",
            "Training:  Iteration No:  276 Worker Num:  4 \n",
            " Loss:  0.1884598731994629\n",
            "Training:  Iteration No:  276 Worker Num:  5 \n",
            " Loss:  0.14372938871383667\n",
            "Training:  Iteration No:  276 Worker Num:  6 \n",
            " Loss:  0.30719342827796936\n",
            "Training:  Iteration No:  276 Worker Num:  7 \n",
            " Loss:  0.17558860778808594\n",
            "Test:  Iteration No:  276 \n",
            " Loss:  0.1752672944795576\n",
            "Test accuracy:  94.82\n",
            "Training:  Iteration No:  277 Worker Num:  0 \n",
            " Loss:  0.21859721839427948\n",
            "Training:  Iteration No:  277 Worker Num:  1 \n",
            " Loss:  0.17987564206123352\n",
            "Training:  Iteration No:  277 Worker Num:  2 \n",
            " Loss:  0.20310857892036438\n",
            "Training:  Iteration No:  277 Worker Num:  3 \n",
            " Loss:  0.14783687889575958\n",
            "Training:  Iteration No:  277 Worker Num:  4 \n",
            " Loss:  0.3034387230873108\n",
            "Training:  Iteration No:  277 Worker Num:  5 \n",
            " Loss:  0.19676384329795837\n",
            "Training:  Iteration No:  277 Worker Num:  6 \n",
            " Loss:  0.2489503026008606\n",
            "Training:  Iteration No:  277 Worker Num:  7 \n",
            " Loss:  0.20285086333751678\n",
            "Test:  Iteration No:  277 \n",
            " Loss:  0.17266637194147216\n",
            "Test accuracy:  94.97\n",
            "Training:  Iteration No:  278 Worker Num:  0 \n",
            " Loss:  0.2005884051322937\n",
            "Training:  Iteration No:  278 Worker Num:  1 \n",
            " Loss:  0.3247668743133545\n",
            "Training:  Iteration No:  278 Worker Num:  2 \n",
            " Loss:  0.21326720714569092\n",
            "Training:  Iteration No:  278 Worker Num:  3 \n",
            " Loss:  0.1539851427078247\n",
            "Training:  Iteration No:  278 Worker Num:  4 \n",
            " Loss:  0.2693426311016083\n",
            "Training:  Iteration No:  278 Worker Num:  5 \n",
            " Loss:  0.2809038758277893\n",
            "Training:  Iteration No:  278 Worker Num:  6 \n",
            " Loss:  0.2758767306804657\n",
            "Training:  Iteration No:  278 Worker Num:  7 \n",
            " Loss:  0.17864422500133514\n",
            "Test:  Iteration No:  278 \n",
            " Loss:  0.1733510573689319\n",
            "Test accuracy:  94.78\n",
            "Training:  Iteration No:  279 Worker Num:  0 \n",
            " Loss:  0.1829906553030014\n",
            "Training:  Iteration No:  279 Worker Num:  1 \n",
            " Loss:  0.27139830589294434\n",
            "Training:  Iteration No:  279 Worker Num:  2 \n",
            " Loss:  0.17351113259792328\n",
            "Training:  Iteration No:  279 Worker Num:  3 \n",
            " Loss:  0.1965082734823227\n",
            "Training:  Iteration No:  279 Worker Num:  4 \n",
            " Loss:  0.2676199972629547\n",
            "Training:  Iteration No:  279 Worker Num:  5 \n",
            " Loss:  0.20481669902801514\n",
            "Training:  Iteration No:  279 Worker Num:  6 \n",
            " Loss:  0.2403108775615692\n",
            "Training:  Iteration No:  279 Worker Num:  7 \n",
            " Loss:  0.22290925681591034\n",
            "Test:  Iteration No:  279 \n",
            " Loss:  0.16895891213789582\n",
            "Test accuracy:  95.0\n",
            "Training:  Iteration No:  280 Worker Num:  0 \n",
            " Loss:  0.1968308985233307\n",
            "Training:  Iteration No:  280 Worker Num:  1 \n",
            " Loss:  0.2818812429904938\n",
            "Training:  Iteration No:  280 Worker Num:  2 \n",
            " Loss:  0.11327116936445236\n",
            "Training:  Iteration No:  280 Worker Num:  3 \n",
            " Loss:  0.2807425856590271\n",
            "Training:  Iteration No:  280 Worker Num:  4 \n",
            " Loss:  0.29913029074668884\n",
            "Training:  Iteration No:  280 Worker Num:  5 \n",
            " Loss:  0.2483779340982437\n",
            "Training:  Iteration No:  280 Worker Num:  6 \n",
            " Loss:  0.20659014582633972\n",
            "Training:  Iteration No:  280 Worker Num:  7 \n",
            " Loss:  0.18205545842647552\n",
            "Test:  Iteration No:  280 \n",
            " Loss:  0.17100010305949592\n",
            "Test accuracy:  95.12\n",
            "Training:  Iteration No:  281 Worker Num:  0 \n",
            " Loss:  0.4064388573169708\n",
            "Training:  Iteration No:  281 Worker Num:  1 \n",
            " Loss:  0.21470853686332703\n",
            "Training:  Iteration No:  281 Worker Num:  2 \n",
            " Loss:  0.2790226638317108\n",
            "Training:  Iteration No:  281 Worker Num:  3 \n",
            " Loss:  0.36856308579444885\n",
            "Training:  Iteration No:  281 Worker Num:  4 \n",
            " Loss:  0.16579164564609528\n",
            "Training:  Iteration No:  281 Worker Num:  5 \n",
            " Loss:  0.18017278611660004\n",
            "Training:  Iteration No:  281 Worker Num:  6 \n",
            " Loss:  0.18129633367061615\n",
            "Training:  Iteration No:  281 Worker Num:  7 \n",
            " Loss:  0.20319834351539612\n",
            "Test:  Iteration No:  281 \n",
            " Loss:  0.16993915413423807\n",
            "Test accuracy:  94.99\n",
            "Training:  Iteration No:  282 Worker Num:  0 \n",
            " Loss:  0.18987900018692017\n",
            "Training:  Iteration No:  282 Worker Num:  1 \n",
            " Loss:  0.12868067622184753\n",
            "Training:  Iteration No:  282 Worker Num:  2 \n",
            " Loss:  0.21340756118297577\n",
            "Training:  Iteration No:  282 Worker Num:  3 \n",
            " Loss:  0.23510785400867462\n",
            "Training:  Iteration No:  282 Worker Num:  4 \n",
            " Loss:  0.1506626307964325\n",
            "Training:  Iteration No:  282 Worker Num:  5 \n",
            " Loss:  0.14661553502082825\n",
            "Training:  Iteration No:  282 Worker Num:  6 \n",
            " Loss:  0.17122730612754822\n",
            "Training:  Iteration No:  282 Worker Num:  7 \n",
            " Loss:  0.31545525789260864\n",
            "Test:  Iteration No:  282 \n",
            " Loss:  0.17226988599411672\n",
            "Test accuracy:  94.99\n",
            "Training:  Iteration No:  283 Worker Num:  0 \n",
            " Loss:  0.24308155477046967\n",
            "Training:  Iteration No:  283 Worker Num:  1 \n",
            " Loss:  0.11156773567199707\n",
            "Training:  Iteration No:  283 Worker Num:  2 \n",
            " Loss:  0.1791989654302597\n",
            "Training:  Iteration No:  283 Worker Num:  3 \n",
            " Loss:  0.2174074947834015\n",
            "Training:  Iteration No:  283 Worker Num:  4 \n",
            " Loss:  0.1836380511522293\n",
            "Training:  Iteration No:  283 Worker Num:  5 \n",
            " Loss:  0.22285214066505432\n",
            "Training:  Iteration No:  283 Worker Num:  6 \n",
            " Loss:  0.25066977739334106\n",
            "Training:  Iteration No:  283 Worker Num:  7 \n",
            " Loss:  0.3887917697429657\n",
            "Test:  Iteration No:  283 \n",
            " Loss:  0.16951960308736638\n",
            "Test accuracy:  95.04\n",
            "Training:  Iteration No:  284 Worker Num:  0 \n",
            " Loss:  0.17537841200828552\n",
            "Training:  Iteration No:  284 Worker Num:  1 \n",
            " Loss:  0.27334293723106384\n",
            "Training:  Iteration No:  284 Worker Num:  2 \n",
            " Loss:  0.10666520893573761\n",
            "Training:  Iteration No:  284 Worker Num:  3 \n",
            " Loss:  0.251676470041275\n",
            "Training:  Iteration No:  284 Worker Num:  4 \n",
            " Loss:  0.29920876026153564\n",
            "Training:  Iteration No:  284 Worker Num:  5 \n",
            " Loss:  0.19285421073436737\n",
            "Training:  Iteration No:  284 Worker Num:  6 \n",
            " Loss:  0.17028659582138062\n",
            "Training:  Iteration No:  284 Worker Num:  7 \n",
            " Loss:  0.17164555191993713\n",
            "Test:  Iteration No:  284 \n",
            " Loss:  0.1709233988665893\n",
            "Test accuracy:  94.96\n",
            "Training:  Iteration No:  285 Worker Num:  0 \n",
            " Loss:  0.2108849436044693\n",
            "Training:  Iteration No:  285 Worker Num:  1 \n",
            " Loss:  0.2358962893486023\n",
            "Training:  Iteration No:  285 Worker Num:  2 \n",
            " Loss:  0.2562844753265381\n",
            "Training:  Iteration No:  285 Worker Num:  3 \n",
            " Loss:  0.18893644213676453\n",
            "Training:  Iteration No:  285 Worker Num:  4 \n",
            " Loss:  0.17999088764190674\n",
            "Training:  Iteration No:  285 Worker Num:  5 \n",
            " Loss:  0.2871381640434265\n",
            "Training:  Iteration No:  285 Worker Num:  6 \n",
            " Loss:  0.18573333323001862\n",
            "Training:  Iteration No:  285 Worker Num:  7 \n",
            " Loss:  0.16596665978431702\n",
            "Test:  Iteration No:  285 \n",
            " Loss:  0.17006068046004336\n",
            "Test accuracy:  94.92\n",
            "Training:  Iteration No:  286 Worker Num:  0 \n",
            " Loss:  0.2609630525112152\n",
            "Training:  Iteration No:  286 Worker Num:  1 \n",
            " Loss:  0.2023155838251114\n",
            "Training:  Iteration No:  286 Worker Num:  2 \n",
            " Loss:  0.2538962960243225\n",
            "Training:  Iteration No:  286 Worker Num:  3 \n",
            " Loss:  0.22187002003192902\n",
            "Training:  Iteration No:  286 Worker Num:  4 \n",
            " Loss:  0.20461128652095795\n",
            "Training:  Iteration No:  286 Worker Num:  5 \n",
            " Loss:  0.21776582300662994\n",
            "Training:  Iteration No:  286 Worker Num:  6 \n",
            " Loss:  0.1668720245361328\n",
            "Training:  Iteration No:  286 Worker Num:  7 \n",
            " Loss:  0.18054933845996857\n",
            "Test:  Iteration No:  286 \n",
            " Loss:  0.1689394779998479\n",
            "Test accuracy:  94.97\n",
            "Training:  Iteration No:  287 Worker Num:  0 \n",
            " Loss:  0.278656542301178\n",
            "Training:  Iteration No:  287 Worker Num:  1 \n",
            " Loss:  0.17512723803520203\n",
            "Training:  Iteration No:  287 Worker Num:  2 \n",
            " Loss:  0.26801785826683044\n",
            "Training:  Iteration No:  287 Worker Num:  3 \n",
            " Loss:  0.18437258899211884\n",
            "Training:  Iteration No:  287 Worker Num:  4 \n",
            " Loss:  0.2412746101617813\n",
            "Training:  Iteration No:  287 Worker Num:  5 \n",
            " Loss:  0.25569266080856323\n",
            "Training:  Iteration No:  287 Worker Num:  6 \n",
            " Loss:  0.20214661955833435\n",
            "Training:  Iteration No:  287 Worker Num:  7 \n",
            " Loss:  0.20800530910491943\n",
            "Test:  Iteration No:  287 \n",
            " Loss:  0.16746342397896172\n",
            "Test accuracy:  95.19\n",
            "Training:  Iteration No:  288 Worker Num:  0 \n",
            " Loss:  0.15741464495658875\n",
            "Training:  Iteration No:  288 Worker Num:  1 \n",
            " Loss:  0.18322394788265228\n",
            "Training:  Iteration No:  288 Worker Num:  2 \n",
            " Loss:  0.17391684651374817\n",
            "Training:  Iteration No:  288 Worker Num:  3 \n",
            " Loss:  0.1486768126487732\n",
            "Training:  Iteration No:  288 Worker Num:  4 \n",
            " Loss:  0.2343064546585083\n",
            "Training:  Iteration No:  288 Worker Num:  5 \n",
            " Loss:  0.1371159851551056\n",
            "Training:  Iteration No:  288 Worker Num:  6 \n",
            " Loss:  0.1854742467403412\n",
            "Training:  Iteration No:  288 Worker Num:  7 \n",
            " Loss:  0.20569756627082825\n",
            "Test:  Iteration No:  288 \n",
            " Loss:  0.16804018897251993\n",
            "Test accuracy:  95.2\n",
            "Training:  Iteration No:  289 Worker Num:  0 \n",
            " Loss:  0.22054260969161987\n",
            "Training:  Iteration No:  289 Worker Num:  1 \n",
            " Loss:  0.1747903972864151\n",
            "Training:  Iteration No:  289 Worker Num:  2 \n",
            " Loss:  0.23493638634681702\n",
            "Training:  Iteration No:  289 Worker Num:  3 \n",
            " Loss:  0.2302369475364685\n",
            "Training:  Iteration No:  289 Worker Num:  4 \n",
            " Loss:  0.22787433862686157\n",
            "Training:  Iteration No:  289 Worker Num:  5 \n",
            " Loss:  0.16568532586097717\n",
            "Training:  Iteration No:  289 Worker Num:  6 \n",
            " Loss:  0.21580955386161804\n",
            "Training:  Iteration No:  289 Worker Num:  7 \n",
            " Loss:  0.11328224092721939\n",
            "Test:  Iteration No:  289 \n",
            " Loss:  0.16664919019028357\n",
            "Test accuracy:  95.26\n",
            "Training:  Iteration No:  290 Worker Num:  0 \n",
            " Loss:  0.11154934018850327\n",
            "Training:  Iteration No:  290 Worker Num:  1 \n",
            " Loss:  0.24516969919204712\n",
            "Training:  Iteration No:  290 Worker Num:  2 \n",
            " Loss:  0.2154625505208969\n",
            "Training:  Iteration No:  290 Worker Num:  3 \n",
            " Loss:  0.2626146674156189\n",
            "Training:  Iteration No:  290 Worker Num:  4 \n",
            " Loss:  0.2915695607662201\n",
            "Training:  Iteration No:  290 Worker Num:  5 \n",
            " Loss:  0.19694092869758606\n",
            "Training:  Iteration No:  290 Worker Num:  6 \n",
            " Loss:  0.13221938908100128\n",
            "Training:  Iteration No:  290 Worker Num:  7 \n",
            " Loss:  0.18304629623889923\n",
            "Test:  Iteration No:  290 \n",
            " Loss:  0.16611568777692282\n",
            "Test accuracy:  95.13\n",
            "Training:  Iteration No:  291 Worker Num:  0 \n",
            " Loss:  0.10061684995889664\n",
            "Training:  Iteration No:  291 Worker Num:  1 \n",
            " Loss:  0.19111618399620056\n",
            "Training:  Iteration No:  291 Worker Num:  2 \n",
            " Loss:  0.1519705206155777\n",
            "Training:  Iteration No:  291 Worker Num:  3 \n",
            " Loss:  0.16949298977851868\n",
            "Training:  Iteration No:  291 Worker Num:  4 \n",
            " Loss:  0.175250843167305\n",
            "Training:  Iteration No:  291 Worker Num:  5 \n",
            " Loss:  0.22964338958263397\n",
            "Training:  Iteration No:  291 Worker Num:  6 \n",
            " Loss:  0.20688283443450928\n",
            "Training:  Iteration No:  291 Worker Num:  7 \n",
            " Loss:  0.1683671474456787\n",
            "Test:  Iteration No:  291 \n",
            " Loss:  0.16496088333970196\n",
            "Test accuracy:  95.15\n",
            "Training:  Iteration No:  292 Worker Num:  0 \n",
            " Loss:  0.2229335457086563\n",
            "Training:  Iteration No:  292 Worker Num:  1 \n",
            " Loss:  0.3095568120479584\n",
            "Training:  Iteration No:  292 Worker Num:  2 \n",
            " Loss:  0.16108176112174988\n",
            "Training:  Iteration No:  292 Worker Num:  3 \n",
            " Loss:  0.1808493435382843\n",
            "Training:  Iteration No:  292 Worker Num:  4 \n",
            " Loss:  0.2507183253765106\n",
            "Training:  Iteration No:  292 Worker Num:  5 \n",
            " Loss:  0.2481430023908615\n",
            "Training:  Iteration No:  292 Worker Num:  6 \n",
            " Loss:  0.19463132321834564\n",
            "Training:  Iteration No:  292 Worker Num:  7 \n",
            " Loss:  0.2595338225364685\n",
            "Test:  Iteration No:  292 \n",
            " Loss:  0.16551567644729645\n",
            "Test accuracy:  95.24\n",
            "Training:  Iteration No:  293 Worker Num:  0 \n",
            " Loss:  0.2523520290851593\n",
            "Training:  Iteration No:  293 Worker Num:  1 \n",
            " Loss:  0.1472761332988739\n",
            "Training:  Iteration No:  293 Worker Num:  2 \n",
            " Loss:  0.24885347485542297\n",
            "Training:  Iteration No:  293 Worker Num:  3 \n",
            " Loss:  0.23147858679294586\n",
            "Training:  Iteration No:  293 Worker Num:  4 \n",
            " Loss:  0.1837543547153473\n",
            "Training:  Iteration No:  293 Worker Num:  5 \n",
            " Loss:  0.19167251884937286\n",
            "Training:  Iteration No:  293 Worker Num:  6 \n",
            " Loss:  0.1914420872926712\n",
            "Training:  Iteration No:  293 Worker Num:  7 \n",
            " Loss:  0.1397121250629425\n",
            "Test:  Iteration No:  293 \n",
            " Loss:  0.16348996368248628\n",
            "Test accuracy:  95.27\n",
            "Training:  Iteration No:  294 Worker Num:  0 \n",
            " Loss:  0.12501204013824463\n",
            "Training:  Iteration No:  294 Worker Num:  1 \n",
            " Loss:  0.23986375331878662\n",
            "Training:  Iteration No:  294 Worker Num:  2 \n",
            " Loss:  0.1793735772371292\n",
            "Training:  Iteration No:  294 Worker Num:  3 \n",
            " Loss:  0.1421886384487152\n",
            "Training:  Iteration No:  294 Worker Num:  4 \n",
            " Loss:  0.20255623757839203\n",
            "Training:  Iteration No:  294 Worker Num:  5 \n",
            " Loss:  0.30194810032844543\n",
            "Training:  Iteration No:  294 Worker Num:  6 \n",
            " Loss:  0.19387555122375488\n",
            "Training:  Iteration No:  294 Worker Num:  7 \n",
            " Loss:  0.12423373013734818\n",
            "Test:  Iteration No:  294 \n",
            " Loss:  0.16453978545445053\n",
            "Test accuracy:  95.23\n",
            "Training:  Iteration No:  295 Worker Num:  0 \n",
            " Loss:  0.28197750449180603\n",
            "Training:  Iteration No:  295 Worker Num:  1 \n",
            " Loss:  0.12796667218208313\n",
            "Training:  Iteration No:  295 Worker Num:  2 \n",
            " Loss:  0.25169405341148376\n",
            "Training:  Iteration No:  295 Worker Num:  3 \n",
            " Loss:  0.22719016671180725\n",
            "Training:  Iteration No:  295 Worker Num:  4 \n",
            " Loss:  0.12247379124164581\n",
            "Training:  Iteration No:  295 Worker Num:  5 \n",
            " Loss:  0.23072513937950134\n",
            "Training:  Iteration No:  295 Worker Num:  6 \n",
            " Loss:  0.14439481496810913\n",
            "Training:  Iteration No:  295 Worker Num:  7 \n",
            " Loss:  0.1916418969631195\n",
            "Test:  Iteration No:  295 \n",
            " Loss:  0.17208079157892284\n",
            "Test accuracy:  94.94\n",
            "Training:  Iteration No:  296 Worker Num:  0 \n",
            " Loss:  0.23872938752174377\n",
            "Training:  Iteration No:  296 Worker Num:  1 \n",
            " Loss:  0.20528550446033478\n",
            "Training:  Iteration No:  296 Worker Num:  2 \n",
            " Loss:  0.21512377262115479\n",
            "Training:  Iteration No:  296 Worker Num:  3 \n",
            " Loss:  0.25897878408432007\n",
            "Training:  Iteration No:  296 Worker Num:  4 \n",
            " Loss:  0.23173367977142334\n",
            "Training:  Iteration No:  296 Worker Num:  5 \n",
            " Loss:  0.19332820177078247\n",
            "Training:  Iteration No:  296 Worker Num:  6 \n",
            " Loss:  0.29889199137687683\n",
            "Training:  Iteration No:  296 Worker Num:  7 \n",
            " Loss:  0.1771959364414215\n",
            "Test:  Iteration No:  296 \n",
            " Loss:  0.16099686867003388\n",
            "Test accuracy:  95.35\n",
            "Training:  Iteration No:  297 Worker Num:  0 \n",
            " Loss:  0.11895298957824707\n",
            "Training:  Iteration No:  297 Worker Num:  1 \n",
            " Loss:  0.1958126425743103\n",
            "Training:  Iteration No:  297 Worker Num:  2 \n",
            " Loss:  0.11488223075866699\n",
            "Training:  Iteration No:  297 Worker Num:  3 \n",
            " Loss:  0.22273708879947662\n",
            "Training:  Iteration No:  297 Worker Num:  4 \n",
            " Loss:  0.09573723375797272\n",
            "Training:  Iteration No:  297 Worker Num:  5 \n",
            " Loss:  0.20994943380355835\n",
            "Training:  Iteration No:  297 Worker Num:  6 \n",
            " Loss:  0.25868165493011475\n",
            "Training:  Iteration No:  297 Worker Num:  7 \n",
            " Loss:  0.308074951171875\n",
            "Test:  Iteration No:  297 \n",
            " Loss:  0.16250625375328184\n",
            "Test accuracy:  95.31\n",
            "Training:  Iteration No:  298 Worker Num:  0 \n",
            " Loss:  0.2076965719461441\n",
            "Training:  Iteration No:  298 Worker Num:  1 \n",
            " Loss:  0.17384999990463257\n",
            "Training:  Iteration No:  298 Worker Num:  2 \n",
            " Loss:  0.15810221433639526\n",
            "Training:  Iteration No:  298 Worker Num:  3 \n",
            " Loss:  0.19969582557678223\n",
            "Training:  Iteration No:  298 Worker Num:  4 \n",
            " Loss:  0.3134625554084778\n",
            "Training:  Iteration No:  298 Worker Num:  5 \n",
            " Loss:  0.17591311037540436\n",
            "Training:  Iteration No:  298 Worker Num:  6 \n",
            " Loss:  0.3091479539871216\n",
            "Training:  Iteration No:  298 Worker Num:  7 \n",
            " Loss:  0.1654539555311203\n",
            "Test:  Iteration No:  298 \n",
            " Loss:  0.16105785260943672\n",
            "Test accuracy:  95.27\n",
            "Training:  Iteration No:  299 Worker Num:  0 \n",
            " Loss:  0.21063683927059174\n",
            "Training:  Iteration No:  299 Worker Num:  1 \n",
            " Loss:  0.20221848785877228\n",
            "Training:  Iteration No:  299 Worker Num:  2 \n",
            " Loss:  0.14457382261753082\n",
            "Training:  Iteration No:  299 Worker Num:  3 \n",
            " Loss:  0.14059214293956757\n",
            "Training:  Iteration No:  299 Worker Num:  4 \n",
            " Loss:  0.14704787731170654\n",
            "Training:  Iteration No:  299 Worker Num:  5 \n",
            " Loss:  0.19046862423419952\n",
            "Training:  Iteration No:  299 Worker Num:  6 \n",
            " Loss:  0.12882298231124878\n",
            "Training:  Iteration No:  299 Worker Num:  7 \n",
            " Loss:  0.17499586939811707\n",
            "Test:  Iteration No:  299 \n",
            " Loss:  0.16072232635762496\n",
            "Test accuracy:  95.22\n",
            "Training:  Iteration No:  300 Worker Num:  0 \n",
            " Loss:  0.19386884570121765\n",
            "Training:  Iteration No:  300 Worker Num:  1 \n",
            " Loss:  0.22767460346221924\n",
            "Training:  Iteration No:  300 Worker Num:  2 \n",
            " Loss:  0.2628054916858673\n",
            "Training:  Iteration No:  300 Worker Num:  3 \n",
            " Loss:  0.13512063026428223\n",
            "Training:  Iteration No:  300 Worker Num:  4 \n",
            " Loss:  0.16082017123699188\n",
            "Training:  Iteration No:  300 Worker Num:  5 \n",
            " Loss:  0.19458553194999695\n",
            "Training:  Iteration No:  300 Worker Num:  6 \n",
            " Loss:  0.22453264892101288\n",
            "Training:  Iteration No:  300 Worker Num:  7 \n",
            " Loss:  0.3097565472126007\n",
            "Test:  Iteration No:  300 \n",
            " Loss:  0.15849624947892338\n",
            "Test accuracy:  95.44\n",
            "Training:  Iteration No:  301 Worker Num:  0 \n",
            " Loss:  0.15418770909309387\n",
            "Training:  Iteration No:  301 Worker Num:  1 \n",
            " Loss:  0.1860315352678299\n",
            "Training:  Iteration No:  301 Worker Num:  2 \n",
            " Loss:  0.1915961354970932\n",
            "Training:  Iteration No:  301 Worker Num:  3 \n",
            " Loss:  0.23936769366264343\n",
            "Training:  Iteration No:  301 Worker Num:  4 \n",
            " Loss:  0.21423213183879852\n",
            "Training:  Iteration No:  301 Worker Num:  5 \n",
            " Loss:  0.12269913405179977\n",
            "Training:  Iteration No:  301 Worker Num:  6 \n",
            " Loss:  0.26143506169319153\n",
            "Training:  Iteration No:  301 Worker Num:  7 \n",
            " Loss:  0.17148812115192413\n",
            "Test:  Iteration No:  301 \n",
            " Loss:  0.1576544922906198\n",
            "Test accuracy:  95.5\n",
            "Training:  Iteration No:  302 Worker Num:  0 \n",
            " Loss:  0.1273544579744339\n",
            "Training:  Iteration No:  302 Worker Num:  1 \n",
            " Loss:  0.1409803032875061\n",
            "Training:  Iteration No:  302 Worker Num:  2 \n",
            " Loss:  0.14910092949867249\n",
            "Training:  Iteration No:  302 Worker Num:  3 \n",
            " Loss:  0.23469442129135132\n",
            "Training:  Iteration No:  302 Worker Num:  4 \n",
            " Loss:  0.26539236307144165\n",
            "Training:  Iteration No:  302 Worker Num:  5 \n",
            " Loss:  0.17388634383678436\n",
            "Training:  Iteration No:  302 Worker Num:  6 \n",
            " Loss:  0.20307514071464539\n",
            "Training:  Iteration No:  302 Worker Num:  7 \n",
            " Loss:  0.19565704464912415\n",
            "Test:  Iteration No:  302 \n",
            " Loss:  0.1590919675824197\n",
            "Test accuracy:  95.4\n",
            "Training:  Iteration No:  303 Worker Num:  0 \n",
            " Loss:  0.29309916496276855\n",
            "Training:  Iteration No:  303 Worker Num:  1 \n",
            " Loss:  0.18023788928985596\n",
            "Training:  Iteration No:  303 Worker Num:  2 \n",
            " Loss:  0.1941119134426117\n",
            "Training:  Iteration No:  303 Worker Num:  3 \n",
            " Loss:  0.20443286001682281\n",
            "Training:  Iteration No:  303 Worker Num:  4 \n",
            " Loss:  0.19294042885303497\n",
            "Training:  Iteration No:  303 Worker Num:  5 \n",
            " Loss:  0.2518158555030823\n",
            "Training:  Iteration No:  303 Worker Num:  6 \n",
            " Loss:  0.24104081094264984\n",
            "Training:  Iteration No:  303 Worker Num:  7 \n",
            " Loss:  0.19650709629058838\n",
            "Test:  Iteration No:  303 \n",
            " Loss:  0.16124001552079675\n",
            "Test accuracy:  95.3\n",
            "Training:  Iteration No:  304 Worker Num:  0 \n",
            " Loss:  0.16589230298995972\n",
            "Training:  Iteration No:  304 Worker Num:  1 \n",
            " Loss:  0.15375861525535583\n",
            "Training:  Iteration No:  304 Worker Num:  2 \n",
            " Loss:  0.1609182208776474\n",
            "Training:  Iteration No:  304 Worker Num:  3 \n",
            " Loss:  0.15966634452342987\n",
            "Training:  Iteration No:  304 Worker Num:  4 \n",
            " Loss:  0.18737810850143433\n",
            "Training:  Iteration No:  304 Worker Num:  5 \n",
            " Loss:  0.15278016030788422\n",
            "Training:  Iteration No:  304 Worker Num:  6 \n",
            " Loss:  0.2660692036151886\n",
            "Training:  Iteration No:  304 Worker Num:  7 \n",
            " Loss:  0.15557050704956055\n",
            "Test:  Iteration No:  304 \n",
            " Loss:  0.15571082265505307\n",
            "Test accuracy:  95.47\n",
            "Training:  Iteration No:  305 Worker Num:  0 \n",
            " Loss:  0.1645122766494751\n",
            "Training:  Iteration No:  305 Worker Num:  1 \n",
            " Loss:  0.140022873878479\n",
            "Training:  Iteration No:  305 Worker Num:  2 \n",
            " Loss:  0.336795449256897\n",
            "Training:  Iteration No:  305 Worker Num:  3 \n",
            " Loss:  0.2084338217973709\n",
            "Training:  Iteration No:  305 Worker Num:  4 \n",
            " Loss:  0.18450337648391724\n",
            "Training:  Iteration No:  305 Worker Num:  5 \n",
            " Loss:  0.2012777179479599\n",
            "Training:  Iteration No:  305 Worker Num:  6 \n",
            " Loss:  0.24082966148853302\n",
            "Training:  Iteration No:  305 Worker Num:  7 \n",
            " Loss:  0.2165263444185257\n",
            "Test:  Iteration No:  305 \n",
            " Loss:  0.15792344296586852\n",
            "Test accuracy:  95.37\n",
            "Training:  Iteration No:  306 Worker Num:  0 \n",
            " Loss:  0.4044415354728699\n",
            "Training:  Iteration No:  306 Worker Num:  1 \n",
            " Loss:  0.22616688907146454\n",
            "Training:  Iteration No:  306 Worker Num:  2 \n",
            " Loss:  0.2643171548843384\n",
            "Training:  Iteration No:  306 Worker Num:  3 \n",
            " Loss:  0.16780157387256622\n",
            "Training:  Iteration No:  306 Worker Num:  4 \n",
            " Loss:  0.16410933434963226\n",
            "Training:  Iteration No:  306 Worker Num:  5 \n",
            " Loss:  0.09261130541563034\n",
            "Training:  Iteration No:  306 Worker Num:  6 \n",
            " Loss:  0.2014027088880539\n",
            "Training:  Iteration No:  306 Worker Num:  7 \n",
            " Loss:  0.20398834347724915\n",
            "Test:  Iteration No:  306 \n",
            " Loss:  0.15431515755178996\n",
            "Test accuracy:  95.45\n",
            "Training:  Iteration No:  307 Worker Num:  0 \n",
            " Loss:  0.11093462258577347\n",
            "Training:  Iteration No:  307 Worker Num:  1 \n",
            " Loss:  0.19502529501914978\n",
            "Training:  Iteration No:  307 Worker Num:  2 \n",
            " Loss:  0.2608879804611206\n",
            "Training:  Iteration No:  307 Worker Num:  3 \n",
            " Loss:  0.13782553374767303\n",
            "Training:  Iteration No:  307 Worker Num:  4 \n",
            " Loss:  0.12335450947284698\n",
            "Training:  Iteration No:  307 Worker Num:  5 \n",
            " Loss:  0.1128356009721756\n",
            "Training:  Iteration No:  307 Worker Num:  6 \n",
            " Loss:  0.10992196947336197\n",
            "Training:  Iteration No:  307 Worker Num:  7 \n",
            " Loss:  0.13107050955295563\n",
            "Test:  Iteration No:  307 \n",
            " Loss:  0.15632718810929527\n",
            "Test accuracy:  95.55\n",
            "Training:  Iteration No:  308 Worker Num:  0 \n",
            " Loss:  0.1689189374446869\n",
            "Training:  Iteration No:  308 Worker Num:  1 \n",
            " Loss:  0.13388343155384064\n",
            "Training:  Iteration No:  308 Worker Num:  2 \n",
            " Loss:  0.18734972178936005\n",
            "Training:  Iteration No:  308 Worker Num:  3 \n",
            " Loss:  0.12305061519145966\n",
            "Training:  Iteration No:  308 Worker Num:  4 \n",
            " Loss:  0.22376172244548798\n",
            "Training:  Iteration No:  308 Worker Num:  5 \n",
            " Loss:  0.1692531704902649\n",
            "Training:  Iteration No:  308 Worker Num:  6 \n",
            " Loss:  0.17009495198726654\n",
            "Training:  Iteration No:  308 Worker Num:  7 \n",
            " Loss:  0.18029676377773285\n",
            "Test:  Iteration No:  308 \n",
            " Loss:  0.15492554093840755\n",
            "Test accuracy:  95.45\n",
            "Training:  Iteration No:  309 Worker Num:  0 \n",
            " Loss:  0.18237459659576416\n",
            "Training:  Iteration No:  309 Worker Num:  1 \n",
            " Loss:  0.2225092649459839\n",
            "Training:  Iteration No:  309 Worker Num:  2 \n",
            " Loss:  0.13660965859889984\n",
            "Training:  Iteration No:  309 Worker Num:  3 \n",
            " Loss:  0.20830315351486206\n",
            "Training:  Iteration No:  309 Worker Num:  4 \n",
            " Loss:  0.1918897032737732\n",
            "Training:  Iteration No:  309 Worker Num:  5 \n",
            " Loss:  0.32650354504585266\n",
            "Training:  Iteration No:  309 Worker Num:  6 \n",
            " Loss:  0.25816014409065247\n",
            "Training:  Iteration No:  309 Worker Num:  7 \n",
            " Loss:  0.1747126430273056\n",
            "Test:  Iteration No:  309 \n",
            " Loss:  0.15943036844388028\n",
            "Test accuracy:  95.4\n",
            "Training:  Iteration No:  310 Worker Num:  0 \n",
            " Loss:  0.11031019687652588\n",
            "Training:  Iteration No:  310 Worker Num:  1 \n",
            " Loss:  0.3406941890716553\n",
            "Training:  Iteration No:  310 Worker Num:  2 \n",
            " Loss:  0.15236391127109528\n",
            "Training:  Iteration No:  310 Worker Num:  3 \n",
            " Loss:  0.07556425034999847\n",
            "Training:  Iteration No:  310 Worker Num:  4 \n",
            " Loss:  0.11861977726221085\n",
            "Training:  Iteration No:  310 Worker Num:  5 \n",
            " Loss:  0.19865304231643677\n",
            "Training:  Iteration No:  310 Worker Num:  6 \n",
            " Loss:  0.1847543865442276\n",
            "Training:  Iteration No:  310 Worker Num:  7 \n",
            " Loss:  0.1496458500623703\n",
            "Test:  Iteration No:  310 \n",
            " Loss:  0.15243982480060828\n",
            "Test accuracy:  95.53\n",
            "Training:  Iteration No:  311 Worker Num:  0 \n",
            " Loss:  0.1713738590478897\n",
            "Training:  Iteration No:  311 Worker Num:  1 \n",
            " Loss:  0.1940624713897705\n",
            "Training:  Iteration No:  311 Worker Num:  2 \n",
            " Loss:  0.12465011328458786\n",
            "Training:  Iteration No:  311 Worker Num:  3 \n",
            " Loss:  0.13817064464092255\n",
            "Training:  Iteration No:  311 Worker Num:  4 \n",
            " Loss:  0.17879782617092133\n",
            "Training:  Iteration No:  311 Worker Num:  5 \n",
            " Loss:  0.16120633482933044\n",
            "Training:  Iteration No:  311 Worker Num:  6 \n",
            " Loss:  0.18518054485321045\n",
            "Training:  Iteration No:  311 Worker Num:  7 \n",
            " Loss:  0.20888124406337738\n",
            "Test:  Iteration No:  311 \n",
            " Loss:  0.1518509599285884\n",
            "Test accuracy:  95.67\n",
            "Training:  Iteration No:  312 Worker Num:  0 \n",
            " Loss:  0.14224740862846375\n",
            "Training:  Iteration No:  312 Worker Num:  1 \n",
            " Loss:  0.2716764509677887\n",
            "Training:  Iteration No:  312 Worker Num:  2 \n",
            " Loss:  0.13428464531898499\n",
            "Training:  Iteration No:  312 Worker Num:  3 \n",
            " Loss:  0.11516633629798889\n",
            "Training:  Iteration No:  312 Worker Num:  4 \n",
            " Loss:  0.3098907470703125\n",
            "Training:  Iteration No:  312 Worker Num:  5 \n",
            " Loss:  0.1037084087729454\n",
            "Training:  Iteration No:  312 Worker Num:  6 \n",
            " Loss:  0.1387154757976532\n",
            "Training:  Iteration No:  312 Worker Num:  7 \n",
            " Loss:  0.10977605730295181\n",
            "Test:  Iteration No:  312 \n",
            " Loss:  0.1548141263885117\n",
            "Test accuracy:  95.45\n",
            "Training:  Iteration No:  313 Worker Num:  0 \n",
            " Loss:  0.178430438041687\n",
            "Training:  Iteration No:  313 Worker Num:  1 \n",
            " Loss:  0.13137096166610718\n",
            "Training:  Iteration No:  313 Worker Num:  2 \n",
            " Loss:  0.17357400059700012\n",
            "Training:  Iteration No:  313 Worker Num:  3 \n",
            " Loss:  0.28167128562927246\n",
            "Training:  Iteration No:  313 Worker Num:  4 \n",
            " Loss:  0.1385713368654251\n",
            "Training:  Iteration No:  313 Worker Num:  5 \n",
            " Loss:  0.16822411119937897\n",
            "Training:  Iteration No:  313 Worker Num:  6 \n",
            " Loss:  0.2012443244457245\n",
            "Training:  Iteration No:  313 Worker Num:  7 \n",
            " Loss:  0.12304935604333878\n",
            "Test:  Iteration No:  313 \n",
            " Loss:  0.15068325775239286\n",
            "Test accuracy:  95.69\n",
            "Training:  Iteration No:  314 Worker Num:  0 \n",
            " Loss:  0.11372947692871094\n",
            "Training:  Iteration No:  314 Worker Num:  1 \n",
            " Loss:  0.2784379720687866\n",
            "Training:  Iteration No:  314 Worker Num:  2 \n",
            " Loss:  0.14541099965572357\n",
            "Training:  Iteration No:  314 Worker Num:  3 \n",
            " Loss:  0.1474248319864273\n",
            "Training:  Iteration No:  314 Worker Num:  4 \n",
            " Loss:  0.28893932700157166\n",
            "Training:  Iteration No:  314 Worker Num:  5 \n",
            " Loss:  0.1811990737915039\n",
            "Training:  Iteration No:  314 Worker Num:  6 \n",
            " Loss:  0.1979302167892456\n",
            "Training:  Iteration No:  314 Worker Num:  7 \n",
            " Loss:  0.13359957933425903\n",
            "Test:  Iteration No:  314 \n",
            " Loss:  0.1536334031025726\n",
            "Test accuracy:  95.42\n",
            "Training:  Iteration No:  315 Worker Num:  0 \n",
            " Loss:  0.1830577254295349\n",
            "Training:  Iteration No:  315 Worker Num:  1 \n",
            " Loss:  0.16898435354232788\n",
            "Training:  Iteration No:  315 Worker Num:  2 \n",
            " Loss:  0.28629353642463684\n",
            "Training:  Iteration No:  315 Worker Num:  3 \n",
            " Loss:  0.22448375821113586\n",
            "Training:  Iteration No:  315 Worker Num:  4 \n",
            " Loss:  0.26124000549316406\n",
            "Training:  Iteration No:  315 Worker Num:  5 \n",
            " Loss:  0.16440001130104065\n",
            "Training:  Iteration No:  315 Worker Num:  6 \n",
            " Loss:  0.16222713887691498\n",
            "Training:  Iteration No:  315 Worker Num:  7 \n",
            " Loss:  0.19131284952163696\n",
            "Test:  Iteration No:  315 \n",
            " Loss:  0.14818594595888937\n",
            "Test accuracy:  95.73\n",
            "Training:  Iteration No:  316 Worker Num:  0 \n",
            " Loss:  0.3026764392852783\n",
            "Training:  Iteration No:  316 Worker Num:  1 \n",
            " Loss:  0.14001521468162537\n",
            "Training:  Iteration No:  316 Worker Num:  2 \n",
            " Loss:  0.20956742763519287\n",
            "Training:  Iteration No:  316 Worker Num:  3 \n",
            " Loss:  0.22291085124015808\n",
            "Training:  Iteration No:  316 Worker Num:  4 \n",
            " Loss:  0.11840754747390747\n",
            "Training:  Iteration No:  316 Worker Num:  5 \n",
            " Loss:  0.14266709983348846\n",
            "Training:  Iteration No:  316 Worker Num:  6 \n",
            " Loss:  0.1543261706829071\n",
            "Training:  Iteration No:  316 Worker Num:  7 \n",
            " Loss:  0.17710283398628235\n",
            "Test:  Iteration No:  316 \n",
            " Loss:  0.1493612169936488\n",
            "Test accuracy:  95.58\n",
            "Training:  Iteration No:  317 Worker Num:  0 \n",
            " Loss:  0.18938295543193817\n",
            "Training:  Iteration No:  317 Worker Num:  1 \n",
            " Loss:  0.23445066809654236\n",
            "Training:  Iteration No:  317 Worker Num:  2 \n",
            " Loss:  0.19912302494049072\n",
            "Training:  Iteration No:  317 Worker Num:  3 \n",
            " Loss:  0.1820310354232788\n",
            "Training:  Iteration No:  317 Worker Num:  4 \n",
            " Loss:  0.17531803250312805\n",
            "Training:  Iteration No:  317 Worker Num:  5 \n",
            " Loss:  0.10434476286172867\n",
            "Training:  Iteration No:  317 Worker Num:  6 \n",
            " Loss:  0.2754744589328766\n",
            "Training:  Iteration No:  317 Worker Num:  7 \n",
            " Loss:  0.16225683689117432\n",
            "Test:  Iteration No:  317 \n",
            " Loss:  0.15189408942396881\n",
            "Test accuracy:  95.48\n",
            "Training:  Iteration No:  318 Worker Num:  0 \n",
            " Loss:  0.13818424940109253\n",
            "Training:  Iteration No:  318 Worker Num:  1 \n",
            " Loss:  0.15363511443138123\n",
            "Training:  Iteration No:  318 Worker Num:  2 \n",
            " Loss:  0.23846805095672607\n",
            "Training:  Iteration No:  318 Worker Num:  3 \n",
            " Loss:  0.1564873307943344\n",
            "Training:  Iteration No:  318 Worker Num:  4 \n",
            " Loss:  0.11923342198133469\n",
            "Training:  Iteration No:  318 Worker Num:  5 \n",
            " Loss:  0.13900405168533325\n",
            "Training:  Iteration No:  318 Worker Num:  6 \n",
            " Loss:  0.10802315920591354\n",
            "Training:  Iteration No:  318 Worker Num:  7 \n",
            " Loss:  0.22922015190124512\n",
            "Test:  Iteration No:  318 \n",
            " Loss:  0.14735506654752395\n",
            "Test accuracy:  95.65\n",
            "Training:  Iteration No:  319 Worker Num:  0 \n",
            " Loss:  0.11548228561878204\n",
            "Training:  Iteration No:  319 Worker Num:  1 \n",
            " Loss:  0.1888442039489746\n",
            "Training:  Iteration No:  319 Worker Num:  2 \n",
            " Loss:  0.06863725930452347\n",
            "Training:  Iteration No:  319 Worker Num:  3 \n",
            " Loss:  0.19804592430591583\n",
            "Training:  Iteration No:  319 Worker Num:  4 \n",
            " Loss:  0.22417688369750977\n",
            "Training:  Iteration No:  319 Worker Num:  5 \n",
            " Loss:  0.19893966615200043\n",
            "Training:  Iteration No:  319 Worker Num:  6 \n",
            " Loss:  0.2560787498950958\n",
            "Training:  Iteration No:  319 Worker Num:  7 \n",
            " Loss:  0.15973898768424988\n",
            "Test:  Iteration No:  319 \n",
            " Loss:  0.15033072594529653\n",
            "Test accuracy:  95.6\n",
            "Training:  Iteration No:  320 Worker Num:  0 \n",
            " Loss:  0.2155964970588684\n",
            "Training:  Iteration No:  320 Worker Num:  1 \n",
            " Loss:  0.09421654045581818\n",
            "Training:  Iteration No:  320 Worker Num:  2 \n",
            " Loss:  0.23796413838863373\n",
            "Training:  Iteration No:  320 Worker Num:  3 \n",
            " Loss:  0.16071082651615143\n",
            "Training:  Iteration No:  320 Worker Num:  4 \n",
            " Loss:  0.07365014404058456\n",
            "Training:  Iteration No:  320 Worker Num:  5 \n",
            " Loss:  0.14897386729717255\n",
            "Training:  Iteration No:  320 Worker Num:  6 \n",
            " Loss:  0.15653567016124725\n",
            "Training:  Iteration No:  320 Worker Num:  7 \n",
            " Loss:  0.2692524194717407\n",
            "Test:  Iteration No:  320 \n",
            " Loss:  0.15044207190151646\n",
            "Test accuracy:  95.54\n",
            "Training:  Iteration No:  321 Worker Num:  0 \n",
            " Loss:  0.32869407534599304\n",
            "Training:  Iteration No:  321 Worker Num:  1 \n",
            " Loss:  0.34903472661972046\n",
            "Training:  Iteration No:  321 Worker Num:  2 \n",
            " Loss:  0.09711413085460663\n",
            "Training:  Iteration No:  321 Worker Num:  3 \n",
            " Loss:  0.21246172487735748\n",
            "Training:  Iteration No:  321 Worker Num:  4 \n",
            " Loss:  0.28200486302375793\n",
            "Training:  Iteration No:  321 Worker Num:  5 \n",
            " Loss:  0.1267460286617279\n",
            "Training:  Iteration No:  321 Worker Num:  6 \n",
            " Loss:  0.1157398670911789\n",
            "Training:  Iteration No:  321 Worker Num:  7 \n",
            " Loss:  0.12470652163028717\n",
            "Test:  Iteration No:  321 \n",
            " Loss:  0.15407154954309704\n",
            "Test accuracy:  95.54\n",
            "Training:  Iteration No:  322 Worker Num:  0 \n",
            " Loss:  0.1497393399477005\n",
            "Training:  Iteration No:  322 Worker Num:  1 \n",
            " Loss:  0.1441788524389267\n",
            "Training:  Iteration No:  322 Worker Num:  2 \n",
            " Loss:  0.26217472553253174\n",
            "Training:  Iteration No:  322 Worker Num:  3 \n",
            " Loss:  0.3405601382255554\n",
            "Training:  Iteration No:  322 Worker Num:  4 \n",
            " Loss:  0.13013353943824768\n",
            "Training:  Iteration No:  322 Worker Num:  5 \n",
            " Loss:  0.13502530753612518\n",
            "Training:  Iteration No:  322 Worker Num:  6 \n",
            " Loss:  0.27955642342567444\n",
            "Training:  Iteration No:  322 Worker Num:  7 \n",
            " Loss:  0.12940295040607452\n",
            "Test:  Iteration No:  322 \n",
            " Loss:  0.14747892699758464\n",
            "Test accuracy:  95.69\n",
            "Training:  Iteration No:  323 Worker Num:  0 \n",
            " Loss:  0.12816686928272247\n",
            "Training:  Iteration No:  323 Worker Num:  1 \n",
            " Loss:  0.1597975194454193\n",
            "Training:  Iteration No:  323 Worker Num:  2 \n",
            " Loss:  0.24947494268417358\n",
            "Training:  Iteration No:  323 Worker Num:  3 \n",
            " Loss:  0.1819545030593872\n",
            "Training:  Iteration No:  323 Worker Num:  4 \n",
            " Loss:  0.15911638736724854\n",
            "Training:  Iteration No:  323 Worker Num:  5 \n",
            " Loss:  0.2013234794139862\n",
            "Training:  Iteration No:  323 Worker Num:  6 \n",
            " Loss:  0.24267148971557617\n",
            "Training:  Iteration No:  323 Worker Num:  7 \n",
            " Loss:  0.1506192982196808\n",
            "Test:  Iteration No:  323 \n",
            " Loss:  0.15187264265657602\n",
            "Test accuracy:  95.71\n",
            "Training:  Iteration No:  324 Worker Num:  0 \n",
            " Loss:  0.19032485783100128\n",
            "Training:  Iteration No:  324 Worker Num:  1 \n",
            " Loss:  0.22112400829792023\n",
            "Training:  Iteration No:  324 Worker Num:  2 \n",
            " Loss:  0.11941775679588318\n",
            "Training:  Iteration No:  324 Worker Num:  3 \n",
            " Loss:  0.1771874576807022\n",
            "Training:  Iteration No:  324 Worker Num:  4 \n",
            " Loss:  0.14059047400951385\n",
            "Training:  Iteration No:  324 Worker Num:  5 \n",
            " Loss:  0.13913749158382416\n",
            "Training:  Iteration No:  324 Worker Num:  6 \n",
            " Loss:  0.23955604434013367\n",
            "Training:  Iteration No:  324 Worker Num:  7 \n",
            " Loss:  0.19018098711967468\n",
            "Test:  Iteration No:  324 \n",
            " Loss:  0.14792601976402198\n",
            "Test accuracy:  95.75\n",
            "Training:  Iteration No:  325 Worker Num:  0 \n",
            " Loss:  0.12695756554603577\n",
            "Training:  Iteration No:  325 Worker Num:  1 \n",
            " Loss:  0.14658023416996002\n",
            "Training:  Iteration No:  325 Worker Num:  2 \n",
            " Loss:  0.22523199021816254\n",
            "Training:  Iteration No:  325 Worker Num:  3 \n",
            " Loss:  0.24834495782852173\n",
            "Training:  Iteration No:  325 Worker Num:  4 \n",
            " Loss:  0.13283121585845947\n",
            "Training:  Iteration No:  325 Worker Num:  5 \n",
            " Loss:  0.18719133734703064\n",
            "Training:  Iteration No:  325 Worker Num:  6 \n",
            " Loss:  0.15549539029598236\n",
            "Training:  Iteration No:  325 Worker Num:  7 \n",
            " Loss:  0.19448038935661316\n",
            "Test:  Iteration No:  325 \n",
            " Loss:  0.14610631581207242\n",
            "Test accuracy:  95.69\n",
            "Training:  Iteration No:  326 Worker Num:  0 \n",
            " Loss:  0.33817043900489807\n",
            "Training:  Iteration No:  326 Worker Num:  1 \n",
            " Loss:  0.14673824608325958\n",
            "Training:  Iteration No:  326 Worker Num:  2 \n",
            " Loss:  0.20184582471847534\n",
            "Training:  Iteration No:  326 Worker Num:  3 \n",
            " Loss:  0.1469486802816391\n",
            "Training:  Iteration No:  326 Worker Num:  4 \n",
            " Loss:  0.09651844948530197\n",
            "Training:  Iteration No:  326 Worker Num:  5 \n",
            " Loss:  0.07982110232114792\n",
            "Training:  Iteration No:  326 Worker Num:  6 \n",
            " Loss:  0.2264179289340973\n",
            "Training:  Iteration No:  326 Worker Num:  7 \n",
            " Loss:  0.13507816195487976\n",
            "Test:  Iteration No:  326 \n",
            " Loss:  0.14713704740463557\n",
            "Test accuracy:  95.6\n",
            "Training:  Iteration No:  327 Worker Num:  0 \n",
            " Loss:  0.17610995471477509\n",
            "Training:  Iteration No:  327 Worker Num:  1 \n",
            " Loss:  0.1178431510925293\n",
            "Training:  Iteration No:  327 Worker Num:  2 \n",
            " Loss:  0.16476328670978546\n",
            "Training:  Iteration No:  327 Worker Num:  3 \n",
            " Loss:  0.13601410388946533\n",
            "Training:  Iteration No:  327 Worker Num:  4 \n",
            " Loss:  0.28052273392677307\n",
            "Training:  Iteration No:  327 Worker Num:  5 \n",
            " Loss:  0.22169412672519684\n",
            "Training:  Iteration No:  327 Worker Num:  6 \n",
            " Loss:  0.13538497686386108\n",
            "Training:  Iteration No:  327 Worker Num:  7 \n",
            " Loss:  0.12327764183282852\n",
            "Test:  Iteration No:  327 \n",
            " Loss:  0.14545725604307047\n",
            "Test accuracy:  95.79\n",
            "Training:  Iteration No:  328 Worker Num:  0 \n",
            " Loss:  0.29680135846138\n",
            "Training:  Iteration No:  328 Worker Num:  1 \n",
            " Loss:  0.19596423208713531\n",
            "Training:  Iteration No:  328 Worker Num:  2 \n",
            " Loss:  0.2174345999956131\n",
            "Training:  Iteration No:  328 Worker Num:  3 \n",
            " Loss:  0.16052746772766113\n",
            "Training:  Iteration No:  328 Worker Num:  4 \n",
            " Loss:  0.160069540143013\n",
            "Training:  Iteration No:  328 Worker Num:  5 \n",
            " Loss:  0.1331319361925125\n",
            "Training:  Iteration No:  328 Worker Num:  6 \n",
            " Loss:  0.1561940759420395\n",
            "Training:  Iteration No:  328 Worker Num:  7 \n",
            " Loss:  0.1603885442018509\n",
            "Test:  Iteration No:  328 \n",
            " Loss:  0.14564472218810381\n",
            "Test accuracy:  95.72\n",
            "Training:  Iteration No:  329 Worker Num:  0 \n",
            " Loss:  0.19377507269382477\n",
            "Training:  Iteration No:  329 Worker Num:  1 \n",
            " Loss:  0.13761557638645172\n",
            "Training:  Iteration No:  329 Worker Num:  2 \n",
            " Loss:  0.12754347920417786\n",
            "Training:  Iteration No:  329 Worker Num:  3 \n",
            " Loss:  0.14526718854904175\n",
            "Training:  Iteration No:  329 Worker Num:  4 \n",
            " Loss:  0.1829797923564911\n",
            "Training:  Iteration No:  329 Worker Num:  5 \n",
            " Loss:  0.24230770766735077\n",
            "Training:  Iteration No:  329 Worker Num:  6 \n",
            " Loss:  0.20917491614818573\n",
            "Training:  Iteration No:  329 Worker Num:  7 \n",
            " Loss:  0.23627272248268127\n",
            "Test:  Iteration No:  329 \n",
            " Loss:  0.14766296225045866\n",
            "Test accuracy:  95.7\n",
            "Training:  Iteration No:  330 Worker Num:  0 \n",
            " Loss:  0.2380339652299881\n",
            "Training:  Iteration No:  330 Worker Num:  1 \n",
            " Loss:  0.146429181098938\n",
            "Training:  Iteration No:  330 Worker Num:  2 \n",
            " Loss:  0.2170492708683014\n",
            "Training:  Iteration No:  330 Worker Num:  3 \n",
            " Loss:  0.1647196114063263\n",
            "Training:  Iteration No:  330 Worker Num:  4 \n",
            " Loss:  0.127153217792511\n",
            "Training:  Iteration No:  330 Worker Num:  5 \n",
            " Loss:  0.17994798719882965\n",
            "Training:  Iteration No:  330 Worker Num:  6 \n",
            " Loss:  0.21621061861515045\n",
            "Training:  Iteration No:  330 Worker Num:  7 \n",
            " Loss:  0.166907399892807\n",
            "Test:  Iteration No:  330 \n",
            " Loss:  0.14598483619975705\n",
            "Test accuracy:  95.71\n",
            "Training:  Iteration No:  331 Worker Num:  0 \n",
            " Loss:  0.12856373190879822\n",
            "Training:  Iteration No:  331 Worker Num:  1 \n",
            " Loss:  0.2171880006790161\n",
            "Training:  Iteration No:  331 Worker Num:  2 \n",
            " Loss:  0.1020447164773941\n",
            "Training:  Iteration No:  331 Worker Num:  3 \n",
            " Loss:  0.14199063181877136\n",
            "Training:  Iteration No:  331 Worker Num:  4 \n",
            " Loss:  0.2441662698984146\n",
            "Training:  Iteration No:  331 Worker Num:  5 \n",
            " Loss:  0.2549745738506317\n",
            "Training:  Iteration No:  331 Worker Num:  6 \n",
            " Loss:  0.35474151372909546\n",
            "Training:  Iteration No:  331 Worker Num:  7 \n",
            " Loss:  0.36172330379486084\n",
            "Test:  Iteration No:  331 \n",
            " Loss:  0.14794344671329931\n",
            "Test accuracy:  95.79\n",
            "Training:  Iteration No:  332 Worker Num:  0 \n",
            " Loss:  0.14470498263835907\n",
            "Training:  Iteration No:  332 Worker Num:  1 \n",
            " Loss:  0.18983706831932068\n",
            "Training:  Iteration No:  332 Worker Num:  2 \n",
            " Loss:  0.1981765478849411\n",
            "Training:  Iteration No:  332 Worker Num:  3 \n",
            " Loss:  0.19643038511276245\n",
            "Training:  Iteration No:  332 Worker Num:  4 \n",
            " Loss:  0.1474539339542389\n",
            "Training:  Iteration No:  332 Worker Num:  5 \n",
            " Loss:  0.22443917393684387\n",
            "Training:  Iteration No:  332 Worker Num:  6 \n",
            " Loss:  0.28768712282180786\n",
            "Training:  Iteration No:  332 Worker Num:  7 \n",
            " Loss:  0.3018578290939331\n",
            "Test:  Iteration No:  332 \n",
            " Loss:  0.14486102463347436\n",
            "Test accuracy:  95.83\n",
            "Training:  Iteration No:  333 Worker Num:  0 \n",
            " Loss:  0.11885978281497955\n",
            "Training:  Iteration No:  333 Worker Num:  1 \n",
            " Loss:  0.17315666377544403\n",
            "Training:  Iteration No:  333 Worker Num:  2 \n",
            " Loss:  0.07481475919485092\n",
            "Training:  Iteration No:  333 Worker Num:  3 \n",
            " Loss:  0.11603696644306183\n",
            "Training:  Iteration No:  333 Worker Num:  4 \n",
            " Loss:  0.24659349024295807\n",
            "Training:  Iteration No:  333 Worker Num:  5 \n",
            " Loss:  0.12184403836727142\n",
            "Training:  Iteration No:  333 Worker Num:  6 \n",
            " Loss:  0.2626332938671112\n",
            "Training:  Iteration No:  333 Worker Num:  7 \n",
            " Loss:  0.22140268981456757\n",
            "Test:  Iteration No:  333 \n",
            " Loss:  0.14098152020191657\n",
            "Test accuracy:  95.98\n",
            "Training:  Iteration No:  334 Worker Num:  0 \n",
            " Loss:  0.14231568574905396\n",
            "Training:  Iteration No:  334 Worker Num:  1 \n",
            " Loss:  0.15849913656711578\n",
            "Training:  Iteration No:  334 Worker Num:  2 \n",
            " Loss:  0.26328209042549133\n",
            "Training:  Iteration No:  334 Worker Num:  3 \n",
            " Loss:  0.11508259922266006\n",
            "Training:  Iteration No:  334 Worker Num:  4 \n",
            " Loss:  0.22577406466007233\n",
            "Training:  Iteration No:  334 Worker Num:  5 \n",
            " Loss:  0.15718834102153778\n",
            "Training:  Iteration No:  334 Worker Num:  6 \n",
            " Loss:  0.21447549760341644\n",
            "Training:  Iteration No:  334 Worker Num:  7 \n",
            " Loss:  0.19175617396831512\n",
            "Test:  Iteration No:  334 \n",
            " Loss:  0.141657859346346\n",
            "Test accuracy:  95.8\n",
            "Training:  Iteration No:  335 Worker Num:  0 \n",
            " Loss:  0.22890809178352356\n",
            "Training:  Iteration No:  335 Worker Num:  1 \n",
            " Loss:  0.2215505689382553\n",
            "Training:  Iteration No:  335 Worker Num:  2 \n",
            " Loss:  0.09985706955194473\n",
            "Training:  Iteration No:  335 Worker Num:  3 \n",
            " Loss:  0.18622314929962158\n",
            "Training:  Iteration No:  335 Worker Num:  4 \n",
            " Loss:  0.10807861387729645\n",
            "Training:  Iteration No:  335 Worker Num:  5 \n",
            " Loss:  0.13775019347667694\n",
            "Training:  Iteration No:  335 Worker Num:  6 \n",
            " Loss:  0.20830215513706207\n",
            "Training:  Iteration No:  335 Worker Num:  7 \n",
            " Loss:  0.22683016955852509\n",
            "Test:  Iteration No:  335 \n",
            " Loss:  0.14135955185688373\n",
            "Test accuracy:  96.01\n",
            "96 % accuracy reached!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(test_accuracy[0:-20])\n",
        "plt.xlabel('Iteration Number')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Test accuracy');"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "9b2d3104-af38-409d-9426-690ccf72790d",
        "id": "fNDi2sBz1DxU"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "avg_train_losses = []\n",
        "for i in range(len(train_losses[0])):\n",
        "    avg_loss = 0\n",
        "    for j in range(num_workers):\n",
        "        avg_loss += train_losses[j][i]\n",
        "\n",
        "    avg_train_losses.append(avg_loss / num_workers)    \n"
      ],
      "metadata": {
        "id": "XYGPps_D1DxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_losses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwlwcpgO1DxU",
        "outputId": "a267305a-b742-4293-b450-feb8979c2eb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "335"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(avg_train_losses[0:-20], label='train')\n",
        "plt.plot(test_losses[0:-20], label='test')\n",
        "plt.xlabel('Iteration Number')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Loss function');"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "73ae4dd1-bfa4-4f09-f9c2-07f21782d972",
        "id": "-c_vPQOX1DxU"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Periodically Connected Communication Matrix"
      ],
      "metadata": {
        "id": "pJiEAZaQUTm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_messages(queue_i, weights_i, alpha_i, with_noise, alpha_update):\n",
        "    while len(queue_i) != 0:\n",
        "        weights_j, alpha_j = queue_i.pop()\n",
        "        for layer in weights_i.keys():\n",
        "            if  with_noise == True:\n",
        "                noise = torch.randn_like(weights_j[layer])\n",
        "            else:\n",
        "                noise = 0    \n",
        "            \n",
        "            weights_i[layer] = (alpha_j/(alpha_i+alpha_j)) * (weights_j[layer] + 0.1*noise) + (alpha_i/(alpha_i+alpha_j)) * weights_i[layer]\n",
        "            if alpha_update == True:\n",
        "                alpha_i += alpha_j\n",
        "\n",
        "    return weights_i, alpha_i        "
      ],
      "metadata": {
        "id": "p1QKTCqowft-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def push_message(queue_j, weights_i, alpha_i):\n",
        "    queue_j.append([weights_i, alpha_i/2])"
      ],
      "metadata": {
        "id": "1LrLibWawft_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(iter_no, worker_num, with_noise=False, alpha_update=True):\n",
        "    net = nets_list[worker_num]\n",
        "    queue = queues_list[worker_num]\n",
        "    alpha = alpha_list[worker_num]\n",
        "\n",
        "    new_weights, new_alpha = process_messages(queue, net.state_dict(), alpha, with_noise, alpha_update)\n",
        "    net.load_state_dict(new_weights)\n",
        "    nets_list[worker_num] = net\n",
        "    alpha_list[worker_num] = new_alpha\n",
        "\n",
        "    net = nets_list[worker_num]\n",
        "    queue = queues_list[worker_num]\n",
        "    alpha = alpha_list[worker_num]\n",
        "\n",
        "    # dataset_size = len(mnist_trainset)\n",
        "    # data_indice = [np.random.choice(list(range(dataset_size)))]\n",
        "    # train_sampler = SubsetRandomSampler(data_indice)\n",
        "    # train_loader = torch.utils.data.DataLoader(dataset=mnist_trainset, shuffle=False, batch_size=batch_size, sampler=train_sampler)\n",
        "    try :\n",
        "        batch_data, batch_labels = data_iters[worker_num].next()\n",
        "    except:\n",
        "        train_loader = torch.utils.data.DataLoader(\n",
        "                 dataset=mnist_trainset,\n",
        "                 batch_size=batch_size,\n",
        "                 shuffle=True\n",
        "                 )\n",
        "    \n",
        "        data_iters[worker_num] = iter(train_loader)\n",
        "\n",
        "        batch_data, batch_labels = data_iters[worker_num].next()\n",
        "\n",
        "\n",
        "    # Transfer to GPU\n",
        "    batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
        "    # zero the parameter gradients\n",
        "    optimizer = optimizers[worker_num]\n",
        "    optimizer.zero_grad()\n",
        "    # Model computations\n",
        "    batch_outputs = net(batch_data)        \n",
        "    loss = criterion(batch_outputs, batch_labels)\n",
        "    loss.backward()\n",
        "    # print statistics\n",
        "    print('Training: ', 'Iteration No: ', iter_no+1, 'Worker Num: ', worker_num, '\\n',\n",
        "    'Loss: ', loss.item())\n",
        "    clipping_value = 1 # arbitrary value of your choosing\n",
        "    torch.nn.utils.clip_grad_norm(net.parameters(), clipping_value)\n",
        "    optimizer.step()\n",
        "    train_losses[worker_num].append(loss.item())\n",
        "\n",
        "    S = np.random.binomial(1, p, 1)\n",
        "    if S == 1:\n",
        "        if 1 in comm_matrix[worker_num, :]:\n",
        "            j = list(comm_matrix[worker_num, :]).index(1)\n",
        "            push_message(queues_list[j], net.state_dict(), alpha)\n",
        "\n",
        "    nets_list[worker_num] = net   \n",
        "    optimizers[worker_num] = optimizer"
      ],
      "metadata": {
        "id": "0mp1XIiTwft_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(iter_no, networks):\n",
        "    layers = networks[0].state_dict().keys()\n",
        "    test_weights = {}\n",
        "    for layer in layers:\n",
        "        test_weights[layer] = 0\n",
        "        for network in networks:\n",
        "            test_weights[layer] += network.state_dict()[layer]\n",
        "\n",
        "        test_weights[layer] /= num_workers\n",
        "\n",
        "    test_network.load_state_dict(test_weights)    \n",
        "\n",
        "    # Validation\n",
        "    with torch.set_grad_enabled(False):\n",
        "        batch_losses = []\n",
        "        counter = 0\n",
        "        total = 0\n",
        "        for batch_data, batch_labels in test_loader:\n",
        "            total += len(batch_data)\n",
        "            # Transfer to GPU\n",
        "            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
        "            # Model computations\n",
        "            batch_outputs = test_network(batch_data)            \n",
        "            loss = criterion(batch_outputs, batch_labels)\n",
        "            batch_losses.append(loss.item())\n",
        "            \n",
        "            # compute accuracy\n",
        "            for i in range(len(batch_data)):\n",
        "                true = int(batch_labels[i])\n",
        "                pred = int(torch.argmax(batch_outputs[i,:]))\n",
        "                if pred == true:\n",
        "                    counter += 1\n",
        "\n",
        "        test_losses.append(sum(batch_losses) / len(batch_losses))\n",
        "        print('Test: ', 'Iteration No: ', iter_no+1,'\\n',\n",
        "            'Loss: ', test_losses[-1])\n",
        "        acc = (counter*100) / total   \n",
        "        test_accuracy.append(acc)    \n",
        "        print('Test accuracy: ', acc) \n",
        "    if acc >= 96:\n",
        "        print('96 % accuracy reached!')\n",
        "        \n",
        "    return acc"
      ],
      "metadata": {
        "id": "WvwOGVb1wft_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_workers = 8\n",
        "all_workers = [i for i in range (num_workers)]"
      ],
      "metadata": {
        "id": "1idPHDlXuh-c"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nets_list = [CNN().to(device)]\n",
        "optimizers = [optim.SGD(nets_list[0].parameters(), lr = 0.1)]\n",
        "# schedulers = [optim.lr_scheduler.MultiStepLR(optimizers[0], milestones=[210], gamma=0.1)]\n",
        "for i in range(1,num_workers):\n",
        "    new_net = CNN().to(device)\n",
        "    new_net.load_state_dict(nets_list[0].state_dict())\n",
        "    nets_list.append(new_net)\n",
        "    optimizer = optim.SGD(new_net.parameters(), lr = 0.1) \n",
        "    optimizers.append(optimizer)\n",
        "    # scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[210], gamma=0.1)\n",
        "    # schedulers.append(scheduler)"
      ],
      "metadata": {
        "id": "TAA37e4Nuh-u"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_network = CNN().to(device)"
      ],
      "metadata": {
        "id": "YzdgfF_muh-v"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "queues_list = [[] for i in range (num_workers)]\n",
        "alpha_list = [1 / num_workers for i in range (num_workers)]"
      ],
      "metadata": {
        "id": "yI9JfG_5uh-v"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_iters = 1000\n",
        "p = 0.5\n",
        "batch_size = 128"
      ],
      "metadata": {
        "id": "3x4jCsEhuh-v"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_iters = []\n",
        "for i in range(num_workers):\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "                 dataset=mnist_trainset,\n",
        "                 batch_size=batch_size,\n",
        "                 shuffle=True\n",
        "                 )\n",
        "    \n",
        "    data_iters.append(iter(train_loader))"
      ],
      "metadata": {
        "id": "wXLqBBjouh-v"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader = torch.utils.data.DataLoader(\n",
        "                dataset=mnist_testset,\n",
        "                batch_size=batch_size,\n",
        "                shuffle=False\n",
        "                )"
      ],
      "metadata": {
        "id": "eZ1VJ49vuh-w"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "train_losses = [[] for i in range (num_workers)]\n",
        "test_losses = []\n",
        "test_accuracy = []"
      ],
      "metadata": {
        "id": "Q6blY91ruh-w"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comm_matrices = []\n",
        "for i in range (num_workers-1):\n",
        "    mat = torch.zeros(num_workers, num_workers)\n",
        "    mat[i, i+1] = 1\n",
        "    mat[i+1, i] = 1\n",
        "    comm_matrices.append(mat)"
      ],
      "metadata": {
        "id": "20ViFSLIuh-x"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ss = 0\n",
        "for iter_no in range(max_iters):\n",
        "    comm_matrix = comm_matrices[ss]\n",
        "    for worker_num in range(num_workers):\n",
        "        train(iter_no, worker_num, with_noise=False, alpha_update=False)\n",
        "    acc = test(iter_no, nets_list)\n",
        "    if acc >= 96:\n",
        "        break\n",
        "\n",
        "    if ss != 6:\n",
        "        ss += 1\n",
        "    else:\n",
        "        ss = 0        \n",
        "\n",
        "\n",
        "\n",
        "    # for scheduler in schedulers:\n",
        "    #     scheduler.step()    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab9cef6d-4e5e-4aae-8df6-1a19a7e76e57",
        "id": "TqlaW3bGuh-x"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training:  Iteration No:  1 Worker Num:  0 \n",
            " Loss:  2.309110641479492\n",
            "Training:  Iteration No:  1 Worker Num:  1 \n",
            " Loss:  2.3120899200439453\n",
            "Training:  Iteration No:  1 Worker Num:  2 \n",
            " Loss:  2.3102314472198486\n",
            "Training:  Iteration No:  1 Worker Num:  3 \n",
            " Loss:  2.301419734954834\n",
            "Training:  Iteration No:  1 Worker Num:  4 \n",
            " Loss:  2.3046247959136963\n",
            "Training:  Iteration No:  1 Worker Num:  5 \n",
            " Loss:  2.3101987838745117\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            " Loss:  0.3769218490659436\n",
            "Test accuracy:  89.18\n",
            "Training:  Iteration No:  104 Worker Num:  0 \n",
            " Loss:  0.4727098345756531\n",
            "Training:  Iteration No:  104 Worker Num:  1 \n",
            " Loss:  0.46219903230667114\n",
            "Training:  Iteration No:  104 Worker Num:  2 \n",
            " Loss:  0.5160382986068726\n",
            "Training:  Iteration No:  104 Worker Num:  3 \n",
            " Loss:  0.5101471543312073\n",
            "Training:  Iteration No:  104 Worker Num:  4 \n",
            " Loss:  0.3392745852470398\n",
            "Training:  Iteration No:  104 Worker Num:  5 \n",
            " Loss:  0.4318375885486603\n",
            "Training:  Iteration No:  104 Worker Num:  6 \n",
            " Loss:  0.43993106484413147\n",
            "Training:  Iteration No:  104 Worker Num:  7 \n",
            " Loss:  0.37398970127105713\n",
            "Test:  Iteration No:  104 \n",
            " Loss:  0.3725698050818866\n",
            "Test accuracy:  89.32\n",
            "Training:  Iteration No:  105 Worker Num:  0 \n",
            " Loss:  0.40500199794769287\n",
            "Training:  Iteration No:  105 Worker Num:  1 \n",
            " Loss:  0.4650942087173462\n",
            "Training:  Iteration No:  105 Worker Num:  2 \n",
            " Loss:  0.46468856930732727\n",
            "Training:  Iteration No:  105 Worker Num:  3 \n",
            " Loss:  0.4255944788455963\n",
            "Training:  Iteration No:  105 Worker Num:  4 \n",
            " Loss:  0.4717714488506317\n",
            "Training:  Iteration No:  105 Worker Num:  5 \n",
            " Loss:  0.3792744278907776\n",
            "Training:  Iteration No:  105 Worker Num:  6 \n",
            " Loss:  0.32260724902153015\n",
            "Training:  Iteration No:  105 Worker Num:  7 \n",
            " Loss:  0.45711255073547363\n",
            "Test:  Iteration No:  105 \n",
            " Loss:  0.36859533360487295\n",
            "Test accuracy:  89.56\n",
            "Training:  Iteration No:  106 Worker Num:  0 \n",
            " Loss:  0.48640432953834534\n",
            "Training:  Iteration No:  106 Worker Num:  1 \n",
            " Loss:  0.3745633661746979\n",
            "Training:  Iteration No:  106 Worker Num:  2 \n",
            " Loss:  0.4974896013736725\n",
            "Training:  Iteration No:  106 Worker Num:  3 \n",
            " Loss:  0.5728464126586914\n",
            "Training:  Iteration No:  106 Worker Num:  4 \n",
            " Loss:  0.3994072675704956\n",
            "Training:  Iteration No:  106 Worker Num:  5 \n",
            " Loss:  0.35007068514823914\n",
            "Training:  Iteration No:  106 Worker Num:  6 \n",
            " Loss:  0.41452986001968384\n",
            "Training:  Iteration No:  106 Worker Num:  7 \n",
            " Loss:  0.38781383633613586\n",
            "Test:  Iteration No:  106 \n",
            " Loss:  0.36673253774642944\n",
            "Test accuracy:  89.69\n",
            "Training:  Iteration No:  107 Worker Num:  0 \n",
            " Loss:  0.4258732795715332\n",
            "Training:  Iteration No:  107 Worker Num:  1 \n",
            " Loss:  0.4393792748451233\n",
            "Training:  Iteration No:  107 Worker Num:  2 \n",
            " Loss:  0.4761297404766083\n",
            "Training:  Iteration No:  107 Worker Num:  3 \n",
            " Loss:  0.4640897512435913\n",
            "Training:  Iteration No:  107 Worker Num:  4 \n",
            " Loss:  0.43474188446998596\n",
            "Training:  Iteration No:  107 Worker Num:  5 \n",
            " Loss:  0.40638816356658936\n",
            "Training:  Iteration No:  107 Worker Num:  6 \n",
            " Loss:  0.2933380901813507\n",
            "Training:  Iteration No:  107 Worker Num:  7 \n",
            " Loss:  0.27154800295829773\n",
            "Test:  Iteration No:  107 \n",
            " Loss:  0.3664758603406858\n",
            "Test accuracy:  89.68\n",
            "Training:  Iteration No:  108 Worker Num:  0 \n",
            " Loss:  0.5011521577835083\n",
            "Training:  Iteration No:  108 Worker Num:  1 \n",
            " Loss:  0.3960494101047516\n",
            "Training:  Iteration No:  108 Worker Num:  2 \n",
            " Loss:  0.5006343722343445\n",
            "Training:  Iteration No:  108 Worker Num:  3 \n",
            " Loss:  0.5439463257789612\n",
            "Training:  Iteration No:  108 Worker Num:  4 \n",
            " Loss:  0.5621532201766968\n",
            "Training:  Iteration No:  108 Worker Num:  5 \n",
            " Loss:  0.48618677258491516\n",
            "Training:  Iteration No:  108 Worker Num:  6 \n",
            " Loss:  0.4709901213645935\n",
            "Training:  Iteration No:  108 Worker Num:  7 \n",
            " Loss:  0.4848003387451172\n",
            "Test:  Iteration No:  108 \n",
            " Loss:  0.3658535293763197\n",
            "Test accuracy:  89.27\n",
            "Training:  Iteration No:  109 Worker Num:  0 \n",
            " Loss:  0.4086051881313324\n",
            "Training:  Iteration No:  109 Worker Num:  1 \n",
            " Loss:  0.2713460922241211\n",
            "Training:  Iteration No:  109 Worker Num:  2 \n",
            " Loss:  0.3138309419155121\n",
            "Training:  Iteration No:  109 Worker Num:  3 \n",
            " Loss:  0.35993441939353943\n",
            "Training:  Iteration No:  109 Worker Num:  4 \n",
            " Loss:  0.4832113981246948\n",
            "Training:  Iteration No:  109 Worker Num:  5 \n",
            " Loss:  0.43331050872802734\n",
            "Training:  Iteration No:  109 Worker Num:  6 \n",
            " Loss:  0.42828497290611267\n",
            "Training:  Iteration No:  109 Worker Num:  7 \n",
            " Loss:  0.5570145845413208\n",
            "Test:  Iteration No:  109 \n",
            " Loss:  0.36499432691290407\n",
            "Test accuracy:  89.49\n",
            "Training:  Iteration No:  110 Worker Num:  0 \n",
            " Loss:  0.40026405453681946\n",
            "Training:  Iteration No:  110 Worker Num:  1 \n",
            " Loss:  0.44878849387168884\n",
            "Training:  Iteration No:  110 Worker Num:  2 \n",
            " Loss:  0.6288116574287415\n",
            "Training:  Iteration No:  110 Worker Num:  3 \n",
            " Loss:  0.2813389301300049\n",
            "Training:  Iteration No:  110 Worker Num:  4 \n",
            " Loss:  0.47855550050735474\n",
            "Training:  Iteration No:  110 Worker Num:  5 \n",
            " Loss:  0.45096999406814575\n",
            "Training:  Iteration No:  110 Worker Num:  6 \n",
            " Loss:  0.3766164779663086\n",
            "Training:  Iteration No:  110 Worker Num:  7 \n",
            " Loss:  0.3181794285774231\n",
            "Test:  Iteration No:  110 \n",
            " Loss:  0.3686778598193881\n",
            "Test accuracy:  89.41\n",
            "Training:  Iteration No:  111 Worker Num:  0 \n",
            " Loss:  0.5155839920043945\n",
            "Training:  Iteration No:  111 Worker Num:  1 \n",
            " Loss:  0.4688793420791626\n",
            "Training:  Iteration No:  111 Worker Num:  2 \n",
            " Loss:  0.36844295263290405\n",
            "Training:  Iteration No:  111 Worker Num:  3 \n",
            " Loss:  0.5058638453483582\n",
            "Training:  Iteration No:  111 Worker Num:  4 \n",
            " Loss:  0.4408318102359772\n",
            "Training:  Iteration No:  111 Worker Num:  5 \n",
            " Loss:  0.49594271183013916\n",
            "Training:  Iteration No:  111 Worker Num:  6 \n",
            " Loss:  0.6724879145622253\n",
            "Training:  Iteration No:  111 Worker Num:  7 \n",
            " Loss:  0.4820522964000702\n",
            "Test:  Iteration No:  111 \n",
            " Loss:  0.35791809852176076\n",
            "Test accuracy:  89.82\n",
            "Training:  Iteration No:  112 Worker Num:  0 \n",
            " Loss:  0.4779618978500366\n",
            "Training:  Iteration No:  112 Worker Num:  1 \n",
            " Loss:  0.5337979197502136\n",
            "Training:  Iteration No:  112 Worker Num:  2 \n",
            " Loss:  0.4639616310596466\n",
            "Training:  Iteration No:  112 Worker Num:  3 \n",
            " Loss:  0.6145285367965698\n",
            "Training:  Iteration No:  112 Worker Num:  4 \n",
            " Loss:  0.3432823717594147\n",
            "Training:  Iteration No:  112 Worker Num:  5 \n",
            " Loss:  0.4703509509563446\n",
            "Training:  Iteration No:  112 Worker Num:  6 \n",
            " Loss:  0.41929304599761963\n",
            "Training:  Iteration No:  112 Worker Num:  7 \n",
            " Loss:  0.4858582317829132\n",
            "Test:  Iteration No:  112 \n",
            " Loss:  0.3568396828597105\n",
            "Test accuracy:  89.82\n",
            "Training:  Iteration No:  113 Worker Num:  0 \n",
            " Loss:  0.48338010907173157\n",
            "Training:  Iteration No:  113 Worker Num:  1 \n",
            " Loss:  0.44797760248184204\n",
            "Training:  Iteration No:  113 Worker Num:  2 \n",
            " Loss:  0.5356205701828003\n",
            "Training:  Iteration No:  113 Worker Num:  3 \n",
            " Loss:  0.4801766872406006\n",
            "Training:  Iteration No:  113 Worker Num:  4 \n",
            " Loss:  0.36825692653656006\n",
            "Training:  Iteration No:  113 Worker Num:  5 \n",
            " Loss:  0.4941309094429016\n",
            "Training:  Iteration No:  113 Worker Num:  6 \n",
            " Loss:  0.4369339942932129\n",
            "Training:  Iteration No:  113 Worker Num:  7 \n",
            " Loss:  0.47148847579956055\n",
            "Test:  Iteration No:  113 \n",
            " Loss:  0.3546623804523975\n",
            "Test accuracy:  89.88\n",
            "Training:  Iteration No:  114 Worker Num:  0 \n",
            " Loss:  0.412169486284256\n",
            "Training:  Iteration No:  114 Worker Num:  1 \n",
            " Loss:  0.36999446153640747\n",
            "Training:  Iteration No:  114 Worker Num:  2 \n",
            " Loss:  0.594668984413147\n",
            "Training:  Iteration No:  114 Worker Num:  3 \n",
            " Loss:  0.5163425207138062\n",
            "Training:  Iteration No:  114 Worker Num:  4 \n",
            " Loss:  0.37559381127357483\n",
            "Training:  Iteration No:  114 Worker Num:  5 \n",
            " Loss:  0.332993745803833\n",
            "Training:  Iteration No:  114 Worker Num:  6 \n",
            " Loss:  0.5348963141441345\n",
            "Training:  Iteration No:  114 Worker Num:  7 \n",
            " Loss:  0.4222973585128784\n",
            "Test:  Iteration No:  114 \n",
            " Loss:  0.35552325359062303\n",
            "Test accuracy:  90.02\n",
            "Training:  Iteration No:  115 Worker Num:  0 \n",
            " Loss:  0.34459686279296875\n",
            "Training:  Iteration No:  115 Worker Num:  1 \n",
            " Loss:  0.31430530548095703\n",
            "Training:  Iteration No:  115 Worker Num:  2 \n",
            " Loss:  0.4341127872467041\n",
            "Training:  Iteration No:  115 Worker Num:  3 \n",
            " Loss:  0.5164369940757751\n",
            "Training:  Iteration No:  115 Worker Num:  4 \n",
            " Loss:  0.41437098383903503\n",
            "Training:  Iteration No:  115 Worker Num:  5 \n",
            " Loss:  0.48874568939208984\n",
            "Training:  Iteration No:  115 Worker Num:  6 \n",
            " Loss:  0.46946951746940613\n",
            "Training:  Iteration No:  115 Worker Num:  7 \n",
            " Loss:  0.3832545876502991\n",
            "Test:  Iteration No:  115 \n",
            " Loss:  0.35786937082870096\n",
            "Test accuracy:  89.75\n",
            "Training:  Iteration No:  116 Worker Num:  0 \n",
            " Loss:  0.5554946064949036\n",
            "Training:  Iteration No:  116 Worker Num:  1 \n",
            " Loss:  0.5011789798736572\n",
            "Training:  Iteration No:  116 Worker Num:  2 \n",
            " Loss:  0.3654003441333771\n",
            "Training:  Iteration No:  116 Worker Num:  3 \n",
            " Loss:  0.4445318579673767\n",
            "Training:  Iteration No:  116 Worker Num:  4 \n",
            " Loss:  0.3648862838745117\n",
            "Training:  Iteration No:  116 Worker Num:  5 \n",
            " Loss:  0.6212995648384094\n",
            "Training:  Iteration No:  116 Worker Num:  6 \n",
            " Loss:  0.41135868430137634\n",
            "Training:  Iteration No:  116 Worker Num:  7 \n",
            " Loss:  0.6042670011520386\n",
            "Test:  Iteration No:  116 \n",
            " Loss:  0.35620872525474695\n",
            "Test accuracy:  89.88\n",
            "Training:  Iteration No:  117 Worker Num:  0 \n",
            " Loss:  0.48671379685401917\n",
            "Training:  Iteration No:  117 Worker Num:  1 \n",
            " Loss:  0.34287163615226746\n",
            "Training:  Iteration No:  117 Worker Num:  2 \n",
            " Loss:  0.37795379757881165\n",
            "Training:  Iteration No:  117 Worker Num:  3 \n",
            " Loss:  0.3001807928085327\n",
            "Training:  Iteration No:  117 Worker Num:  4 \n",
            " Loss:  0.4472692012786865\n",
            "Training:  Iteration No:  117 Worker Num:  5 \n",
            " Loss:  0.34860166907310486\n",
            "Training:  Iteration No:  117 Worker Num:  6 \n",
            " Loss:  0.49129122495651245\n",
            "Training:  Iteration No:  117 Worker Num:  7 \n",
            " Loss:  0.4323314130306244\n",
            "Test:  Iteration No:  117 \n",
            " Loss:  0.35199558951809434\n",
            "Test accuracy:  89.84\n",
            "Training:  Iteration No:  118 Worker Num:  0 \n",
            " Loss:  0.558600902557373\n",
            "Training:  Iteration No:  118 Worker Num:  1 \n",
            " Loss:  0.3410942852497101\n",
            "Training:  Iteration No:  118 Worker Num:  2 \n",
            " Loss:  0.44796842336654663\n",
            "Training:  Iteration No:  118 Worker Num:  3 \n",
            " Loss:  0.38699889183044434\n",
            "Training:  Iteration No:  118 Worker Num:  4 \n",
            " Loss:  0.34974855184555054\n",
            "Training:  Iteration No:  118 Worker Num:  5 \n",
            " Loss:  0.3530030846595764\n",
            "Training:  Iteration No:  118 Worker Num:  6 \n",
            " Loss:  0.3620178997516632\n",
            "Training:  Iteration No:  118 Worker Num:  7 \n",
            " Loss:  0.42640191316604614\n",
            "Test:  Iteration No:  118 \n",
            " Loss:  0.3466224150284182\n",
            "Test accuracy:  90.35\n",
            "Training:  Iteration No:  119 Worker Num:  0 \n",
            " Loss:  0.410165935754776\n",
            "Training:  Iteration No:  119 Worker Num:  1 \n",
            " Loss:  0.3838905990123749\n",
            "Training:  Iteration No:  119 Worker Num:  2 \n",
            " Loss:  0.30759483575820923\n",
            "Training:  Iteration No:  119 Worker Num:  3 \n",
            " Loss:  0.4962528645992279\n",
            "Training:  Iteration No:  119 Worker Num:  4 \n",
            " Loss:  0.34097617864608765\n",
            "Training:  Iteration No:  119 Worker Num:  5 \n",
            " Loss:  0.36910536885261536\n",
            "Training:  Iteration No:  119 Worker Num:  6 \n",
            " Loss:  0.353666752576828\n",
            "Training:  Iteration No:  119 Worker Num:  7 \n",
            " Loss:  0.5095986127853394\n",
            "Test:  Iteration No:  119 \n",
            " Loss:  0.3461924888476541\n",
            "Test accuracy:  90.11\n",
            "Training:  Iteration No:  120 Worker Num:  0 \n",
            " Loss:  0.5800924301147461\n",
            "Training:  Iteration No:  120 Worker Num:  1 \n",
            " Loss:  0.4857887327671051\n",
            "Training:  Iteration No:  120 Worker Num:  2 \n",
            " Loss:  0.4905531108379364\n",
            "Training:  Iteration No:  120 Worker Num:  3 \n",
            " Loss:  0.307744562625885\n",
            "Training:  Iteration No:  120 Worker Num:  4 \n",
            " Loss:  0.4275010824203491\n",
            "Training:  Iteration No:  120 Worker Num:  5 \n",
            " Loss:  0.6327455043792725\n",
            "Training:  Iteration No:  120 Worker Num:  6 \n",
            " Loss:  0.37823688983917236\n",
            "Training:  Iteration No:  120 Worker Num:  7 \n",
            " Loss:  0.3787408471107483\n",
            "Test:  Iteration No:  120 \n",
            " Loss:  0.34235016525357587\n",
            "Test accuracy:  90.17\n",
            "Training:  Iteration No:  121 Worker Num:  0 \n",
            " Loss:  0.3333560526371002\n",
            "Training:  Iteration No:  121 Worker Num:  1 \n",
            " Loss:  0.3685574531555176\n",
            "Training:  Iteration No:  121 Worker Num:  2 \n",
            " Loss:  0.26325637102127075\n",
            "Training:  Iteration No:  121 Worker Num:  3 \n",
            " Loss:  0.2778194844722748\n",
            "Training:  Iteration No:  121 Worker Num:  4 \n",
            " Loss:  0.4037148952484131\n",
            "Training:  Iteration No:  121 Worker Num:  5 \n",
            " Loss:  0.38718101382255554\n",
            "Training:  Iteration No:  121 Worker Num:  6 \n",
            " Loss:  0.48875585198402405\n",
            "Training:  Iteration No:  121 Worker Num:  7 \n",
            " Loss:  0.3608996570110321\n",
            "Test:  Iteration No:  121 \n",
            " Loss:  0.3502282380963428\n",
            "Test accuracy:  89.8\n",
            "Training:  Iteration No:  122 Worker Num:  0 \n",
            " Loss:  0.4502488970756531\n",
            "Training:  Iteration No:  122 Worker Num:  1 \n",
            " Loss:  0.38626253604888916\n",
            "Training:  Iteration No:  122 Worker Num:  2 \n",
            " Loss:  0.3970697820186615\n",
            "Training:  Iteration No:  122 Worker Num:  3 \n",
            " Loss:  0.4719245731830597\n",
            "Training:  Iteration No:  122 Worker Num:  4 \n",
            " Loss:  0.4756132960319519\n",
            "Training:  Iteration No:  122 Worker Num:  5 \n",
            " Loss:  0.3953326642513275\n",
            "Training:  Iteration No:  122 Worker Num:  6 \n",
            " Loss:  0.29110971093177795\n",
            "Training:  Iteration No:  122 Worker Num:  7 \n",
            " Loss:  0.6304870843887329\n",
            "Test:  Iteration No:  122 \n",
            " Loss:  0.34103254290132584\n",
            "Test accuracy:  90.35\n",
            "Training:  Iteration No:  123 Worker Num:  0 \n",
            " Loss:  0.46642112731933594\n",
            "Training:  Iteration No:  123 Worker Num:  1 \n",
            " Loss:  0.35513293743133545\n",
            "Training:  Iteration No:  123 Worker Num:  2 \n",
            " Loss:  0.3308599591255188\n",
            "Training:  Iteration No:  123 Worker Num:  3 \n",
            " Loss:  0.5257737040519714\n",
            "Training:  Iteration No:  123 Worker Num:  4 \n",
            " Loss:  0.3761647045612335\n",
            "Training:  Iteration No:  123 Worker Num:  5 \n",
            " Loss:  0.35764092206954956\n",
            "Training:  Iteration No:  123 Worker Num:  6 \n",
            " Loss:  0.4776223301887512\n",
            "Training:  Iteration No:  123 Worker Num:  7 \n",
            " Loss:  0.5139380097389221\n",
            "Test:  Iteration No:  123 \n",
            " Loss:  0.34055448483817186\n",
            "Test accuracy:  90.35\n",
            "Training:  Iteration No:  124 Worker Num:  0 \n",
            " Loss:  0.517881453037262\n",
            "Training:  Iteration No:  124 Worker Num:  1 \n",
            " Loss:  0.3459545075893402\n",
            "Training:  Iteration No:  124 Worker Num:  2 \n",
            " Loss:  0.38905996084213257\n",
            "Training:  Iteration No:  124 Worker Num:  3 \n",
            " Loss:  0.3927158713340759\n",
            "Training:  Iteration No:  124 Worker Num:  4 \n",
            " Loss:  0.40596166253089905\n",
            "Training:  Iteration No:  124 Worker Num:  5 \n",
            " Loss:  0.4230159521102905\n",
            "Training:  Iteration No:  124 Worker Num:  6 \n",
            " Loss:  0.42553043365478516\n",
            "Training:  Iteration No:  124 Worker Num:  7 \n",
            " Loss:  0.3245615065097809\n",
            "Test:  Iteration No:  124 \n",
            " Loss:  0.34533706213100046\n",
            "Test accuracy:  90.21\n",
            "Training:  Iteration No:  125 Worker Num:  0 \n",
            " Loss:  0.4794692397117615\n",
            "Training:  Iteration No:  125 Worker Num:  1 \n",
            " Loss:  0.38674384355545044\n",
            "Training:  Iteration No:  125 Worker Num:  2 \n",
            " Loss:  0.3846146762371063\n",
            "Training:  Iteration No:  125 Worker Num:  3 \n",
            " Loss:  0.3402639329433441\n",
            "Training:  Iteration No:  125 Worker Num:  4 \n",
            " Loss:  0.3617744445800781\n",
            "Training:  Iteration No:  125 Worker Num:  5 \n",
            " Loss:  0.4006407558917999\n",
            "Training:  Iteration No:  125 Worker Num:  6 \n",
            " Loss:  0.4006764590740204\n",
            "Training:  Iteration No:  125 Worker Num:  7 \n",
            " Loss:  0.5649310946464539\n",
            "Test:  Iteration No:  125 \n",
            " Loss:  0.3344923447393164\n",
            "Test accuracy:  90.66\n",
            "Training:  Iteration No:  126 Worker Num:  0 \n",
            " Loss:  0.44802120327949524\n",
            "Training:  Iteration No:  126 Worker Num:  1 \n",
            " Loss:  0.38765567541122437\n",
            "Training:  Iteration No:  126 Worker Num:  2 \n",
            " Loss:  0.443462073802948\n",
            "Training:  Iteration No:  126 Worker Num:  3 \n",
            " Loss:  0.3960134983062744\n",
            "Training:  Iteration No:  126 Worker Num:  4 \n",
            " Loss:  0.44723963737487793\n",
            "Training:  Iteration No:  126 Worker Num:  5 \n",
            " Loss:  0.3582706153392792\n",
            "Training:  Iteration No:  126 Worker Num:  6 \n",
            " Loss:  0.5161644220352173\n",
            "Training:  Iteration No:  126 Worker Num:  7 \n",
            " Loss:  0.41141214966773987\n",
            "Test:  Iteration No:  126 \n",
            " Loss:  0.339098474694581\n",
            "Test accuracy:  90.48\n",
            "Training:  Iteration No:  127 Worker Num:  0 \n",
            " Loss:  0.26531746983528137\n",
            "Training:  Iteration No:  127 Worker Num:  1 \n",
            " Loss:  0.48766303062438965\n",
            "Training:  Iteration No:  127 Worker Num:  2 \n",
            " Loss:  0.2412891536951065\n",
            "Training:  Iteration No:  127 Worker Num:  3 \n",
            " Loss:  0.4881088137626648\n",
            "Training:  Iteration No:  127 Worker Num:  4 \n",
            " Loss:  0.3172691762447357\n",
            "Training:  Iteration No:  127 Worker Num:  5 \n",
            " Loss:  0.3604090213775635\n",
            "Training:  Iteration No:  127 Worker Num:  6 \n",
            " Loss:  0.3328021466732025\n",
            "Training:  Iteration No:  127 Worker Num:  7 \n",
            " Loss:  0.4640088677406311\n",
            "Test:  Iteration No:  127 \n",
            " Loss:  0.3296424191963824\n",
            "Test accuracy:  90.54\n",
            "Training:  Iteration No:  128 Worker Num:  0 \n",
            " Loss:  0.3828207850456238\n",
            "Training:  Iteration No:  128 Worker Num:  1 \n",
            " Loss:  0.43762677907943726\n",
            "Training:  Iteration No:  128 Worker Num:  2 \n",
            " Loss:  0.415858656167984\n",
            "Training:  Iteration No:  128 Worker Num:  3 \n",
            " Loss:  0.4060008227825165\n",
            "Training:  Iteration No:  128 Worker Num:  4 \n",
            " Loss:  0.3798540532588959\n",
            "Training:  Iteration No:  128 Worker Num:  5 \n",
            " Loss:  0.3472859263420105\n",
            "Training:  Iteration No:  128 Worker Num:  6 \n",
            " Loss:  0.3935561776161194\n",
            "Training:  Iteration No:  128 Worker Num:  7 \n",
            " Loss:  0.35363101959228516\n",
            "Test:  Iteration No:  128 \n",
            " Loss:  0.32914270828419095\n",
            "Test accuracy:  90.59\n",
            "Training:  Iteration No:  129 Worker Num:  0 \n",
            " Loss:  0.3500549793243408\n",
            "Training:  Iteration No:  129 Worker Num:  1 \n",
            " Loss:  0.599986732006073\n",
            "Training:  Iteration No:  129 Worker Num:  2 \n",
            " Loss:  0.4734342694282532\n",
            "Training:  Iteration No:  129 Worker Num:  3 \n",
            " Loss:  0.3574658930301666\n",
            "Training:  Iteration No:  129 Worker Num:  4 \n",
            " Loss:  0.38040587306022644\n",
            "Training:  Iteration No:  129 Worker Num:  5 \n",
            " Loss:  0.3087258040904999\n",
            "Training:  Iteration No:  129 Worker Num:  6 \n",
            " Loss:  0.5043869614601135\n",
            "Training:  Iteration No:  129 Worker Num:  7 \n",
            " Loss:  0.41569074988365173\n",
            "Test:  Iteration No:  129 \n",
            " Loss:  0.3287360242839101\n",
            "Test accuracy:  90.67\n",
            "Training:  Iteration No:  130 Worker Num:  0 \n",
            " Loss:  0.5832741856575012\n",
            "Training:  Iteration No:  130 Worker Num:  1 \n",
            " Loss:  0.5027698278427124\n",
            "Training:  Iteration No:  130 Worker Num:  2 \n",
            " Loss:  0.3591761887073517\n",
            "Training:  Iteration No:  130 Worker Num:  3 \n",
            " Loss:  0.3922662138938904\n",
            "Training:  Iteration No:  130 Worker Num:  4 \n",
            " Loss:  0.5112413763999939\n",
            "Training:  Iteration No:  130 Worker Num:  5 \n",
            " Loss:  0.3854970932006836\n",
            "Training:  Iteration No:  130 Worker Num:  6 \n",
            " Loss:  0.45552071928977966\n",
            "Training:  Iteration No:  130 Worker Num:  7 \n",
            " Loss:  0.39601683616638184\n",
            "Test:  Iteration No:  130 \n",
            " Loss:  0.3295737205992771\n",
            "Test accuracy:  90.74\n",
            "Training:  Iteration No:  131 Worker Num:  0 \n",
            " Loss:  0.34755393862724304\n",
            "Training:  Iteration No:  131 Worker Num:  1 \n",
            " Loss:  0.4512275457382202\n",
            "Training:  Iteration No:  131 Worker Num:  2 \n",
            " Loss:  0.3239222764968872\n",
            "Training:  Iteration No:  131 Worker Num:  3 \n",
            " Loss:  0.2951945960521698\n",
            "Training:  Iteration No:  131 Worker Num:  4 \n",
            " Loss:  0.32014700770378113\n",
            "Training:  Iteration No:  131 Worker Num:  5 \n",
            " Loss:  0.3600127100944519\n",
            "Training:  Iteration No:  131 Worker Num:  6 \n",
            " Loss:  0.42714858055114746\n",
            "Training:  Iteration No:  131 Worker Num:  7 \n",
            " Loss:  0.4137920141220093\n",
            "Test:  Iteration No:  131 \n",
            " Loss:  0.3289645689103422\n",
            "Test accuracy:  90.56\n",
            "Training:  Iteration No:  132 Worker Num:  0 \n",
            " Loss:  0.2504183351993561\n",
            "Training:  Iteration No:  132 Worker Num:  1 \n",
            " Loss:  0.3978576064109802\n",
            "Training:  Iteration No:  132 Worker Num:  2 \n",
            " Loss:  0.2998288571834564\n",
            "Training:  Iteration No:  132 Worker Num:  3 \n",
            " Loss:  0.4042649567127228\n",
            "Training:  Iteration No:  132 Worker Num:  4 \n",
            " Loss:  0.26827868819236755\n",
            "Training:  Iteration No:  132 Worker Num:  5 \n",
            " Loss:  0.44889432191848755\n",
            "Training:  Iteration No:  132 Worker Num:  6 \n",
            " Loss:  0.44299739599227905\n",
            "Training:  Iteration No:  132 Worker Num:  7 \n",
            " Loss:  0.3936612904071808\n",
            "Test:  Iteration No:  132 \n",
            " Loss:  0.3254265092501912\n",
            "Test accuracy:  90.43\n",
            "Training:  Iteration No:  133 Worker Num:  0 \n",
            " Loss:  0.48250165581703186\n",
            "Training:  Iteration No:  133 Worker Num:  1 \n",
            " Loss:  0.3625245690345764\n",
            "Training:  Iteration No:  133 Worker Num:  2 \n",
            " Loss:  0.3584330976009369\n",
            "Training:  Iteration No:  133 Worker Num:  3 \n",
            " Loss:  0.39560821652412415\n",
            "Training:  Iteration No:  133 Worker Num:  4 \n",
            " Loss:  0.4421216547489166\n",
            "Training:  Iteration No:  133 Worker Num:  5 \n",
            " Loss:  0.21793612837791443\n",
            "Training:  Iteration No:  133 Worker Num:  6 \n",
            " Loss:  0.3499299883842468\n",
            "Training:  Iteration No:  133 Worker Num:  7 \n",
            " Loss:  0.4593352675437927\n",
            "Test:  Iteration No:  133 \n",
            " Loss:  0.32168846497241455\n",
            "Test accuracy:  90.88\n",
            "Training:  Iteration No:  134 Worker Num:  0 \n",
            " Loss:  0.48384585976600647\n",
            "Training:  Iteration No:  134 Worker Num:  1 \n",
            " Loss:  0.41054651141166687\n",
            "Training:  Iteration No:  134 Worker Num:  2 \n",
            " Loss:  0.33550018072128296\n",
            "Training:  Iteration No:  134 Worker Num:  3 \n",
            " Loss:  0.3981125056743622\n",
            "Training:  Iteration No:  134 Worker Num:  4 \n",
            " Loss:  0.31983697414398193\n",
            "Training:  Iteration No:  134 Worker Num:  5 \n",
            " Loss:  0.3514278531074524\n",
            "Training:  Iteration No:  134 Worker Num:  6 \n",
            " Loss:  0.3642844557762146\n",
            "Training:  Iteration No:  134 Worker Num:  7 \n",
            " Loss:  0.4266090989112854\n",
            "Test:  Iteration No:  134 \n",
            " Loss:  0.3214192565389072\n",
            "Test accuracy:  90.56\n",
            "Training:  Iteration No:  135 Worker Num:  0 \n",
            " Loss:  0.40169277787208557\n",
            "Training:  Iteration No:  135 Worker Num:  1 \n",
            " Loss:  0.36965689063072205\n",
            "Training:  Iteration No:  135 Worker Num:  2 \n",
            " Loss:  0.3432024419307709\n",
            "Training:  Iteration No:  135 Worker Num:  3 \n",
            " Loss:  0.34832555055618286\n",
            "Training:  Iteration No:  135 Worker Num:  4 \n",
            " Loss:  0.25775402784347534\n",
            "Training:  Iteration No:  135 Worker Num:  5 \n",
            " Loss:  0.3946182131767273\n",
            "Training:  Iteration No:  135 Worker Num:  6 \n",
            " Loss:  0.3533124029636383\n",
            "Training:  Iteration No:  135 Worker Num:  7 \n",
            " Loss:  0.4242591857910156\n",
            "Test:  Iteration No:  135 \n",
            " Loss:  0.3184864819992947\n",
            "Test accuracy:  90.78\n",
            "Training:  Iteration No:  136 Worker Num:  0 \n",
            " Loss:  0.3356485962867737\n",
            "Training:  Iteration No:  136 Worker Num:  1 \n",
            " Loss:  0.37605154514312744\n",
            "Training:  Iteration No:  136 Worker Num:  2 \n",
            " Loss:  0.41778838634490967\n",
            "Training:  Iteration No:  136 Worker Num:  3 \n",
            " Loss:  0.49320662021636963\n",
            "Training:  Iteration No:  136 Worker Num:  4 \n",
            " Loss:  0.45609718561172485\n",
            "Training:  Iteration No:  136 Worker Num:  5 \n",
            " Loss:  0.405397891998291\n",
            "Training:  Iteration No:  136 Worker Num:  6 \n",
            " Loss:  0.4473935067653656\n",
            "Training:  Iteration No:  136 Worker Num:  7 \n",
            " Loss:  0.5307329893112183\n",
            "Test:  Iteration No:  136 \n",
            " Loss:  0.3164003213183789\n",
            "Test accuracy:  90.93\n",
            "Training:  Iteration No:  137 Worker Num:  0 \n",
            " Loss:  0.4824909567832947\n",
            "Training:  Iteration No:  137 Worker Num:  1 \n",
            " Loss:  0.43691322207450867\n",
            "Training:  Iteration No:  137 Worker Num:  2 \n",
            " Loss:  0.2614712119102478\n",
            "Training:  Iteration No:  137 Worker Num:  3 \n",
            " Loss:  0.3525691330432892\n",
            "Training:  Iteration No:  137 Worker Num:  4 \n",
            " Loss:  0.38359612226486206\n",
            "Training:  Iteration No:  137 Worker Num:  5 \n",
            " Loss:  0.41311678290367126\n",
            "Training:  Iteration No:  137 Worker Num:  6 \n",
            " Loss:  0.3935169577598572\n",
            "Training:  Iteration No:  137 Worker Num:  7 \n",
            " Loss:  0.3397446274757385\n",
            "Test:  Iteration No:  137 \n",
            " Loss:  0.31780228810974315\n",
            "Test accuracy:  91.02\n",
            "Training:  Iteration No:  138 Worker Num:  0 \n",
            " Loss:  0.2722546458244324\n",
            "Training:  Iteration No:  138 Worker Num:  1 \n",
            " Loss:  0.37112051248550415\n",
            "Training:  Iteration No:  138 Worker Num:  2 \n",
            " Loss:  0.49959978461265564\n",
            "Training:  Iteration No:  138 Worker Num:  3 \n",
            " Loss:  0.4344242811203003\n",
            "Training:  Iteration No:  138 Worker Num:  4 \n",
            " Loss:  0.4492676556110382\n",
            "Training:  Iteration No:  138 Worker Num:  5 \n",
            " Loss:  0.4696432650089264\n",
            "Training:  Iteration No:  138 Worker Num:  6 \n",
            " Loss:  0.3899305462837219\n",
            "Training:  Iteration No:  138 Worker Num:  7 \n",
            " Loss:  0.4499376118183136\n",
            "Test:  Iteration No:  138 \n",
            " Loss:  0.31263158286485493\n",
            "Test accuracy:  91.08\n",
            "Training:  Iteration No:  139 Worker Num:  0 \n",
            " Loss:  0.4125576317310333\n",
            "Training:  Iteration No:  139 Worker Num:  1 \n",
            " Loss:  0.362326443195343\n",
            "Training:  Iteration No:  139 Worker Num:  2 \n",
            " Loss:  0.3221011459827423\n",
            "Training:  Iteration No:  139 Worker Num:  3 \n",
            " Loss:  0.35377103090286255\n",
            "Training:  Iteration No:  139 Worker Num:  4 \n",
            " Loss:  0.517005443572998\n",
            "Training:  Iteration No:  139 Worker Num:  5 \n",
            " Loss:  0.3646484613418579\n",
            "Training:  Iteration No:  139 Worker Num:  6 \n",
            " Loss:  0.375054270029068\n",
            "Training:  Iteration No:  139 Worker Num:  7 \n",
            " Loss:  0.4345507025718689\n",
            "Test:  Iteration No:  139 \n",
            " Loss:  0.3124486330566527\n",
            "Test accuracy:  91.16\n",
            "Training:  Iteration No:  140 Worker Num:  0 \n",
            " Loss:  0.3873414099216461\n",
            "Training:  Iteration No:  140 Worker Num:  1 \n",
            " Loss:  0.40607452392578125\n",
            "Training:  Iteration No:  140 Worker Num:  2 \n",
            " Loss:  0.514954686164856\n",
            "Training:  Iteration No:  140 Worker Num:  3 \n",
            " Loss:  0.4697239398956299\n",
            "Training:  Iteration No:  140 Worker Num:  4 \n",
            " Loss:  0.3769538998603821\n",
            "Training:  Iteration No:  140 Worker Num:  5 \n",
            " Loss:  0.32415661215782166\n",
            "Training:  Iteration No:  140 Worker Num:  6 \n",
            " Loss:  0.3638468086719513\n",
            "Training:  Iteration No:  140 Worker Num:  7 \n",
            " Loss:  0.42483556270599365\n",
            "Test:  Iteration No:  140 \n",
            " Loss:  0.31450836770708046\n",
            "Test accuracy:  91.07\n",
            "Training:  Iteration No:  141 Worker Num:  0 \n",
            " Loss:  0.39726340770721436\n",
            "Training:  Iteration No:  141 Worker Num:  1 \n",
            " Loss:  0.34540441632270813\n",
            "Training:  Iteration No:  141 Worker Num:  2 \n",
            " Loss:  0.42181459069252014\n",
            "Training:  Iteration No:  141 Worker Num:  3 \n",
            " Loss:  0.3896785378456116\n",
            "Training:  Iteration No:  141 Worker Num:  4 \n",
            " Loss:  0.5055411458015442\n",
            "Training:  Iteration No:  141 Worker Num:  5 \n",
            " Loss:  0.36704859137535095\n",
            "Training:  Iteration No:  141 Worker Num:  6 \n",
            " Loss:  0.4437524676322937\n",
            "Training:  Iteration No:  141 Worker Num:  7 \n",
            " Loss:  0.3936714828014374\n",
            "Test:  Iteration No:  141 \n",
            " Loss:  0.30993426284646686\n",
            "Test accuracy:  91.14\n",
            "Training:  Iteration No:  142 Worker Num:  0 \n",
            " Loss:  0.362560510635376\n",
            "Training:  Iteration No:  142 Worker Num:  1 \n",
            " Loss:  0.37806275486946106\n",
            "Training:  Iteration No:  142 Worker Num:  2 \n",
            " Loss:  0.616651177406311\n",
            "Training:  Iteration No:  142 Worker Num:  3 \n",
            " Loss:  0.396650105714798\n",
            "Training:  Iteration No:  142 Worker Num:  4 \n",
            " Loss:  0.49941501021385193\n",
            "Training:  Iteration No:  142 Worker Num:  5 \n",
            " Loss:  0.2207077294588089\n",
            "Training:  Iteration No:  142 Worker Num:  6 \n",
            " Loss:  0.41346707940101624\n",
            "Training:  Iteration No:  142 Worker Num:  7 \n",
            " Loss:  0.44278135895729065\n",
            "Test:  Iteration No:  142 \n",
            " Loss:  0.3123301704284511\n",
            "Test accuracy:  91.03\n",
            "Training:  Iteration No:  143 Worker Num:  0 \n",
            " Loss:  0.46741464734077454\n",
            "Training:  Iteration No:  143 Worker Num:  1 \n",
            " Loss:  0.39530929923057556\n",
            "Training:  Iteration No:  143 Worker Num:  2 \n",
            " Loss:  0.4335164427757263\n",
            "Training:  Iteration No:  143 Worker Num:  3 \n",
            " Loss:  0.4387068450450897\n",
            "Training:  Iteration No:  143 Worker Num:  4 \n",
            " Loss:  0.391597181558609\n",
            "Training:  Iteration No:  143 Worker Num:  5 \n",
            " Loss:  0.39438337087631226\n",
            "Training:  Iteration No:  143 Worker Num:  6 \n",
            " Loss:  0.317346453666687\n",
            "Training:  Iteration No:  143 Worker Num:  7 \n",
            " Loss:  0.41118964552879333\n",
            "Test:  Iteration No:  143 \n",
            " Loss:  0.31449471961093856\n",
            "Test accuracy:  91.09\n",
            "Training:  Iteration No:  144 Worker Num:  0 \n",
            " Loss:  0.44001057744026184\n",
            "Training:  Iteration No:  144 Worker Num:  1 \n",
            " Loss:  0.4124438166618347\n",
            "Training:  Iteration No:  144 Worker Num:  2 \n",
            " Loss:  0.37330448627471924\n",
            "Training:  Iteration No:  144 Worker Num:  3 \n",
            " Loss:  0.36437833309173584\n",
            "Training:  Iteration No:  144 Worker Num:  4 \n",
            " Loss:  0.3045123219490051\n",
            "Training:  Iteration No:  144 Worker Num:  5 \n",
            " Loss:  0.35858532786369324\n",
            "Training:  Iteration No:  144 Worker Num:  6 \n",
            " Loss:  0.3888801634311676\n",
            "Training:  Iteration No:  144 Worker Num:  7 \n",
            " Loss:  0.4819566607475281\n",
            "Test:  Iteration No:  144 \n",
            " Loss:  0.3139840140938759\n",
            "Test accuracy:  90.99\n",
            "Training:  Iteration No:  145 Worker Num:  0 \n",
            " Loss:  0.2865979075431824\n",
            "Training:  Iteration No:  145 Worker Num:  1 \n",
            " Loss:  0.37911951541900635\n",
            "Training:  Iteration No:  145 Worker Num:  2 \n",
            " Loss:  0.6390312314033508\n",
            "Training:  Iteration No:  145 Worker Num:  3 \n",
            " Loss:  0.3786858022212982\n",
            "Training:  Iteration No:  145 Worker Num:  4 \n",
            " Loss:  0.4694925844669342\n",
            "Training:  Iteration No:  145 Worker Num:  5 \n",
            " Loss:  0.43300706148147583\n",
            "Training:  Iteration No:  145 Worker Num:  6 \n",
            " Loss:  0.34888458251953125\n",
            "Training:  Iteration No:  145 Worker Num:  7 \n",
            " Loss:  0.3659806251525879\n",
            "Test:  Iteration No:  145 \n",
            " Loss:  0.3071486698581448\n",
            "Test accuracy:  91.21\n",
            "Training:  Iteration No:  146 Worker Num:  0 \n",
            " Loss:  0.36555755138397217\n",
            "Training:  Iteration No:  146 Worker Num:  1 \n",
            " Loss:  0.39098814129829407\n",
            "Training:  Iteration No:  146 Worker Num:  2 \n",
            " Loss:  0.29828473925590515\n",
            "Training:  Iteration No:  146 Worker Num:  3 \n",
            " Loss:  0.50794917345047\n",
            "Training:  Iteration No:  146 Worker Num:  4 \n",
            " Loss:  0.3533823490142822\n",
            "Training:  Iteration No:  146 Worker Num:  5 \n",
            " Loss:  0.36381879448890686\n",
            "Training:  Iteration No:  146 Worker Num:  6 \n",
            " Loss:  0.3826845586299896\n",
            "Training:  Iteration No:  146 Worker Num:  7 \n",
            " Loss:  0.4354695975780487\n",
            "Test:  Iteration No:  146 \n",
            " Loss:  0.3050435590027254\n",
            "Test accuracy:  91.24\n",
            "Training:  Iteration No:  147 Worker Num:  0 \n",
            " Loss:  0.285738468170166\n",
            "Training:  Iteration No:  147 Worker Num:  1 \n",
            " Loss:  0.32198917865753174\n",
            "Training:  Iteration No:  147 Worker Num:  2 \n",
            " Loss:  0.4281328022480011\n",
            "Training:  Iteration No:  147 Worker Num:  3 \n",
            " Loss:  0.42497262358665466\n",
            "Training:  Iteration No:  147 Worker Num:  4 \n",
            " Loss:  0.27393388748168945\n",
            "Training:  Iteration No:  147 Worker Num:  5 \n",
            " Loss:  0.29147017002105713\n",
            "Training:  Iteration No:  147 Worker Num:  6 \n",
            " Loss:  0.32217127084732056\n",
            "Training:  Iteration No:  147 Worker Num:  7 \n",
            " Loss:  0.22654762864112854\n",
            "Test:  Iteration No:  147 \n",
            " Loss:  0.30714675831266597\n",
            "Test accuracy:  91.23\n",
            "Training:  Iteration No:  148 Worker Num:  0 \n",
            " Loss:  0.4371170997619629\n",
            "Training:  Iteration No:  148 Worker Num:  1 \n",
            " Loss:  0.27372121810913086\n",
            "Training:  Iteration No:  148 Worker Num:  2 \n",
            " Loss:  0.2814052402973175\n",
            "Training:  Iteration No:  148 Worker Num:  3 \n",
            " Loss:  0.32864707708358765\n",
            "Training:  Iteration No:  148 Worker Num:  4 \n",
            " Loss:  0.24311089515686035\n",
            "Training:  Iteration No:  148 Worker Num:  5 \n",
            " Loss:  0.36482471227645874\n",
            "Training:  Iteration No:  148 Worker Num:  6 \n",
            " Loss:  0.4114450514316559\n",
            "Training:  Iteration No:  148 Worker Num:  7 \n",
            " Loss:  0.3608076274394989\n",
            "Test:  Iteration No:  148 \n",
            " Loss:  0.3033317727378652\n",
            "Test accuracy:  91.19\n",
            "Training:  Iteration No:  149 Worker Num:  0 \n",
            " Loss:  0.39060914516448975\n",
            "Training:  Iteration No:  149 Worker Num:  1 \n",
            " Loss:  0.2769486606121063\n",
            "Training:  Iteration No:  149 Worker Num:  2 \n",
            " Loss:  0.4129219353199005\n",
            "Training:  Iteration No:  149 Worker Num:  3 \n",
            " Loss:  0.37549594044685364\n",
            "Training:  Iteration No:  149 Worker Num:  4 \n",
            " Loss:  0.35317373275756836\n",
            "Training:  Iteration No:  149 Worker Num:  5 \n",
            " Loss:  0.34085986018180847\n",
            "Training:  Iteration No:  149 Worker Num:  6 \n",
            " Loss:  0.40439605712890625\n",
            "Training:  Iteration No:  149 Worker Num:  7 \n",
            " Loss:  0.49575069546699524\n",
            "Test:  Iteration No:  149 \n",
            " Loss:  0.30471042949187604\n",
            "Test accuracy:  91.34\n",
            "Training:  Iteration No:  150 Worker Num:  0 \n",
            " Loss:  0.3482387065887451\n",
            "Training:  Iteration No:  150 Worker Num:  1 \n",
            " Loss:  0.42509034276008606\n",
            "Training:  Iteration No:  150 Worker Num:  2 \n",
            " Loss:  0.3030444383621216\n",
            "Training:  Iteration No:  150 Worker Num:  3 \n",
            " Loss:  0.4164544641971588\n",
            "Training:  Iteration No:  150 Worker Num:  4 \n",
            " Loss:  0.2886408865451813\n",
            "Training:  Iteration No:  150 Worker Num:  5 \n",
            " Loss:  0.40541839599609375\n",
            "Training:  Iteration No:  150 Worker Num:  6 \n",
            " Loss:  0.3340146541595459\n",
            "Training:  Iteration No:  150 Worker Num:  7 \n",
            " Loss:  0.4504621624946594\n",
            "Test:  Iteration No:  150 \n",
            " Loss:  0.29727212448097484\n",
            "Test accuracy:  91.45\n",
            "Training:  Iteration No:  151 Worker Num:  0 \n",
            " Loss:  0.5233864784240723\n",
            "Training:  Iteration No:  151 Worker Num:  1 \n",
            " Loss:  0.550194501876831\n",
            "Training:  Iteration No:  151 Worker Num:  2 \n",
            " Loss:  0.45070356130599976\n",
            "Training:  Iteration No:  151 Worker Num:  3 \n",
            " Loss:  0.381564199924469\n",
            "Training:  Iteration No:  151 Worker Num:  4 \n",
            " Loss:  0.42419859766960144\n",
            "Training:  Iteration No:  151 Worker Num:  5 \n",
            " Loss:  0.3010694086551666\n",
            "Training:  Iteration No:  151 Worker Num:  6 \n",
            " Loss:  0.5528777837753296\n",
            "Training:  Iteration No:  151 Worker Num:  7 \n",
            " Loss:  0.3440951704978943\n",
            "Test:  Iteration No:  151 \n",
            " Loss:  0.2995671342728259\n",
            "Test accuracy:  91.54\n",
            "Training:  Iteration No:  152 Worker Num:  0 \n",
            " Loss:  0.3435244858264923\n",
            "Training:  Iteration No:  152 Worker Num:  1 \n",
            " Loss:  0.3247814178466797\n",
            "Training:  Iteration No:  152 Worker Num:  2 \n",
            " Loss:  0.46248292922973633\n",
            "Training:  Iteration No:  152 Worker Num:  3 \n",
            " Loss:  0.3866281509399414\n",
            "Training:  Iteration No:  152 Worker Num:  4 \n",
            " Loss:  0.25327691435813904\n",
            "Training:  Iteration No:  152 Worker Num:  5 \n",
            " Loss:  0.39940303564071655\n",
            "Training:  Iteration No:  152 Worker Num:  6 \n",
            " Loss:  0.30154311656951904\n",
            "Training:  Iteration No:  152 Worker Num:  7 \n",
            " Loss:  0.328624963760376\n",
            "Test:  Iteration No:  152 \n",
            " Loss:  0.2944515477158601\n",
            "Test accuracy:  91.69\n",
            "Training:  Iteration No:  153 Worker Num:  0 \n",
            " Loss:  0.2768937051296234\n",
            "Training:  Iteration No:  153 Worker Num:  1 \n",
            " Loss:  0.26055073738098145\n",
            "Training:  Iteration No:  153 Worker Num:  2 \n",
            " Loss:  0.5109856128692627\n",
            "Training:  Iteration No:  153 Worker Num:  3 \n",
            " Loss:  0.38369086384773254\n",
            "Training:  Iteration No:  153 Worker Num:  4 \n",
            " Loss:  0.3889425992965698\n",
            "Training:  Iteration No:  153 Worker Num:  5 \n",
            " Loss:  0.5077409148216248\n",
            "Training:  Iteration No:  153 Worker Num:  6 \n",
            " Loss:  0.4226457178592682\n",
            "Training:  Iteration No:  153 Worker Num:  7 \n",
            " Loss:  0.35272181034088135\n",
            "Test:  Iteration No:  153 \n",
            " Loss:  0.30002887462136113\n",
            "Test accuracy:  91.22\n",
            "Training:  Iteration No:  154 Worker Num:  0 \n",
            " Loss:  0.3712829351425171\n",
            "Training:  Iteration No:  154 Worker Num:  1 \n",
            " Loss:  0.4057740867137909\n",
            "Training:  Iteration No:  154 Worker Num:  2 \n",
            " Loss:  0.33170413970947266\n",
            "Training:  Iteration No:  154 Worker Num:  3 \n",
            " Loss:  0.3437545895576477\n",
            "Training:  Iteration No:  154 Worker Num:  4 \n",
            " Loss:  0.3309415578842163\n",
            "Training:  Iteration No:  154 Worker Num:  5 \n",
            " Loss:  0.33793285489082336\n",
            "Training:  Iteration No:  154 Worker Num:  6 \n",
            " Loss:  0.4319945275783539\n",
            "Training:  Iteration No:  154 Worker Num:  7 \n",
            " Loss:  0.38217490911483765\n",
            "Test:  Iteration No:  154 \n",
            " Loss:  0.29283955386733707\n",
            "Test accuracy:  91.57\n",
            "Training:  Iteration No:  155 Worker Num:  0 \n",
            " Loss:  0.3101421594619751\n",
            "Training:  Iteration No:  155 Worker Num:  1 \n",
            " Loss:  0.25980162620544434\n",
            "Training:  Iteration No:  155 Worker Num:  2 \n",
            " Loss:  0.35633933544158936\n",
            "Training:  Iteration No:  155 Worker Num:  3 \n",
            " Loss:  0.36388537287712097\n",
            "Training:  Iteration No:  155 Worker Num:  4 \n",
            " Loss:  0.4492482841014862\n",
            "Training:  Iteration No:  155 Worker Num:  5 \n",
            " Loss:  0.27494314312934875\n",
            "Training:  Iteration No:  155 Worker Num:  6 \n",
            " Loss:  0.31949084997177124\n",
            "Training:  Iteration No:  155 Worker Num:  7 \n",
            " Loss:  0.29999271035194397\n",
            "Test:  Iteration No:  155 \n",
            " Loss:  0.29173343949302843\n",
            "Test accuracy:  91.56\n",
            "Training:  Iteration No:  156 Worker Num:  0 \n",
            " Loss:  0.3105454444885254\n",
            "Training:  Iteration No:  156 Worker Num:  1 \n",
            " Loss:  0.4542531371116638\n",
            "Training:  Iteration No:  156 Worker Num:  2 \n",
            " Loss:  0.3888568580150604\n",
            "Training:  Iteration No:  156 Worker Num:  3 \n",
            " Loss:  0.3298955261707306\n",
            "Training:  Iteration No:  156 Worker Num:  4 \n",
            " Loss:  0.5174497365951538\n",
            "Training:  Iteration No:  156 Worker Num:  5 \n",
            " Loss:  0.3067813515663147\n",
            "Training:  Iteration No:  156 Worker Num:  6 \n",
            " Loss:  0.31975993514060974\n",
            "Training:  Iteration No:  156 Worker Num:  7 \n",
            " Loss:  0.4021526277065277\n",
            "Test:  Iteration No:  156 \n",
            " Loss:  0.2894066772600518\n",
            "Test accuracy:  91.83\n",
            "Training:  Iteration No:  157 Worker Num:  0 \n",
            " Loss:  0.39578425884246826\n",
            "Training:  Iteration No:  157 Worker Num:  1 \n",
            " Loss:  0.41097429394721985\n",
            "Training:  Iteration No:  157 Worker Num:  2 \n",
            " Loss:  0.2790543735027313\n",
            "Training:  Iteration No:  157 Worker Num:  3 \n",
            " Loss:  0.47012564539909363\n",
            "Training:  Iteration No:  157 Worker Num:  4 \n",
            " Loss:  0.36649268865585327\n",
            "Training:  Iteration No:  157 Worker Num:  5 \n",
            " Loss:  0.46118494868278503\n",
            "Training:  Iteration No:  157 Worker Num:  6 \n",
            " Loss:  0.42001840472221375\n",
            "Training:  Iteration No:  157 Worker Num:  7 \n",
            " Loss:  0.3454960882663727\n",
            "Test:  Iteration No:  157 \n",
            " Loss:  0.29298621472678604\n",
            "Test accuracy:  91.67\n",
            "Training:  Iteration No:  158 Worker Num:  0 \n",
            " Loss:  0.31656965613365173\n",
            "Training:  Iteration No:  158 Worker Num:  1 \n",
            " Loss:  0.3253675699234009\n",
            "Training:  Iteration No:  158 Worker Num:  2 \n",
            " Loss:  0.35754644870758057\n",
            "Training:  Iteration No:  158 Worker Num:  3 \n",
            " Loss:  0.3381093740463257\n",
            "Training:  Iteration No:  158 Worker Num:  4 \n",
            " Loss:  0.3401566743850708\n",
            "Training:  Iteration No:  158 Worker Num:  5 \n",
            " Loss:  0.3284870982170105\n",
            "Training:  Iteration No:  158 Worker Num:  6 \n",
            " Loss:  0.4020875096321106\n",
            "Training:  Iteration No:  158 Worker Num:  7 \n",
            " Loss:  0.3706493079662323\n",
            "Test:  Iteration No:  158 \n",
            " Loss:  0.28873063232513924\n",
            "Test accuracy:  91.73\n",
            "Training:  Iteration No:  159 Worker Num:  0 \n",
            " Loss:  0.3507584035396576\n",
            "Training:  Iteration No:  159 Worker Num:  1 \n",
            " Loss:  0.44348305463790894\n",
            "Training:  Iteration No:  159 Worker Num:  2 \n",
            " Loss:  0.41307342052459717\n",
            "Training:  Iteration No:  159 Worker Num:  3 \n",
            " Loss:  0.42210912704467773\n",
            "Training:  Iteration No:  159 Worker Num:  4 \n",
            " Loss:  0.44561728835105896\n",
            "Training:  Iteration No:  159 Worker Num:  5 \n",
            " Loss:  0.3723079562187195\n",
            "Training:  Iteration No:  159 Worker Num:  6 \n",
            " Loss:  0.27216634154319763\n",
            "Training:  Iteration No:  159 Worker Num:  7 \n",
            " Loss:  0.3706977367401123\n",
            "Test:  Iteration No:  159 \n",
            " Loss:  0.2927114839983892\n",
            "Test accuracy:  91.64\n",
            "Training:  Iteration No:  160 Worker Num:  0 \n",
            " Loss:  0.27338656783103943\n",
            "Training:  Iteration No:  160 Worker Num:  1 \n",
            " Loss:  0.3903404772281647\n",
            "Training:  Iteration No:  160 Worker Num:  2 \n",
            " Loss:  0.31183913350105286\n",
            "Training:  Iteration No:  160 Worker Num:  3 \n",
            " Loss:  0.35678768157958984\n",
            "Training:  Iteration No:  160 Worker Num:  4 \n",
            " Loss:  0.2934125065803528\n",
            "Training:  Iteration No:  160 Worker Num:  5 \n",
            " Loss:  0.3531409204006195\n",
            "Training:  Iteration No:  160 Worker Num:  6 \n",
            " Loss:  0.21433785557746887\n",
            "Training:  Iteration No:  160 Worker Num:  7 \n",
            " Loss:  0.4352760314941406\n",
            "Test:  Iteration No:  160 \n",
            " Loss:  0.2881016205006008\n",
            "Test accuracy:  91.68\n",
            "Training:  Iteration No:  161 Worker Num:  0 \n",
            " Loss:  0.3376004993915558\n",
            "Training:  Iteration No:  161 Worker Num:  1 \n",
            " Loss:  0.5248279571533203\n",
            "Training:  Iteration No:  161 Worker Num:  2 \n",
            " Loss:  0.516577422618866\n",
            "Training:  Iteration No:  161 Worker Num:  3 \n",
            " Loss:  0.33542174100875854\n",
            "Training:  Iteration No:  161 Worker Num:  4 \n",
            " Loss:  0.24531979858875275\n",
            "Training:  Iteration No:  161 Worker Num:  5 \n",
            " Loss:  0.2751256823539734\n",
            "Training:  Iteration No:  161 Worker Num:  6 \n",
            " Loss:  0.35613003373146057\n",
            "Training:  Iteration No:  161 Worker Num:  7 \n",
            " Loss:  0.3546784818172455\n",
            "Test:  Iteration No:  161 \n",
            " Loss:  0.2907959398212312\n",
            "Test accuracy:  91.41\n",
            "Training:  Iteration No:  162 Worker Num:  0 \n",
            " Loss:  0.3135264813899994\n",
            "Training:  Iteration No:  162 Worker Num:  1 \n",
            " Loss:  0.39493417739868164\n",
            "Training:  Iteration No:  162 Worker Num:  2 \n",
            " Loss:  0.36196935176849365\n",
            "Training:  Iteration No:  162 Worker Num:  3 \n",
            " Loss:  0.3346251845359802\n",
            "Training:  Iteration No:  162 Worker Num:  4 \n",
            " Loss:  0.3230225145816803\n",
            "Training:  Iteration No:  162 Worker Num:  5 \n",
            " Loss:  0.34860649704933167\n",
            "Training:  Iteration No:  162 Worker Num:  6 \n",
            " Loss:  0.35889488458633423\n",
            "Training:  Iteration No:  162 Worker Num:  7 \n",
            " Loss:  0.25105273723602295\n",
            "Test:  Iteration No:  162 \n",
            " Loss:  0.2845450225698797\n",
            "Test accuracy:  92.01\n",
            "Training:  Iteration No:  163 Worker Num:  0 \n",
            " Loss:  0.3589079678058624\n",
            "Training:  Iteration No:  163 Worker Num:  1 \n",
            " Loss:  0.38750186562538147\n",
            "Training:  Iteration No:  163 Worker Num:  2 \n",
            " Loss:  0.319429874420166\n",
            "Training:  Iteration No:  163 Worker Num:  3 \n",
            " Loss:  0.3464473783969879\n",
            "Training:  Iteration No:  163 Worker Num:  4 \n",
            " Loss:  0.35324445366859436\n",
            "Training:  Iteration No:  163 Worker Num:  5 \n",
            " Loss:  0.2966180741786957\n",
            "Training:  Iteration No:  163 Worker Num:  6 \n",
            " Loss:  0.30270248651504517\n",
            "Training:  Iteration No:  163 Worker Num:  7 \n",
            " Loss:  0.2852064371109009\n",
            "Test:  Iteration No:  163 \n",
            " Loss:  0.28084966526189936\n",
            "Test accuracy:  92.06\n",
            "Training:  Iteration No:  164 Worker Num:  0 \n",
            " Loss:  0.3071489632129669\n",
            "Training:  Iteration No:  164 Worker Num:  1 \n",
            " Loss:  0.24718129634857178\n",
            "Training:  Iteration No:  164 Worker Num:  2 \n",
            " Loss:  0.21746282279491425\n",
            "Training:  Iteration No:  164 Worker Num:  3 \n",
            " Loss:  0.30788102746009827\n",
            "Training:  Iteration No:  164 Worker Num:  4 \n",
            " Loss:  0.41520747542381287\n",
            "Training:  Iteration No:  164 Worker Num:  5 \n",
            " Loss:  0.3357278108596802\n",
            "Training:  Iteration No:  164 Worker Num:  6 \n",
            " Loss:  0.385652631521225\n",
            "Training:  Iteration No:  164 Worker Num:  7 \n",
            " Loss:  0.3627462387084961\n",
            "Test:  Iteration No:  164 \n",
            " Loss:  0.27951240563128565\n",
            "Test accuracy:  91.93\n",
            "Training:  Iteration No:  165 Worker Num:  0 \n",
            " Loss:  0.29379045963287354\n",
            "Training:  Iteration No:  165 Worker Num:  1 \n",
            " Loss:  0.4352060854434967\n",
            "Training:  Iteration No:  165 Worker Num:  2 \n",
            " Loss:  0.38119396567344666\n",
            "Training:  Iteration No:  165 Worker Num:  3 \n",
            " Loss:  0.309614360332489\n",
            "Training:  Iteration No:  165 Worker Num:  4 \n",
            " Loss:  0.4189969003200531\n",
            "Training:  Iteration No:  165 Worker Num:  5 \n",
            " Loss:  0.3076152503490448\n",
            "Training:  Iteration No:  165 Worker Num:  6 \n",
            " Loss:  0.41230255365371704\n",
            "Training:  Iteration No:  165 Worker Num:  7 \n",
            " Loss:  0.27459481358528137\n",
            "Test:  Iteration No:  165 \n",
            " Loss:  0.28141020653368554\n",
            "Test accuracy:  91.85\n",
            "Training:  Iteration No:  166 Worker Num:  0 \n",
            " Loss:  0.2598634362220764\n",
            "Training:  Iteration No:  166 Worker Num:  1 \n",
            " Loss:  0.48870140314102173\n",
            "Training:  Iteration No:  166 Worker Num:  2 \n",
            " Loss:  0.4126594364643097\n",
            "Training:  Iteration No:  166 Worker Num:  3 \n",
            " Loss:  0.23100370168685913\n",
            "Training:  Iteration No:  166 Worker Num:  4 \n",
            " Loss:  0.3353807032108307\n",
            "Training:  Iteration No:  166 Worker Num:  5 \n",
            " Loss:  0.29677554965019226\n",
            "Training:  Iteration No:  166 Worker Num:  6 \n",
            " Loss:  0.2427976280450821\n",
            "Training:  Iteration No:  166 Worker Num:  7 \n",
            " Loss:  0.3444366157054901\n",
            "Test:  Iteration No:  166 \n",
            " Loss:  0.27867262322408487\n",
            "Test accuracy:  92.05\n",
            "Training:  Iteration No:  167 Worker Num:  0 \n",
            " Loss:  0.30029743909835815\n",
            "Training:  Iteration No:  167 Worker Num:  1 \n",
            " Loss:  0.46760401129722595\n",
            "Training:  Iteration No:  167 Worker Num:  2 \n",
            " Loss:  0.3418087959289551\n",
            "Training:  Iteration No:  167 Worker Num:  3 \n",
            " Loss:  0.23902714252471924\n",
            "Training:  Iteration No:  167 Worker Num:  4 \n",
            " Loss:  0.24902521073818207\n",
            "Training:  Iteration No:  167 Worker Num:  5 \n",
            " Loss:  0.35557374358177185\n",
            "Training:  Iteration No:  167 Worker Num:  6 \n",
            " Loss:  0.3163362741470337\n",
            "Training:  Iteration No:  167 Worker Num:  7 \n",
            " Loss:  0.2715313136577606\n",
            "Test:  Iteration No:  167 \n",
            " Loss:  0.2777617357529794\n",
            "Test accuracy:  92.09\n",
            "Training:  Iteration No:  168 Worker Num:  0 \n",
            " Loss:  0.30782490968704224\n",
            "Training:  Iteration No:  168 Worker Num:  1 \n",
            " Loss:  0.36132481694221497\n",
            "Training:  Iteration No:  168 Worker Num:  2 \n",
            " Loss:  0.19913798570632935\n",
            "Training:  Iteration No:  168 Worker Num:  3 \n",
            " Loss:  0.3886052370071411\n",
            "Training:  Iteration No:  168 Worker Num:  4 \n",
            " Loss:  0.2933083772659302\n",
            "Training:  Iteration No:  168 Worker Num:  5 \n",
            " Loss:  0.2931041121482849\n",
            "Training:  Iteration No:  168 Worker Num:  6 \n",
            " Loss:  0.46243149042129517\n",
            "Training:  Iteration No:  168 Worker Num:  7 \n",
            " Loss:  0.3092539608478546\n",
            "Test:  Iteration No:  168 \n",
            " Loss:  0.2784762461021354\n",
            "Test accuracy:  92.02\n",
            "Training:  Iteration No:  169 Worker Num:  0 \n",
            " Loss:  0.34219080209732056\n",
            "Training:  Iteration No:  169 Worker Num:  1 \n",
            " Loss:  0.3885478079319\n",
            "Training:  Iteration No:  169 Worker Num:  2 \n",
            " Loss:  0.2883847951889038\n",
            "Training:  Iteration No:  169 Worker Num:  3 \n",
            " Loss:  0.2616971433162689\n",
            "Training:  Iteration No:  169 Worker Num:  4 \n",
            " Loss:  0.2826303243637085\n",
            "Training:  Iteration No:  169 Worker Num:  5 \n",
            " Loss:  0.40884363651275635\n",
            "Training:  Iteration No:  169 Worker Num:  6 \n",
            " Loss:  0.33154749870300293\n",
            "Training:  Iteration No:  169 Worker Num:  7 \n",
            " Loss:  0.42052406072616577\n",
            "Test:  Iteration No:  169 \n",
            " Loss:  0.2801556308058244\n",
            "Test accuracy:  91.96\n",
            "Training:  Iteration No:  170 Worker Num:  0 \n",
            " Loss:  0.38558506965637207\n",
            "Training:  Iteration No:  170 Worker Num:  1 \n",
            " Loss:  0.36198344826698303\n",
            "Training:  Iteration No:  170 Worker Num:  2 \n",
            " Loss:  0.5458984971046448\n",
            "Training:  Iteration No:  170 Worker Num:  3 \n",
            " Loss:  0.31106510758399963\n",
            "Training:  Iteration No:  170 Worker Num:  4 \n",
            " Loss:  0.33598339557647705\n",
            "Training:  Iteration No:  170 Worker Num:  5 \n",
            " Loss:  0.4587574601173401\n",
            "Training:  Iteration No:  170 Worker Num:  6 \n",
            " Loss:  0.334820419549942\n",
            "Training:  Iteration No:  170 Worker Num:  7 \n",
            " Loss:  0.28233131766319275\n",
            "Test:  Iteration No:  170 \n",
            " Loss:  0.27203535778990273\n",
            "Test accuracy:  92.12\n",
            "Training:  Iteration No:  171 Worker Num:  0 \n",
            " Loss:  0.33324748277664185\n",
            "Training:  Iteration No:  171 Worker Num:  1 \n",
            " Loss:  0.3289451003074646\n",
            "Training:  Iteration No:  171 Worker Num:  2 \n",
            " Loss:  0.32861006259918213\n",
            "Training:  Iteration No:  171 Worker Num:  3 \n",
            " Loss:  0.24544134736061096\n",
            "Training:  Iteration No:  171 Worker Num:  4 \n",
            " Loss:  0.32185566425323486\n",
            "Training:  Iteration No:  171 Worker Num:  5 \n",
            " Loss:  0.2830239236354828\n",
            "Training:  Iteration No:  171 Worker Num:  6 \n",
            " Loss:  0.30893898010253906\n",
            "Training:  Iteration No:  171 Worker Num:  7 \n",
            " Loss:  0.3450537621974945\n",
            "Test:  Iteration No:  171 \n",
            " Loss:  0.2741713046650343\n",
            "Test accuracy:  91.97\n",
            "Training:  Iteration No:  172 Worker Num:  0 \n",
            " Loss:  0.2761231064796448\n",
            "Training:  Iteration No:  172 Worker Num:  1 \n",
            " Loss:  0.29339954257011414\n",
            "Training:  Iteration No:  172 Worker Num:  2 \n",
            " Loss:  0.339793860912323\n",
            "Training:  Iteration No:  172 Worker Num:  3 \n",
            " Loss:  0.32287874817848206\n",
            "Training:  Iteration No:  172 Worker Num:  4 \n",
            " Loss:  0.4407651722431183\n",
            "Training:  Iteration No:  172 Worker Num:  5 \n",
            " Loss:  0.3479498326778412\n",
            "Training:  Iteration No:  172 Worker Num:  6 \n",
            " Loss:  0.32422173023223877\n",
            "Training:  Iteration No:  172 Worker Num:  7 \n",
            " Loss:  0.32907870411872864\n",
            "Test:  Iteration No:  172 \n",
            " Loss:  0.2730992617014843\n",
            "Test accuracy:  92.18\n",
            "Training:  Iteration No:  173 Worker Num:  0 \n",
            " Loss:  0.22866345942020416\n",
            "Training:  Iteration No:  173 Worker Num:  1 \n",
            " Loss:  0.40647706389427185\n",
            "Training:  Iteration No:  173 Worker Num:  2 \n",
            " Loss:  0.2983350455760956\n",
            "Training:  Iteration No:  173 Worker Num:  3 \n",
            " Loss:  0.23421671986579895\n",
            "Training:  Iteration No:  173 Worker Num:  4 \n",
            " Loss:  0.3589531481266022\n",
            "Training:  Iteration No:  173 Worker Num:  5 \n",
            " Loss:  0.3783756196498871\n",
            "Training:  Iteration No:  173 Worker Num:  6 \n",
            " Loss:  0.30490729212760925\n",
            "Training:  Iteration No:  173 Worker Num:  7 \n",
            " Loss:  0.2757617235183716\n",
            "Test:  Iteration No:  173 \n",
            " Loss:  0.27058458000515834\n",
            "Test accuracy:  92.22\n",
            "Training:  Iteration No:  174 Worker Num:  0 \n",
            " Loss:  0.32213735580444336\n",
            "Training:  Iteration No:  174 Worker Num:  1 \n",
            " Loss:  0.30462172627449036\n",
            "Training:  Iteration No:  174 Worker Num:  2 \n",
            " Loss:  0.3660595715045929\n",
            "Training:  Iteration No:  174 Worker Num:  3 \n",
            " Loss:  0.23034191131591797\n",
            "Training:  Iteration No:  174 Worker Num:  4 \n",
            " Loss:  0.3224286437034607\n",
            "Training:  Iteration No:  174 Worker Num:  5 \n",
            " Loss:  0.28652021288871765\n",
            "Training:  Iteration No:  174 Worker Num:  6 \n",
            " Loss:  0.23307175934314728\n",
            "Training:  Iteration No:  174 Worker Num:  7 \n",
            " Loss:  0.3695155382156372\n",
            "Test:  Iteration No:  174 \n",
            " Loss:  0.2674018567593037\n",
            "Test accuracy:  92.39\n",
            "Training:  Iteration No:  175 Worker Num:  0 \n",
            " Loss:  0.3415440618991852\n",
            "Training:  Iteration No:  175 Worker Num:  1 \n",
            " Loss:  0.32893675565719604\n",
            "Training:  Iteration No:  175 Worker Num:  2 \n",
            " Loss:  0.3172428607940674\n",
            "Training:  Iteration No:  175 Worker Num:  3 \n",
            " Loss:  0.2569899260997772\n",
            "Training:  Iteration No:  175 Worker Num:  4 \n",
            " Loss:  0.3692157566547394\n",
            "Training:  Iteration No:  175 Worker Num:  5 \n",
            " Loss:  0.2538509666919708\n",
            "Training:  Iteration No:  175 Worker Num:  6 \n",
            " Loss:  0.2569634020328522\n",
            "Training:  Iteration No:  175 Worker Num:  7 \n",
            " Loss:  0.3208577036857605\n",
            "Test:  Iteration No:  175 \n",
            " Loss:  0.26772055036942416\n",
            "Test accuracy:  92.44\n",
            "Training:  Iteration No:  176 Worker Num:  0 \n",
            " Loss:  0.3176286220550537\n",
            "Training:  Iteration No:  176 Worker Num:  1 \n",
            " Loss:  0.344273179769516\n",
            "Training:  Iteration No:  176 Worker Num:  2 \n",
            " Loss:  0.3404097259044647\n",
            "Training:  Iteration No:  176 Worker Num:  3 \n",
            " Loss:  0.34940212965011597\n",
            "Training:  Iteration No:  176 Worker Num:  4 \n",
            " Loss:  0.23108702898025513\n",
            "Training:  Iteration No:  176 Worker Num:  5 \n",
            " Loss:  0.3133475184440613\n",
            "Training:  Iteration No:  176 Worker Num:  6 \n",
            " Loss:  0.438387006521225\n",
            "Training:  Iteration No:  176 Worker Num:  7 \n",
            " Loss:  0.20432835817337036\n",
            "Test:  Iteration No:  176 \n",
            " Loss:  0.2682724316852002\n",
            "Test accuracy:  92.21\n",
            "Training:  Iteration No:  177 Worker Num:  0 \n",
            " Loss:  0.3899833858013153\n",
            "Training:  Iteration No:  177 Worker Num:  1 \n",
            " Loss:  0.3627623915672302\n",
            "Training:  Iteration No:  177 Worker Num:  2 \n",
            " Loss:  0.2501544952392578\n",
            "Training:  Iteration No:  177 Worker Num:  3 \n",
            " Loss:  0.33768734335899353\n",
            "Training:  Iteration No:  177 Worker Num:  4 \n",
            " Loss:  0.278046190738678\n",
            "Training:  Iteration No:  177 Worker Num:  5 \n",
            " Loss:  0.3627791106700897\n",
            "Training:  Iteration No:  177 Worker Num:  6 \n",
            " Loss:  0.3680843412876129\n",
            "Training:  Iteration No:  177 Worker Num:  7 \n",
            " Loss:  0.3826254606246948\n",
            "Test:  Iteration No:  177 \n",
            " Loss:  0.2655098289958661\n",
            "Test accuracy:  92.34\n",
            "Training:  Iteration No:  178 Worker Num:  0 \n",
            " Loss:  0.3301151990890503\n",
            "Training:  Iteration No:  178 Worker Num:  1 \n",
            " Loss:  0.339518666267395\n",
            "Training:  Iteration No:  178 Worker Num:  2 \n",
            " Loss:  0.27852779626846313\n",
            "Training:  Iteration No:  178 Worker Num:  3 \n",
            " Loss:  0.21537597477436066\n",
            "Training:  Iteration No:  178 Worker Num:  4 \n",
            " Loss:  0.3628862202167511\n",
            "Training:  Iteration No:  178 Worker Num:  5 \n",
            " Loss:  0.33385398983955383\n",
            "Training:  Iteration No:  178 Worker Num:  6 \n",
            " Loss:  0.28987917304039\n",
            "Training:  Iteration No:  178 Worker Num:  7 \n",
            " Loss:  0.36560189723968506\n",
            "Test:  Iteration No:  178 \n",
            " Loss:  0.26443394804019715\n",
            "Test accuracy:  92.21\n",
            "Training:  Iteration No:  179 Worker Num:  0 \n",
            " Loss:  0.32285547256469727\n",
            "Training:  Iteration No:  179 Worker Num:  1 \n",
            " Loss:  0.38440874218940735\n",
            "Training:  Iteration No:  179 Worker Num:  2 \n",
            " Loss:  0.3572338819503784\n",
            "Training:  Iteration No:  179 Worker Num:  3 \n",
            " Loss:  0.40177494287490845\n",
            "Training:  Iteration No:  179 Worker Num:  4 \n",
            " Loss:  0.22762155532836914\n",
            "Training:  Iteration No:  179 Worker Num:  5 \n",
            " Loss:  0.18629349768161774\n",
            "Training:  Iteration No:  179 Worker Num:  6 \n",
            " Loss:  0.4306117296218872\n",
            "Training:  Iteration No:  179 Worker Num:  7 \n",
            " Loss:  0.42777150869369507\n",
            "Test:  Iteration No:  179 \n",
            " Loss:  0.25993032321994064\n",
            "Test accuracy:  92.52\n",
            "Training:  Iteration No:  180 Worker Num:  0 \n",
            " Loss:  0.3384650945663452\n",
            "Training:  Iteration No:  180 Worker Num:  1 \n",
            " Loss:  0.29302021861076355\n",
            "Training:  Iteration No:  180 Worker Num:  2 \n",
            " Loss:  0.3649038076400757\n",
            "Training:  Iteration No:  180 Worker Num:  3 \n",
            " Loss:  0.29427605867385864\n",
            "Training:  Iteration No:  180 Worker Num:  4 \n",
            " Loss:  0.5141182541847229\n",
            "Training:  Iteration No:  180 Worker Num:  5 \n",
            " Loss:  0.31414535641670227\n",
            "Training:  Iteration No:  180 Worker Num:  6 \n",
            " Loss:  0.2664071321487427\n",
            "Training:  Iteration No:  180 Worker Num:  7 \n",
            " Loss:  0.20438241958618164\n",
            "Test:  Iteration No:  180 \n",
            " Loss:  0.2620124532074868\n",
            "Test accuracy:  92.51\n",
            "Training:  Iteration No:  181 Worker Num:  0 \n",
            " Loss:  0.20393845438957214\n",
            "Training:  Iteration No:  181 Worker Num:  1 \n",
            " Loss:  0.28284981846809387\n",
            "Training:  Iteration No:  181 Worker Num:  2 \n",
            " Loss:  0.3568996489048004\n",
            "Training:  Iteration No:  181 Worker Num:  3 \n",
            " Loss:  0.47792455554008484\n",
            "Training:  Iteration No:  181 Worker Num:  4 \n",
            " Loss:  0.3363476097583771\n",
            "Training:  Iteration No:  181 Worker Num:  5 \n",
            " Loss:  0.36794477701187134\n",
            "Training:  Iteration No:  181 Worker Num:  6 \n",
            " Loss:  0.366333931684494\n",
            "Training:  Iteration No:  181 Worker Num:  7 \n",
            " Loss:  0.45088571310043335\n",
            "Test:  Iteration No:  181 \n",
            " Loss:  0.2625787262863751\n",
            "Test accuracy:  92.47\n",
            "Training:  Iteration No:  182 Worker Num:  0 \n",
            " Loss:  0.2733122408390045\n",
            "Training:  Iteration No:  182 Worker Num:  1 \n",
            " Loss:  0.38502761721611023\n",
            "Training:  Iteration No:  182 Worker Num:  2 \n",
            " Loss:  0.32493022084236145\n",
            "Training:  Iteration No:  182 Worker Num:  3 \n",
            " Loss:  0.4449768364429474\n",
            "Training:  Iteration No:  182 Worker Num:  4 \n",
            " Loss:  0.266616553068161\n",
            "Training:  Iteration No:  182 Worker Num:  5 \n",
            " Loss:  0.2791140079498291\n",
            "Training:  Iteration No:  182 Worker Num:  6 \n",
            " Loss:  0.32114318013191223\n",
            "Training:  Iteration No:  182 Worker Num:  7 \n",
            " Loss:  0.3579767942428589\n",
            "Test:  Iteration No:  182 \n",
            " Loss:  0.2594054054846115\n",
            "Test accuracy:  92.37\n",
            "Training:  Iteration No:  183 Worker Num:  0 \n",
            " Loss:  0.340285062789917\n",
            "Training:  Iteration No:  183 Worker Num:  1 \n",
            " Loss:  0.3210282623767853\n",
            "Training:  Iteration No:  183 Worker Num:  2 \n",
            " Loss:  0.32653504610061646\n",
            "Training:  Iteration No:  183 Worker Num:  3 \n",
            " Loss:  0.25599944591522217\n",
            "Training:  Iteration No:  183 Worker Num:  4 \n",
            " Loss:  0.26385560631752014\n",
            "Training:  Iteration No:  183 Worker Num:  5 \n",
            " Loss:  0.3590070307254791\n",
            "Training:  Iteration No:  183 Worker Num:  6 \n",
            " Loss:  0.5240939855575562\n",
            "Training:  Iteration No:  183 Worker Num:  7 \n",
            " Loss:  0.2450159639120102\n",
            "Test:  Iteration No:  183 \n",
            " Loss:  0.25806864263796353\n",
            "Test accuracy:  92.52\n",
            "Training:  Iteration No:  184 Worker Num:  0 \n",
            " Loss:  0.2691386938095093\n",
            "Training:  Iteration No:  184 Worker Num:  1 \n",
            " Loss:  0.29053837060928345\n",
            "Training:  Iteration No:  184 Worker Num:  2 \n",
            " Loss:  0.6239956617355347\n",
            "Training:  Iteration No:  184 Worker Num:  3 \n",
            " Loss:  0.32828548550605774\n",
            "Training:  Iteration No:  184 Worker Num:  4 \n",
            " Loss:  0.2740033268928528\n",
            "Training:  Iteration No:  184 Worker Num:  5 \n",
            " Loss:  0.253131240606308\n",
            "Training:  Iteration No:  184 Worker Num:  6 \n",
            " Loss:  0.2843450605869293\n",
            "Training:  Iteration No:  184 Worker Num:  7 \n",
            " Loss:  0.33142969012260437\n",
            "Test:  Iteration No:  184 \n",
            " Loss:  0.256723367455828\n",
            "Test accuracy:  92.56\n",
            "Training:  Iteration No:  185 Worker Num:  0 \n",
            " Loss:  0.279934823513031\n",
            "Training:  Iteration No:  185 Worker Num:  1 \n",
            " Loss:  0.26658210158348083\n",
            "Training:  Iteration No:  185 Worker Num:  2 \n",
            " Loss:  0.24146009981632233\n",
            "Training:  Iteration No:  185 Worker Num:  3 \n",
            " Loss:  0.18181867897510529\n",
            "Training:  Iteration No:  185 Worker Num:  4 \n",
            " Loss:  0.2876666486263275\n",
            "Training:  Iteration No:  185 Worker Num:  5 \n",
            " Loss:  0.2387925684452057\n",
            "Training:  Iteration No:  185 Worker Num:  6 \n",
            " Loss:  0.2265455573797226\n",
            "Training:  Iteration No:  185 Worker Num:  7 \n",
            " Loss:  0.3560296297073364\n",
            "Test:  Iteration No:  185 \n",
            " Loss:  0.2538410788048295\n",
            "Test accuracy:  92.7\n",
            "Training:  Iteration No:  186 Worker Num:  0 \n",
            " Loss:  0.2946113646030426\n",
            "Training:  Iteration No:  186 Worker Num:  1 \n",
            " Loss:  0.14956405758857727\n",
            "Training:  Iteration No:  186 Worker Num:  2 \n",
            " Loss:  0.22236977517604828\n",
            "Training:  Iteration No:  186 Worker Num:  3 \n",
            " Loss:  0.39382433891296387\n",
            "Training:  Iteration No:  186 Worker Num:  4 \n",
            " Loss:  0.3799930214881897\n",
            "Training:  Iteration No:  186 Worker Num:  5 \n",
            " Loss:  0.2982947826385498\n",
            "Training:  Iteration No:  186 Worker Num:  6 \n",
            " Loss:  0.2414415180683136\n",
            "Training:  Iteration No:  186 Worker Num:  7 \n",
            " Loss:  0.16396911442279816\n",
            "Test:  Iteration No:  186 \n",
            " Loss:  0.252824990951185\n",
            "Test accuracy:  92.8\n",
            "Training:  Iteration No:  187 Worker Num:  0 \n",
            " Loss:  0.3604971468448639\n",
            "Training:  Iteration No:  187 Worker Num:  1 \n",
            " Loss:  0.33393394947052\n",
            "Training:  Iteration No:  187 Worker Num:  2 \n",
            " Loss:  0.31820160150527954\n",
            "Training:  Iteration No:  187 Worker Num:  3 \n",
            " Loss:  0.38873589038848877\n",
            "Training:  Iteration No:  187 Worker Num:  4 \n",
            " Loss:  0.17314690351486206\n",
            "Training:  Iteration No:  187 Worker Num:  5 \n",
            " Loss:  0.2902075946331024\n",
            "Training:  Iteration No:  187 Worker Num:  6 \n",
            " Loss:  0.23052890598773956\n",
            "Training:  Iteration No:  187 Worker Num:  7 \n",
            " Loss:  0.3283763527870178\n",
            "Test:  Iteration No:  187 \n",
            " Loss:  0.2529728815336771\n",
            "Test accuracy:  92.69\n",
            "Training:  Iteration No:  188 Worker Num:  0 \n",
            " Loss:  0.15747584402561188\n",
            "Training:  Iteration No:  188 Worker Num:  1 \n",
            " Loss:  0.3723215162754059\n",
            "Training:  Iteration No:  188 Worker Num:  2 \n",
            " Loss:  0.33506423234939575\n",
            "Training:  Iteration No:  188 Worker Num:  3 \n",
            " Loss:  0.37138357758522034\n",
            "Training:  Iteration No:  188 Worker Num:  4 \n",
            " Loss:  0.40210893750190735\n",
            "Training:  Iteration No:  188 Worker Num:  5 \n",
            " Loss:  0.31174859404563904\n",
            "Training:  Iteration No:  188 Worker Num:  6 \n",
            " Loss:  0.36496543884277344\n",
            "Training:  Iteration No:  188 Worker Num:  7 \n",
            " Loss:  0.34296083450317383\n",
            "Test:  Iteration No:  188 \n",
            " Loss:  0.25315321248637723\n",
            "Test accuracy:  92.68\n",
            "Training:  Iteration No:  189 Worker Num:  0 \n",
            " Loss:  0.2503919303417206\n",
            "Training:  Iteration No:  189 Worker Num:  1 \n",
            " Loss:  0.33479389548301697\n",
            "Training:  Iteration No:  189 Worker Num:  2 \n",
            " Loss:  0.40509742498397827\n",
            "Training:  Iteration No:  189 Worker Num:  3 \n",
            " Loss:  0.35320138931274414\n",
            "Training:  Iteration No:  189 Worker Num:  4 \n",
            " Loss:  0.37547817826271057\n",
            "Training:  Iteration No:  189 Worker Num:  5 \n",
            " Loss:  0.22982440888881683\n",
            "Training:  Iteration No:  189 Worker Num:  6 \n",
            " Loss:  0.22849372029304504\n",
            "Training:  Iteration No:  189 Worker Num:  7 \n",
            " Loss:  0.2893050014972687\n",
            "Test:  Iteration No:  189 \n",
            " Loss:  0.25039547632295117\n",
            "Test accuracy:  92.87\n",
            "Training:  Iteration No:  190 Worker Num:  0 \n",
            " Loss:  0.3260987401008606\n",
            "Training:  Iteration No:  190 Worker Num:  1 \n",
            " Loss:  0.3009134829044342\n",
            "Training:  Iteration No:  190 Worker Num:  2 \n",
            " Loss:  0.2630496323108673\n",
            "Training:  Iteration No:  190 Worker Num:  3 \n",
            " Loss:  0.37140369415283203\n",
            "Training:  Iteration No:  190 Worker Num:  4 \n",
            " Loss:  0.3383932113647461\n",
            "Training:  Iteration No:  190 Worker Num:  5 \n",
            " Loss:  0.39363712072372437\n",
            "Training:  Iteration No:  190 Worker Num:  6 \n",
            " Loss:  0.40403491258621216\n",
            "Training:  Iteration No:  190 Worker Num:  7 \n",
            " Loss:  0.2148144543170929\n",
            "Test:  Iteration No:  190 \n",
            " Loss:  0.2530891490227814\n",
            "Test accuracy:  92.73\n",
            "Training:  Iteration No:  191 Worker Num:  0 \n",
            " Loss:  0.4156302511692047\n",
            "Training:  Iteration No:  191 Worker Num:  1 \n",
            " Loss:  0.21701355278491974\n",
            "Training:  Iteration No:  191 Worker Num:  2 \n",
            " Loss:  0.3741911053657532\n",
            "Training:  Iteration No:  191 Worker Num:  3 \n",
            " Loss:  0.5282080769538879\n",
            "Training:  Iteration No:  191 Worker Num:  4 \n",
            " Loss:  0.2838108241558075\n",
            "Training:  Iteration No:  191 Worker Num:  5 \n",
            " Loss:  0.25443553924560547\n",
            "Training:  Iteration No:  191 Worker Num:  6 \n",
            " Loss:  0.2442101687192917\n",
            "Training:  Iteration No:  191 Worker Num:  7 \n",
            " Loss:  0.32890021800994873\n",
            "Test:  Iteration No:  191 \n",
            " Loss:  0.24916353906634486\n",
            "Test accuracy:  92.82\n",
            "Training:  Iteration No:  192 Worker Num:  0 \n",
            " Loss:  0.39728009700775146\n",
            "Training:  Iteration No:  192 Worker Num:  1 \n",
            " Loss:  0.17852814495563507\n",
            "Training:  Iteration No:  192 Worker Num:  2 \n",
            " Loss:  0.21991869807243347\n",
            "Training:  Iteration No:  192 Worker Num:  3 \n",
            " Loss:  0.3956165313720703\n",
            "Training:  Iteration No:  192 Worker Num:  4 \n",
            " Loss:  0.2600611448287964\n",
            "Training:  Iteration No:  192 Worker Num:  5 \n",
            " Loss:  0.28040650486946106\n",
            "Training:  Iteration No:  192 Worker Num:  6 \n",
            " Loss:  0.25482913851737976\n",
            "Training:  Iteration No:  192 Worker Num:  7 \n",
            " Loss:  0.30397284030914307\n",
            "Test:  Iteration No:  192 \n",
            " Loss:  0.25523396979875956\n",
            "Test accuracy:  92.62\n",
            "Training:  Iteration No:  193 Worker Num:  0 \n",
            " Loss:  0.3794110417366028\n",
            "Training:  Iteration No:  193 Worker Num:  1 \n",
            " Loss:  0.3515935242176056\n",
            "Training:  Iteration No:  193 Worker Num:  2 \n",
            " Loss:  0.39477968215942383\n",
            "Training:  Iteration No:  193 Worker Num:  3 \n",
            " Loss:  0.3151293992996216\n",
            "Training:  Iteration No:  193 Worker Num:  4 \n",
            " Loss:  0.29169711470603943\n",
            "Training:  Iteration No:  193 Worker Num:  5 \n",
            " Loss:  0.2026871293783188\n",
            "Training:  Iteration No:  193 Worker Num:  6 \n",
            " Loss:  0.2918277084827423\n",
            "Training:  Iteration No:  193 Worker Num:  7 \n",
            " Loss:  0.28784969449043274\n",
            "Test:  Iteration No:  193 \n",
            " Loss:  0.24785257824048212\n",
            "Test accuracy:  92.89\n",
            "Training:  Iteration No:  194 Worker Num:  0 \n",
            " Loss:  0.3333539068698883\n",
            "Training:  Iteration No:  194 Worker Num:  1 \n",
            " Loss:  0.30900660157203674\n",
            "Training:  Iteration No:  194 Worker Num:  2 \n",
            " Loss:  0.2805224359035492\n",
            "Training:  Iteration No:  194 Worker Num:  3 \n",
            " Loss:  0.2510845363140106\n",
            "Training:  Iteration No:  194 Worker Num:  4 \n",
            " Loss:  0.19315502047538757\n",
            "Training:  Iteration No:  194 Worker Num:  5 \n",
            " Loss:  0.2974229156970978\n",
            "Training:  Iteration No:  194 Worker Num:  6 \n",
            " Loss:  0.1841120570898056\n",
            "Training:  Iteration No:  194 Worker Num:  7 \n",
            " Loss:  0.3242599070072174\n",
            "Test:  Iteration No:  194 \n",
            " Loss:  0.2515762985177055\n",
            "Test accuracy:  92.61\n",
            "Training:  Iteration No:  195 Worker Num:  0 \n",
            " Loss:  0.25520601868629456\n",
            "Training:  Iteration No:  195 Worker Num:  1 \n",
            " Loss:  0.22970141470432281\n",
            "Training:  Iteration No:  195 Worker Num:  2 \n",
            " Loss:  0.27830788493156433\n",
            "Training:  Iteration No:  195 Worker Num:  3 \n",
            " Loss:  0.24630136787891388\n",
            "Training:  Iteration No:  195 Worker Num:  4 \n",
            " Loss:  0.27198168635368347\n",
            "Training:  Iteration No:  195 Worker Num:  5 \n",
            " Loss:  0.3536462187767029\n",
            "Training:  Iteration No:  195 Worker Num:  6 \n",
            " Loss:  0.3360536992549896\n",
            "Training:  Iteration No:  195 Worker Num:  7 \n",
            " Loss:  0.3600534498691559\n",
            "Test:  Iteration No:  195 \n",
            " Loss:  0.24625513688484324\n",
            "Test accuracy:  92.82\n",
            "Training:  Iteration No:  196 Worker Num:  0 \n",
            " Loss:  0.4106362760066986\n",
            "Training:  Iteration No:  196 Worker Num:  1 \n",
            " Loss:  0.4496881663799286\n",
            "Training:  Iteration No:  196 Worker Num:  2 \n",
            " Loss:  0.24328172206878662\n",
            "Training:  Iteration No:  196 Worker Num:  3 \n",
            " Loss:  0.25969403982162476\n",
            "Training:  Iteration No:  196 Worker Num:  4 \n",
            " Loss:  0.30552586913108826\n",
            "Training:  Iteration No:  196 Worker Num:  5 \n",
            " Loss:  0.3107222318649292\n",
            "Training:  Iteration No:  196 Worker Num:  6 \n",
            " Loss:  0.3959987461566925\n",
            "Training:  Iteration No:  196 Worker Num:  7 \n",
            " Loss:  0.24411357939243317\n",
            "Test:  Iteration No:  196 \n",
            " Loss:  0.24267548211862014\n",
            "Test accuracy:  93.06\n",
            "Training:  Iteration No:  197 Worker Num:  0 \n",
            " Loss:  0.32781699299812317\n",
            "Training:  Iteration No:  197 Worker Num:  1 \n",
            " Loss:  0.31773725152015686\n",
            "Training:  Iteration No:  197 Worker Num:  2 \n",
            " Loss:  0.31943875551223755\n",
            "Training:  Iteration No:  197 Worker Num:  3 \n",
            " Loss:  0.2862359583377838\n",
            "Training:  Iteration No:  197 Worker Num:  4 \n",
            " Loss:  0.37620291113853455\n",
            "Training:  Iteration No:  197 Worker Num:  5 \n",
            " Loss:  0.22538712620735168\n",
            "Training:  Iteration No:  197 Worker Num:  6 \n",
            " Loss:  0.20302064716815948\n",
            "Training:  Iteration No:  197 Worker Num:  7 \n",
            " Loss:  0.32210230827331543\n",
            "Test:  Iteration No:  197 \n",
            " Loss:  0.24638559996987444\n",
            "Test accuracy:  92.84\n",
            "Training:  Iteration No:  198 Worker Num:  0 \n",
            " Loss:  0.3438263237476349\n",
            "Training:  Iteration No:  198 Worker Num:  1 \n",
            " Loss:  0.2774008810520172\n",
            "Training:  Iteration No:  198 Worker Num:  2 \n",
            " Loss:  0.24455128610134125\n",
            "Training:  Iteration No:  198 Worker Num:  3 \n",
            " Loss:  0.27016696333885193\n",
            "Training:  Iteration No:  198 Worker Num:  4 \n",
            " Loss:  0.23778046667575836\n",
            "Training:  Iteration No:  198 Worker Num:  5 \n",
            " Loss:  0.41335389018058777\n",
            "Training:  Iteration No:  198 Worker Num:  6 \n",
            " Loss:  0.19937515258789062\n",
            "Training:  Iteration No:  198 Worker Num:  7 \n",
            " Loss:  0.2673797905445099\n",
            "Test:  Iteration No:  198 \n",
            " Loss:  0.24289163164322888\n",
            "Test accuracy:  92.84\n",
            "Training:  Iteration No:  199 Worker Num:  0 \n",
            " Loss:  0.288357138633728\n",
            "Training:  Iteration No:  199 Worker Num:  1 \n",
            " Loss:  0.2324722707271576\n",
            "Training:  Iteration No:  199 Worker Num:  2 \n",
            " Loss:  0.37997671961784363\n",
            "Training:  Iteration No:  199 Worker Num:  3 \n",
            " Loss:  0.2547716796398163\n",
            "Training:  Iteration No:  199 Worker Num:  4 \n",
            " Loss:  0.23485681414604187\n",
            "Training:  Iteration No:  199 Worker Num:  5 \n",
            " Loss:  0.385469913482666\n",
            "Training:  Iteration No:  199 Worker Num:  6 \n",
            " Loss:  0.30163800716400146\n",
            "Training:  Iteration No:  199 Worker Num:  7 \n",
            " Loss:  0.29039907455444336\n",
            "Test:  Iteration No:  199 \n",
            " Loss:  0.24242284087629257\n",
            "Test accuracy:  93.12\n",
            "Training:  Iteration No:  200 Worker Num:  0 \n",
            " Loss:  0.43237850069999695\n",
            "Training:  Iteration No:  200 Worker Num:  1 \n",
            " Loss:  0.3762618899345398\n",
            "Training:  Iteration No:  200 Worker Num:  2 \n",
            " Loss:  0.3411405086517334\n",
            "Training:  Iteration No:  200 Worker Num:  3 \n",
            " Loss:  0.3454710841178894\n",
            "Training:  Iteration No:  200 Worker Num:  4 \n",
            " Loss:  0.17035146057605743\n",
            "Training:  Iteration No:  200 Worker Num:  5 \n",
            " Loss:  0.32333749532699585\n",
            "Training:  Iteration No:  200 Worker Num:  6 \n",
            " Loss:  0.40479281544685364\n",
            "Training:  Iteration No:  200 Worker Num:  7 \n",
            " Loss:  0.3748501241207123\n",
            "Test:  Iteration No:  200 \n",
            " Loss:  0.24178754677406594\n",
            "Test accuracy:  92.98\n",
            "Training:  Iteration No:  201 Worker Num:  0 \n",
            " Loss:  0.31994199752807617\n",
            "Training:  Iteration No:  201 Worker Num:  1 \n",
            " Loss:  0.30484285950660706\n",
            "Training:  Iteration No:  201 Worker Num:  2 \n",
            " Loss:  0.2570164203643799\n",
            "Training:  Iteration No:  201 Worker Num:  3 \n",
            " Loss:  0.18995799124240875\n",
            "Training:  Iteration No:  201 Worker Num:  4 \n",
            " Loss:  0.34301432967185974\n",
            "Training:  Iteration No:  201 Worker Num:  5 \n",
            " Loss:  0.22220438718795776\n",
            "Training:  Iteration No:  201 Worker Num:  6 \n",
            " Loss:  0.308158814907074\n",
            "Training:  Iteration No:  201 Worker Num:  7 \n",
            " Loss:  0.33726340532302856\n",
            "Test:  Iteration No:  201 \n",
            " Loss:  0.23888346043568623\n",
            "Test accuracy:  93.09\n",
            "Training:  Iteration No:  202 Worker Num:  0 \n",
            " Loss:  0.4796221852302551\n",
            "Training:  Iteration No:  202 Worker Num:  1 \n",
            " Loss:  0.4353759288787842\n",
            "Training:  Iteration No:  202 Worker Num:  2 \n",
            " Loss:  0.2775566577911377\n",
            "Training:  Iteration No:  202 Worker Num:  3 \n",
            " Loss:  0.2122950404882431\n",
            "Training:  Iteration No:  202 Worker Num:  4 \n",
            " Loss:  0.18940842151641846\n",
            "Training:  Iteration No:  202 Worker Num:  5 \n",
            " Loss:  0.33290737867355347\n",
            "Training:  Iteration No:  202 Worker Num:  6 \n",
            " Loss:  0.2323026955127716\n",
            "Training:  Iteration No:  202 Worker Num:  7 \n",
            " Loss:  0.19647172093391418\n",
            "Test:  Iteration No:  202 \n",
            " Loss:  0.23994022140963167\n",
            "Test accuracy:  93.05\n",
            "Training:  Iteration No:  203 Worker Num:  0 \n",
            " Loss:  0.3130945563316345\n",
            "Training:  Iteration No:  203 Worker Num:  1 \n",
            " Loss:  0.21436241269111633\n",
            "Training:  Iteration No:  203 Worker Num:  2 \n",
            " Loss:  0.43833431601524353\n",
            "Training:  Iteration No:  203 Worker Num:  3 \n",
            " Loss:  0.16188167035579681\n",
            "Training:  Iteration No:  203 Worker Num:  4 \n",
            " Loss:  0.35305309295654297\n",
            "Training:  Iteration No:  203 Worker Num:  5 \n",
            " Loss:  0.41029852628707886\n",
            "Training:  Iteration No:  203 Worker Num:  6 \n",
            " Loss:  0.196171835064888\n",
            "Training:  Iteration No:  203 Worker Num:  7 \n",
            " Loss:  0.31414490938186646\n",
            "Test:  Iteration No:  203 \n",
            " Loss:  0.24031392826781242\n",
            "Test accuracy:  92.89\n",
            "Training:  Iteration No:  204 Worker Num:  0 \n",
            " Loss:  0.4138588607311249\n",
            "Training:  Iteration No:  204 Worker Num:  1 \n",
            " Loss:  0.26781755685806274\n",
            "Training:  Iteration No:  204 Worker Num:  2 \n",
            " Loss:  0.32486122846603394\n",
            "Training:  Iteration No:  204 Worker Num:  3 \n",
            " Loss:  0.23985381424427032\n",
            "Training:  Iteration No:  204 Worker Num:  4 \n",
            " Loss:  0.33681678771972656\n",
            "Training:  Iteration No:  204 Worker Num:  5 \n",
            " Loss:  0.2942086458206177\n",
            "Training:  Iteration No:  204 Worker Num:  6 \n",
            " Loss:  0.22591149806976318\n",
            "Training:  Iteration No:  204 Worker Num:  7 \n",
            " Loss:  0.22511310875415802\n",
            "Test:  Iteration No:  204 \n",
            " Loss:  0.2363421615637556\n",
            "Test accuracy:  93.12\n",
            "Training:  Iteration No:  205 Worker Num:  0 \n",
            " Loss:  0.3292671740055084\n",
            "Training:  Iteration No:  205 Worker Num:  1 \n",
            " Loss:  0.23432831466197968\n",
            "Training:  Iteration No:  205 Worker Num:  2 \n",
            " Loss:  0.2887616455554962\n",
            "Training:  Iteration No:  205 Worker Num:  3 \n",
            " Loss:  0.26736870408058167\n",
            "Training:  Iteration No:  205 Worker Num:  4 \n",
            " Loss:  0.3597603738307953\n",
            "Training:  Iteration No:  205 Worker Num:  5 \n",
            " Loss:  0.31071770191192627\n",
            "Training:  Iteration No:  205 Worker Num:  6 \n",
            " Loss:  0.30337095260620117\n",
            "Training:  Iteration No:  205 Worker Num:  7 \n",
            " Loss:  0.1823236495256424\n",
            "Test:  Iteration No:  205 \n",
            " Loss:  0.23485301343040377\n",
            "Test accuracy:  93.27\n",
            "Training:  Iteration No:  206 Worker Num:  0 \n",
            " Loss:  0.29070019721984863\n",
            "Training:  Iteration No:  206 Worker Num:  1 \n",
            " Loss:  0.27498871088027954\n",
            "Training:  Iteration No:  206 Worker Num:  2 \n",
            " Loss:  0.34251514077186584\n",
            "Training:  Iteration No:  206 Worker Num:  3 \n",
            " Loss:  0.35727906227111816\n",
            "Training:  Iteration No:  206 Worker Num:  4 \n",
            " Loss:  0.24705588817596436\n",
            "Training:  Iteration No:  206 Worker Num:  5 \n",
            " Loss:  0.2282189130783081\n",
            "Training:  Iteration No:  206 Worker Num:  6 \n",
            " Loss:  0.26384955644607544\n",
            "Training:  Iteration No:  206 Worker Num:  7 \n",
            " Loss:  0.2338392585515976\n",
            "Test:  Iteration No:  206 \n",
            " Loss:  0.23941769903596444\n",
            "Test accuracy:  93.08\n",
            "Training:  Iteration No:  207 Worker Num:  0 \n",
            " Loss:  0.34079378843307495\n",
            "Training:  Iteration No:  207 Worker Num:  1 \n",
            " Loss:  0.39083167910575867\n",
            "Training:  Iteration No:  207 Worker Num:  2 \n",
            " Loss:  0.14354433119297028\n",
            "Training:  Iteration No:  207 Worker Num:  3 \n",
            " Loss:  0.19195999205112457\n",
            "Training:  Iteration No:  207 Worker Num:  4 \n",
            " Loss:  0.1839960664510727\n",
            "Training:  Iteration No:  207 Worker Num:  5 \n",
            " Loss:  0.20863577723503113\n",
            "Training:  Iteration No:  207 Worker Num:  6 \n",
            " Loss:  0.317346453666687\n",
            "Training:  Iteration No:  207 Worker Num:  7 \n",
            " Loss:  0.2584536671638489\n",
            "Test:  Iteration No:  207 \n",
            " Loss:  0.23448552320935304\n",
            "Test accuracy:  93.16\n",
            "Training:  Iteration No:  208 Worker Num:  0 \n",
            " Loss:  0.22443880140781403\n",
            "Training:  Iteration No:  208 Worker Num:  1 \n",
            " Loss:  0.3280261754989624\n",
            "Training:  Iteration No:  208 Worker Num:  2 \n",
            " Loss:  0.37271642684936523\n",
            "Training:  Iteration No:  208 Worker Num:  3 \n",
            " Loss:  0.2315346896648407\n",
            "Training:  Iteration No:  208 Worker Num:  4 \n",
            " Loss:  0.2603812515735626\n",
            "Training:  Iteration No:  208 Worker Num:  5 \n",
            " Loss:  0.25500407814979553\n",
            "Training:  Iteration No:  208 Worker Num:  6 \n",
            " Loss:  0.16581234335899353\n",
            "Training:  Iteration No:  208 Worker Num:  7 \n",
            " Loss:  0.3102455139160156\n",
            "Test:  Iteration No:  208 \n",
            " Loss:  0.234519498965031\n",
            "Test accuracy:  93.32\n",
            "Training:  Iteration No:  209 Worker Num:  0 \n",
            " Loss:  0.35086187720298767\n",
            "Training:  Iteration No:  209 Worker Num:  1 \n",
            " Loss:  0.2888520658016205\n",
            "Training:  Iteration No:  209 Worker Num:  2 \n",
            " Loss:  0.28610023856163025\n",
            "Training:  Iteration No:  209 Worker Num:  3 \n",
            " Loss:  0.2780858874320984\n",
            "Training:  Iteration No:  209 Worker Num:  4 \n",
            " Loss:  0.18577776849269867\n",
            "Training:  Iteration No:  209 Worker Num:  5 \n",
            " Loss:  0.4623180329799652\n",
            "Training:  Iteration No:  209 Worker Num:  6 \n",
            " Loss:  0.3885203003883362\n",
            "Training:  Iteration No:  209 Worker Num:  7 \n",
            " Loss:  0.28253424167633057\n",
            "Test:  Iteration No:  209 \n",
            " Loss:  0.2325962843679929\n",
            "Test accuracy:  93.4\n",
            "Training:  Iteration No:  210 Worker Num:  0 \n",
            " Loss:  0.26719868183135986\n",
            "Training:  Iteration No:  210 Worker Num:  1 \n",
            " Loss:  0.3448999524116516\n",
            "Training:  Iteration No:  210 Worker Num:  2 \n",
            " Loss:  0.4153675436973572\n",
            "Training:  Iteration No:  210 Worker Num:  3 \n",
            " Loss:  0.31130626797676086\n",
            "Training:  Iteration No:  210 Worker Num:  4 \n",
            " Loss:  0.2604342997074127\n",
            "Training:  Iteration No:  210 Worker Num:  5 \n",
            " Loss:  0.16324740648269653\n",
            "Training:  Iteration No:  210 Worker Num:  6 \n",
            " Loss:  0.28766554594039917\n",
            "Training:  Iteration No:  210 Worker Num:  7 \n",
            " Loss:  0.3974345624446869\n",
            "Test:  Iteration No:  210 \n",
            " Loss:  0.2350341049081917\n",
            "Test accuracy:  93.32\n",
            "Training:  Iteration No:  211 Worker Num:  0 \n",
            " Loss:  0.2218094766139984\n",
            "Training:  Iteration No:  211 Worker Num:  1 \n",
            " Loss:  0.2582937777042389\n",
            "Training:  Iteration No:  211 Worker Num:  2 \n",
            " Loss:  0.463954359292984\n",
            "Training:  Iteration No:  211 Worker Num:  3 \n",
            " Loss:  0.23950283229351044\n",
            "Training:  Iteration No:  211 Worker Num:  4 \n",
            " Loss:  0.1528625339269638\n",
            "Training:  Iteration No:  211 Worker Num:  5 \n",
            " Loss:  0.4533810615539551\n",
            "Training:  Iteration No:  211 Worker Num:  6 \n",
            " Loss:  0.30260610580444336\n",
            "Training:  Iteration No:  211 Worker Num:  7 \n",
            " Loss:  0.27707988023757935\n",
            "Test:  Iteration No:  211 \n",
            " Loss:  0.229900699886906\n",
            "Test accuracy:  93.55\n",
            "Training:  Iteration No:  212 Worker Num:  0 \n",
            " Loss:  0.2512647211551666\n",
            "Training:  Iteration No:  212 Worker Num:  1 \n",
            " Loss:  0.1971047818660736\n",
            "Training:  Iteration No:  212 Worker Num:  2 \n",
            " Loss:  0.26192203164100647\n",
            "Training:  Iteration No:  212 Worker Num:  3 \n",
            " Loss:  0.28722310066223145\n",
            "Training:  Iteration No:  212 Worker Num:  4 \n",
            " Loss:  0.2751760482788086\n",
            "Training:  Iteration No:  212 Worker Num:  5 \n",
            " Loss:  0.23508277535438538\n",
            "Training:  Iteration No:  212 Worker Num:  6 \n",
            " Loss:  0.3575957715511322\n",
            "Training:  Iteration No:  212 Worker Num:  7 \n",
            " Loss:  0.4003850519657135\n",
            "Test:  Iteration No:  212 \n",
            " Loss:  0.23257246231542358\n",
            "Test accuracy:  93.41\n",
            "Training:  Iteration No:  213 Worker Num:  0 \n",
            " Loss:  0.17547908425331116\n",
            "Training:  Iteration No:  213 Worker Num:  1 \n",
            " Loss:  0.26339560747146606\n",
            "Training:  Iteration No:  213 Worker Num:  2 \n",
            " Loss:  0.30026766657829285\n",
            "Training:  Iteration No:  213 Worker Num:  3 \n",
            " Loss:  0.23601239919662476\n",
            "Training:  Iteration No:  213 Worker Num:  4 \n",
            " Loss:  0.1526230424642563\n",
            "Training:  Iteration No:  213 Worker Num:  5 \n",
            " Loss:  0.3771525025367737\n",
            "Training:  Iteration No:  213 Worker Num:  6 \n",
            " Loss:  0.23208259046077728\n",
            "Training:  Iteration No:  213 Worker Num:  7 \n",
            " Loss:  0.3733738660812378\n",
            "Test:  Iteration No:  213 \n",
            " Loss:  0.2283156334151384\n",
            "Test accuracy:  93.45\n",
            "Training:  Iteration No:  214 Worker Num:  0 \n",
            " Loss:  0.28130099177360535\n",
            "Training:  Iteration No:  214 Worker Num:  1 \n",
            " Loss:  0.29999494552612305\n",
            "Training:  Iteration No:  214 Worker Num:  2 \n",
            " Loss:  0.31266114115715027\n",
            "Training:  Iteration No:  214 Worker Num:  3 \n",
            " Loss:  0.288079172372818\n",
            "Training:  Iteration No:  214 Worker Num:  4 \n",
            " Loss:  0.276784747838974\n",
            "Training:  Iteration No:  214 Worker Num:  5 \n",
            " Loss:  0.22814954817295074\n",
            "Training:  Iteration No:  214 Worker Num:  6 \n",
            " Loss:  0.24033960700035095\n",
            "Training:  Iteration No:  214 Worker Num:  7 \n",
            " Loss:  0.36386242508888245\n",
            "Test:  Iteration No:  214 \n",
            " Loss:  0.23080265727129917\n",
            "Test accuracy:  93.49\n",
            "Training:  Iteration No:  215 Worker Num:  0 \n",
            " Loss:  0.2714797258377075\n",
            "Training:  Iteration No:  215 Worker Num:  1 \n",
            " Loss:  0.23760569095611572\n",
            "Training:  Iteration No:  215 Worker Num:  2 \n",
            " Loss:  0.19711339473724365\n",
            "Training:  Iteration No:  215 Worker Num:  3 \n",
            " Loss:  0.34829673171043396\n",
            "Training:  Iteration No:  215 Worker Num:  4 \n",
            " Loss:  0.27288898825645447\n",
            "Training:  Iteration No:  215 Worker Num:  5 \n",
            " Loss:  0.30001091957092285\n",
            "Training:  Iteration No:  215 Worker Num:  6 \n",
            " Loss:  0.23092608153820038\n",
            "Training:  Iteration No:  215 Worker Num:  7 \n",
            " Loss:  0.23991578817367554\n",
            "Test:  Iteration No:  215 \n",
            " Loss:  0.2251505701790882\n",
            "Test accuracy:  93.39\n",
            "Training:  Iteration No:  216 Worker Num:  0 \n",
            " Loss:  0.32533782720565796\n",
            "Training:  Iteration No:  216 Worker Num:  1 \n",
            " Loss:  0.32226717472076416\n",
            "Training:  Iteration No:  216 Worker Num:  2 \n",
            " Loss:  0.35460755228996277\n",
            "Training:  Iteration No:  216 Worker Num:  3 \n",
            " Loss:  0.1811697781085968\n",
            "Training:  Iteration No:  216 Worker Num:  4 \n",
            " Loss:  0.4117172360420227\n",
            "Training:  Iteration No:  216 Worker Num:  5 \n",
            " Loss:  0.31456536054611206\n",
            "Training:  Iteration No:  216 Worker Num:  6 \n",
            " Loss:  0.15162162482738495\n",
            "Training:  Iteration No:  216 Worker Num:  7 \n",
            " Loss:  0.28622114658355713\n",
            "Test:  Iteration No:  216 \n",
            " Loss:  0.22522120736539364\n",
            "Test accuracy:  93.5\n",
            "Training:  Iteration No:  217 Worker Num:  0 \n",
            " Loss:  0.1532973200082779\n",
            "Training:  Iteration No:  217 Worker Num:  1 \n",
            " Loss:  0.29104387760162354\n",
            "Training:  Iteration No:  217 Worker Num:  2 \n",
            " Loss:  0.2767060101032257\n",
            "Training:  Iteration No:  217 Worker Num:  3 \n",
            " Loss:  0.2909030616283417\n",
            "Training:  Iteration No:  217 Worker Num:  4 \n",
            " Loss:  0.2797499895095825\n",
            "Training:  Iteration No:  217 Worker Num:  5 \n",
            " Loss:  0.29105040431022644\n",
            "Training:  Iteration No:  217 Worker Num:  6 \n",
            " Loss:  0.3761841058731079\n",
            "Training:  Iteration No:  217 Worker Num:  7 \n",
            " Loss:  0.2558123767375946\n",
            "Test:  Iteration No:  217 \n",
            " Loss:  0.2264730526676661\n",
            "Test accuracy:  93.34\n",
            "Training:  Iteration No:  218 Worker Num:  0 \n",
            " Loss:  0.27978193759918213\n",
            "Training:  Iteration No:  218 Worker Num:  1 \n",
            " Loss:  0.19598762691020966\n",
            "Training:  Iteration No:  218 Worker Num:  2 \n",
            " Loss:  0.30169937014579773\n",
            "Training:  Iteration No:  218 Worker Num:  3 \n",
            " Loss:  0.27544817328453064\n",
            "Training:  Iteration No:  218 Worker Num:  4 \n",
            " Loss:  0.3322761058807373\n",
            "Training:  Iteration No:  218 Worker Num:  5 \n",
            " Loss:  0.18186341226100922\n",
            "Training:  Iteration No:  218 Worker Num:  6 \n",
            " Loss:  0.33480459451675415\n",
            "Training:  Iteration No:  218 Worker Num:  7 \n",
            " Loss:  0.239498108625412\n",
            "Test:  Iteration No:  218 \n",
            " Loss:  0.22519866485573067\n",
            "Test accuracy:  93.45\n",
            "Training:  Iteration No:  219 Worker Num:  0 \n",
            " Loss:  0.1898701786994934\n",
            "Training:  Iteration No:  219 Worker Num:  1 \n",
            " Loss:  0.26537230610847473\n",
            "Training:  Iteration No:  219 Worker Num:  2 \n",
            " Loss:  0.17359957098960876\n",
            "Training:  Iteration No:  219 Worker Num:  3 \n",
            " Loss:  0.23331588506698608\n",
            "Training:  Iteration No:  219 Worker Num:  4 \n",
            " Loss:  0.314307302236557\n",
            "Training:  Iteration No:  219 Worker Num:  5 \n",
            " Loss:  0.24399612843990326\n",
            "Training:  Iteration No:  219 Worker Num:  6 \n",
            " Loss:  0.31805646419525146\n",
            "Training:  Iteration No:  219 Worker Num:  7 \n",
            " Loss:  0.2172429859638214\n",
            "Test:  Iteration No:  219 \n",
            " Loss:  0.2219840154004625\n",
            "Test accuracy:  93.61\n",
            "Training:  Iteration No:  220 Worker Num:  0 \n",
            " Loss:  0.28317150473594666\n",
            "Training:  Iteration No:  220 Worker Num:  1 \n",
            " Loss:  0.26121291518211365\n",
            "Training:  Iteration No:  220 Worker Num:  2 \n",
            " Loss:  0.19224461913108826\n",
            "Training:  Iteration No:  220 Worker Num:  3 \n",
            " Loss:  0.2770538926124573\n",
            "Training:  Iteration No:  220 Worker Num:  4 \n",
            " Loss:  0.3287287652492523\n",
            "Training:  Iteration No:  220 Worker Num:  5 \n",
            " Loss:  0.30749520659446716\n",
            "Training:  Iteration No:  220 Worker Num:  6 \n",
            " Loss:  0.28045713901519775\n",
            "Training:  Iteration No:  220 Worker Num:  7 \n",
            " Loss:  0.17424435913562775\n",
            "Test:  Iteration No:  220 \n",
            " Loss:  0.22233232443185547\n",
            "Test accuracy:  93.52\n",
            "Training:  Iteration No:  221 Worker Num:  0 \n",
            " Loss:  0.24495051801204681\n",
            "Training:  Iteration No:  221 Worker Num:  1 \n",
            " Loss:  0.30345863103866577\n",
            "Training:  Iteration No:  221 Worker Num:  2 \n",
            " Loss:  0.2462891936302185\n",
            "Training:  Iteration No:  221 Worker Num:  3 \n",
            " Loss:  0.18049730360507965\n",
            "Training:  Iteration No:  221 Worker Num:  4 \n",
            " Loss:  0.30578017234802246\n",
            "Training:  Iteration No:  221 Worker Num:  5 \n",
            " Loss:  0.35136741399765015\n",
            "Training:  Iteration No:  221 Worker Num:  6 \n",
            " Loss:  0.3093971014022827\n",
            "Training:  Iteration No:  221 Worker Num:  7 \n",
            " Loss:  0.29611024260520935\n",
            "Test:  Iteration No:  221 \n",
            " Loss:  0.22165349724737904\n",
            "Test accuracy:  93.71\n",
            "Training:  Iteration No:  222 Worker Num:  0 \n",
            " Loss:  0.25113368034362793\n",
            "Training:  Iteration No:  222 Worker Num:  1 \n",
            " Loss:  0.21734505891799927\n",
            "Training:  Iteration No:  222 Worker Num:  2 \n",
            " Loss:  0.2737257182598114\n",
            "Training:  Iteration No:  222 Worker Num:  3 \n",
            " Loss:  0.20409998297691345\n",
            "Training:  Iteration No:  222 Worker Num:  4 \n",
            " Loss:  0.26674196124076843\n",
            "Training:  Iteration No:  222 Worker Num:  5 \n",
            " Loss:  0.19072702527046204\n",
            "Training:  Iteration No:  222 Worker Num:  6 \n",
            " Loss:  0.3585318326950073\n",
            "Training:  Iteration No:  222 Worker Num:  7 \n",
            " Loss:  0.17252016067504883\n",
            "Test:  Iteration No:  222 \n",
            " Loss:  0.22014986149542318\n",
            "Test accuracy:  93.71\n",
            "Training:  Iteration No:  223 Worker Num:  0 \n",
            " Loss:  0.3346025347709656\n",
            "Training:  Iteration No:  223 Worker Num:  1 \n",
            " Loss:  0.37341776490211487\n",
            "Training:  Iteration No:  223 Worker Num:  2 \n",
            " Loss:  0.25320199131965637\n",
            "Training:  Iteration No:  223 Worker Num:  3 \n",
            " Loss:  0.1265125721693039\n",
            "Training:  Iteration No:  223 Worker Num:  4 \n",
            " Loss:  0.33534473180770874\n",
            "Training:  Iteration No:  223 Worker Num:  5 \n",
            " Loss:  0.20211544632911682\n",
            "Training:  Iteration No:  223 Worker Num:  6 \n",
            " Loss:  0.13484124839305878\n",
            "Training:  Iteration No:  223 Worker Num:  7 \n",
            " Loss:  0.35214629769325256\n",
            "Test:  Iteration No:  223 \n",
            " Loss:  0.21865264083482797\n",
            "Test accuracy:  93.65\n",
            "Training:  Iteration No:  224 Worker Num:  0 \n",
            " Loss:  0.21948085725307465\n",
            "Training:  Iteration No:  224 Worker Num:  1 \n",
            " Loss:  0.26295262575149536\n",
            "Training:  Iteration No:  224 Worker Num:  2 \n",
            " Loss:  0.22066572308540344\n",
            "Training:  Iteration No:  224 Worker Num:  3 \n",
            " Loss:  0.37005341053009033\n",
            "Training:  Iteration No:  224 Worker Num:  4 \n",
            " Loss:  0.2906067371368408\n",
            "Training:  Iteration No:  224 Worker Num:  5 \n",
            " Loss:  0.4018040597438812\n",
            "Training:  Iteration No:  224 Worker Num:  6 \n",
            " Loss:  0.2600021958351135\n",
            "Training:  Iteration No:  224 Worker Num:  7 \n",
            " Loss:  0.2573600709438324\n",
            "Test:  Iteration No:  224 \n",
            " Loss:  0.21981452727289516\n",
            "Test accuracy:  93.52\n",
            "Training:  Iteration No:  225 Worker Num:  0 \n",
            " Loss:  0.32920655608177185\n",
            "Training:  Iteration No:  225 Worker Num:  1 \n",
            " Loss:  0.24635136127471924\n",
            "Training:  Iteration No:  225 Worker Num:  2 \n",
            " Loss:  0.2660418152809143\n",
            "Training:  Iteration No:  225 Worker Num:  3 \n",
            " Loss:  0.18565131723880768\n",
            "Training:  Iteration No:  225 Worker Num:  4 \n",
            " Loss:  0.27366864681243896\n",
            "Training:  Iteration No:  225 Worker Num:  5 \n",
            " Loss:  0.16356796026229858\n",
            "Training:  Iteration No:  225 Worker Num:  6 \n",
            " Loss:  0.2941073477268219\n",
            "Training:  Iteration No:  225 Worker Num:  7 \n",
            " Loss:  0.23361122608184814\n",
            "Test:  Iteration No:  225 \n",
            " Loss:  0.21856676167042194\n",
            "Test accuracy:  93.51\n",
            "Training:  Iteration No:  226 Worker Num:  0 \n",
            " Loss:  0.17167486250400543\n",
            "Training:  Iteration No:  226 Worker Num:  1 \n",
            " Loss:  0.2256542146205902\n",
            "Training:  Iteration No:  226 Worker Num:  2 \n",
            " Loss:  0.3163941204547882\n",
            "Training:  Iteration No:  226 Worker Num:  3 \n",
            " Loss:  0.31758737564086914\n",
            "Training:  Iteration No:  226 Worker Num:  4 \n",
            " Loss:  0.2672814726829529\n",
            "Training:  Iteration No:  226 Worker Num:  5 \n",
            " Loss:  0.1887955665588379\n",
            "Training:  Iteration No:  226 Worker Num:  6 \n",
            " Loss:  0.15596900880336761\n",
            "Training:  Iteration No:  226 Worker Num:  7 \n",
            " Loss:  0.19925493001937866\n",
            "Test:  Iteration No:  226 \n",
            " Loss:  0.21996730728711508\n",
            "Test accuracy:  93.47\n",
            "Training:  Iteration No:  227 Worker Num:  0 \n",
            " Loss:  0.21052050590515137\n",
            "Training:  Iteration No:  227 Worker Num:  1 \n",
            " Loss:  0.38207143545150757\n",
            "Training:  Iteration No:  227 Worker Num:  2 \n",
            " Loss:  0.43418604135513306\n",
            "Training:  Iteration No:  227 Worker Num:  3 \n",
            " Loss:  0.16826865077018738\n",
            "Training:  Iteration No:  227 Worker Num:  4 \n",
            " Loss:  0.3804188668727875\n",
            "Training:  Iteration No:  227 Worker Num:  5 \n",
            " Loss:  0.3439311683177948\n",
            "Training:  Iteration No:  227 Worker Num:  6 \n",
            " Loss:  0.20183376967906952\n",
            "Training:  Iteration No:  227 Worker Num:  7 \n",
            " Loss:  0.31794556975364685\n",
            "Test:  Iteration No:  227 \n",
            " Loss:  0.2199761704718576\n",
            "Test accuracy:  93.53\n",
            "Training:  Iteration No:  228 Worker Num:  0 \n",
            " Loss:  0.2476212978363037\n",
            "Training:  Iteration No:  228 Worker Num:  1 \n",
            " Loss:  0.2711791396141052\n",
            "Training:  Iteration No:  228 Worker Num:  2 \n",
            " Loss:  0.24790441989898682\n",
            "Training:  Iteration No:  228 Worker Num:  3 \n",
            " Loss:  0.28792595863342285\n",
            "Training:  Iteration No:  228 Worker Num:  4 \n",
            " Loss:  0.2996824383735657\n",
            "Training:  Iteration No:  228 Worker Num:  5 \n",
            " Loss:  0.2804073393344879\n",
            "Training:  Iteration No:  228 Worker Num:  6 \n",
            " Loss:  0.24336817860603333\n",
            "Training:  Iteration No:  228 Worker Num:  7 \n",
            " Loss:  0.20698407292366028\n",
            "Test:  Iteration No:  228 \n",
            " Loss:  0.21389894535222762\n",
            "Test accuracy:  93.86\n",
            "Training:  Iteration No:  229 Worker Num:  0 \n",
            " Loss:  0.27496397495269775\n",
            "Training:  Iteration No:  229 Worker Num:  1 \n",
            " Loss:  0.33277690410614014\n",
            "Training:  Iteration No:  229 Worker Num:  2 \n",
            " Loss:  0.311808317899704\n",
            "Training:  Iteration No:  229 Worker Num:  3 \n",
            " Loss:  0.3094693720340729\n",
            "Training:  Iteration No:  229 Worker Num:  4 \n",
            " Loss:  0.3246121406555176\n",
            "Training:  Iteration No:  229 Worker Num:  5 \n",
            " Loss:  0.2906879782676697\n",
            "Training:  Iteration No:  229 Worker Num:  6 \n",
            " Loss:  0.21818776428699493\n",
            "Training:  Iteration No:  229 Worker Num:  7 \n",
            " Loss:  0.26909148693084717\n",
            "Test:  Iteration No:  229 \n",
            " Loss:  0.21471152698644733\n",
            "Test accuracy:  93.76\n",
            "Training:  Iteration No:  230 Worker Num:  0 \n",
            " Loss:  0.3295539915561676\n",
            "Training:  Iteration No:  230 Worker Num:  1 \n",
            " Loss:  0.2502879798412323\n",
            "Training:  Iteration No:  230 Worker Num:  2 \n",
            " Loss:  0.3004911541938782\n",
            "Training:  Iteration No:  230 Worker Num:  3 \n",
            " Loss:  0.21554094552993774\n",
            "Training:  Iteration No:  230 Worker Num:  4 \n",
            " Loss:  0.22460907697677612\n",
            "Training:  Iteration No:  230 Worker Num:  5 \n",
            " Loss:  0.4139569103717804\n",
            "Training:  Iteration No:  230 Worker Num:  6 \n",
            " Loss:  0.248702272772789\n",
            "Training:  Iteration No:  230 Worker Num:  7 \n",
            " Loss:  0.18327674269676208\n",
            "Test:  Iteration No:  230 \n",
            " Loss:  0.2144017584645484\n",
            "Test accuracy:  93.75\n",
            "Training:  Iteration No:  231 Worker Num:  0 \n",
            " Loss:  0.25295501947402954\n",
            "Training:  Iteration No:  231 Worker Num:  1 \n",
            " Loss:  0.24915502965450287\n",
            "Training:  Iteration No:  231 Worker Num:  2 \n",
            " Loss:  0.3854731619358063\n",
            "Training:  Iteration No:  231 Worker Num:  3 \n",
            " Loss:  0.3306407928466797\n",
            "Training:  Iteration No:  231 Worker Num:  4 \n",
            " Loss:  0.2974447011947632\n",
            "Training:  Iteration No:  231 Worker Num:  5 \n",
            " Loss:  0.15032903850078583\n",
            "Training:  Iteration No:  231 Worker Num:  6 \n",
            " Loss:  0.29200664162635803\n",
            "Training:  Iteration No:  231 Worker Num:  7 \n",
            " Loss:  0.3080998957157135\n",
            "Test:  Iteration No:  231 \n",
            " Loss:  0.21404512383373855\n",
            "Test accuracy:  93.63\n",
            "Training:  Iteration No:  232 Worker Num:  0 \n",
            " Loss:  0.2819836139678955\n",
            "Training:  Iteration No:  232 Worker Num:  1 \n",
            " Loss:  0.2857876121997833\n",
            "Training:  Iteration No:  232 Worker Num:  2 \n",
            " Loss:  0.2453291267156601\n",
            "Training:  Iteration No:  232 Worker Num:  3 \n",
            " Loss:  0.21564625203609467\n",
            "Training:  Iteration No:  232 Worker Num:  4 \n",
            " Loss:  0.17358070611953735\n",
            "Training:  Iteration No:  232 Worker Num:  5 \n",
            " Loss:  0.2595793306827545\n",
            "Training:  Iteration No:  232 Worker Num:  6 \n",
            " Loss:  0.3687710762023926\n",
            "Training:  Iteration No:  232 Worker Num:  7 \n",
            " Loss:  0.31276822090148926\n",
            "Test:  Iteration No:  232 \n",
            " Loss:  0.2133216282022716\n",
            "Test accuracy:  93.94\n",
            "Training:  Iteration No:  233 Worker Num:  0 \n",
            " Loss:  0.40847861766815186\n",
            "Training:  Iteration No:  233 Worker Num:  1 \n",
            " Loss:  0.21311454474925995\n",
            "Training:  Iteration No:  233 Worker Num:  2 \n",
            " Loss:  0.2543206214904785\n",
            "Training:  Iteration No:  233 Worker Num:  3 \n",
            " Loss:  0.24701949954032898\n",
            "Training:  Iteration No:  233 Worker Num:  4 \n",
            " Loss:  0.24285981059074402\n",
            "Training:  Iteration No:  233 Worker Num:  5 \n",
            " Loss:  0.24005761742591858\n",
            "Training:  Iteration No:  233 Worker Num:  6 \n",
            " Loss:  0.2261599451303482\n",
            "Training:  Iteration No:  233 Worker Num:  7 \n",
            " Loss:  0.3214477002620697\n",
            "Test:  Iteration No:  233 \n",
            " Loss:  0.21420981169124192\n",
            "Test accuracy:  93.78\n",
            "Training:  Iteration No:  234 Worker Num:  0 \n",
            " Loss:  0.24716432392597198\n",
            "Training:  Iteration No:  234 Worker Num:  1 \n",
            " Loss:  0.23750054836273193\n",
            "Training:  Iteration No:  234 Worker Num:  2 \n",
            " Loss:  0.3605368137359619\n",
            "Training:  Iteration No:  234 Worker Num:  3 \n",
            " Loss:  0.3290089964866638\n",
            "Training:  Iteration No:  234 Worker Num:  4 \n",
            " Loss:  0.2310386449098587\n",
            "Training:  Iteration No:  234 Worker Num:  5 \n",
            " Loss:  0.2808064818382263\n",
            "Training:  Iteration No:  234 Worker Num:  6 \n",
            " Loss:  0.3210514187812805\n",
            "Training:  Iteration No:  234 Worker Num:  7 \n",
            " Loss:  0.3225942552089691\n",
            "Test:  Iteration No:  234 \n",
            " Loss:  0.21212327407225023\n",
            "Test accuracy:  93.83\n",
            "Training:  Iteration No:  235 Worker Num:  0 \n",
            " Loss:  0.23899947106838226\n",
            "Training:  Iteration No:  235 Worker Num:  1 \n",
            " Loss:  0.2862612307071686\n",
            "Training:  Iteration No:  235 Worker Num:  2 \n",
            " Loss:  0.2159307450056076\n",
            "Training:  Iteration No:  235 Worker Num:  3 \n",
            " Loss:  0.2538716495037079\n",
            "Training:  Iteration No:  235 Worker Num:  4 \n",
            " Loss:  0.20580880343914032\n",
            "Training:  Iteration No:  235 Worker Num:  5 \n",
            " Loss:  0.1292532980442047\n",
            "Training:  Iteration No:  235 Worker Num:  6 \n",
            " Loss:  0.3293200433254242\n",
            "Training:  Iteration No:  235 Worker Num:  7 \n",
            " Loss:  0.36390626430511475\n",
            "Test:  Iteration No:  235 \n",
            " Loss:  0.20870430712128366\n",
            "Test accuracy:  93.92\n",
            "Training:  Iteration No:  236 Worker Num:  0 \n",
            " Loss:  0.28752875328063965\n",
            "Training:  Iteration No:  236 Worker Num:  1 \n",
            " Loss:  0.37540847063064575\n",
            "Training:  Iteration No:  236 Worker Num:  2 \n",
            " Loss:  0.2542623281478882\n",
            "Training:  Iteration No:  236 Worker Num:  3 \n",
            " Loss:  0.15203851461410522\n",
            "Training:  Iteration No:  236 Worker Num:  4 \n",
            " Loss:  0.4115593433380127\n",
            "Training:  Iteration No:  236 Worker Num:  5 \n",
            " Loss:  0.285068154335022\n",
            "Training:  Iteration No:  236 Worker Num:  6 \n",
            " Loss:  0.1692216843366623\n",
            "Training:  Iteration No:  236 Worker Num:  7 \n",
            " Loss:  0.32143548130989075\n",
            "Test:  Iteration No:  236 \n",
            " Loss:  0.21029127297239214\n",
            "Test accuracy:  93.93\n",
            "Training:  Iteration No:  237 Worker Num:  0 \n",
            " Loss:  0.21350064873695374\n",
            "Training:  Iteration No:  237 Worker Num:  1 \n",
            " Loss:  0.27108049392700195\n",
            "Training:  Iteration No:  237 Worker Num:  2 \n",
            " Loss:  0.15782740712165833\n",
            "Training:  Iteration No:  237 Worker Num:  3 \n",
            " Loss:  0.3911503553390503\n",
            "Training:  Iteration No:  237 Worker Num:  4 \n",
            " Loss:  0.16444344818592072\n",
            "Training:  Iteration No:  237 Worker Num:  5 \n",
            " Loss:  0.30494824051856995\n",
            "Training:  Iteration No:  237 Worker Num:  6 \n",
            " Loss:  0.4037163257598877\n",
            "Training:  Iteration No:  237 Worker Num:  7 \n",
            " Loss:  0.21646814048290253\n",
            "Test:  Iteration No:  237 \n",
            " Loss:  0.2092031695750318\n",
            "Test accuracy:  93.87\n",
            "Training:  Iteration No:  238 Worker Num:  0 \n",
            " Loss:  0.22675122320652008\n",
            "Training:  Iteration No:  238 Worker Num:  1 \n",
            " Loss:  0.2716284692287445\n",
            "Training:  Iteration No:  238 Worker Num:  2 \n",
            " Loss:  0.2981295585632324\n",
            "Training:  Iteration No:  238 Worker Num:  3 \n",
            " Loss:  0.2792840600013733\n",
            "Training:  Iteration No:  238 Worker Num:  4 \n",
            " Loss:  0.21099181473255157\n",
            "Training:  Iteration No:  238 Worker Num:  5 \n",
            " Loss:  0.26589757204055786\n",
            "Training:  Iteration No:  238 Worker Num:  6 \n",
            " Loss:  0.18748657405376434\n",
            "Training:  Iteration No:  238 Worker Num:  7 \n",
            " Loss:  0.34825950860977173\n",
            "Test:  Iteration No:  238 \n",
            " Loss:  0.21118110410067475\n",
            "Test accuracy:  94.05\n",
            "Training:  Iteration No:  239 Worker Num:  0 \n",
            " Loss:  0.22606782615184784\n",
            "Training:  Iteration No:  239 Worker Num:  1 \n",
            " Loss:  0.2906581163406372\n",
            "Training:  Iteration No:  239 Worker Num:  2 \n",
            " Loss:  0.35181868076324463\n",
            "Training:  Iteration No:  239 Worker Num:  3 \n",
            " Loss:  0.22093188762664795\n",
            "Training:  Iteration No:  239 Worker Num:  4 \n",
            " Loss:  0.239420086145401\n",
            "Training:  Iteration No:  239 Worker Num:  5 \n",
            " Loss:  0.13708727061748505\n",
            "Training:  Iteration No:  239 Worker Num:  6 \n",
            " Loss:  0.22967395186424255\n",
            "Training:  Iteration No:  239 Worker Num:  7 \n",
            " Loss:  0.20479944348335266\n",
            "Test:  Iteration No:  239 \n",
            " Loss:  0.2073253052238422\n",
            "Test accuracy:  94.01\n",
            "Training:  Iteration No:  240 Worker Num:  0 \n",
            " Loss:  0.3623345196247101\n",
            "Training:  Iteration No:  240 Worker Num:  1 \n",
            " Loss:  0.17802733182907104\n",
            "Training:  Iteration No:  240 Worker Num:  2 \n",
            " Loss:  0.31141233444213867\n",
            "Training:  Iteration No:  240 Worker Num:  3 \n",
            " Loss:  0.22070284187793732\n",
            "Training:  Iteration No:  240 Worker Num:  4 \n",
            " Loss:  0.25990989804267883\n",
            "Training:  Iteration No:  240 Worker Num:  5 \n",
            " Loss:  0.17410808801651\n",
            "Training:  Iteration No:  240 Worker Num:  6 \n",
            " Loss:  0.20226562023162842\n",
            "Training:  Iteration No:  240 Worker Num:  7 \n",
            " Loss:  0.26738250255584717\n",
            "Test:  Iteration No:  240 \n",
            " Loss:  0.20560541268013702\n",
            "Test accuracy:  94.0\n",
            "Training:  Iteration No:  241 Worker Num:  0 \n",
            " Loss:  0.302933007478714\n",
            "Training:  Iteration No:  241 Worker Num:  1 \n",
            " Loss:  0.2093488872051239\n",
            "Training:  Iteration No:  241 Worker Num:  2 \n",
            " Loss:  0.25145235657691956\n",
            "Training:  Iteration No:  241 Worker Num:  3 \n",
            " Loss:  0.317876398563385\n",
            "Training:  Iteration No:  241 Worker Num:  4 \n",
            " Loss:  0.18260447680950165\n",
            "Training:  Iteration No:  241 Worker Num:  5 \n",
            " Loss:  0.34957754611968994\n",
            "Training:  Iteration No:  241 Worker Num:  6 \n",
            " Loss:  0.3776906430721283\n",
            "Training:  Iteration No:  241 Worker Num:  7 \n",
            " Loss:  0.22791413962841034\n",
            "Test:  Iteration No:  241 \n",
            " Loss:  0.20608220708002395\n",
            "Test accuracy:  94.02\n",
            "Training:  Iteration No:  242 Worker Num:  0 \n",
            " Loss:  0.22817359864711761\n",
            "Training:  Iteration No:  242 Worker Num:  1 \n",
            " Loss:  0.3066890239715576\n",
            "Training:  Iteration No:  242 Worker Num:  2 \n",
            " Loss:  0.23061563074588776\n",
            "Training:  Iteration No:  242 Worker Num:  3 \n",
            " Loss:  0.25602155923843384\n",
            "Training:  Iteration No:  242 Worker Num:  4 \n",
            " Loss:  0.3743537962436676\n",
            "Training:  Iteration No:  242 Worker Num:  5 \n",
            " Loss:  0.18019315600395203\n",
            "Training:  Iteration No:  242 Worker Num:  6 \n",
            " Loss:  0.2741520404815674\n",
            "Training:  Iteration No:  242 Worker Num:  7 \n",
            " Loss:  0.31344953179359436\n",
            "Test:  Iteration No:  242 \n",
            " Loss:  0.20717150227555745\n",
            "Test accuracy:  93.91\n",
            "Training:  Iteration No:  243 Worker Num:  0 \n",
            " Loss:  0.30710384249687195\n",
            "Training:  Iteration No:  243 Worker Num:  1 \n",
            " Loss:  0.2858208417892456\n",
            "Training:  Iteration No:  243 Worker Num:  2 \n",
            " Loss:  0.19417887926101685\n",
            "Training:  Iteration No:  243 Worker Num:  3 \n",
            " Loss:  0.2532128095626831\n",
            "Training:  Iteration No:  243 Worker Num:  4 \n",
            " Loss:  0.3225526809692383\n",
            "Training:  Iteration No:  243 Worker Num:  5 \n",
            " Loss:  0.1641533374786377\n",
            "Training:  Iteration No:  243 Worker Num:  6 \n",
            " Loss:  0.23123735189437866\n",
            "Training:  Iteration No:  243 Worker Num:  7 \n",
            " Loss:  0.15443238615989685\n",
            "Test:  Iteration No:  243 \n",
            " Loss:  0.21025393813659873\n",
            "Test accuracy:  93.76\n",
            "Training:  Iteration No:  244 Worker Num:  0 \n",
            " Loss:  0.15927764773368835\n",
            "Training:  Iteration No:  244 Worker Num:  1 \n",
            " Loss:  0.36815008521080017\n",
            "Training:  Iteration No:  244 Worker Num:  2 \n",
            " Loss:  0.2206861525774002\n",
            "Training:  Iteration No:  244 Worker Num:  3 \n",
            " Loss:  0.26520249247550964\n",
            "Training:  Iteration No:  244 Worker Num:  4 \n",
            " Loss:  0.250274658203125\n",
            "Training:  Iteration No:  244 Worker Num:  5 \n",
            " Loss:  0.23298987746238708\n",
            "Training:  Iteration No:  244 Worker Num:  6 \n",
            " Loss:  0.18414084613323212\n",
            "Training:  Iteration No:  244 Worker Num:  7 \n",
            " Loss:  0.3000198006629944\n",
            "Test:  Iteration No:  244 \n",
            " Loss:  0.20196187464355292\n",
            "Test accuracy:  94.14\n",
            "Training:  Iteration No:  245 Worker Num:  0 \n",
            " Loss:  0.3255099952220917\n",
            "Training:  Iteration No:  245 Worker Num:  1 \n",
            " Loss:  0.24928206205368042\n",
            "Training:  Iteration No:  245 Worker Num:  2 \n",
            " Loss:  0.21656255424022675\n",
            "Training:  Iteration No:  245 Worker Num:  3 \n",
            " Loss:  0.3705889582633972\n",
            "Training:  Iteration No:  245 Worker Num:  4 \n",
            " Loss:  0.18728598952293396\n",
            "Training:  Iteration No:  245 Worker Num:  5 \n",
            " Loss:  0.2326354831457138\n",
            "Training:  Iteration No:  245 Worker Num:  6 \n",
            " Loss:  0.26420924067497253\n",
            "Training:  Iteration No:  245 Worker Num:  7 \n",
            " Loss:  0.19262127578258514\n",
            "Test:  Iteration No:  245 \n",
            " Loss:  0.20479440176270053\n",
            "Test accuracy:  94.13\n",
            "Training:  Iteration No:  246 Worker Num:  0 \n",
            " Loss:  0.22177985310554504\n",
            "Training:  Iteration No:  246 Worker Num:  1 \n",
            " Loss:  0.29766014218330383\n",
            "Training:  Iteration No:  246 Worker Num:  2 \n",
            " Loss:  0.243820920586586\n",
            "Training:  Iteration No:  246 Worker Num:  3 \n",
            " Loss:  0.16813698410987854\n",
            "Training:  Iteration No:  246 Worker Num:  4 \n",
            " Loss:  0.33287808299064636\n",
            "Training:  Iteration No:  246 Worker Num:  5 \n",
            " Loss:  0.3021746575832367\n",
            "Training:  Iteration No:  246 Worker Num:  6 \n",
            " Loss:  0.2637062668800354\n",
            "Training:  Iteration No:  246 Worker Num:  7 \n",
            " Loss:  0.26111122965812683\n",
            "Test:  Iteration No:  246 \n",
            " Loss:  0.2018382017017354\n",
            "Test accuracy:  94.26\n",
            "Training:  Iteration No:  247 Worker Num:  0 \n",
            " Loss:  0.2257293313741684\n",
            "Training:  Iteration No:  247 Worker Num:  1 \n",
            " Loss:  0.24854128062725067\n",
            "Training:  Iteration No:  247 Worker Num:  2 \n",
            " Loss:  0.17420101165771484\n",
            "Training:  Iteration No:  247 Worker Num:  3 \n",
            " Loss:  0.2952243685722351\n",
            "Training:  Iteration No:  247 Worker Num:  4 \n",
            " Loss:  0.25392815470695496\n",
            "Training:  Iteration No:  247 Worker Num:  5 \n",
            " Loss:  0.15403331816196442\n",
            "Training:  Iteration No:  247 Worker Num:  6 \n",
            " Loss:  0.23147502541542053\n",
            "Training:  Iteration No:  247 Worker Num:  7 \n",
            " Loss:  0.15474143624305725\n",
            "Test:  Iteration No:  247 \n",
            " Loss:  0.21097600669894792\n",
            "Test accuracy:  93.73\n",
            "Training:  Iteration No:  248 Worker Num:  0 \n",
            " Loss:  0.29138684272766113\n",
            "Training:  Iteration No:  248 Worker Num:  1 \n",
            " Loss:  0.2033163458108902\n",
            "Training:  Iteration No:  248 Worker Num:  2 \n",
            " Loss:  0.3176637589931488\n",
            "Training:  Iteration No:  248 Worker Num:  3 \n",
            " Loss:  0.208973228931427\n",
            "Training:  Iteration No:  248 Worker Num:  4 \n",
            " Loss:  0.20646096765995026\n",
            "Training:  Iteration No:  248 Worker Num:  5 \n",
            " Loss:  0.32670658826828003\n",
            "Training:  Iteration No:  248 Worker Num:  6 \n",
            " Loss:  0.2322734147310257\n",
            "Training:  Iteration No:  248 Worker Num:  7 \n",
            " Loss:  0.22140595316886902\n",
            "Test:  Iteration No:  248 \n",
            " Loss:  0.2022687473770561\n",
            "Test accuracy:  94.14\n",
            "Training:  Iteration No:  249 Worker Num:  0 \n",
            " Loss:  0.22561036050319672\n",
            "Training:  Iteration No:  249 Worker Num:  1 \n",
            " Loss:  0.29363787174224854\n",
            "Training:  Iteration No:  249 Worker Num:  2 \n",
            " Loss:  0.20384374260902405\n",
            "Training:  Iteration No:  249 Worker Num:  3 \n",
            " Loss:  0.414287269115448\n",
            "Training:  Iteration No:  249 Worker Num:  4 \n",
            " Loss:  0.25666654109954834\n",
            "Training:  Iteration No:  249 Worker Num:  5 \n",
            " Loss:  0.2199617624282837\n",
            "Training:  Iteration No:  249 Worker Num:  6 \n",
            " Loss:  0.29518458247184753\n",
            "Training:  Iteration No:  249 Worker Num:  7 \n",
            " Loss:  0.1337340772151947\n",
            "Test:  Iteration No:  249 \n",
            " Loss:  0.20448032125264784\n",
            "Test accuracy:  93.83\n",
            "Training:  Iteration No:  250 Worker Num:  0 \n",
            " Loss:  0.307766318321228\n",
            "Training:  Iteration No:  250 Worker Num:  1 \n",
            " Loss:  0.25897330045700073\n",
            "Training:  Iteration No:  250 Worker Num:  2 \n",
            " Loss:  0.3707979619503021\n",
            "Training:  Iteration No:  250 Worker Num:  3 \n",
            " Loss:  0.24191777408123016\n",
            "Training:  Iteration No:  250 Worker Num:  4 \n",
            " Loss:  0.18634188175201416\n",
            "Training:  Iteration No:  250 Worker Num:  5 \n",
            " Loss:  0.18849056959152222\n",
            "Training:  Iteration No:  250 Worker Num:  6 \n",
            " Loss:  0.2511281669139862\n",
            "Training:  Iteration No:  250 Worker Num:  7 \n",
            " Loss:  0.1815803200006485\n",
            "Test:  Iteration No:  250 \n",
            " Loss:  0.20455996424572753\n",
            "Test accuracy:  94.08\n",
            "Training:  Iteration No:  251 Worker Num:  0 \n",
            " Loss:  0.12369637191295624\n",
            "Training:  Iteration No:  251 Worker Num:  1 \n",
            " Loss:  0.19193828105926514\n",
            "Training:  Iteration No:  251 Worker Num:  2 \n",
            " Loss:  0.265160471200943\n",
            "Training:  Iteration No:  251 Worker Num:  3 \n",
            " Loss:  0.19418169558048248\n",
            "Training:  Iteration No:  251 Worker Num:  4 \n",
            " Loss:  0.3012310862541199\n",
            "Training:  Iteration No:  251 Worker Num:  5 \n",
            " Loss:  0.2640005052089691\n",
            "Training:  Iteration No:  251 Worker Num:  6 \n",
            " Loss:  0.2556878328323364\n",
            "Training:  Iteration No:  251 Worker Num:  7 \n",
            " Loss:  0.289640873670578\n",
            "Test:  Iteration No:  251 \n",
            " Loss:  0.19984353985637426\n",
            "Test accuracy:  94.15\n",
            "Training:  Iteration No:  252 Worker Num:  0 \n",
            " Loss:  0.23248755931854248\n",
            "Training:  Iteration No:  252 Worker Num:  1 \n",
            " Loss:  0.21547167003154755\n",
            "Training:  Iteration No:  252 Worker Num:  2 \n",
            " Loss:  0.31695353984832764\n",
            "Training:  Iteration No:  252 Worker Num:  3 \n",
            " Loss:  0.19893929362297058\n",
            "Training:  Iteration No:  252 Worker Num:  4 \n",
            " Loss:  0.3174728453159332\n",
            "Training:  Iteration No:  252 Worker Num:  5 \n",
            " Loss:  0.2345276176929474\n",
            "Training:  Iteration No:  252 Worker Num:  6 \n",
            " Loss:  0.20453223586082458\n",
            "Training:  Iteration No:  252 Worker Num:  7 \n",
            " Loss:  0.3105672001838684\n",
            "Test:  Iteration No:  252 \n",
            " Loss:  0.19712749214347783\n",
            "Test accuracy:  94.39\n",
            "Training:  Iteration No:  253 Worker Num:  0 \n",
            " Loss:  0.3439170718193054\n",
            "Training:  Iteration No:  253 Worker Num:  1 \n",
            " Loss:  0.27802774310112\n",
            "Training:  Iteration No:  253 Worker Num:  2 \n",
            " Loss:  0.3010278046131134\n",
            "Training:  Iteration No:  253 Worker Num:  3 \n",
            " Loss:  0.2883637845516205\n",
            "Training:  Iteration No:  253 Worker Num:  4 \n",
            " Loss:  0.26045218110084534\n",
            "Training:  Iteration No:  253 Worker Num:  5 \n",
            " Loss:  0.37010014057159424\n",
            "Training:  Iteration No:  253 Worker Num:  6 \n",
            " Loss:  0.23346373438835144\n",
            "Training:  Iteration No:  253 Worker Num:  7 \n",
            " Loss:  0.21959584951400757\n",
            "Test:  Iteration No:  253 \n",
            " Loss:  0.19799080979126163\n",
            "Test accuracy:  94.29\n",
            "Training:  Iteration No:  254 Worker Num:  0 \n",
            " Loss:  0.21533221006393433\n",
            "Training:  Iteration No:  254 Worker Num:  1 \n",
            " Loss:  0.2954142391681671\n",
            "Training:  Iteration No:  254 Worker Num:  2 \n",
            " Loss:  0.17084543406963348\n",
            "Training:  Iteration No:  254 Worker Num:  3 \n",
            " Loss:  0.20681744813919067\n",
            "Training:  Iteration No:  254 Worker Num:  4 \n",
            " Loss:  0.19825950264930725\n",
            "Training:  Iteration No:  254 Worker Num:  5 \n",
            " Loss:  0.23124077916145325\n",
            "Training:  Iteration No:  254 Worker Num:  6 \n",
            " Loss:  0.28245311975479126\n",
            "Training:  Iteration No:  254 Worker Num:  7 \n",
            " Loss:  0.22745633125305176\n",
            "Test:  Iteration No:  254 \n",
            " Loss:  0.19871802470069141\n",
            "Test accuracy:  94.19\n",
            "Training:  Iteration No:  255 Worker Num:  0 \n",
            " Loss:  0.18718716502189636\n",
            "Training:  Iteration No:  255 Worker Num:  1 \n",
            " Loss:  0.23250703513622284\n",
            "Training:  Iteration No:  255 Worker Num:  2 \n",
            " Loss:  0.20175832509994507\n",
            "Training:  Iteration No:  255 Worker Num:  3 \n",
            " Loss:  0.28299033641815186\n",
            "Training:  Iteration No:  255 Worker Num:  4 \n",
            " Loss:  0.23599834740161896\n",
            "Training:  Iteration No:  255 Worker Num:  5 \n",
            " Loss:  0.34480011463165283\n",
            "Training:  Iteration No:  255 Worker Num:  6 \n",
            " Loss:  0.3130176067352295\n",
            "Training:  Iteration No:  255 Worker Num:  7 \n",
            " Loss:  0.22033759951591492\n",
            "Test:  Iteration No:  255 \n",
            " Loss:  0.1981072638985477\n",
            "Test accuracy:  94.18\n",
            "Training:  Iteration No:  256 Worker Num:  0 \n",
            " Loss:  0.17481450736522675\n",
            "Training:  Iteration No:  256 Worker Num:  1 \n",
            " Loss:  0.2724578380584717\n",
            "Training:  Iteration No:  256 Worker Num:  2 \n",
            " Loss:  0.2319728434085846\n",
            "Training:  Iteration No:  256 Worker Num:  3 \n",
            " Loss:  0.26672467589378357\n",
            "Training:  Iteration No:  256 Worker Num:  4 \n",
            " Loss:  0.19731082022190094\n",
            "Training:  Iteration No:  256 Worker Num:  5 \n",
            " Loss:  0.25654757022857666\n",
            "Training:  Iteration No:  256 Worker Num:  6 \n",
            " Loss:  0.2327626347541809\n",
            "Training:  Iteration No:  256 Worker Num:  7 \n",
            " Loss:  0.20577594637870789\n",
            "Test:  Iteration No:  256 \n",
            " Loss:  0.19780690057934086\n",
            "Test accuracy:  94.33\n",
            "Training:  Iteration No:  257 Worker Num:  0 \n",
            " Loss:  0.22567468881607056\n",
            "Training:  Iteration No:  257 Worker Num:  1 \n",
            " Loss:  0.1935289353132248\n",
            "Training:  Iteration No:  257 Worker Num:  2 \n",
            " Loss:  0.39421167969703674\n",
            "Training:  Iteration No:  257 Worker Num:  3 \n",
            " Loss:  0.27907490730285645\n",
            "Training:  Iteration No:  257 Worker Num:  4 \n",
            " Loss:  0.22382992506027222\n",
            "Training:  Iteration No:  257 Worker Num:  5 \n",
            " Loss:  0.1586391180753708\n",
            "Training:  Iteration No:  257 Worker Num:  6 \n",
            " Loss:  0.3216925859451294\n",
            "Training:  Iteration No:  257 Worker Num:  7 \n",
            " Loss:  0.13996995985507965\n",
            "Test:  Iteration No:  257 \n",
            " Loss:  0.1944272652787121\n",
            "Test accuracy:  94.34\n",
            "Training:  Iteration No:  258 Worker Num:  0 \n",
            " Loss:  0.24597413837909698\n",
            "Training:  Iteration No:  258 Worker Num:  1 \n",
            " Loss:  0.3255571722984314\n",
            "Training:  Iteration No:  258 Worker Num:  2 \n",
            " Loss:  0.18469400703907013\n",
            "Training:  Iteration No:  258 Worker Num:  3 \n",
            " Loss:  0.2891097664833069\n",
            "Training:  Iteration No:  258 Worker Num:  4 \n",
            " Loss:  0.14144189655780792\n",
            "Training:  Iteration No:  258 Worker Num:  5 \n",
            " Loss:  0.19040393829345703\n",
            "Training:  Iteration No:  258 Worker Num:  6 \n",
            " Loss:  0.26377180218696594\n",
            "Training:  Iteration No:  258 Worker Num:  7 \n",
            " Loss:  0.24758243560791016\n",
            "Test:  Iteration No:  258 \n",
            " Loss:  0.19546807716494496\n",
            "Test accuracy:  94.5\n",
            "Training:  Iteration No:  259 Worker Num:  0 \n",
            " Loss:  0.350660502910614\n",
            "Training:  Iteration No:  259 Worker Num:  1 \n",
            " Loss:  0.262558251619339\n",
            "Training:  Iteration No:  259 Worker Num:  2 \n",
            " Loss:  0.21733398735523224\n",
            "Training:  Iteration No:  259 Worker Num:  3 \n",
            " Loss:  0.17692691087722778\n",
            "Training:  Iteration No:  259 Worker Num:  4 \n",
            " Loss:  0.29221397638320923\n",
            "Training:  Iteration No:  259 Worker Num:  5 \n",
            " Loss:  0.2082529366016388\n",
            "Training:  Iteration No:  259 Worker Num:  6 \n",
            " Loss:  0.25820767879486084\n",
            "Training:  Iteration No:  259 Worker Num:  7 \n",
            " Loss:  0.2303062081336975\n",
            "Test:  Iteration No:  259 \n",
            " Loss:  0.1919716117149101\n",
            "Test accuracy:  94.44\n",
            "Training:  Iteration No:  260 Worker Num:  0 \n",
            " Loss:  0.11817554384469986\n",
            "Training:  Iteration No:  260 Worker Num:  1 \n",
            " Loss:  0.21922710537910461\n",
            "Training:  Iteration No:  260 Worker Num:  2 \n",
            " Loss:  0.24937620759010315\n",
            "Training:  Iteration No:  260 Worker Num:  3 \n",
            " Loss:  0.24633222818374634\n",
            "Training:  Iteration No:  260 Worker Num:  4 \n",
            " Loss:  0.2617969214916229\n",
            "Training:  Iteration No:  260 Worker Num:  5 \n",
            " Loss:  0.2875101566314697\n",
            "Training:  Iteration No:  260 Worker Num:  6 \n",
            " Loss:  0.23741695284843445\n",
            "Training:  Iteration No:  260 Worker Num:  7 \n",
            " Loss:  0.1774105727672577\n",
            "Test:  Iteration No:  260 \n",
            " Loss:  0.19381707582528454\n",
            "Test accuracy:  94.32\n",
            "Training:  Iteration No:  261 Worker Num:  0 \n",
            " Loss:  0.22259429097175598\n",
            "Training:  Iteration No:  261 Worker Num:  1 \n",
            " Loss:  0.12435603141784668\n",
            "Training:  Iteration No:  261 Worker Num:  2 \n",
            " Loss:  0.18322905898094177\n",
            "Training:  Iteration No:  261 Worker Num:  3 \n",
            " Loss:  0.26221993565559387\n",
            "Training:  Iteration No:  261 Worker Num:  4 \n",
            " Loss:  0.2525225877761841\n",
            "Training:  Iteration No:  261 Worker Num:  5 \n",
            " Loss:  0.21873445808887482\n",
            "Training:  Iteration No:  261 Worker Num:  6 \n",
            " Loss:  0.21792438626289368\n",
            "Training:  Iteration No:  261 Worker Num:  7 \n",
            " Loss:  0.30605098605155945\n",
            "Test:  Iteration No:  261 \n",
            " Loss:  0.19164831204269128\n",
            "Test accuracy:  94.27\n",
            "Training:  Iteration No:  262 Worker Num:  0 \n",
            " Loss:  0.1999322921037674\n",
            "Training:  Iteration No:  262 Worker Num:  1 \n",
            " Loss:  0.12469084560871124\n",
            "Training:  Iteration No:  262 Worker Num:  2 \n",
            " Loss:  0.22709767520427704\n",
            "Training:  Iteration No:  262 Worker Num:  3 \n",
            " Loss:  0.3622673451900482\n",
            "Training:  Iteration No:  262 Worker Num:  4 \n",
            " Loss:  0.5066394805908203\n",
            "Training:  Iteration No:  262 Worker Num:  5 \n",
            " Loss:  0.24778033792972565\n",
            "Training:  Iteration No:  262 Worker Num:  6 \n",
            " Loss:  0.1944669634103775\n",
            "Training:  Iteration No:  262 Worker Num:  7 \n",
            " Loss:  0.2069539576768875\n",
            "Test:  Iteration No:  262 \n",
            " Loss:  0.19121445639978482\n",
            "Test accuracy:  94.45\n",
            "Training:  Iteration No:  263 Worker Num:  0 \n",
            " Loss:  0.18667738139629364\n",
            "Training:  Iteration No:  263 Worker Num:  1 \n",
            " Loss:  0.25737106800079346\n",
            "Training:  Iteration No:  263 Worker Num:  2 \n",
            " Loss:  0.10222915560007095\n",
            "Training:  Iteration No:  263 Worker Num:  3 \n",
            " Loss:  0.17834734916687012\n",
            "Training:  Iteration No:  263 Worker Num:  4 \n",
            " Loss:  0.1747848093509674\n",
            "Training:  Iteration No:  263 Worker Num:  5 \n",
            " Loss:  0.20995134115219116\n",
            "Training:  Iteration No:  263 Worker Num:  6 \n",
            " Loss:  0.25020119547843933\n",
            "Training:  Iteration No:  263 Worker Num:  7 \n",
            " Loss:  0.29264506697654724\n",
            "Test:  Iteration No:  263 \n",
            " Loss:  0.18966337299280905\n",
            "Test accuracy:  94.55\n",
            "Training:  Iteration No:  264 Worker Num:  0 \n",
            " Loss:  0.2992773652076721\n",
            "Training:  Iteration No:  264 Worker Num:  1 \n",
            " Loss:  0.28514420986175537\n",
            "Training:  Iteration No:  264 Worker Num:  2 \n",
            " Loss:  0.22498701512813568\n",
            "Training:  Iteration No:  264 Worker Num:  3 \n",
            " Loss:  0.1841541975736618\n",
            "Training:  Iteration No:  264 Worker Num:  4 \n",
            " Loss:  0.19489969313144684\n",
            "Training:  Iteration No:  264 Worker Num:  5 \n",
            " Loss:  0.13709703087806702\n",
            "Training:  Iteration No:  264 Worker Num:  6 \n",
            " Loss:  0.1873818337917328\n",
            "Training:  Iteration No:  264 Worker Num:  7 \n",
            " Loss:  0.2587970197200775\n",
            "Test:  Iteration No:  264 \n",
            " Loss:  0.18823008458542673\n",
            "Test accuracy:  94.53\n",
            "Training:  Iteration No:  265 Worker Num:  0 \n",
            " Loss:  0.23006081581115723\n",
            "Training:  Iteration No:  265 Worker Num:  1 \n",
            " Loss:  0.2238340973854065\n",
            "Training:  Iteration No:  265 Worker Num:  2 \n",
            " Loss:  0.1869306117296219\n",
            "Training:  Iteration No:  265 Worker Num:  3 \n",
            " Loss:  0.188079833984375\n",
            "Training:  Iteration No:  265 Worker Num:  4 \n",
            " Loss:  0.26772257685661316\n",
            "Training:  Iteration No:  265 Worker Num:  5 \n",
            " Loss:  0.15526863932609558\n",
            "Training:  Iteration No:  265 Worker Num:  6 \n",
            " Loss:  0.2679224908351898\n",
            "Training:  Iteration No:  265 Worker Num:  7 \n",
            " Loss:  0.2575775384902954\n",
            "Test:  Iteration No:  265 \n",
            " Loss:  0.18915900000948696\n",
            "Test accuracy:  94.37\n",
            "Training:  Iteration No:  266 Worker Num:  0 \n",
            " Loss:  0.26610520482063293\n",
            "Training:  Iteration No:  266 Worker Num:  1 \n",
            " Loss:  0.2673167288303375\n",
            "Training:  Iteration No:  266 Worker Num:  2 \n",
            " Loss:  0.13124698400497437\n",
            "Training:  Iteration No:  266 Worker Num:  3 \n",
            " Loss:  0.11940272897481918\n",
            "Training:  Iteration No:  266 Worker Num:  4 \n",
            " Loss:  0.20765751600265503\n",
            "Training:  Iteration No:  266 Worker Num:  5 \n",
            " Loss:  0.2634938061237335\n",
            "Training:  Iteration No:  266 Worker Num:  6 \n",
            " Loss:  0.1694752424955368\n",
            "Training:  Iteration No:  266 Worker Num:  7 \n",
            " Loss:  0.22382491827011108\n",
            "Test:  Iteration No:  266 \n",
            " Loss:  0.18660497808050883\n",
            "Test accuracy:  94.52\n",
            "Training:  Iteration No:  267 Worker Num:  0 \n",
            " Loss:  0.47971248626708984\n",
            "Training:  Iteration No:  267 Worker Num:  1 \n",
            " Loss:  0.14564679563045502\n",
            "Training:  Iteration No:  267 Worker Num:  2 \n",
            " Loss:  0.11581666022539139\n",
            "Training:  Iteration No:  267 Worker Num:  3 \n",
            " Loss:  0.4529082775115967\n",
            "Training:  Iteration No:  267 Worker Num:  4 \n",
            " Loss:  0.26678329706192017\n",
            "Training:  Iteration No:  267 Worker Num:  5 \n",
            " Loss:  0.29251807928085327\n",
            "Training:  Iteration No:  267 Worker Num:  6 \n",
            " Loss:  0.2406371831893921\n",
            "Training:  Iteration No:  267 Worker Num:  7 \n",
            " Loss:  0.23302365839481354\n",
            "Test:  Iteration No:  267 \n",
            " Loss:  0.19251335682229528\n",
            "Test accuracy:  94.23\n",
            "Training:  Iteration No:  268 Worker Num:  0 \n",
            " Loss:  0.15155544877052307\n",
            "Training:  Iteration No:  268 Worker Num:  1 \n",
            " Loss:  0.22905990481376648\n",
            "Training:  Iteration No:  268 Worker Num:  2 \n",
            " Loss:  0.15902742743492126\n",
            "Training:  Iteration No:  268 Worker Num:  3 \n",
            " Loss:  0.2252158671617508\n",
            "Training:  Iteration No:  268 Worker Num:  4 \n",
            " Loss:  0.39064979553222656\n",
            "Training:  Iteration No:  268 Worker Num:  5 \n",
            " Loss:  0.25014835596084595\n",
            "Training:  Iteration No:  268 Worker Num:  6 \n",
            " Loss:  0.1878347098827362\n",
            "Training:  Iteration No:  268 Worker Num:  7 \n",
            " Loss:  0.26169049739837646\n",
            "Test:  Iteration No:  268 \n",
            " Loss:  0.18406380419442547\n",
            "Test accuracy:  94.68\n",
            "Training:  Iteration No:  269 Worker Num:  0 \n",
            " Loss:  0.18944060802459717\n",
            "Training:  Iteration No:  269 Worker Num:  1 \n",
            " Loss:  0.1980513632297516\n",
            "Training:  Iteration No:  269 Worker Num:  2 \n",
            " Loss:  0.14687785506248474\n",
            "Training:  Iteration No:  269 Worker Num:  3 \n",
            " Loss:  0.3146962821483612\n",
            "Training:  Iteration No:  269 Worker Num:  4 \n",
            " Loss:  0.211457297205925\n",
            "Training:  Iteration No:  269 Worker Num:  5 \n",
            " Loss:  0.20873254537582397\n",
            "Training:  Iteration No:  269 Worker Num:  6 \n",
            " Loss:  0.2493331879377365\n",
            "Training:  Iteration No:  269 Worker Num:  7 \n",
            " Loss:  0.24598459899425507\n",
            "Test:  Iteration No:  269 \n",
            " Loss:  0.18449721156466234\n",
            "Test accuracy:  94.57\n",
            "Training:  Iteration No:  270 Worker Num:  0 \n",
            " Loss:  0.255329430103302\n",
            "Training:  Iteration No:  270 Worker Num:  1 \n",
            " Loss:  0.13841435313224792\n",
            "Training:  Iteration No:  270 Worker Num:  2 \n",
            " Loss:  0.24860481917858124\n",
            "Training:  Iteration No:  270 Worker Num:  3 \n",
            " Loss:  0.18333294987678528\n",
            "Training:  Iteration No:  270 Worker Num:  4 \n",
            " Loss:  0.13617530465126038\n",
            "Training:  Iteration No:  270 Worker Num:  5 \n",
            " Loss:  0.17921915650367737\n",
            "Training:  Iteration No:  270 Worker Num:  6 \n",
            " Loss:  0.12690579891204834\n",
            "Training:  Iteration No:  270 Worker Num:  7 \n",
            " Loss:  0.2450922429561615\n",
            "Test:  Iteration No:  270 \n",
            " Loss:  0.18618517844124308\n",
            "Test accuracy:  94.54\n",
            "Training:  Iteration No:  271 Worker Num:  0 \n",
            " Loss:  0.2233809381723404\n",
            "Training:  Iteration No:  271 Worker Num:  1 \n",
            " Loss:  0.23763325810432434\n",
            "Training:  Iteration No:  271 Worker Num:  2 \n",
            " Loss:  0.36555346846580505\n",
            "Training:  Iteration No:  271 Worker Num:  3 \n",
            " Loss:  0.23350438475608826\n",
            "Training:  Iteration No:  271 Worker Num:  4 \n",
            " Loss:  0.21450649201869965\n",
            "Training:  Iteration No:  271 Worker Num:  5 \n",
            " Loss:  0.1277860850095749\n",
            "Training:  Iteration No:  271 Worker Num:  6 \n",
            " Loss:  0.11549500375986099\n",
            "Training:  Iteration No:  271 Worker Num:  7 \n",
            " Loss:  0.14963524043560028\n",
            "Test:  Iteration No:  271 \n",
            " Loss:  0.1837201750118144\n",
            "Test accuracy:  94.52\n",
            "Training:  Iteration No:  272 Worker Num:  0 \n",
            " Loss:  0.26576822996139526\n",
            "Training:  Iteration No:  272 Worker Num:  1 \n",
            " Loss:  0.25485146045684814\n",
            "Training:  Iteration No:  272 Worker Num:  2 \n",
            " Loss:  0.2035718411207199\n",
            "Training:  Iteration No:  272 Worker Num:  3 \n",
            " Loss:  0.19298946857452393\n",
            "Training:  Iteration No:  272 Worker Num:  4 \n",
            " Loss:  0.20571786165237427\n",
            "Training:  Iteration No:  272 Worker Num:  5 \n",
            " Loss:  0.09633169323205948\n",
            "Training:  Iteration No:  272 Worker Num:  6 \n",
            " Loss:  0.14213678240776062\n",
            "Training:  Iteration No:  272 Worker Num:  7 \n",
            " Loss:  0.12660394608974457\n",
            "Test:  Iteration No:  272 \n",
            " Loss:  0.18364881775400874\n",
            "Test accuracy:  94.55\n",
            "Training:  Iteration No:  273 Worker Num:  0 \n",
            " Loss:  0.2874274253845215\n",
            "Training:  Iteration No:  273 Worker Num:  1 \n",
            " Loss:  0.20901170372962952\n",
            "Training:  Iteration No:  273 Worker Num:  2 \n",
            " Loss:  0.1931062936782837\n",
            "Training:  Iteration No:  273 Worker Num:  3 \n",
            " Loss:  0.17340686917304993\n",
            "Training:  Iteration No:  273 Worker Num:  4 \n",
            " Loss:  0.17412661015987396\n",
            "Training:  Iteration No:  273 Worker Num:  5 \n",
            " Loss:  0.2063726931810379\n",
            "Training:  Iteration No:  273 Worker Num:  6 \n",
            " Loss:  0.2719554305076599\n",
            "Training:  Iteration No:  273 Worker Num:  7 \n",
            " Loss:  0.12626466155052185\n",
            "Test:  Iteration No:  273 \n",
            " Loss:  0.18456614223792206\n",
            "Test accuracy:  94.51\n",
            "Training:  Iteration No:  274 Worker Num:  0 \n",
            " Loss:  0.21185202896595\n",
            "Training:  Iteration No:  274 Worker Num:  1 \n",
            " Loss:  0.2406376153230667\n",
            "Training:  Iteration No:  274 Worker Num:  2 \n",
            " Loss:  0.14948903024196625\n",
            "Training:  Iteration No:  274 Worker Num:  3 \n",
            " Loss:  0.10190117359161377\n",
            "Training:  Iteration No:  274 Worker Num:  4 \n",
            " Loss:  0.31180238723754883\n",
            "Training:  Iteration No:  274 Worker Num:  5 \n",
            " Loss:  0.25920599699020386\n",
            "Training:  Iteration No:  274 Worker Num:  6 \n",
            " Loss:  0.25529950857162476\n",
            "Training:  Iteration No:  274 Worker Num:  7 \n",
            " Loss:  0.2003427892923355\n",
            "Test:  Iteration No:  274 \n",
            " Loss:  0.18283989974828083\n",
            "Test accuracy:  94.58\n",
            "Training:  Iteration No:  275 Worker Num:  0 \n",
            " Loss:  0.21664801239967346\n",
            "Training:  Iteration No:  275 Worker Num:  1 \n",
            " Loss:  0.288556307554245\n",
            "Training:  Iteration No:  275 Worker Num:  2 \n",
            " Loss:  0.2604331374168396\n",
            "Training:  Iteration No:  275 Worker Num:  3 \n",
            " Loss:  0.15630051493644714\n",
            "Training:  Iteration No:  275 Worker Num:  4 \n",
            " Loss:  0.12172041833400726\n",
            "Training:  Iteration No:  275 Worker Num:  5 \n",
            " Loss:  0.1275668740272522\n",
            "Training:  Iteration No:  275 Worker Num:  6 \n",
            " Loss:  0.4354868531227112\n",
            "Training:  Iteration No:  275 Worker Num:  7 \n",
            " Loss:  0.14392831921577454\n",
            "Test:  Iteration No:  275 \n",
            " Loss:  0.18526529208344372\n",
            "Test accuracy:  94.36\n",
            "Training:  Iteration No:  276 Worker Num:  0 \n",
            " Loss:  0.24335560202598572\n",
            "Training:  Iteration No:  276 Worker Num:  1 \n",
            " Loss:  0.23137810826301575\n",
            "Training:  Iteration No:  276 Worker Num:  2 \n",
            " Loss:  0.27794429659843445\n",
            "Training:  Iteration No:  276 Worker Num:  3 \n",
            " Loss:  0.23235146701335907\n",
            "Training:  Iteration No:  276 Worker Num:  4 \n",
            " Loss:  0.3080044090747833\n",
            "Training:  Iteration No:  276 Worker Num:  5 \n",
            " Loss:  0.19080428779125214\n",
            "Training:  Iteration No:  276 Worker Num:  6 \n",
            " Loss:  0.20905475318431854\n",
            "Training:  Iteration No:  276 Worker Num:  7 \n",
            " Loss:  0.2712729275226593\n",
            "Test:  Iteration No:  276 \n",
            " Loss:  0.18517264291125385\n",
            "Test accuracy:  94.68\n",
            "Training:  Iteration No:  277 Worker Num:  0 \n",
            " Loss:  0.16200630366802216\n",
            "Training:  Iteration No:  277 Worker Num:  1 \n",
            " Loss:  0.14035344123840332\n",
            "Training:  Iteration No:  277 Worker Num:  2 \n",
            " Loss:  0.21779891848564148\n",
            "Training:  Iteration No:  277 Worker Num:  3 \n",
            " Loss:  0.36481061577796936\n",
            "Training:  Iteration No:  277 Worker Num:  4 \n",
            " Loss:  0.15839536488056183\n",
            "Training:  Iteration No:  277 Worker Num:  5 \n",
            " Loss:  0.2600959837436676\n",
            "Training:  Iteration No:  277 Worker Num:  6 \n",
            " Loss:  0.15575505793094635\n",
            "Training:  Iteration No:  277 Worker Num:  7 \n",
            " Loss:  0.27198365330696106\n",
            "Test:  Iteration No:  277 \n",
            " Loss:  0.18196005641801072\n",
            "Test accuracy:  94.52\n",
            "Training:  Iteration No:  278 Worker Num:  0 \n",
            " Loss:  0.2985917925834656\n",
            "Training:  Iteration No:  278 Worker Num:  1 \n",
            " Loss:  0.217705637216568\n",
            "Training:  Iteration No:  278 Worker Num:  2 \n",
            " Loss:  0.25554516911506653\n",
            "Training:  Iteration No:  278 Worker Num:  3 \n",
            " Loss:  0.22649416327476501\n",
            "Training:  Iteration No:  278 Worker Num:  4 \n",
            " Loss:  0.23683270812034607\n",
            "Training:  Iteration No:  278 Worker Num:  5 \n",
            " Loss:  0.2921021580696106\n",
            "Training:  Iteration No:  278 Worker Num:  6 \n",
            " Loss:  0.13988499343395233\n",
            "Training:  Iteration No:  278 Worker Num:  7 \n",
            " Loss:  0.18278945982456207\n",
            "Test:  Iteration No:  278 \n",
            " Loss:  0.18882800840265765\n",
            "Test accuracy:  94.47\n",
            "Training:  Iteration No:  279 Worker Num:  0 \n",
            " Loss:  0.13798648118972778\n",
            "Training:  Iteration No:  279 Worker Num:  1 \n",
            " Loss:  0.2949744760990143\n",
            "Training:  Iteration No:  279 Worker Num:  2 \n",
            " Loss:  0.3219878673553467\n",
            "Training:  Iteration No:  279 Worker Num:  3 \n",
            " Loss:  0.17350125312805176\n",
            "Training:  Iteration No:  279 Worker Num:  4 \n",
            " Loss:  0.22572404146194458\n",
            "Training:  Iteration No:  279 Worker Num:  5 \n",
            " Loss:  0.26989200711250305\n",
            "Training:  Iteration No:  279 Worker Num:  6 \n",
            " Loss:  0.11627330631017685\n",
            "Training:  Iteration No:  279 Worker Num:  7 \n",
            " Loss:  0.26706182956695557\n",
            "Test:  Iteration No:  279 \n",
            " Loss:  0.1795669912325242\n",
            "Test accuracy:  94.67\n",
            "Training:  Iteration No:  280 Worker Num:  0 \n",
            " Loss:  0.22419847548007965\n",
            "Training:  Iteration No:  280 Worker Num:  1 \n",
            " Loss:  0.23964747786521912\n",
            "Training:  Iteration No:  280 Worker Num:  2 \n",
            " Loss:  0.1810861974954605\n",
            "Training:  Iteration No:  280 Worker Num:  3 \n",
            " Loss:  0.18046051263809204\n",
            "Training:  Iteration No:  280 Worker Num:  4 \n",
            " Loss:  0.20596468448638916\n",
            "Training:  Iteration No:  280 Worker Num:  5 \n",
            " Loss:  0.1728563755750656\n",
            "Training:  Iteration No:  280 Worker Num:  6 \n",
            " Loss:  0.15874820947647095\n",
            "Training:  Iteration No:  280 Worker Num:  7 \n",
            " Loss:  0.1798645257949829\n",
            "Test:  Iteration No:  280 \n",
            " Loss:  0.1772460675432901\n",
            "Test accuracy:  94.8\n",
            "Training:  Iteration No:  281 Worker Num:  0 \n",
            " Loss:  0.1502641886472702\n",
            "Training:  Iteration No:  281 Worker Num:  1 \n",
            " Loss:  0.24596454203128815\n",
            "Training:  Iteration No:  281 Worker Num:  2 \n",
            " Loss:  0.37187063694000244\n",
            "Training:  Iteration No:  281 Worker Num:  3 \n",
            " Loss:  0.1922791451215744\n",
            "Training:  Iteration No:  281 Worker Num:  4 \n",
            " Loss:  0.3117835819721222\n",
            "Training:  Iteration No:  281 Worker Num:  5 \n",
            " Loss:  0.2826276123523712\n",
            "Training:  Iteration No:  281 Worker Num:  6 \n",
            " Loss:  0.2583533525466919\n",
            "Training:  Iteration No:  281 Worker Num:  7 \n",
            " Loss:  0.15831822156906128\n",
            "Test:  Iteration No:  281 \n",
            " Loss:  0.17933109097324218\n",
            "Test accuracy:  94.79\n",
            "Training:  Iteration No:  282 Worker Num:  0 \n",
            " Loss:  0.1852072775363922\n",
            "Training:  Iteration No:  282 Worker Num:  1 \n",
            " Loss:  0.2766692340373993\n",
            "Training:  Iteration No:  282 Worker Num:  2 \n",
            " Loss:  0.3806705176830292\n",
            "Training:  Iteration No:  282 Worker Num:  3 \n",
            " Loss:  0.1993655264377594\n",
            "Training:  Iteration No:  282 Worker Num:  4 \n",
            " Loss:  0.18873485922813416\n",
            "Training:  Iteration No:  282 Worker Num:  5 \n",
            " Loss:  0.24173463881015778\n",
            "Training:  Iteration No:  282 Worker Num:  6 \n",
            " Loss:  0.22736535966396332\n",
            "Training:  Iteration No:  282 Worker Num:  7 \n",
            " Loss:  0.2593786418437958\n",
            "Test:  Iteration No:  282 \n",
            " Loss:  0.17981578057773318\n",
            "Test accuracy:  94.75\n",
            "Training:  Iteration No:  283 Worker Num:  0 \n",
            " Loss:  0.1798306405544281\n",
            "Training:  Iteration No:  283 Worker Num:  1 \n",
            " Loss:  0.15421858429908752\n",
            "Training:  Iteration No:  283 Worker Num:  2 \n",
            " Loss:  0.20335669815540314\n",
            "Training:  Iteration No:  283 Worker Num:  3 \n",
            " Loss:  0.28112560510635376\n",
            "Training:  Iteration No:  283 Worker Num:  4 \n",
            " Loss:  0.14202123880386353\n",
            "Training:  Iteration No:  283 Worker Num:  5 \n",
            " Loss:  0.2698473632335663\n",
            "Training:  Iteration No:  283 Worker Num:  6 \n",
            " Loss:  0.19888117909431458\n",
            "Training:  Iteration No:  283 Worker Num:  7 \n",
            " Loss:  0.15926186740398407\n",
            "Test:  Iteration No:  283 \n",
            " Loss:  0.17744087242179468\n",
            "Test accuracy:  94.8\n",
            "Training:  Iteration No:  284 Worker Num:  0 \n",
            " Loss:  0.2658035159111023\n",
            "Training:  Iteration No:  284 Worker Num:  1 \n",
            " Loss:  0.2842549681663513\n",
            "Training:  Iteration No:  284 Worker Num:  2 \n",
            " Loss:  0.22696994245052338\n",
            "Training:  Iteration No:  284 Worker Num:  3 \n",
            " Loss:  0.1711745411157608\n",
            "Training:  Iteration No:  284 Worker Num:  4 \n",
            " Loss:  0.2676044702529907\n",
            "Training:  Iteration No:  284 Worker Num:  5 \n",
            " Loss:  0.20345841348171234\n",
            "Training:  Iteration No:  284 Worker Num:  6 \n",
            " Loss:  0.23555761575698853\n",
            "Training:  Iteration No:  284 Worker Num:  7 \n",
            " Loss:  0.13039778172969818\n",
            "Test:  Iteration No:  284 \n",
            " Loss:  0.17668942656792416\n",
            "Test accuracy:  94.66\n",
            "Training:  Iteration No:  285 Worker Num:  0 \n",
            " Loss:  0.15835490822792053\n",
            "Training:  Iteration No:  285 Worker Num:  1 \n",
            " Loss:  0.18820863962173462\n",
            "Training:  Iteration No:  285 Worker Num:  2 \n",
            " Loss:  0.18693652749061584\n",
            "Training:  Iteration No:  285 Worker Num:  3 \n",
            " Loss:  0.3520326316356659\n",
            "Training:  Iteration No:  285 Worker Num:  4 \n",
            " Loss:  0.13231080770492554\n",
            "Training:  Iteration No:  285 Worker Num:  5 \n",
            " Loss:  0.1601770520210266\n",
            "Training:  Iteration No:  285 Worker Num:  6 \n",
            " Loss:  0.13014265894889832\n",
            "Training:  Iteration No:  285 Worker Num:  7 \n",
            " Loss:  0.1451796144247055\n",
            "Test:  Iteration No:  285 \n",
            " Loss:  0.17386356030319688\n",
            "Test accuracy:  94.82\n",
            "Training:  Iteration No:  286 Worker Num:  0 \n",
            " Loss:  0.37728390097618103\n",
            "Training:  Iteration No:  286 Worker Num:  1 \n",
            " Loss:  0.3394956588745117\n",
            "Training:  Iteration No:  286 Worker Num:  2 \n",
            " Loss:  0.20163701474666595\n",
            "Training:  Iteration No:  286 Worker Num:  3 \n",
            " Loss:  0.12545444071292877\n",
            "Training:  Iteration No:  286 Worker Num:  4 \n",
            " Loss:  0.21965067088603973\n",
            "Training:  Iteration No:  286 Worker Num:  5 \n",
            " Loss:  0.12765288352966309\n",
            "Training:  Iteration No:  286 Worker Num:  6 \n",
            " Loss:  0.18516390025615692\n",
            "Training:  Iteration No:  286 Worker Num:  7 \n",
            " Loss:  0.1549552083015442\n",
            "Test:  Iteration No:  286 \n",
            " Loss:  0.1735898001956506\n",
            "Test accuracy:  94.83\n",
            "Training:  Iteration No:  287 Worker Num:  0 \n",
            " Loss:  0.22609733045101166\n",
            "Training:  Iteration No:  287 Worker Num:  1 \n",
            " Loss:  0.15881335735321045\n",
            "Training:  Iteration No:  287 Worker Num:  2 \n",
            " Loss:  0.2768436670303345\n",
            "Training:  Iteration No:  287 Worker Num:  3 \n",
            " Loss:  0.20186373591423035\n",
            "Training:  Iteration No:  287 Worker Num:  4 \n",
            " Loss:  0.12073156237602234\n",
            "Training:  Iteration No:  287 Worker Num:  5 \n",
            " Loss:  0.10538306832313538\n",
            "Training:  Iteration No:  287 Worker Num:  6 \n",
            " Loss:  0.1418328881263733\n",
            "Training:  Iteration No:  287 Worker Num:  7 \n",
            " Loss:  0.27309590578079224\n",
            "Test:  Iteration No:  287 \n",
            " Loss:  0.1753062256788717\n",
            "Test accuracy:  94.74\n",
            "Training:  Iteration No:  288 Worker Num:  0 \n",
            " Loss:  0.2392863929271698\n",
            "Training:  Iteration No:  288 Worker Num:  1 \n",
            " Loss:  0.2426987737417221\n",
            "Training:  Iteration No:  288 Worker Num:  2 \n",
            " Loss:  0.18762050569057465\n",
            "Training:  Iteration No:  288 Worker Num:  3 \n",
            " Loss:  0.3245875835418701\n",
            "Training:  Iteration No:  288 Worker Num:  4 \n",
            " Loss:  0.25308918952941895\n",
            "Training:  Iteration No:  288 Worker Num:  5 \n",
            " Loss:  0.13895079493522644\n",
            "Training:  Iteration No:  288 Worker Num:  6 \n",
            " Loss:  0.13080884516239166\n",
            "Training:  Iteration No:  288 Worker Num:  7 \n",
            " Loss:  0.16300782561302185\n",
            "Test:  Iteration No:  288 \n",
            " Loss:  0.173776151559351\n",
            "Test accuracy:  94.67\n",
            "Training:  Iteration No:  289 Worker Num:  0 \n",
            " Loss:  0.251819908618927\n",
            "Training:  Iteration No:  289 Worker Num:  1 \n",
            " Loss:  0.2726914584636688\n",
            "Training:  Iteration No:  289 Worker Num:  2 \n",
            " Loss:  0.15413615107536316\n",
            "Training:  Iteration No:  289 Worker Num:  3 \n",
            " Loss:  0.11508866399526596\n",
            "Training:  Iteration No:  289 Worker Num:  4 \n",
            " Loss:  0.30088019371032715\n",
            "Training:  Iteration No:  289 Worker Num:  5 \n",
            " Loss:  0.18422560393810272\n",
            "Training:  Iteration No:  289 Worker Num:  6 \n",
            " Loss:  0.328025758266449\n",
            "Training:  Iteration No:  289 Worker Num:  7 \n",
            " Loss:  0.3526093661785126\n",
            "Test:  Iteration No:  289 \n",
            " Loss:  0.17432905457725253\n",
            "Test accuracy:  94.79\n",
            "Training:  Iteration No:  290 Worker Num:  0 \n",
            " Loss:  0.2273363173007965\n",
            "Training:  Iteration No:  290 Worker Num:  1 \n",
            " Loss:  0.1787595897912979\n",
            "Training:  Iteration No:  290 Worker Num:  2 \n",
            " Loss:  0.25882306694984436\n",
            "Training:  Iteration No:  290 Worker Num:  3 \n",
            " Loss:  0.1701810210943222\n",
            "Training:  Iteration No:  290 Worker Num:  4 \n",
            " Loss:  0.22374457120895386\n",
            "Training:  Iteration No:  290 Worker Num:  5 \n",
            " Loss:  0.2141728699207306\n",
            "Training:  Iteration No:  290 Worker Num:  6 \n",
            " Loss:  0.1831819862127304\n",
            "Training:  Iteration No:  290 Worker Num:  7 \n",
            " Loss:  0.15484941005706787\n",
            "Test:  Iteration No:  290 \n",
            " Loss:  0.17379342737501557\n",
            "Test accuracy:  94.67\n",
            "Training:  Iteration No:  291 Worker Num:  0 \n",
            " Loss:  0.21571488678455353\n",
            "Training:  Iteration No:  291 Worker Num:  1 \n",
            " Loss:  0.26492637395858765\n",
            "Training:  Iteration No:  291 Worker Num:  2 \n",
            " Loss:  0.35596126317977905\n",
            "Training:  Iteration No:  291 Worker Num:  3 \n",
            " Loss:  0.24252130091190338\n",
            "Training:  Iteration No:  291 Worker Num:  4 \n",
            " Loss:  0.27947118878364563\n",
            "Training:  Iteration No:  291 Worker Num:  5 \n",
            " Loss:  0.22714397311210632\n",
            "Training:  Iteration No:  291 Worker Num:  6 \n",
            " Loss:  0.18788857758045197\n",
            "Training:  Iteration No:  291 Worker Num:  7 \n",
            " Loss:  0.2431749403476715\n",
            "Test:  Iteration No:  291 \n",
            " Loss:  0.17884710841352428\n",
            "Test accuracy:  94.81\n",
            "Training:  Iteration No:  292 Worker Num:  0 \n",
            " Loss:  0.2619357407093048\n",
            "Training:  Iteration No:  292 Worker Num:  1 \n",
            " Loss:  0.10248515009880066\n",
            "Training:  Iteration No:  292 Worker Num:  2 \n",
            " Loss:  0.1230369433760643\n",
            "Training:  Iteration No:  292 Worker Num:  3 \n",
            " Loss:  0.37790828943252563\n",
            "Training:  Iteration No:  292 Worker Num:  4 \n",
            " Loss:  0.16632594168186188\n",
            "Training:  Iteration No:  292 Worker Num:  5 \n",
            " Loss:  0.17584572732448578\n",
            "Training:  Iteration No:  292 Worker Num:  6 \n",
            " Loss:  0.23673534393310547\n",
            "Training:  Iteration No:  292 Worker Num:  7 \n",
            " Loss:  0.2256879359483719\n",
            "Test:  Iteration No:  292 \n",
            " Loss:  0.17130386802261766\n",
            "Test accuracy:  94.89\n",
            "Training:  Iteration No:  293 Worker Num:  0 \n",
            " Loss:  0.36775827407836914\n",
            "Training:  Iteration No:  293 Worker Num:  1 \n",
            " Loss:  0.15332509577274323\n",
            "Training:  Iteration No:  293 Worker Num:  2 \n",
            " Loss:  0.20263323187828064\n",
            "Training:  Iteration No:  293 Worker Num:  3 \n",
            " Loss:  0.1809113323688507\n",
            "Training:  Iteration No:  293 Worker Num:  4 \n",
            " Loss:  0.17856116592884064\n",
            "Training:  Iteration No:  293 Worker Num:  5 \n",
            " Loss:  0.31772616505622864\n",
            "Training:  Iteration No:  293 Worker Num:  6 \n",
            " Loss:  0.1858992725610733\n",
            "Training:  Iteration No:  293 Worker Num:  7 \n",
            " Loss:  0.18140654265880585\n",
            "Test:  Iteration No:  293 \n",
            " Loss:  0.17139089487115794\n",
            "Test accuracy:  95.11\n",
            "Training:  Iteration No:  294 Worker Num:  0 \n",
            " Loss:  0.29083991050720215\n",
            "Training:  Iteration No:  294 Worker Num:  1 \n",
            " Loss:  0.24842724204063416\n",
            "Training:  Iteration No:  294 Worker Num:  2 \n",
            " Loss:  0.26586809754371643\n",
            "Training:  Iteration No:  294 Worker Num:  3 \n",
            " Loss:  0.12733733654022217\n",
            "Training:  Iteration No:  294 Worker Num:  4 \n",
            " Loss:  0.24624168872833252\n",
            "Training:  Iteration No:  294 Worker Num:  5 \n",
            " Loss:  0.22518983483314514\n",
            "Training:  Iteration No:  294 Worker Num:  6 \n",
            " Loss:  0.09671752899885178\n",
            "Training:  Iteration No:  294 Worker Num:  7 \n",
            " Loss:  0.27390414476394653\n",
            "Test:  Iteration No:  294 \n",
            " Loss:  0.17038110064790596\n",
            "Test accuracy:  94.83\n",
            "Training:  Iteration No:  295 Worker Num:  0 \n",
            " Loss:  0.1909431517124176\n",
            "Training:  Iteration No:  295 Worker Num:  1 \n",
            " Loss:  0.17813582718372345\n",
            "Training:  Iteration No:  295 Worker Num:  2 \n",
            " Loss:  0.3150738477706909\n",
            "Training:  Iteration No:  295 Worker Num:  3 \n",
            " Loss:  0.20466704666614532\n",
            "Training:  Iteration No:  295 Worker Num:  4 \n",
            " Loss:  0.30407702922821045\n",
            "Training:  Iteration No:  295 Worker Num:  5 \n",
            " Loss:  0.2283705770969391\n",
            "Training:  Iteration No:  295 Worker Num:  6 \n",
            " Loss:  0.19016757607460022\n",
            "Training:  Iteration No:  295 Worker Num:  7 \n",
            " Loss:  0.21148207783699036\n",
            "Test:  Iteration No:  295 \n",
            " Loss:  0.17214112983474247\n",
            "Test accuracy:  94.93\n",
            "Training:  Iteration No:  296 Worker Num:  0 \n",
            " Loss:  0.21688498556613922\n",
            "Training:  Iteration No:  296 Worker Num:  1 \n",
            " Loss:  0.2073696106672287\n",
            "Training:  Iteration No:  296 Worker Num:  2 \n",
            " Loss:  0.21501369774341583\n",
            "Training:  Iteration No:  296 Worker Num:  3 \n",
            " Loss:  0.25434333086013794\n",
            "Training:  Iteration No:  296 Worker Num:  4 \n",
            " Loss:  0.23291759192943573\n",
            "Training:  Iteration No:  296 Worker Num:  5 \n",
            " Loss:  0.31370648741722107\n",
            "Training:  Iteration No:  296 Worker Num:  6 \n",
            " Loss:  0.19293183088302612\n",
            "Training:  Iteration No:  296 Worker Num:  7 \n",
            " Loss:  0.11556152999401093\n",
            "Test:  Iteration No:  296 \n",
            " Loss:  0.16806339352875005\n",
            "Test accuracy:  95.02\n",
            "Training:  Iteration No:  297 Worker Num:  0 \n",
            " Loss:  0.26625651121139526\n",
            "Training:  Iteration No:  297 Worker Num:  1 \n",
            " Loss:  0.2108086496591568\n",
            "Training:  Iteration No:  297 Worker Num:  2 \n",
            " Loss:  0.2262582778930664\n",
            "Training:  Iteration No:  297 Worker Num:  3 \n",
            " Loss:  0.20817482471466064\n",
            "Training:  Iteration No:  297 Worker Num:  4 \n",
            " Loss:  0.15917852520942688\n",
            "Training:  Iteration No:  297 Worker Num:  5 \n",
            " Loss:  0.14115369319915771\n",
            "Training:  Iteration No:  297 Worker Num:  6 \n",
            " Loss:  0.1639854609966278\n",
            "Training:  Iteration No:  297 Worker Num:  7 \n",
            " Loss:  0.309663861989975\n",
            "Test:  Iteration No:  297 \n",
            " Loss:  0.1676430180315164\n",
            "Test accuracy:  94.92\n",
            "Training:  Iteration No:  298 Worker Num:  0 \n",
            " Loss:  0.2408280372619629\n",
            "Training:  Iteration No:  298 Worker Num:  1 \n",
            " Loss:  0.18286696076393127\n",
            "Training:  Iteration No:  298 Worker Num:  2 \n",
            " Loss:  0.2314993143081665\n",
            "Training:  Iteration No:  298 Worker Num:  3 \n",
            " Loss:  0.18217134475708008\n",
            "Training:  Iteration No:  298 Worker Num:  4 \n",
            " Loss:  0.1897565722465515\n",
            "Training:  Iteration No:  298 Worker Num:  5 \n",
            " Loss:  0.18354980647563934\n",
            "Training:  Iteration No:  298 Worker Num:  6 \n",
            " Loss:  0.2505457401275635\n",
            "Training:  Iteration No:  298 Worker Num:  7 \n",
            " Loss:  0.16414600610733032\n",
            "Test:  Iteration No:  298 \n",
            " Loss:  0.16714114663367974\n",
            "Test accuracy:  95.14\n",
            "Training:  Iteration No:  299 Worker Num:  0 \n",
            " Loss:  0.24365685880184174\n",
            "Training:  Iteration No:  299 Worker Num:  1 \n",
            " Loss:  0.2526923716068268\n",
            "Training:  Iteration No:  299 Worker Num:  2 \n",
            " Loss:  0.16765721142292023\n",
            "Training:  Iteration No:  299 Worker Num:  3 \n",
            " Loss:  0.2254536747932434\n",
            "Training:  Iteration No:  299 Worker Num:  4 \n",
            " Loss:  0.18767546117305756\n",
            "Training:  Iteration No:  299 Worker Num:  5 \n",
            " Loss:  0.1228073239326477\n",
            "Training:  Iteration No:  299 Worker Num:  6 \n",
            " Loss:  0.13354012370109558\n",
            "Training:  Iteration No:  299 Worker Num:  7 \n",
            " Loss:  0.19047515094280243\n",
            "Test:  Iteration No:  299 \n",
            " Loss:  0.1682076380069403\n",
            "Test accuracy:  95.15\n",
            "Training:  Iteration No:  300 Worker Num:  0 \n",
            " Loss:  0.2113528698682785\n",
            "Training:  Iteration No:  300 Worker Num:  1 \n",
            " Loss:  0.2823454439640045\n",
            "Training:  Iteration No:  300 Worker Num:  2 \n",
            " Loss:  0.2803047299385071\n",
            "Training:  Iteration No:  300 Worker Num:  3 \n",
            " Loss:  0.1204460933804512\n",
            "Training:  Iteration No:  300 Worker Num:  4 \n",
            " Loss:  0.14756126701831818\n",
            "Training:  Iteration No:  300 Worker Num:  5 \n",
            " Loss:  0.24440962076187134\n",
            "Training:  Iteration No:  300 Worker Num:  6 \n",
            " Loss:  0.24077065289020538\n",
            "Training:  Iteration No:  300 Worker Num:  7 \n",
            " Loss:  0.1829722374677658\n",
            "Test:  Iteration No:  300 \n",
            " Loss:  0.16330401281696522\n",
            "Test accuracy:  95.15\n",
            "Training:  Iteration No:  301 Worker Num:  0 \n",
            " Loss:  0.14184287190437317\n",
            "Training:  Iteration No:  301 Worker Num:  1 \n",
            " Loss:  0.15232503414154053\n",
            "Training:  Iteration No:  301 Worker Num:  2 \n",
            " Loss:  0.20037779211997986\n",
            "Training:  Iteration No:  301 Worker Num:  3 \n",
            " Loss:  0.237448588013649\n",
            "Training:  Iteration No:  301 Worker Num:  4 \n",
            " Loss:  0.14287765324115753\n",
            "Training:  Iteration No:  301 Worker Num:  5 \n",
            " Loss:  0.20863115787506104\n",
            "Training:  Iteration No:  301 Worker Num:  6 \n",
            " Loss:  0.15021872520446777\n",
            "Training:  Iteration No:  301 Worker Num:  7 \n",
            " Loss:  0.09120115637779236\n",
            "Test:  Iteration No:  301 \n",
            " Loss:  0.16572246045062813\n",
            "Test accuracy:  95.08\n",
            "Training:  Iteration No:  302 Worker Num:  0 \n",
            " Loss:  0.2813336253166199\n",
            "Training:  Iteration No:  302 Worker Num:  1 \n",
            " Loss:  0.2540425956249237\n",
            "Training:  Iteration No:  302 Worker Num:  2 \n",
            " Loss:  0.3672420382499695\n",
            "Training:  Iteration No:  302 Worker Num:  3 \n",
            " Loss:  0.2596133351325989\n",
            "Training:  Iteration No:  302 Worker Num:  4 \n",
            " Loss:  0.25407272577285767\n",
            "Training:  Iteration No:  302 Worker Num:  5 \n",
            " Loss:  0.17252029478549957\n",
            "Training:  Iteration No:  302 Worker Num:  6 \n",
            " Loss:  0.23941375315189362\n",
            "Training:  Iteration No:  302 Worker Num:  7 \n",
            " Loss:  0.2220987230539322\n",
            "Test:  Iteration No:  302 \n",
            " Loss:  0.16864057423994888\n",
            "Test accuracy:  94.86\n",
            "Training:  Iteration No:  303 Worker Num:  0 \n",
            " Loss:  0.2474692165851593\n",
            "Training:  Iteration No:  303 Worker Num:  1 \n",
            " Loss:  0.19645389914512634\n",
            "Training:  Iteration No:  303 Worker Num:  2 \n",
            " Loss:  0.1682918667793274\n",
            "Training:  Iteration No:  303 Worker Num:  3 \n",
            " Loss:  0.12229587137699127\n",
            "Training:  Iteration No:  303 Worker Num:  4 \n",
            " Loss:  0.19099055230617523\n",
            "Training:  Iteration No:  303 Worker Num:  5 \n",
            " Loss:  0.28303441405296326\n",
            "Training:  Iteration No:  303 Worker Num:  6 \n",
            " Loss:  0.1999271810054779\n",
            "Training:  Iteration No:  303 Worker Num:  7 \n",
            " Loss:  0.0786581039428711\n",
            "Test:  Iteration No:  303 \n",
            " Loss:  0.16344867373781302\n",
            "Test accuracy:  95.16\n",
            "Training:  Iteration No:  304 Worker Num:  0 \n",
            " Loss:  0.20043355226516724\n",
            "Training:  Iteration No:  304 Worker Num:  1 \n",
            " Loss:  0.18560433387756348\n",
            "Training:  Iteration No:  304 Worker Num:  2 \n",
            " Loss:  0.10237690061330795\n",
            "Training:  Iteration No:  304 Worker Num:  3 \n",
            " Loss:  0.20304827392101288\n",
            "Training:  Iteration No:  304 Worker Num:  4 \n",
            " Loss:  0.1958598792552948\n",
            "Training:  Iteration No:  304 Worker Num:  5 \n",
            " Loss:  0.17251001298427582\n",
            "Training:  Iteration No:  304 Worker Num:  6 \n",
            " Loss:  0.16994303464889526\n",
            "Training:  Iteration No:  304 Worker Num:  7 \n",
            " Loss:  0.16493044793605804\n",
            "Test:  Iteration No:  304 \n",
            " Loss:  0.1620694701854564\n",
            "Test accuracy:  95.05\n",
            "Training:  Iteration No:  305 Worker Num:  0 \n",
            " Loss:  0.14582520723342896\n",
            "Training:  Iteration No:  305 Worker Num:  1 \n",
            " Loss:  0.1219940185546875\n",
            "Training:  Iteration No:  305 Worker Num:  2 \n",
            " Loss:  0.19608129560947418\n",
            "Training:  Iteration No:  305 Worker Num:  3 \n",
            " Loss:  0.2987201511859894\n",
            "Training:  Iteration No:  305 Worker Num:  4 \n",
            " Loss:  0.183171808719635\n",
            "Training:  Iteration No:  305 Worker Num:  5 \n",
            " Loss:  0.19482970237731934\n",
            "Training:  Iteration No:  305 Worker Num:  6 \n",
            " Loss:  0.25270187854766846\n",
            "Training:  Iteration No:  305 Worker Num:  7 \n",
            " Loss:  0.2883722484111786\n",
            "Test:  Iteration No:  305 \n",
            " Loss:  0.16287914976548357\n",
            "Test accuracy:  95.08\n",
            "Training:  Iteration No:  306 Worker Num:  0 \n",
            " Loss:  0.20232418179512024\n",
            "Training:  Iteration No:  306 Worker Num:  1 \n",
            " Loss:  0.3248419761657715\n",
            "Training:  Iteration No:  306 Worker Num:  2 \n",
            " Loss:  0.2460261583328247\n",
            "Training:  Iteration No:  306 Worker Num:  3 \n",
            " Loss:  0.16345234215259552\n",
            "Training:  Iteration No:  306 Worker Num:  4 \n",
            " Loss:  0.12320071458816528\n",
            "Training:  Iteration No:  306 Worker Num:  5 \n",
            " Loss:  0.19955120980739594\n",
            "Training:  Iteration No:  306 Worker Num:  6 \n",
            " Loss:  0.2338142693042755\n",
            "Training:  Iteration No:  306 Worker Num:  7 \n",
            " Loss:  0.14318490028381348\n",
            "Test:  Iteration No:  306 \n",
            " Loss:  0.16236188809116242\n",
            "Test accuracy:  95.16\n",
            "Training:  Iteration No:  307 Worker Num:  0 \n",
            " Loss:  0.1194210797548294\n",
            "Training:  Iteration No:  307 Worker Num:  1 \n",
            " Loss:  0.1752598136663437\n",
            "Training:  Iteration No:  307 Worker Num:  2 \n",
            " Loss:  0.15374720096588135\n",
            "Training:  Iteration No:  307 Worker Num:  3 \n",
            " Loss:  0.33727672696113586\n",
            "Training:  Iteration No:  307 Worker Num:  4 \n",
            " Loss:  0.2860738933086395\n",
            "Training:  Iteration No:  307 Worker Num:  5 \n",
            " Loss:  0.3120778501033783\n",
            "Training:  Iteration No:  307 Worker Num:  6 \n",
            " Loss:  0.34557658433914185\n",
            "Training:  Iteration No:  307 Worker Num:  7 \n",
            " Loss:  0.1737722009420395\n",
            "Test:  Iteration No:  307 \n",
            " Loss:  0.16260109649664617\n",
            "Test accuracy:  95.2\n",
            "Training:  Iteration No:  308 Worker Num:  0 \n",
            " Loss:  0.19812320172786713\n",
            "Training:  Iteration No:  308 Worker Num:  1 \n",
            " Loss:  0.21240954101085663\n",
            "Training:  Iteration No:  308 Worker Num:  2 \n",
            " Loss:  0.16091430187225342\n",
            "Training:  Iteration No:  308 Worker Num:  3 \n",
            " Loss:  0.15593762695789337\n",
            "Training:  Iteration No:  308 Worker Num:  4 \n",
            " Loss:  0.14083287119865417\n",
            "Training:  Iteration No:  308 Worker Num:  5 \n",
            " Loss:  0.18115831911563873\n",
            "Training:  Iteration No:  308 Worker Num:  6 \n",
            " Loss:  0.1386737823486328\n",
            "Training:  Iteration No:  308 Worker Num:  7 \n",
            " Loss:  0.11567287892103195\n",
            "Test:  Iteration No:  308 \n",
            " Loss:  0.16072617438798653\n",
            "Test accuracy:  95.17\n",
            "Training:  Iteration No:  309 Worker Num:  0 \n",
            " Loss:  0.2326459437608719\n",
            "Training:  Iteration No:  309 Worker Num:  1 \n",
            " Loss:  0.19366921484470367\n",
            "Training:  Iteration No:  309 Worker Num:  2 \n",
            " Loss:  0.21590504050254822\n",
            "Training:  Iteration No:  309 Worker Num:  3 \n",
            " Loss:  0.19175677001476288\n",
            "Training:  Iteration No:  309 Worker Num:  4 \n",
            " Loss:  0.15678037703037262\n",
            "Training:  Iteration No:  309 Worker Num:  5 \n",
            " Loss:  0.1868390440940857\n",
            "Training:  Iteration No:  309 Worker Num:  6 \n",
            " Loss:  0.2837165594100952\n",
            "Training:  Iteration No:  309 Worker Num:  7 \n",
            " Loss:  0.15346312522888184\n",
            "Test:  Iteration No:  309 \n",
            " Loss:  0.16321025864232944\n",
            "Test accuracy:  95.13\n",
            "Training:  Iteration No:  310 Worker Num:  0 \n",
            " Loss:  0.16951832175254822\n",
            "Training:  Iteration No:  310 Worker Num:  1 \n",
            " Loss:  0.2293902337551117\n",
            "Training:  Iteration No:  310 Worker Num:  2 \n",
            " Loss:  0.22883325815200806\n",
            "Training:  Iteration No:  310 Worker Num:  3 \n",
            " Loss:  0.15128850936889648\n",
            "Training:  Iteration No:  310 Worker Num:  4 \n",
            " Loss:  0.18852651119232178\n",
            "Training:  Iteration No:  310 Worker Num:  5 \n",
            " Loss:  0.26274964213371277\n",
            "Training:  Iteration No:  310 Worker Num:  6 \n",
            " Loss:  0.16858553886413574\n",
            "Training:  Iteration No:  310 Worker Num:  7 \n",
            " Loss:  0.2755967974662781\n",
            "Test:  Iteration No:  310 \n",
            " Loss:  0.164945380202247\n",
            "Test accuracy:  94.99\n",
            "Training:  Iteration No:  311 Worker Num:  0 \n",
            " Loss:  0.1400855928659439\n",
            "Training:  Iteration No:  311 Worker Num:  1 \n",
            " Loss:  0.15303553640842438\n",
            "Training:  Iteration No:  311 Worker Num:  2 \n",
            " Loss:  0.20380344986915588\n",
            "Training:  Iteration No:  311 Worker Num:  3 \n",
            " Loss:  0.13615453243255615\n",
            "Training:  Iteration No:  311 Worker Num:  4 \n",
            " Loss:  0.26989448070526123\n",
            "Training:  Iteration No:  311 Worker Num:  5 \n",
            " Loss:  0.16593332588672638\n",
            "Training:  Iteration No:  311 Worker Num:  6 \n",
            " Loss:  0.3075094223022461\n",
            "Training:  Iteration No:  311 Worker Num:  7 \n",
            " Loss:  0.1720850169658661\n",
            "Test:  Iteration No:  311 \n",
            " Loss:  0.1620229919900811\n",
            "Test accuracy:  95.19\n",
            "Training:  Iteration No:  312 Worker Num:  0 \n",
            " Loss:  0.24646346271038055\n",
            "Training:  Iteration No:  312 Worker Num:  1 \n",
            " Loss:  0.1322658658027649\n",
            "Training:  Iteration No:  312 Worker Num:  2 \n",
            " Loss:  0.1417187601327896\n",
            "Training:  Iteration No:  312 Worker Num:  3 \n",
            " Loss:  0.1669130027294159\n",
            "Training:  Iteration No:  312 Worker Num:  4 \n",
            " Loss:  0.1327318698167801\n",
            "Training:  Iteration No:  312 Worker Num:  5 \n",
            " Loss:  0.11898999661207199\n",
            "Training:  Iteration No:  312 Worker Num:  6 \n",
            " Loss:  0.2774149477481842\n",
            "Training:  Iteration No:  312 Worker Num:  7 \n",
            " Loss:  0.2136814445257187\n",
            "Test:  Iteration No:  312 \n",
            " Loss:  0.15897621035788068\n",
            "Test accuracy:  95.28\n",
            "Training:  Iteration No:  313 Worker Num:  0 \n",
            " Loss:  0.217548206448555\n",
            "Training:  Iteration No:  313 Worker Num:  1 \n",
            " Loss:  0.1723654717206955\n",
            "Training:  Iteration No:  313 Worker Num:  2 \n",
            " Loss:  0.224320650100708\n",
            "Training:  Iteration No:  313 Worker Num:  3 \n",
            " Loss:  0.1632593274116516\n",
            "Training:  Iteration No:  313 Worker Num:  4 \n",
            " Loss:  0.15874330699443817\n",
            "Training:  Iteration No:  313 Worker Num:  5 \n",
            " Loss:  0.3386930227279663\n",
            "Training:  Iteration No:  313 Worker Num:  6 \n",
            " Loss:  0.19285278022289276\n",
            "Training:  Iteration No:  313 Worker Num:  7 \n",
            " Loss:  0.1782284528017044\n",
            "Test:  Iteration No:  313 \n",
            " Loss:  0.16064129552886455\n",
            "Test accuracy:  95.32\n",
            "Training:  Iteration No:  314 Worker Num:  0 \n",
            " Loss:  0.12893858551979065\n",
            "Training:  Iteration No:  314 Worker Num:  1 \n",
            " Loss:  0.23706269264221191\n",
            "Training:  Iteration No:  314 Worker Num:  2 \n",
            " Loss:  0.15708541870117188\n",
            "Training:  Iteration No:  314 Worker Num:  3 \n",
            " Loss:  0.1912541389465332\n",
            "Training:  Iteration No:  314 Worker Num:  4 \n",
            " Loss:  0.14398156106472015\n",
            "Training:  Iteration No:  314 Worker Num:  5 \n",
            " Loss:  0.09949649125337601\n",
            "Training:  Iteration No:  314 Worker Num:  6 \n",
            " Loss:  0.2655717134475708\n",
            "Training:  Iteration No:  314 Worker Num:  7 \n",
            " Loss:  0.19800756871700287\n",
            "Test:  Iteration No:  314 \n",
            " Loss:  0.1607700638779545\n",
            "Test accuracy:  95.24\n",
            "Training:  Iteration No:  315 Worker Num:  0 \n",
            " Loss:  0.19928725063800812\n",
            "Training:  Iteration No:  315 Worker Num:  1 \n",
            " Loss:  0.26539739966392517\n",
            "Training:  Iteration No:  315 Worker Num:  2 \n",
            " Loss:  0.22641685605049133\n",
            "Training:  Iteration No:  315 Worker Num:  3 \n",
            " Loss:  0.20292672514915466\n",
            "Training:  Iteration No:  315 Worker Num:  4 \n",
            " Loss:  0.20328791439533234\n",
            "Training:  Iteration No:  315 Worker Num:  5 \n",
            " Loss:  0.2508559226989746\n",
            "Training:  Iteration No:  315 Worker Num:  6 \n",
            " Loss:  0.2240702509880066\n",
            "Training:  Iteration No:  315 Worker Num:  7 \n",
            " Loss:  0.2568753957748413\n",
            "Test:  Iteration No:  315 \n",
            " Loss:  0.1595258494320361\n",
            "Test accuracy:  95.24\n",
            "Training:  Iteration No:  316 Worker Num:  0 \n",
            " Loss:  0.23234625160694122\n",
            "Training:  Iteration No:  316 Worker Num:  1 \n",
            " Loss:  0.24491055309772491\n",
            "Training:  Iteration No:  316 Worker Num:  2 \n",
            " Loss:  0.280988872051239\n",
            "Training:  Iteration No:  316 Worker Num:  3 \n",
            " Loss:  0.10797805339097977\n",
            "Training:  Iteration No:  316 Worker Num:  4 \n",
            " Loss:  0.2747715413570404\n",
            "Training:  Iteration No:  316 Worker Num:  5 \n",
            " Loss:  0.12010469287633896\n",
            "Training:  Iteration No:  316 Worker Num:  6 \n",
            " Loss:  0.21782948076725006\n",
            "Training:  Iteration No:  316 Worker Num:  7 \n",
            " Loss:  0.07241877168416977\n",
            "Test:  Iteration No:  316 \n",
            " Loss:  0.15701241061515822\n",
            "Test accuracy:  95.33\n",
            "Training:  Iteration No:  317 Worker Num:  0 \n",
            " Loss:  0.20936046540737152\n",
            "Training:  Iteration No:  317 Worker Num:  1 \n",
            " Loss:  0.18980377912521362\n",
            "Training:  Iteration No:  317 Worker Num:  2 \n",
            " Loss:  0.3623674809932709\n",
            "Training:  Iteration No:  317 Worker Num:  3 \n",
            " Loss:  0.154446542263031\n",
            "Training:  Iteration No:  317 Worker Num:  4 \n",
            " Loss:  0.23934252560138702\n",
            "Training:  Iteration No:  317 Worker Num:  5 \n",
            " Loss:  0.19468674063682556\n",
            "Training:  Iteration No:  317 Worker Num:  6 \n",
            " Loss:  0.15861555933952332\n",
            "Training:  Iteration No:  317 Worker Num:  7 \n",
            " Loss:  0.3190881907939911\n",
            "Test:  Iteration No:  317 \n",
            " Loss:  0.15715615686524328\n",
            "Test accuracy:  95.33\n",
            "Training:  Iteration No:  318 Worker Num:  0 \n",
            " Loss:  0.2683109939098358\n",
            "Training:  Iteration No:  318 Worker Num:  1 \n",
            " Loss:  0.2889534533023834\n",
            "Training:  Iteration No:  318 Worker Num:  2 \n",
            " Loss:  0.1382879912853241\n",
            "Training:  Iteration No:  318 Worker Num:  3 \n",
            " Loss:  0.19629862904548645\n",
            "Training:  Iteration No:  318 Worker Num:  4 \n",
            " Loss:  0.32847997546195984\n",
            "Training:  Iteration No:  318 Worker Num:  5 \n",
            " Loss:  0.2285267561674118\n",
            "Training:  Iteration No:  318 Worker Num:  6 \n",
            " Loss:  0.22271747887134552\n",
            "Training:  Iteration No:  318 Worker Num:  7 \n",
            " Loss:  0.17633205652236938\n",
            "Test:  Iteration No:  318 \n",
            " Loss:  0.155721077023522\n",
            "Test accuracy:  95.31\n",
            "Training:  Iteration No:  319 Worker Num:  0 \n",
            " Loss:  0.19311095774173737\n",
            "Training:  Iteration No:  319 Worker Num:  1 \n",
            " Loss:  0.10355126112699509\n",
            "Training:  Iteration No:  319 Worker Num:  2 \n",
            " Loss:  0.13325217366218567\n",
            "Training:  Iteration No:  319 Worker Num:  3 \n",
            " Loss:  0.1987767070531845\n",
            "Training:  Iteration No:  319 Worker Num:  4 \n",
            " Loss:  0.16435697674751282\n",
            "Training:  Iteration No:  319 Worker Num:  5 \n",
            " Loss:  0.16986802220344543\n",
            "Training:  Iteration No:  319 Worker Num:  6 \n",
            " Loss:  0.21960893273353577\n",
            "Training:  Iteration No:  319 Worker Num:  7 \n",
            " Loss:  0.1525414139032364\n",
            "Test:  Iteration No:  319 \n",
            " Loss:  0.15552167506605574\n",
            "Test accuracy:  95.26\n",
            "Training:  Iteration No:  320 Worker Num:  0 \n",
            " Loss:  0.19821442663669586\n",
            "Training:  Iteration No:  320 Worker Num:  1 \n",
            " Loss:  0.17366747558116913\n",
            "Training:  Iteration No:  320 Worker Num:  2 \n",
            " Loss:  0.2784970998764038\n",
            "Training:  Iteration No:  320 Worker Num:  3 \n",
            " Loss:  0.2070837765932083\n",
            "Training:  Iteration No:  320 Worker Num:  4 \n",
            " Loss:  0.3473415970802307\n",
            "Training:  Iteration No:  320 Worker Num:  5 \n",
            " Loss:  0.16822880506515503\n",
            "Training:  Iteration No:  320 Worker Num:  6 \n",
            " Loss:  0.13328775763511658\n",
            "Training:  Iteration No:  320 Worker Num:  7 \n",
            " Loss:  0.276884526014328\n",
            "Test:  Iteration No:  320 \n",
            " Loss:  0.1561840678258693\n",
            "Test accuracy:  95.3\n",
            "Training:  Iteration No:  321 Worker Num:  0 \n",
            " Loss:  0.2568390369415283\n",
            "Training:  Iteration No:  321 Worker Num:  1 \n",
            " Loss:  0.19800348579883575\n",
            "Training:  Iteration No:  321 Worker Num:  2 \n",
            " Loss:  0.16890205442905426\n",
            "Training:  Iteration No:  321 Worker Num:  3 \n",
            " Loss:  0.14838717877864838\n",
            "Training:  Iteration No:  321 Worker Num:  4 \n",
            " Loss:  0.24108614027500153\n",
            "Training:  Iteration No:  321 Worker Num:  5 \n",
            " Loss:  0.16313926875591278\n",
            "Training:  Iteration No:  321 Worker Num:  6 \n",
            " Loss:  0.24003776907920837\n",
            "Training:  Iteration No:  321 Worker Num:  7 \n",
            " Loss:  0.36518725752830505\n",
            "Test:  Iteration No:  321 \n",
            " Loss:  0.15286072843436954\n",
            "Test accuracy:  95.44\n",
            "Training:  Iteration No:  322 Worker Num:  0 \n",
            " Loss:  0.14693425595760345\n",
            "Training:  Iteration No:  322 Worker Num:  1 \n",
            " Loss:  0.176373690366745\n",
            "Training:  Iteration No:  322 Worker Num:  2 \n",
            " Loss:  0.1567113697528839\n",
            "Training:  Iteration No:  322 Worker Num:  3 \n",
            " Loss:  0.30394819378852844\n",
            "Training:  Iteration No:  322 Worker Num:  4 \n",
            " Loss:  0.10932665318250656\n",
            "Training:  Iteration No:  322 Worker Num:  5 \n",
            " Loss:  0.09830686450004578\n",
            "Training:  Iteration No:  322 Worker Num:  6 \n",
            " Loss:  0.19631227850914001\n",
            "Training:  Iteration No:  322 Worker Num:  7 \n",
            " Loss:  0.21657618880271912\n",
            "Test:  Iteration No:  322 \n",
            " Loss:  0.1543948063214274\n",
            "Test accuracy:  95.38\n",
            "Training:  Iteration No:  323 Worker Num:  0 \n",
            " Loss:  0.22512602806091309\n",
            "Training:  Iteration No:  323 Worker Num:  1 \n",
            " Loss:  0.16278591752052307\n",
            "Training:  Iteration No:  323 Worker Num:  2 \n",
            " Loss:  0.2575070261955261\n",
            "Training:  Iteration No:  323 Worker Num:  3 \n",
            " Loss:  0.12300723046064377\n",
            "Training:  Iteration No:  323 Worker Num:  4 \n",
            " Loss:  0.23192347586154938\n",
            "Training:  Iteration No:  323 Worker Num:  5 \n",
            " Loss:  0.09743302315473557\n",
            "Training:  Iteration No:  323 Worker Num:  6 \n",
            " Loss:  0.2702028751373291\n",
            "Training:  Iteration No:  323 Worker Num:  7 \n",
            " Loss:  0.24564328789710999\n",
            "Test:  Iteration No:  323 \n",
            " Loss:  0.15206529583805536\n",
            "Test accuracy:  95.48\n",
            "Training:  Iteration No:  324 Worker Num:  0 \n",
            " Loss:  0.187774196267128\n",
            "Training:  Iteration No:  324 Worker Num:  1 \n",
            " Loss:  0.196528360247612\n",
            "Training:  Iteration No:  324 Worker Num:  2 \n",
            " Loss:  0.23571257293224335\n",
            "Training:  Iteration No:  324 Worker Num:  3 \n",
            " Loss:  0.1562633514404297\n",
            "Training:  Iteration No:  324 Worker Num:  4 \n",
            " Loss:  0.20381081104278564\n",
            "Training:  Iteration No:  324 Worker Num:  5 \n",
            " Loss:  0.24549250304698944\n",
            "Training:  Iteration No:  324 Worker Num:  6 \n",
            " Loss:  0.1798945516347885\n",
            "Training:  Iteration No:  324 Worker Num:  7 \n",
            " Loss:  0.31720125675201416\n",
            "Test:  Iteration No:  324 \n",
            " Loss:  0.15286285236855096\n",
            "Test accuracy:  95.43\n",
            "Training:  Iteration No:  325 Worker Num:  0 \n",
            " Loss:  0.29929131269454956\n",
            "Training:  Iteration No:  325 Worker Num:  1 \n",
            " Loss:  0.11524231731891632\n",
            "Training:  Iteration No:  325 Worker Num:  2 \n",
            " Loss:  0.17590637505054474\n",
            "Training:  Iteration No:  325 Worker Num:  3 \n",
            " Loss:  0.12251363694667816\n",
            "Training:  Iteration No:  325 Worker Num:  4 \n",
            " Loss:  0.1843530535697937\n",
            "Training:  Iteration No:  325 Worker Num:  5 \n",
            " Loss:  0.18169741332530975\n",
            "Training:  Iteration No:  325 Worker Num:  6 \n",
            " Loss:  0.11716174334287643\n",
            "Training:  Iteration No:  325 Worker Num:  7 \n",
            " Loss:  0.12009754776954651\n",
            "Test:  Iteration No:  325 \n",
            " Loss:  0.15196509124664\n",
            "Test accuracy:  95.5\n",
            "Training:  Iteration No:  326 Worker Num:  0 \n",
            " Loss:  0.1705259382724762\n",
            "Training:  Iteration No:  326 Worker Num:  1 \n",
            " Loss:  0.13058559596538544\n",
            "Training:  Iteration No:  326 Worker Num:  2 \n",
            " Loss:  0.13250039517879486\n",
            "Training:  Iteration No:  326 Worker Num:  3 \n",
            " Loss:  0.1613561511039734\n",
            "Training:  Iteration No:  326 Worker Num:  4 \n",
            " Loss:  0.1754005402326584\n",
            "Training:  Iteration No:  326 Worker Num:  5 \n",
            " Loss:  0.18585047125816345\n",
            "Training:  Iteration No:  326 Worker Num:  6 \n",
            " Loss:  0.2291204333305359\n",
            "Training:  Iteration No:  326 Worker Num:  7 \n",
            " Loss:  0.17166541516780853\n",
            "Test:  Iteration No:  326 \n",
            " Loss:  0.15386681801556976\n",
            "Test accuracy:  95.43\n",
            "Training:  Iteration No:  327 Worker Num:  0 \n",
            " Loss:  0.1894107162952423\n",
            "Training:  Iteration No:  327 Worker Num:  1 \n",
            " Loss:  0.2185213714838028\n",
            "Training:  Iteration No:  327 Worker Num:  2 \n",
            " Loss:  0.287679523229599\n",
            "Training:  Iteration No:  327 Worker Num:  3 \n",
            " Loss:  0.16175733506679535\n",
            "Training:  Iteration No:  327 Worker Num:  4 \n",
            " Loss:  0.1744583696126938\n",
            "Training:  Iteration No:  327 Worker Num:  5 \n",
            " Loss:  0.2134578973054886\n",
            "Training:  Iteration No:  327 Worker Num:  6 \n",
            " Loss:  0.15141162276268005\n",
            "Training:  Iteration No:  327 Worker Num:  7 \n",
            " Loss:  0.17722389101982117\n",
            "Test:  Iteration No:  327 \n",
            " Loss:  0.15299455423049535\n",
            "Test accuracy:  95.44\n",
            "Training:  Iteration No:  328 Worker Num:  0 \n",
            " Loss:  0.11018455773591995\n",
            "Training:  Iteration No:  328 Worker Num:  1 \n",
            " Loss:  0.12547567486763\n",
            "Training:  Iteration No:  328 Worker Num:  2 \n",
            " Loss:  0.1198464035987854\n",
            "Training:  Iteration No:  328 Worker Num:  3 \n",
            " Loss:  0.20391784608364105\n",
            "Training:  Iteration No:  328 Worker Num:  4 \n",
            " Loss:  0.18759401142597198\n",
            "Training:  Iteration No:  328 Worker Num:  5 \n",
            " Loss:  0.2506991922855377\n",
            "Training:  Iteration No:  328 Worker Num:  6 \n",
            " Loss:  0.3042721450328827\n",
            "Training:  Iteration No:  328 Worker Num:  7 \n",
            " Loss:  0.2339569330215454\n",
            "Test:  Iteration No:  328 \n",
            " Loss:  0.1513144726027982\n",
            "Test accuracy:  95.44\n",
            "Training:  Iteration No:  329 Worker Num:  0 \n",
            " Loss:  0.2123422771692276\n",
            "Training:  Iteration No:  329 Worker Num:  1 \n",
            " Loss:  0.15717171132564545\n",
            "Training:  Iteration No:  329 Worker Num:  2 \n",
            " Loss:  0.1673126220703125\n",
            "Training:  Iteration No:  329 Worker Num:  3 \n",
            " Loss:  0.27973806858062744\n",
            "Training:  Iteration No:  329 Worker Num:  4 \n",
            " Loss:  0.20500999689102173\n",
            "Training:  Iteration No:  329 Worker Num:  5 \n",
            " Loss:  0.16499340534210205\n",
            "Training:  Iteration No:  329 Worker Num:  6 \n",
            " Loss:  0.2530811131000519\n",
            "Training:  Iteration No:  329 Worker Num:  7 \n",
            " Loss:  0.18600279092788696\n",
            "Test:  Iteration No:  329 \n",
            " Loss:  0.15005788392729208\n",
            "Test accuracy:  95.53\n",
            "Training:  Iteration No:  330 Worker Num:  0 \n",
            " Loss:  0.057953063398599625\n",
            "Training:  Iteration No:  330 Worker Num:  1 \n",
            " Loss:  0.1704496145248413\n",
            "Training:  Iteration No:  330 Worker Num:  2 \n",
            " Loss:  0.20444239675998688\n",
            "Training:  Iteration No:  330 Worker Num:  3 \n",
            " Loss:  0.19307471811771393\n",
            "Training:  Iteration No:  330 Worker Num:  4 \n",
            " Loss:  0.21516768634319305\n",
            "Training:  Iteration No:  330 Worker Num:  5 \n",
            " Loss:  0.26872143149375916\n",
            "Training:  Iteration No:  330 Worker Num:  6 \n",
            " Loss:  0.12778812646865845\n",
            "Training:  Iteration No:  330 Worker Num:  7 \n",
            " Loss:  0.12398877739906311\n",
            "Test:  Iteration No:  330 \n",
            " Loss:  0.15173630201835422\n",
            "Test accuracy:  95.33\n",
            "Training:  Iteration No:  331 Worker Num:  0 \n",
            " Loss:  0.1278800070285797\n",
            "Training:  Iteration No:  331 Worker Num:  1 \n",
            " Loss:  0.1949957013130188\n",
            "Training:  Iteration No:  331 Worker Num:  2 \n",
            " Loss:  0.12200599163770676\n",
            "Training:  Iteration No:  331 Worker Num:  3 \n",
            " Loss:  0.1046387255191803\n",
            "Training:  Iteration No:  331 Worker Num:  4 \n",
            " Loss:  0.2571563422679901\n",
            "Training:  Iteration No:  331 Worker Num:  5 \n",
            " Loss:  0.22005154192447662\n",
            "Training:  Iteration No:  331 Worker Num:  6 \n",
            " Loss:  0.20640155673027039\n",
            "Training:  Iteration No:  331 Worker Num:  7 \n",
            " Loss:  0.3335014879703522\n",
            "Test:  Iteration No:  331 \n",
            " Loss:  0.1515154451414754\n",
            "Test accuracy:  95.59\n",
            "Training:  Iteration No:  332 Worker Num:  0 \n",
            " Loss:  0.37431907653808594\n",
            "Training:  Iteration No:  332 Worker Num:  1 \n",
            " Loss:  0.2213771939277649\n",
            "Training:  Iteration No:  332 Worker Num:  2 \n",
            " Loss:  0.1910635083913803\n",
            "Training:  Iteration No:  332 Worker Num:  3 \n",
            " Loss:  0.08526510745286942\n",
            "Training:  Iteration No:  332 Worker Num:  4 \n",
            " Loss:  0.20344649255275726\n",
            "Training:  Iteration No:  332 Worker Num:  5 \n",
            " Loss:  0.15843388438224792\n",
            "Training:  Iteration No:  332 Worker Num:  6 \n",
            " Loss:  0.22302338480949402\n",
            "Training:  Iteration No:  332 Worker Num:  7 \n",
            " Loss:  0.11577276140451431\n",
            "Test:  Iteration No:  332 \n",
            " Loss:  0.1496991126597682\n",
            "Test accuracy:  95.63\n",
            "Training:  Iteration No:  333 Worker Num:  0 \n",
            " Loss:  0.14794892072677612\n",
            "Training:  Iteration No:  333 Worker Num:  1 \n",
            " Loss:  0.16646556556224823\n",
            "Training:  Iteration No:  333 Worker Num:  2 \n",
            " Loss:  0.14846518635749817\n",
            "Training:  Iteration No:  333 Worker Num:  3 \n",
            " Loss:  0.249030202627182\n",
            "Training:  Iteration No:  333 Worker Num:  4 \n",
            " Loss:  0.11526388674974442\n",
            "Training:  Iteration No:  333 Worker Num:  5 \n",
            " Loss:  0.18945789337158203\n",
            "Training:  Iteration No:  333 Worker Num:  6 \n",
            " Loss:  0.18964612483978271\n",
            "Training:  Iteration No:  333 Worker Num:  7 \n",
            " Loss:  0.11294224113225937\n",
            "Test:  Iteration No:  333 \n",
            " Loss:  0.1514652958611333\n",
            "Test accuracy:  95.58\n",
            "Training:  Iteration No:  334 Worker Num:  0 \n",
            " Loss:  0.16124683618545532\n",
            "Training:  Iteration No:  334 Worker Num:  1 \n",
            " Loss:  0.26274144649505615\n",
            "Training:  Iteration No:  334 Worker Num:  2 \n",
            " Loss:  0.3559682369232178\n",
            "Training:  Iteration No:  334 Worker Num:  3 \n",
            " Loss:  0.20771165192127228\n",
            "Training:  Iteration No:  334 Worker Num:  4 \n",
            " Loss:  0.1328336000442505\n",
            "Training:  Iteration No:  334 Worker Num:  5 \n",
            " Loss:  0.16750194132328033\n",
            "Training:  Iteration No:  334 Worker Num:  6 \n",
            " Loss:  0.22196796536445618\n",
            "Training:  Iteration No:  334 Worker Num:  7 \n",
            " Loss:  0.18782258033752441\n",
            "Test:  Iteration No:  334 \n",
            " Loss:  0.14872560024120007\n",
            "Test accuracy:  95.61\n",
            "Training:  Iteration No:  335 Worker Num:  0 \n",
            " Loss:  0.15871652960777283\n",
            "Training:  Iteration No:  335 Worker Num:  1 \n",
            " Loss:  0.2744416296482086\n",
            "Training:  Iteration No:  335 Worker Num:  2 \n",
            " Loss:  0.21483644843101501\n",
            "Training:  Iteration No:  335 Worker Num:  3 \n",
            " Loss:  0.3178510367870331\n",
            "Training:  Iteration No:  335 Worker Num:  4 \n",
            " Loss:  0.20440499484539032\n",
            "Training:  Iteration No:  335 Worker Num:  5 \n",
            " Loss:  0.12377460300922394\n",
            "Training:  Iteration No:  335 Worker Num:  6 \n",
            " Loss:  0.22554431855678558\n",
            "Training:  Iteration No:  335 Worker Num:  7 \n",
            " Loss:  0.14488549530506134\n",
            "Test:  Iteration No:  335 \n",
            " Loss:  0.15465608558063454\n",
            "Test accuracy:  95.6\n",
            "Training:  Iteration No:  336 Worker Num:  0 \n",
            " Loss:  0.14080291986465454\n",
            "Training:  Iteration No:  336 Worker Num:  1 \n",
            " Loss:  0.09264197200536728\n",
            "Training:  Iteration No:  336 Worker Num:  2 \n",
            " Loss:  0.18823762238025665\n",
            "Training:  Iteration No:  336 Worker Num:  3 \n",
            " Loss:  0.18869638442993164\n",
            "Training:  Iteration No:  336 Worker Num:  4 \n",
            " Loss:  0.24541684985160828\n",
            "Training:  Iteration No:  336 Worker Num:  5 \n",
            " Loss:  0.17950353026390076\n",
            "Training:  Iteration No:  336 Worker Num:  6 \n",
            " Loss:  0.3222542405128479\n",
            "Training:  Iteration No:  336 Worker Num:  7 \n",
            " Loss:  0.2101030796766281\n",
            "Test:  Iteration No:  336 \n",
            " Loss:  0.15455712461136753\n",
            "Test accuracy:  95.31\n",
            "Training:  Iteration No:  337 Worker Num:  0 \n",
            " Loss:  0.1984957456588745\n",
            "Training:  Iteration No:  337 Worker Num:  1 \n",
            " Loss:  0.1193864494562149\n",
            "Training:  Iteration No:  337 Worker Num:  2 \n",
            " Loss:  0.1783335953950882\n",
            "Training:  Iteration No:  337 Worker Num:  3 \n",
            " Loss:  0.20690862834453583\n",
            "Training:  Iteration No:  337 Worker Num:  4 \n",
            " Loss:  0.1517840325832367\n",
            "Training:  Iteration No:  337 Worker Num:  5 \n",
            " Loss:  0.14964762330055237\n",
            "Training:  Iteration No:  337 Worker Num:  6 \n",
            " Loss:  0.17645931243896484\n",
            "Training:  Iteration No:  337 Worker Num:  7 \n",
            " Loss:  0.17493565380573273\n",
            "Test:  Iteration No:  337 \n",
            " Loss:  0.1490935965712312\n",
            "Test accuracy:  95.62\n",
            "Training:  Iteration No:  338 Worker Num:  0 \n",
            " Loss:  0.107310451567173\n",
            "Training:  Iteration No:  338 Worker Num:  1 \n",
            " Loss:  0.24442453682422638\n",
            "Training:  Iteration No:  338 Worker Num:  2 \n",
            " Loss:  0.19870305061340332\n",
            "Training:  Iteration No:  338 Worker Num:  3 \n",
            " Loss:  0.1763264536857605\n",
            "Training:  Iteration No:  338 Worker Num:  4 \n",
            " Loss:  0.13727246224880219\n",
            "Training:  Iteration No:  338 Worker Num:  5 \n",
            " Loss:  0.18630684912204742\n",
            "Training:  Iteration No:  338 Worker Num:  6 \n",
            " Loss:  0.16233448684215546\n",
            "Training:  Iteration No:  338 Worker Num:  7 \n",
            " Loss:  0.22122223675251007\n",
            "Test:  Iteration No:  338 \n",
            " Loss:  0.14955788732305925\n",
            "Test accuracy:  95.39\n",
            "Training:  Iteration No:  339 Worker Num:  0 \n",
            " Loss:  0.14406278729438782\n",
            "Training:  Iteration No:  339 Worker Num:  1 \n",
            " Loss:  0.1557827591896057\n",
            "Training:  Iteration No:  339 Worker Num:  2 \n",
            " Loss:  0.1055695116519928\n",
            "Training:  Iteration No:  339 Worker Num:  3 \n",
            " Loss:  0.1749478578567505\n",
            "Training:  Iteration No:  339 Worker Num:  4 \n",
            " Loss:  0.1524895876646042\n",
            "Training:  Iteration No:  339 Worker Num:  5 \n",
            " Loss:  0.14376141130924225\n",
            "Training:  Iteration No:  339 Worker Num:  6 \n",
            " Loss:  0.14159102737903595\n",
            "Training:  Iteration No:  339 Worker Num:  7 \n",
            " Loss:  0.18141034245491028\n",
            "Test:  Iteration No:  339 \n",
            " Loss:  0.14651578628234094\n",
            "Test accuracy:  95.5\n",
            "Training:  Iteration No:  340 Worker Num:  0 \n",
            " Loss:  0.21180997788906097\n",
            "Training:  Iteration No:  340 Worker Num:  1 \n",
            " Loss:  0.11485135555267334\n",
            "Training:  Iteration No:  340 Worker Num:  2 \n",
            " Loss:  0.10436996817588806\n",
            "Training:  Iteration No:  340 Worker Num:  3 \n",
            " Loss:  0.201828271150589\n",
            "Training:  Iteration No:  340 Worker Num:  4 \n",
            " Loss:  0.11208575963973999\n",
            "Training:  Iteration No:  340 Worker Num:  5 \n",
            " Loss:  0.1602373570203781\n",
            "Training:  Iteration No:  340 Worker Num:  6 \n",
            " Loss:  0.257908433675766\n",
            "Training:  Iteration No:  340 Worker Num:  7 \n",
            " Loss:  0.13225966691970825\n",
            "Test:  Iteration No:  340 \n",
            " Loss:  0.14562874729450367\n",
            "Test accuracy:  95.6\n",
            "Training:  Iteration No:  341 Worker Num:  0 \n",
            " Loss:  0.1850547343492508\n",
            "Training:  Iteration No:  341 Worker Num:  1 \n",
            " Loss:  0.1381797045469284\n",
            "Training:  Iteration No:  341 Worker Num:  2 \n",
            " Loss:  0.12541060149669647\n",
            "Training:  Iteration No:  341 Worker Num:  3 \n",
            " Loss:  0.14826585352420807\n",
            "Training:  Iteration No:  341 Worker Num:  4 \n",
            " Loss:  0.1170654296875\n",
            "Training:  Iteration No:  341 Worker Num:  5 \n",
            " Loss:  0.26008331775665283\n",
            "Training:  Iteration No:  341 Worker Num:  6 \n",
            " Loss:  0.21827267110347748\n",
            "Training:  Iteration No:  341 Worker Num:  7 \n",
            " Loss:  0.2142123132944107\n",
            "Test:  Iteration No:  341 \n",
            " Loss:  0.1452158033388042\n",
            "Test accuracy:  95.69\n",
            "Training:  Iteration No:  342 Worker Num:  0 \n",
            " Loss:  0.2335217446088791\n",
            "Training:  Iteration No:  342 Worker Num:  1 \n",
            " Loss:  0.1620209813117981\n",
            "Training:  Iteration No:  342 Worker Num:  2 \n",
            " Loss:  0.24462558329105377\n",
            "Training:  Iteration No:  342 Worker Num:  3 \n",
            " Loss:  0.16486354172229767\n",
            "Training:  Iteration No:  342 Worker Num:  4 \n",
            " Loss:  0.19624394178390503\n",
            "Training:  Iteration No:  342 Worker Num:  5 \n",
            " Loss:  0.18168886005878448\n",
            "Training:  Iteration No:  342 Worker Num:  6 \n",
            " Loss:  0.13684290647506714\n",
            "Training:  Iteration No:  342 Worker Num:  7 \n",
            " Loss:  0.17590020596981049\n",
            "Test:  Iteration No:  342 \n",
            " Loss:  0.14673521663828554\n",
            "Test accuracy:  95.49\n",
            "Training:  Iteration No:  343 Worker Num:  0 \n",
            " Loss:  0.19205549359321594\n",
            "Training:  Iteration No:  343 Worker Num:  1 \n",
            " Loss:  0.13266822695732117\n",
            "Training:  Iteration No:  343 Worker Num:  2 \n",
            " Loss:  0.18050029873847961\n",
            "Training:  Iteration No:  343 Worker Num:  3 \n",
            " Loss:  0.2013547122478485\n",
            "Training:  Iteration No:  343 Worker Num:  4 \n",
            " Loss:  0.19400201737880707\n",
            "Training:  Iteration No:  343 Worker Num:  5 \n",
            " Loss:  0.20721065998077393\n",
            "Training:  Iteration No:  343 Worker Num:  6 \n",
            " Loss:  0.20524126291275024\n",
            "Training:  Iteration No:  343 Worker Num:  7 \n",
            " Loss:  0.1250665932893753\n",
            "Test:  Iteration No:  343 \n",
            " Loss:  0.14766541046172946\n",
            "Test accuracy:  95.55\n",
            "Training:  Iteration No:  344 Worker Num:  0 \n",
            " Loss:  0.24603645503520966\n",
            "Training:  Iteration No:  344 Worker Num:  1 \n",
            " Loss:  0.140211284160614\n",
            "Training:  Iteration No:  344 Worker Num:  2 \n",
            " Loss:  0.15020333230495453\n",
            "Training:  Iteration No:  344 Worker Num:  3 \n",
            " Loss:  0.18343128263950348\n",
            "Training:  Iteration No:  344 Worker Num:  4 \n",
            " Loss:  0.107564777135849\n",
            "Training:  Iteration No:  344 Worker Num:  5 \n",
            " Loss:  0.11327354609966278\n",
            "Training:  Iteration No:  344 Worker Num:  6 \n",
            " Loss:  0.21938574314117432\n",
            "Training:  Iteration No:  344 Worker Num:  7 \n",
            " Loss:  0.118911974132061\n",
            "Test:  Iteration No:  344 \n",
            " Loss:  0.14593092376319103\n",
            "Test accuracy:  95.73\n",
            "Training:  Iteration No:  345 Worker Num:  0 \n",
            " Loss:  0.11805409938097\n",
            "Training:  Iteration No:  345 Worker Num:  1 \n",
            " Loss:  0.19341112673282623\n",
            "Training:  Iteration No:  345 Worker Num:  2 \n",
            " Loss:  0.09737128764390945\n",
            "Training:  Iteration No:  345 Worker Num:  3 \n",
            " Loss:  0.09224355965852737\n",
            "Training:  Iteration No:  345 Worker Num:  4 \n",
            " Loss:  0.2504461705684662\n",
            "Training:  Iteration No:  345 Worker Num:  5 \n",
            " Loss:  0.1524357944726944\n",
            "Training:  Iteration No:  345 Worker Num:  6 \n",
            " Loss:  0.2029442936182022\n",
            "Training:  Iteration No:  345 Worker Num:  7 \n",
            " Loss:  0.14046524465084076\n",
            "Test:  Iteration No:  345 \n",
            " Loss:  0.14404557125430695\n",
            "Test accuracy:  95.6\n",
            "Training:  Iteration No:  346 Worker Num:  0 \n",
            " Loss:  0.20358328521251678\n",
            "Training:  Iteration No:  346 Worker Num:  1 \n",
            " Loss:  0.16650274395942688\n",
            "Training:  Iteration No:  346 Worker Num:  2 \n",
            " Loss:  0.19790631532669067\n",
            "Training:  Iteration No:  346 Worker Num:  3 \n",
            " Loss:  0.13347090780735016\n",
            "Training:  Iteration No:  346 Worker Num:  4 \n",
            " Loss:  0.10771813988685608\n",
            "Training:  Iteration No:  346 Worker Num:  5 \n",
            " Loss:  0.2901737689971924\n",
            "Training:  Iteration No:  346 Worker Num:  6 \n",
            " Loss:  0.2895571291446686\n",
            "Training:  Iteration No:  346 Worker Num:  7 \n",
            " Loss:  0.24573908746242523\n",
            "Test:  Iteration No:  346 \n",
            " Loss:  0.14507024228879356\n",
            "Test accuracy:  95.66\n",
            "Training:  Iteration No:  347 Worker Num:  0 \n",
            " Loss:  0.2168012112379074\n",
            "Training:  Iteration No:  347 Worker Num:  1 \n",
            " Loss:  0.21118195354938507\n",
            "Training:  Iteration No:  347 Worker Num:  2 \n",
            " Loss:  0.12438143789768219\n",
            "Training:  Iteration No:  347 Worker Num:  3 \n",
            " Loss:  0.12877191603183746\n",
            "Training:  Iteration No:  347 Worker Num:  4 \n",
            " Loss:  0.21098823845386505\n",
            "Training:  Iteration No:  347 Worker Num:  5 \n",
            " Loss:  0.06213703379034996\n",
            "Training:  Iteration No:  347 Worker Num:  6 \n",
            " Loss:  0.16955628991127014\n",
            "Training:  Iteration No:  347 Worker Num:  7 \n",
            " Loss:  0.1592232882976532\n",
            "Test:  Iteration No:  347 \n",
            " Loss:  0.14290623625598942\n",
            "Test accuracy:  95.67\n",
            "Training:  Iteration No:  348 Worker Num:  0 \n",
            " Loss:  0.18967461585998535\n",
            "Training:  Iteration No:  348 Worker Num:  1 \n",
            " Loss:  0.2509365379810333\n",
            "Training:  Iteration No:  348 Worker Num:  2 \n",
            " Loss:  0.2954423129558563\n",
            "Training:  Iteration No:  348 Worker Num:  3 \n",
            " Loss:  0.3303678333759308\n",
            "Training:  Iteration No:  348 Worker Num:  4 \n",
            " Loss:  0.2526817321777344\n",
            "Training:  Iteration No:  348 Worker Num:  5 \n",
            " Loss:  0.13056714832782745\n",
            "Training:  Iteration No:  348 Worker Num:  6 \n",
            " Loss:  0.18826454877853394\n",
            "Training:  Iteration No:  348 Worker Num:  7 \n",
            " Loss:  0.1668204516172409\n",
            "Test:  Iteration No:  348 \n",
            " Loss:  0.144185448301178\n",
            "Test accuracy:  95.6\n",
            "Training:  Iteration No:  349 Worker Num:  0 \n",
            " Loss:  0.11528804898262024\n",
            "Training:  Iteration No:  349 Worker Num:  1 \n",
            " Loss:  0.152204230427742\n",
            "Training:  Iteration No:  349 Worker Num:  2 \n",
            " Loss:  0.2138865739107132\n",
            "Training:  Iteration No:  349 Worker Num:  3 \n",
            " Loss:  0.12620145082473755\n",
            "Training:  Iteration No:  349 Worker Num:  4 \n",
            " Loss:  0.12503603100776672\n",
            "Training:  Iteration No:  349 Worker Num:  5 \n",
            " Loss:  0.25981438159942627\n",
            "Training:  Iteration No:  349 Worker Num:  6 \n",
            " Loss:  0.1899871677160263\n",
            "Training:  Iteration No:  349 Worker Num:  7 \n",
            " Loss:  0.19257231056690216\n",
            "Test:  Iteration No:  349 \n",
            " Loss:  0.1415104236088316\n",
            "Test accuracy:  95.88\n",
            "Training:  Iteration No:  350 Worker Num:  0 \n",
            " Loss:  0.23623467981815338\n",
            "Training:  Iteration No:  350 Worker Num:  1 \n",
            " Loss:  0.14651450514793396\n",
            "Training:  Iteration No:  350 Worker Num:  2 \n",
            " Loss:  0.13523918390274048\n",
            "Training:  Iteration No:  350 Worker Num:  3 \n",
            " Loss:  0.30339571833610535\n",
            "Training:  Iteration No:  350 Worker Num:  4 \n",
            " Loss:  0.3462485074996948\n",
            "Training:  Iteration No:  350 Worker Num:  5 \n",
            " Loss:  0.24596147239208221\n",
            "Training:  Iteration No:  350 Worker Num:  6 \n",
            " Loss:  0.27320244908332825\n",
            "Training:  Iteration No:  350 Worker Num:  7 \n",
            " Loss:  0.15821607410907745\n",
            "Test:  Iteration No:  350 \n",
            " Loss:  0.14347236408981717\n",
            "Test accuracy:  95.56\n",
            "Training:  Iteration No:  351 Worker Num:  0 \n",
            " Loss:  0.20244543254375458\n",
            "Training:  Iteration No:  351 Worker Num:  1 \n",
            " Loss:  0.11358166486024857\n",
            "Training:  Iteration No:  351 Worker Num:  2 \n",
            " Loss:  0.08851148933172226\n",
            "Training:  Iteration No:  351 Worker Num:  3 \n",
            " Loss:  0.10216756165027618\n",
            "Training:  Iteration No:  351 Worker Num:  4 \n",
            " Loss:  0.2614196836948395\n",
            "Training:  Iteration No:  351 Worker Num:  5 \n",
            " Loss:  0.13648517429828644\n",
            "Training:  Iteration No:  351 Worker Num:  6 \n",
            " Loss:  0.18451392650604248\n",
            "Training:  Iteration No:  351 Worker Num:  7 \n",
            " Loss:  0.16217514872550964\n",
            "Test:  Iteration No:  351 \n",
            " Loss:  0.13956571234790963\n",
            "Test accuracy:  95.77\n",
            "Training:  Iteration No:  352 Worker Num:  0 \n",
            " Loss:  0.10644067078828812\n",
            "Training:  Iteration No:  352 Worker Num:  1 \n",
            " Loss:  0.09942836314439774\n",
            "Training:  Iteration No:  352 Worker Num:  2 \n",
            " Loss:  0.20319126546382904\n",
            "Training:  Iteration No:  352 Worker Num:  3 \n",
            " Loss:  0.11147074401378632\n",
            "Training:  Iteration No:  352 Worker Num:  4 \n",
            " Loss:  0.14056363701820374\n",
            "Training:  Iteration No:  352 Worker Num:  5 \n",
            " Loss:  0.11479600518941879\n",
            "Training:  Iteration No:  352 Worker Num:  6 \n",
            " Loss:  0.16186189651489258\n",
            "Training:  Iteration No:  352 Worker Num:  7 \n",
            " Loss:  0.2038901150226593\n",
            "Test:  Iteration No:  352 \n",
            " Loss:  0.14098770854570256\n",
            "Test accuracy:  95.78\n",
            "Training:  Iteration No:  353 Worker Num:  0 \n",
            " Loss:  0.16332405805587769\n",
            "Training:  Iteration No:  353 Worker Num:  1 \n",
            " Loss:  0.14054381847381592\n",
            "Training:  Iteration No:  353 Worker Num:  2 \n",
            " Loss:  0.1392153948545456\n",
            "Training:  Iteration No:  353 Worker Num:  3 \n",
            " Loss:  0.2464880645275116\n",
            "Training:  Iteration No:  353 Worker Num:  4 \n",
            " Loss:  0.14685861766338348\n",
            "Training:  Iteration No:  353 Worker Num:  5 \n",
            " Loss:  0.16746482253074646\n",
            "Training:  Iteration No:  353 Worker Num:  6 \n",
            " Loss:  0.1463809609413147\n",
            "Training:  Iteration No:  353 Worker Num:  7 \n",
            " Loss:  0.1333853155374527\n",
            "Test:  Iteration No:  353 \n",
            " Loss:  0.14221709225138154\n",
            "Test accuracy:  95.81\n",
            "Training:  Iteration No:  354 Worker Num:  0 \n",
            " Loss:  0.16753728687763214\n",
            "Training:  Iteration No:  354 Worker Num:  1 \n",
            " Loss:  0.16333697736263275\n",
            "Training:  Iteration No:  354 Worker Num:  2 \n",
            " Loss:  0.15969431400299072\n",
            "Training:  Iteration No:  354 Worker Num:  3 \n",
            " Loss:  0.21562349796295166\n",
            "Training:  Iteration No:  354 Worker Num:  4 \n",
            " Loss:  0.1946309208869934\n",
            "Training:  Iteration No:  354 Worker Num:  5 \n",
            " Loss:  0.13916364312171936\n",
            "Training:  Iteration No:  354 Worker Num:  6 \n",
            " Loss:  0.162645623087883\n",
            "Training:  Iteration No:  354 Worker Num:  7 \n",
            " Loss:  0.193630188703537\n",
            "Test:  Iteration No:  354 \n",
            " Loss:  0.14328206666521257\n",
            "Test accuracy:  95.61\n",
            "Training:  Iteration No:  355 Worker Num:  0 \n",
            " Loss:  0.18360121548175812\n",
            "Training:  Iteration No:  355 Worker Num:  1 \n",
            " Loss:  0.21375249326229095\n",
            "Training:  Iteration No:  355 Worker Num:  2 \n",
            " Loss:  0.14539048075675964\n",
            "Training:  Iteration No:  355 Worker Num:  3 \n",
            " Loss:  0.2637922167778015\n",
            "Training:  Iteration No:  355 Worker Num:  4 \n",
            " Loss:  0.1109805479645729\n",
            "Training:  Iteration No:  355 Worker Num:  5 \n",
            " Loss:  0.15044888854026794\n",
            "Training:  Iteration No:  355 Worker Num:  6 \n",
            " Loss:  0.15782156586647034\n",
            "Training:  Iteration No:  355 Worker Num:  7 \n",
            " Loss:  0.28507936000823975\n",
            "Test:  Iteration No:  355 \n",
            " Loss:  0.13950547695018445\n",
            "Test accuracy:  95.9\n",
            "Training:  Iteration No:  356 Worker Num:  0 \n",
            " Loss:  0.21730659902095795\n",
            "Training:  Iteration No:  356 Worker Num:  1 \n",
            " Loss:  0.16591660678386688\n",
            "Training:  Iteration No:  356 Worker Num:  2 \n",
            " Loss:  0.15804269909858704\n",
            "Training:  Iteration No:  356 Worker Num:  3 \n",
            " Loss:  0.19829143583774567\n",
            "Training:  Iteration No:  356 Worker Num:  4 \n",
            " Loss:  0.2397230714559555\n",
            "Training:  Iteration No:  356 Worker Num:  5 \n",
            " Loss:  0.11355233937501907\n",
            "Training:  Iteration No:  356 Worker Num:  6 \n",
            " Loss:  0.08449315279722214\n",
            "Training:  Iteration No:  356 Worker Num:  7 \n",
            " Loss:  0.18383780121803284\n",
            "Test:  Iteration No:  356 \n",
            " Loss:  0.1403020737115058\n",
            "Test accuracy:  95.73\n",
            "Training:  Iteration No:  357 Worker Num:  0 \n",
            " Loss:  0.19993463158607483\n",
            "Training:  Iteration No:  357 Worker Num:  1 \n",
            " Loss:  0.22220593690872192\n",
            "Training:  Iteration No:  357 Worker Num:  2 \n",
            " Loss:  0.14404737949371338\n",
            "Training:  Iteration No:  357 Worker Num:  3 \n",
            " Loss:  0.25053533911705017\n",
            "Training:  Iteration No:  357 Worker Num:  4 \n",
            " Loss:  0.13129310309886932\n",
            "Training:  Iteration No:  357 Worker Num:  5 \n",
            " Loss:  0.11075551062822342\n",
            "Training:  Iteration No:  357 Worker Num:  6 \n",
            " Loss:  0.151367649435997\n",
            "Training:  Iteration No:  357 Worker Num:  7 \n",
            " Loss:  0.25695210695266724\n",
            "Test:  Iteration No:  357 \n",
            " Loss:  0.1372119273070859\n",
            "Test accuracy:  95.99\n",
            "Training:  Iteration No:  358 Worker Num:  0 \n",
            " Loss:  0.1364443302154541\n",
            "Training:  Iteration No:  358 Worker Num:  1 \n",
            " Loss:  0.2383153885602951\n",
            "Training:  Iteration No:  358 Worker Num:  2 \n",
            " Loss:  0.1713627129793167\n",
            "Training:  Iteration No:  358 Worker Num:  3 \n",
            " Loss:  0.14182159304618835\n",
            "Training:  Iteration No:  358 Worker Num:  4 \n",
            " Loss:  0.23099572956562042\n",
            "Training:  Iteration No:  358 Worker Num:  5 \n",
            " Loss:  0.19861513376235962\n",
            "Training:  Iteration No:  358 Worker Num:  6 \n",
            " Loss:  0.23006272315979004\n",
            "Training:  Iteration No:  358 Worker Num:  7 \n",
            " Loss:  0.3345518112182617\n",
            "Test:  Iteration No:  358 \n",
            " Loss:  0.1375663099586492\n",
            "Test accuracy:  95.84\n",
            "Training:  Iteration No:  359 Worker Num:  0 \n",
            " Loss:  0.28557634353637695\n",
            "Training:  Iteration No:  359 Worker Num:  1 \n",
            " Loss:  0.170408695936203\n",
            "Training:  Iteration No:  359 Worker Num:  2 \n",
            " Loss:  0.1747555136680603\n",
            "Training:  Iteration No:  359 Worker Num:  3 \n",
            " Loss:  0.25527065992355347\n",
            "Training:  Iteration No:  359 Worker Num:  4 \n",
            " Loss:  0.1488526165485382\n",
            "Training:  Iteration No:  359 Worker Num:  5 \n",
            " Loss:  0.20540213584899902\n",
            "Training:  Iteration No:  359 Worker Num:  6 \n",
            " Loss:  0.19733662903308868\n",
            "Training:  Iteration No:  359 Worker Num:  7 \n",
            " Loss:  0.15484243631362915\n",
            "Test:  Iteration No:  359 \n",
            " Loss:  0.13826255840426194\n",
            "Test accuracy:  95.9\n",
            "Training:  Iteration No:  360 Worker Num:  0 \n",
            " Loss:  0.15536314249038696\n",
            "Training:  Iteration No:  360 Worker Num:  1 \n",
            " Loss:  0.12377456575632095\n",
            "Training:  Iteration No:  360 Worker Num:  2 \n",
            " Loss:  0.1547011435031891\n",
            "Training:  Iteration No:  360 Worker Num:  3 \n",
            " Loss:  0.24666255712509155\n",
            "Training:  Iteration No:  360 Worker Num:  4 \n",
            " Loss:  0.1310468316078186\n",
            "Training:  Iteration No:  360 Worker Num:  5 \n",
            " Loss:  0.12493222206830978\n",
            "Training:  Iteration No:  360 Worker Num:  6 \n",
            " Loss:  0.27778467535972595\n",
            "Training:  Iteration No:  360 Worker Num:  7 \n",
            " Loss:  0.16354089975357056\n",
            "Test:  Iteration No:  360 \n",
            " Loss:  0.14031469039736857\n",
            "Test accuracy:  95.64\n",
            "Training:  Iteration No:  361 Worker Num:  0 \n",
            " Loss:  0.31246575713157654\n",
            "Training:  Iteration No:  361 Worker Num:  1 \n",
            " Loss:  0.17908135056495667\n",
            "Training:  Iteration No:  361 Worker Num:  2 \n",
            " Loss:  0.09676297008991241\n",
            "Training:  Iteration No:  361 Worker Num:  3 \n",
            " Loss:  0.2163449078798294\n",
            "Training:  Iteration No:  361 Worker Num:  4 \n",
            " Loss:  0.19900304079055786\n",
            "Training:  Iteration No:  361 Worker Num:  5 \n",
            " Loss:  0.2458200454711914\n",
            "Training:  Iteration No:  361 Worker Num:  6 \n",
            " Loss:  0.14283592998981476\n",
            "Training:  Iteration No:  361 Worker Num:  7 \n",
            " Loss:  0.16651606559753418\n",
            "Test:  Iteration No:  361 \n",
            " Loss:  0.13980865925904126\n",
            "Test accuracy:  95.77\n",
            "Training:  Iteration No:  362 Worker Num:  0 \n",
            " Loss:  0.1437406837940216\n",
            "Training:  Iteration No:  362 Worker Num:  1 \n",
            " Loss:  0.19424036145210266\n",
            "Training:  Iteration No:  362 Worker Num:  2 \n",
            " Loss:  0.18205593526363373\n",
            "Training:  Iteration No:  362 Worker Num:  3 \n",
            " Loss:  0.17046858370304108\n",
            "Training:  Iteration No:  362 Worker Num:  4 \n",
            " Loss:  0.20274019241333008\n",
            "Training:  Iteration No:  362 Worker Num:  5 \n",
            " Loss:  0.1894850879907608\n",
            "Training:  Iteration No:  362 Worker Num:  6 \n",
            " Loss:  0.17232315242290497\n",
            "Training:  Iteration No:  362 Worker Num:  7 \n",
            " Loss:  0.15965299308300018\n",
            "Test:  Iteration No:  362 \n",
            " Loss:  0.13679363721793023\n",
            "Test accuracy:  95.77\n",
            "Training:  Iteration No:  363 Worker Num:  0 \n",
            " Loss:  0.2105303406715393\n",
            "Training:  Iteration No:  363 Worker Num:  1 \n",
            " Loss:  0.10779058933258057\n",
            "Training:  Iteration No:  363 Worker Num:  2 \n",
            " Loss:  0.15335309505462646\n",
            "Training:  Iteration No:  363 Worker Num:  3 \n",
            " Loss:  0.12613819539546967\n",
            "Training:  Iteration No:  363 Worker Num:  4 \n",
            " Loss:  0.11010038107633591\n",
            "Training:  Iteration No:  363 Worker Num:  5 \n",
            " Loss:  0.12232566624879837\n",
            "Training:  Iteration No:  363 Worker Num:  6 \n",
            " Loss:  0.14753368496894836\n",
            "Training:  Iteration No:  363 Worker Num:  7 \n",
            " Loss:  0.25719815492630005\n",
            "Test:  Iteration No:  363 \n",
            " Loss:  0.13641813424521987\n",
            "Test accuracy:  95.95\n",
            "Training:  Iteration No:  364 Worker Num:  0 \n",
            " Loss:  0.14327558875083923\n",
            "Training:  Iteration No:  364 Worker Num:  1 \n",
            " Loss:  0.14877018332481384\n",
            "Training:  Iteration No:  364 Worker Num:  2 \n",
            " Loss:  0.2250206619501114\n",
            "Training:  Iteration No:  364 Worker Num:  3 \n",
            " Loss:  0.17827147245407104\n",
            "Training:  Iteration No:  364 Worker Num:  4 \n",
            " Loss:  0.10656628757715225\n",
            "Training:  Iteration No:  364 Worker Num:  5 \n",
            " Loss:  0.25348523259162903\n",
            "Training:  Iteration No:  364 Worker Num:  6 \n",
            " Loss:  0.2789035737514496\n",
            "Training:  Iteration No:  364 Worker Num:  7 \n",
            " Loss:  0.1541765332221985\n",
            "Test:  Iteration No:  364 \n",
            " Loss:  0.13655751429592508\n",
            "Test accuracy:  95.9\n",
            "Training:  Iteration No:  365 Worker Num:  0 \n",
            " Loss:  0.22900031507015228\n",
            "Training:  Iteration No:  365 Worker Num:  1 \n",
            " Loss:  0.13130347430706024\n",
            "Training:  Iteration No:  365 Worker Num:  2 \n",
            " Loss:  0.1548704355955124\n",
            "Training:  Iteration No:  365 Worker Num:  3 \n",
            " Loss:  0.17194344103336334\n",
            "Training:  Iteration No:  365 Worker Num:  4 \n",
            " Loss:  0.07271559536457062\n",
            "Training:  Iteration No:  365 Worker Num:  5 \n",
            " Loss:  0.2719401717185974\n",
            "Training:  Iteration No:  365 Worker Num:  6 \n",
            " Loss:  0.17181885242462158\n",
            "Training:  Iteration No:  365 Worker Num:  7 \n",
            " Loss:  0.13875173032283783\n",
            "Test:  Iteration No:  365 \n",
            " Loss:  0.13541141676103485\n",
            "Test accuracy:  95.77\n",
            "Training:  Iteration No:  366 Worker Num:  0 \n",
            " Loss:  0.15330398082733154\n",
            "Training:  Iteration No:  366 Worker Num:  1 \n",
            " Loss:  0.1757429540157318\n",
            "Training:  Iteration No:  366 Worker Num:  2 \n",
            " Loss:  0.11705605685710907\n",
            "Training:  Iteration No:  366 Worker Num:  3 \n",
            " Loss:  0.22851496934890747\n",
            "Training:  Iteration No:  366 Worker Num:  4 \n",
            " Loss:  0.08523134142160416\n",
            "Training:  Iteration No:  366 Worker Num:  5 \n",
            " Loss:  0.16968393325805664\n",
            "Training:  Iteration No:  366 Worker Num:  6 \n",
            " Loss:  0.222959503531456\n",
            "Training:  Iteration No:  366 Worker Num:  7 \n",
            " Loss:  0.12950575351715088\n",
            "Test:  Iteration No:  366 \n",
            " Loss:  0.1350709659377514\n",
            "Test accuracy:  96.08\n",
            "96 % accuracy reached!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(test_accuracy)\n",
        "plt.xlabel('Iteration Number')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Test accuracy');"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "7bf6d9b3-9490-4559-e462-029fbe2a379f",
        "id": "myYzdrVUuh-x"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxddZ3/8dfn3uzN1jRJ972FUqDQWpBNVOogIAouo+ioOKOijjqOjoO4/GZ05uGMy7jrQ2XcGEVkUQFXBGRR9raUUiilC92XpM2+3u3z++OcXNI2SdM0yUly38/HI4+c5d5zPzltzjvn+z3ne8zdERERAYhFXYCIiIwdCgUREclSKIiISJZCQUREshQKIiKSpVAQEZEshYKIiGQpFGTcMbO2Xl8ZM+vsNf93Q9je/Wb2npGoVWS8yYu6AJHj5e6lPdNmth14j7vfE11FI8vM8tw9FXUdkht0piAThpnFzOw6M9tqZofM7BYzqwrXFZnZz8LlTWb2hJlNNbPPAy8Dvh2eaXy7n23famb7zazZzB40s1N7rSs2s6+Y2Y5w/V/NrDhcd4GZPRx+5i4ze1e4/LCzEzN7l5n9tde8m9kHzWwzsDlc9o1wGy1mtsbMXtbr9XEz+1T4s7eG62eb2XfM7CtH/Cx3mtlHT3yPy0SkUJCJ5MPAlcDLgRlAI/CdcN3VQAUwG5gCvB/odPdPA38BPuTupe7+oX62/QdgMVALrAVu7LXuf4CXAOcBVcC1QMbM5obv+xZQA5wJrDuOn+dK4KXA0nD+iXAbVcDPgVvNrChc9zHgrcBlQDnwD0AHcAPwVjOLAZhZNfCq8P0iR1HzkUwk7yc4uO8GMLPPAjvN7B1AkiAMFrn7emDN8WzY3X/UMx1ut9HMKoBWggPwOe6+J3zJw+Hr3gbc4+43hcsPhV+D9d/u3tCrhp/1WvcVM/sMcDLwFPAe4Fp33xSuf6rnM82sGVgF3A1cBdzv7geOow7JITpTkIlkLvDrsKmmCdgIpIGpwE+Bu4BfmNleM/uSmeUPZqNh08wXwqaZFmB7uKo6/CoCtvbx1tn9LB+sXUfU8XEz2xg2UTURnPlUD+KzbgDeHk6/nWBfiPRJoSATyS7gUnev7PVV5O573D3p7p9z96UEzTyXA+8M33esoYLfBlxB0OxSAcwLlxtwEOgCFvZTT1/LAdqBkl7z0/p4TbausP/gWuDNwGR3rwSawxqO9Vk/A64wszOAU4Db+3mdiEJBJpTvAZ8P2/IxsxozuyKcfqWZnW5mcaCFoDkpE77vALBggO2WAd0ETT8lwH/1rHD3DPAj4KtmNiM8qzjXzAoJ+h1eZWZvNrM8M5tiZmeGb10HvMHMSsxsEfDuY/xsZUAKqAfyzOzfCPoOevwA+E8zW2yBZWY2JaxxN0F/xE+BX7p75zE+S3KYQkEmkm8AdwJ/MrNW4FGCjloI/hK/jSAQNgIP8GIzyjeAN5lZo5l9s4/t/h+wA9gDPBtut7ePA08THHgbgC8CMXffSdDx+y/h8nXAGeF7vgYkCALpBg7vuO7LXcAfgefDWro4vHnpq8AtwJ/Cn/GHQHGv9TcAp6OmIzkG00N2RCY+M7uQoBlpruuXXgagMwWRCS7sUP8I8AMFghyLQkFkAjOzU4AmYDrw9YjLkXFgxELBzH5kZnVmtqHXsiozu9vMNoffJ4fLzcy+aWZbzGy9ma0YqbpEcom7b3T3Se5+nru3RF2PjH0jeabwE+CSI5ZdB9zr7ouBe8N5gEsJ7hZdDFwDfHcE6xIRkX6MaEezmc0Dfuvup4Xzm4BXuPs+M5tOcGflyWb2/XD6piNfN9D2q6urfd68eSNWv4jIRLRmzZqD7l7T17rRHuZiaq8D/X6CO00BZnL45XW7w2VHhYKZXUNwNsGcOXNYvXr1yFUrIjIBmdmO/tZF1tEcXgVx3Kcp7n69u69095U1NX0GnYiIDNFoh8KBsNmI8HtduHwPwdgtPWaFy0REZBSNdijcSTCEMeH3O3otf2d4FdI5QPOx+hNERGT4jVifgpndBLwCqDaz3cC/A18AbjGzdxPcqv/m8OW/JxgOYAvBGPB/P1J1iYhI/0YsFNz9rf2sWtXHax344EjVIiIig6M7mkVEJEuhICIiWQoFEZGIZDJHX5XflUzT3JFk84FWupJpem4w7kik2FrfRkcixX/9fiP7mkfmsRh6RrOIyADcnY5EGjNo7EgyrbwIgJ0NHVSVFHD/83U0tieoLCkgmc6wcl4V63c38fKTanhmbwuPbjvEkmnltHenWLuzkdlVJTR3JulKpvnV2j286pRaNte10d6dYnZVCQ9tOUhJQR5t3SkAzpxdSX7c2FzXRlNHkryYkco4c6pKePs5c4f951UoiMi4tvNQBzVlhRQXxHF3th1s5/n9rTiwuLaU8uJ8tta3UVtWxAsH29nd2EFlST6La8t44WA7Dz5fz6H2BHOqSqhv7eZQezdF+XGqSgp4dl8LOw510J1K08cf9cRjRrqvFf0ozIvRncpQkBcjkcowo6KI29ft5byFU5hZWcx9m+o4f1E1pYV5rJxXRV1LF796cg8zKoq4cHENFyyq5sldTZy7cAqvO2PG8O3EXsb1Q3ZWrlzpGuZCZPyoa+0ik4FpFUVHrdtS18acqhI6k2l+u34ve5uC5pGivDg1ZYU8u6+F6tJCTppaxp6mTsoK87hvUx1/fGY/syeXEI8Ze5o6SWf8sAN1z4G4P1WTCqgpLQzDooCZlcV0JtMcautm1uQSls4op7woj8L8OOXF+dS3dhMzmDKpgEdfaOB1Z8xgxZzJtHYlae1K8e37tvDS+VU0tCd4ydzJnDW/ijXbGykvzmfZrAo6k2nKi/Jp705RlB9nf0sXMyuDh+R1p9IU5sWHea8fzczWuPvKPtcpFERkIKl0hk0HWjllWjmxmAHQ0J7gpsd3MqeqhJXzJvPs3hae29/KA5vqqS4r4GWLa9i0v5W7nz1Axp1TZ5STTDvP7G3hYFs3ZUV5lBTESWecZNqZUlrAtvp2pkwqIJVxmjuTxGOGAanwAF9WmEdbIkXvQ9bU8kIuWlLL+t3NTCsvYtbkYooK4rx22QzM4J5n61i9o4G3nDWbVNqZO6Uk23zzxAsNVJbkc/HSadmfK1cMFApqPhLJAe7Ok7uaqCktpKaskPs31eEOFSX57DjUQVtXiuf2t7K/pZOywnziceOVJ9dy6+pdrNnRSCrjnDm7kpbOJAtrS/nL5nq6kkf/9X3y1DJ2NXbw+6f3kxczVp1SS8yM1TsaSaQydCbSvOOcucRjRkciRTxmuMMDz9dz5ZkzMAvay99zwXyWzarAzDjU1s3uxk6WzaqgpSvF8wdamVZeRHNnkqXTywc8oJ86o6LP5dWlhSysKR22/TuR6ExBZBzoTqXZtL+VjkSah7YcZPHUMhraupk8qYCmjiSLa0v583N17G3u5MmdTRxqT7CgehLnLJhCftx44WAH92w8AEB+3Eimj/69LymIs2RaGa1dKVq7Uuxv6SIvZrzj3Lnc/ewBdjd2UhCPYQZXnTWbt5w1h+5Ummf2tjC1vIiXLqiivCgfd2fHoQ5KCuPUlr3YTHSwrZv27hRzp0w66rPdHbPc+ms9Smo+EhljUukM8Zhxz8Y6qksLKCvK5zdP7WVKaXCQP3VGOb9bv481OxtZMWcy922qo6kjOeA282LG1PIi5lSVcPqsCp7a1cT63c2YBeve9tK5zKkqYVt9G5Ul+Zwxu5KYGXOnlFBamEdePEZpYdB4kMk4a3Y2Eo8ZK+ZMpr07xT0bD/CyxTUkUpk++wRk/FDzkcgI6P3X7f7mLjoSKaaUFoLD03uaeXBzPY9sPcSi2lI217XS2J6kpCBOdWkhT+5qZOn0ctbubAL6voqlKD/G6TMruH9THefMn8LlZ0ynO5nh3IVTaOxIUFNWyIHmbooL4mypa+MlcydTU1Y4LD9bLGacNa8qOz+pMI8rzpw5LNuWsU2hINJLdypNW1dwcO9IpNjV0EltWSExM+rbutjX3MXDWw9x57q9tHQlOX9hNcl0hvufr88e1HuaZ/LjxhmzKnl460Eqiws4ZXo5HYkUq7c3Mq+6hOcPtPFvly8lmc7Q0pXk9ctnkRczassLefD5g5w6o5zZVSV91jkjvFqlp3lmUa3ax2V4KBQkZzR1JGjuTDJ3yiTqWru45YldLKwppSAvRm1ZER2JFO/5v9W0dqVYVFtKXUsXLV3BDUQ9Nwz1uPCkGmIGT2xvIC9urFpSy2uWTWdXQwdNHUlecXItp8+soKIk/6g6epqOEulMv5cfXnLatJHZCSLHoFCQce/ITkp3Z2dDBx2JNPnxGGt3NvK79ft4ZOshMFhYU8pz+1voqzttckk+1126hIe2HGRhzSQuO306+5u7aGhPMKOymMK8GFcun0lR/tCvJc+LB6PLjMb16CLHS6EgY14648QMupIZ1uxoZPmcSn7z1F4e3nqIdbua2NfcSWVJAYV5MSYVBP+lNx1oPWwbc6eUcNnp0/j90/vZcaid9124kMtOn0Z3KkMm4+xv6eKnj+zgvRcu4NWnTuP9L18YxY8qEjldfSSR6UikKMyLh9eqO7saOjGDa29bz/yaSXQm0mzc18LW+uBO18qSAtbsaKSkIE5HIs208iLOnF3JjMpimjuTtHYlSWWcg23dXL5sOrMnl9CVSjN3yiSWz67EzNjf3EUilWHOlL7b6kVyga4+kjGhuSPJ+j1NnD2/iubOJK/71kOUFMZZNrOCvU1dPL69IfvaR7YdYlp5EadML+Nli6t5dFsDa3Y0ctGSWjoTaT76Nydx1rzJx31tuy6lFBmYQkFOmLvz/IE2FteWsqepk1tX76KlK8XV582jojifL9/1HHc/e4BD7QncobTwxREgi/PjbKtvB+DipVOZXlHE+YuqWXXKVOKxw/sJ6lu7qS3XQV1kJCkU5Lg9s7eZnz+2k0mFeVy4uIbfPb2Pmx7fyfzqSexr7iSZduIx4ycPb8++59LTpnHytDJqy4p4aOtB5k+ZxHkLp3Deomo2H2jlN0/t5YMXLeq389XMFAgio0B9CtKvzkSaovwYOxs62Livhe89sA0zeHp3M0X5cbqS6exlmstmVfDM3hYuOXUan37NKRxqS/CZ25/mZYtrWFRbyhXhuDYiEj31KUi/XjjYTsxg7pRJdCRS/Pm5OhraE2za38otq3exbFYlG/Y0Hzb08BtWzOTfLl9Kc2eS3Y2dzKuexMzKYrqS6eylmjMqi7njQxdE9WOJyBApFHLIhj3BODinzqigrrWLr9z1PL96cjcF8RhXLJ/J6u0NPH+gDYCCvBgvP6mGtTubcIdVS2qpLS/iv15/WvYv/sqSgsMGNzuRa/dFZGxQKOSAZ/e2cPu6Pfzkoe3EYvDFNy7jxkd3sm53E69fPpNt9e3ctWE/FcX5fO/tK1g5r4rJJQXZ8Xhau5JUlhRE/WOIyChQKEwwG/Y0c8PD2/n860/nyZ2N/P7pfdzwyA7y48a5C6upa+niI79YB8B1ly455k1a8ZgpEERyiEJhAkmmM3zuN8/wxPZGGtoT3PtcHRCMff/JS0+hoiSfdMZ5dNshDrZ1c9np0yOuWETGGoXCONNztdiepk5uenwndz61l8tOn05hPMZ37t+aHanz3ufquPCkGr551ZmH/aUfjxnnL6qOpHYRGfsUCuPECwfb+fljO7j3uTp2HOrIHvxPmV7O9x/YBgT3AqycV0U6k+GeZ+v4ztuWU1Z09CidIiL9USiMUR2J4JGINaWF/Pjh7Xzrz5vpSKQ5ZVoZ733ZAqZXFHHWvCoWTy3ltd/6KwDffOty8sMROK+5UAO6icjxUyiMUf/5243c9PhOzls4hYe3HuLs+VV8+U3L+ny+7a/+8TxiZtlAEBEZKoXCGPWHDfsA2LS/lWsvOZkPvHxhv3cElxTon1FEhoeOJmNMVzLN9Q9uo6kjyacvO4X3Xrgg6pJEJIcoFCK0r7mTaeVF2TOAHz/0Ag9tOcQ9Gw8AsHLe5CjLE5EcpFCIQGtXkvrWbi76ygNctKSWU2eUs6uhg9vX7c2+5q1nz+G0mRURVikiuUihEIFX/s8DHGzrBuDPz9Xx5/AmMwgeEP+La85h5byqqMoTkRymUIhATyAA/PID51HX0sUHblwLwIbPvVoDy4lIZBQKo6yhPXHY/EvmBv0GJQVxppQWKBBEJFIKhVFQ19rFpII8JhXmsa2+Lbv8S29clp1+9FOriOkhNCISMYXCCEulM7z6aw/S2JHk+ne8hP/47bMAPPCvrzjsRrRyDUchImNAJLfAmtlHzewZM9tgZjeZWZGZzTezx8xsi5ndbGYTYrzm9XuaaexIAnDNT9ewu7ETgFmTS6IsS0SkT6MeCmY2E/gnYKW7nwbEgauALwJfc/dFQCPw7tGubbh1p9L8eu0eIHiEJcCUSQW87+ULiMfUVCQiY09UzUd5QLGZJYESYB9wEfC2cP0NwGeB70ZS3TD5f7dv4JbVuzl5ahnXvnoJiVSGz77uVKpLC6MuTUSkT6N+puDue4D/AXYShEEzsAZocvdU+LLdwMy+3m9m15jZajNbXV9fPxolD0lLV5I7n9rL5JJ8vvDG05lWUcS337ZCgSAiY1oUzUeTgSuA+cAMYBJwyWDf7+7Xu/tKd19ZU1MzQlWeuLs27KcrmeHHf382y+douAoRGR+i6Gh+FfCCu9e7exL4FXA+UGlmPc1Zs4A9EdQ2bLbWt5MfN5ZpqAoRGUeiCIWdwDlmVmLBSHCrgGeB+4A3ha+5GrgjgtqGzf7mTqZVFBFTh7KIjCNR9Ck8BtwGrAWeDmu4HvgE8DEz2wJMAX442rUNp73NXUwvL466DBGR4xLJ1Ufu/u/Avx+xeBtwdgTljIj9zV2cObsy6jJERI6Lnt84Atyd/c1dTK8siroUEZHjolAYZi1dSc76/D0k0hmmlysURGR8USgMs0e2HuJgWzASanWZ7kkQkfFFoTDMHtvWAAQPyzljlvoURGR80Sipw+zRbYc4b+EUbnzPS7PPXhYRGS90pjCMOhIpNu5vYeW8KgWCiIxLCoUTlEhluGX1LlLpDM/tb8UdTptRHnVZIiJDouajE/T7p/dx7W3rKS/Koz7sYD5VQ1uIyDilUDhBj28POpbve66eWAwqivOZUaFLUUVkfFIonKAnXghC4ebVuyjIi/HS+epPEJHxS30KJ6CxPcHmujYuWFRNQTxGQTzGZ16zNOqyRESGTGcKJ2D1jkYA/mnVYhbVltKVTDOjUoPgicj4pVA4Aau3N1AQj7FsVgVF+fGoyxEROWFqPjoBT2xv4HQFgohMIAqFIUpnnA17WlgxR0NZiMjEoVAYot2NHSTSGRbVlkZdiojIsFEoDNG2g+0ALKhRKIjIxKFQGIKWriQPbT4IwILqSRFXIyIyfHT10RB87OanuGfjAQCqJhVEXI2IyPDRmcIQ9AQCoLuXRWRC0ZnCIN342A5eqG/nqrNnA3DG7Eo+8eqTI65KRGR4KRQGIZnO8OlfbwBg3a4mAP714pM5b1F1lGWJiAw7NR8Nwr6mruz06h2NxGPG6bM0PLaITDwKhUHY1dgBwJJpZQC84qQaKorzoyxJRGREKBQGYWdDEAofv/hkppUXce0lSyKuSERkZKhPYQAtXUme2dPCroYO8mLGK5fU8uinVkVdlojIiFEoDOCzdzzDr57cw/I5lcycXEw8pstPRWRiU/PRAHY3dQLw5M4m5lSVRFyNiMjIUygMoLr0xbuVLz1teoSViIiMDoXCAA62JrLTrz1DoSAiE5/6FAZQ19rF8jmV/L/Ll1JWpEtQRWTi05nCAOpbu1k+ezIr5kyOuhQRkVGhM4U+NHcmOeNzfwKgtrww4mpEREaPzhT6sONQe3a6plShICK5Q6HQh4Nt3dnpSYU6mRKR3HHMUDCz15pZToVHfWsQCkumlXHugikRVyMiMnoGc7B/C7DZzL5kZjkx6E9PKNzxofOpKNFVRyKSO44ZCu7+dmA5sBX4iZk9YmbXmFnZUD/UzCrN7DYze87MNprZuWZWZWZ3m9nm8Htkl/zUtXZTUZxPYV48qhJERCIxqGYhd28BbgN+AUwHXg+sNbMPD/FzvwH80d2XAGcAG4HrgHvdfTFwbzgfifrWbmrK1MEsIrlnMH0KrzOzXwP3A/nA2e5+KcHB/F+O9wPNrAK4EPghgLsn3L0JuAK4IXzZDcCVx7vt4VLf2q2rjkQkJw3m0po3Al9z9wd7L3T3DjN79xA+cz5QD/zYzM4A1gAfAaa6+77wNfuBqX292cyuAa4BmDNnzhA+/tjqWrtZPqdyRLYtIjKWDab56LPA4z0zZlZsZvMA3P3eIXxmHrAC+K67LwfaOaKpyN0d8L7e7O7Xu/tKd19ZU1MzhI8fWCbjHGjpolbNRyKSgwYTCrcCmV7z6XDZUO0Gdrv7Y+H8bQQhccDMpgOE3+tO4DOGbE9TJ92pDAtqSqP4eBGRSA0mFPLcPTtcaDhdMMDrB+Tu+4FdZnZyuGgV8CxwJ3B1uOxq4I6hfsaJ2FzXCsDiWoWCiOSewfQp1JvZ69z9TgAzuwI4eIKf+2HgRjMrALYBf08QULeE/RQ7gDef4GcMyeYDbQAsrh3yFbciIuPWYELh/QQH8G8DBuwC3nkiH+ru64CVfayK/AHIm+vaqC0r1E1rIpKTjhkK7r4VOMfMSsP5thGvKkIvHGxnQc2kqMsQEYnEoEZ7M7PXAKcCRWbBw+vd/T9GsK7IdCbSVE0ujroMEZFIDObmte8RjH/0YYLmo78F5o5wXZFJpjMUxHNq/D8RkazBHP3Oc/d3Ao3u/jngXOCkkS0rOsl0hvy4RV2GiEgkBhMKXeH3DjObASQJxj+akJJpJ19nCiKSowbTp/AbM6sEvgysJbjT+H9HtKoIJdIZ8vMUCiKSmwYMhfDhOveGA9b90sx+CxS5e/OoVBcB9SmISC4b8Ojn7hngO73muydyIAAkU+pTEJHcNZg/ie81szdaz7WoE1winVGfgojkrMEc/d5HMABet5m1mFmrmbWMcF2RcHd1NItIThvMHc05MwhQMh2M1l2gjmYRyVHHDAUzu7Cv5Uc+dGciSKaDEcLVpyAiuWowl6T+a6/pIuBsgqelXTQiFUXoxVDQmYKI5KbBNB+9tve8mc0Gvj5iFUUooVAQkRw3lKPfbuCU4S5kLMj2KSgURCRHDaZP4Vu8+LzkGHAmwZ3NE04yFZ4p5KlPQURy02D6FFb3mk4BN7n7QyNUT6TUpyAiuW4woXAb0OXuaQAzi5tZibt3jGxpo099CiKS6wZ1RzPQ+6kzxcA9I1NOtNSnICK5bjBHv6Lej+AMp0tGrqToqPlIRHLdYI5+7Wa2omfGzF4CdI5cSdHJdjTr5jURyVGD6VP4Z+BWM9tL8DjOaQSP55xwsn0KGuZCRHLUYG5ee8LMlgAnh4s2uXtyZMuKhvoURCTXHfPoZ2YfBCa5+wZ33wCUmtk/jnxpo099CiKS6wZz9Htv+OQ1ANy9EXjvyJUUHQ2IJyK5bjChEO/9gB0ziwMFI1dSdBIpnSmISG4bTEfzH4Gbzez74fz7gD+MXEnRaO5Ics/GA4CepyAiuWswofAJ4Brg/eH8eoIrkCaUD920lr9sPgjoTEFEctcxj37ungEeA7YTPEvhImDjyJY1+rbWZe/PU5+CiOSsfs8UzOwk4K3h10HgZgB3f+XolDa6ejcZ6UxBRHLVQM1HzwF/AS539y0AZvbRUakqAr1DQfcpiEiuGujo9wZgH3Cfmf2vma0iuKN5QuodCrHYhP0xRUQG1G8ouPvt7n4VsAS4j2C4i1oz+66ZXTxaBY6W8BYFEZGcNpiO5nZ3/3n4rOZZwJMEVyRNKC2dE3LkDhGR43Jcjefu3uju17v7qpEqKCpNHYmoSxARiZx6VAmGt2hPpKMuQ0QkcoO5eW3Caw6bji5eOpULFldHXI2ISHQUCrwYCq9ZNp0rzpwZcTUiItGJrPnIzOJm9qSZ/Tacn29mj5nZFjO72cxGbdC9nlAoL84frY8UERmTouxT+AiHD5fxReBr7r4IaATePVqF9IRChUJBRHJcJKFgZrOA1wA/COeNYEyl28KX3ABcOVr1dHQHncylhWpNE5HcFtWZwteBa4GeW8amAE3ungrndwN9Nu6b2TVmttrMVtfX1w9LMe2J4GOL8+PDsj0RkfFq1EPBzC4H6tx9zVDeH94nsdLdV9bU1AxLTZ3h5aglBQoFEcltUbSXnA+8zswuA4qAcuAbQKWZ5YVnC7OAPaNVUEcYCpPUfCQiOW7UzxTc/ZPuPsvd5wFXAX92978jGF/pTeHLrgbuGK2aOhIpzKBQT1wTkRw3lo6CnwA+ZmZbCPoYfjhaH9yRSFOSH6fXo6hFRHJSpO0l7n4/cH84vY3gyW6jriORokRNRyIiY+pMITIdibQ6mUVEUCgA0N6dpqRAZwoiIgoFoDOZ0pmCiAgKBUDNRyIiPRQKBMNcKBRERBQKAHQkU+pTEBFBoQAEZwrFOlMQEVEoQNCnMEmhICKiUMhknM5kmmI1H4mIKBQ6kxohVUSkR86HQnaEVIWCiIhCoa07eMCOrj4SEVEoZJ/PXFmi5zOLiCgUwlCoKFYoiIgoFBQKIiJZCoWOBKBQEBEBhUL2TKFcoSAiolBo7kxSlB+jKF+XpIqIKBQ6k2o6EhEJ5XwoNHUoFEREeuR8KDR3JqksLoi6DBGRMUGh0JlUJ7OISEihoD4FEZGsnA4Fd6epI6khLkREQjkdCg3tCTqTaWZUFkddiojImJDTobCjoQOAuVUlEVciIjI25HQo7OoJhSkKBRERyPFQ2HEoCIXZOlMQEQFyPBR2NnQwtbxQQ1yIiIRyPhTm6CxBRCQrp0OhtSulexRERHrJ6VBIpNIU5OX0LhAROUxOHxET6QwF8ZzeBSIih8npI2IildGZgohILzl9RFQoiIgcLqePiIlUhoK4LkcVEemR26GQ1pmCiEhvOXtEzGScZNoVCiIivYz6EdHMZpvZfWb2rJk9Y2YfCZdXmdndZrY5/D55JOtIpDMAFCoURESyojgipoB/cfelwDnAB81sKXAdcK+7LwbuDYAiYYQAAAnnSURBVOdHTE8o6JJUEZEXjfoR0d33ufvacLoV2AjMBK4AbghfdgNw5UjWkUiFoaAzBRGRrEiPiGY2D1gOPAZMdfd94ar9wNR+3nONma02s9X19fVD/myFgojI0SI7IppZKfBL4J/dvaX3Ond3wPt6n7tf7+4r3X1lTU3NkD8/GwpqPhIRyYrkiGhm+QSBcKO7/ypcfMDMpofrpwN1I1lDtk9BZwoiIllRXH1kwA+Bje7+1V6r7gSuDqevBu4YyTrUfCQicrS8CD7zfOAdwNNmti5c9ingC8AtZvZuYAfw5pEsoluhICJylFEPBXf/K2D9rF41WnX0nCkUqk9BRCQrZ4+I6lMQETlazh4R1acgInK0nD0iKhRERI6Ws0fERDoN6D4FEZHecvaIqDMFEZGj5ewRUaEgInK0nD0idmcvSdWT10REeuRsKOiSVBGRo+XsEVHNRyIiR8vZI2IilSEeM+Kx/m6uFhHJPTkdCrocVUTkcDl7VEykM2o6EhE5Qs4eFRMphYKIyJFy8qh4yxO7uH3dHmZUFEVdiojImBLF8xQiN696EquWTOVTrzkl6lJERMaUnAyFs+dXcfb8qqjLEBEZc3Ky+UhERPqmUBARkSyFgoiIZCkUREQkS6EgIiJZCgUREclSKIiISJZCQUREsszdo65hyMysHtgxxLdXAweHsZyRojqHz3ioEVTncBoPNcLo1znX3Wv6WjGuQ+FEmNlqd18ZdR3HojqHz3ioEVTncBoPNcLYqlPNRyIikqVQEBGRrFwOheujLmCQVOfwGQ81guocTuOhRhhDdeZsn4KIiBwtl88URETkCAoFERHJyslQMLNLzGyTmW0xs+uirqeHmW03s6fNbJ2ZrQ6XVZnZ3Wa2Ofw+OYK6fmRmdWa2odeyPuuywDfDfbvezFZEXOdnzWxPuE/XmdllvdZ9Mqxzk5m9epRqnG1m95nZs2b2jJl9JFw+pvbnAHWOtf1ZZGaPm9lTYZ2fC5fPN7PHwnpuNrOCcHlhOL8lXD8vwhp/YmYv9NqXZ4bLI/sdAsDdc+oLiANbgQVAAfAUsDTqusLatgPVRyz7EnBdOH0d8MUI6roQWAFsOFZdwGXAHwADzgEei7jOzwIf7+O1S8N/+0Jgfvh/Ij4KNU4HVoTTZcDzYS1jan8OUOdY258GlIbT+cBj4X66BbgqXP494APh9D8C3wunrwJujrDGnwBv6uP1kf0OuXtOnimcDWxx923ungB+AVwRcU0DuQK4IZy+AbhytAtw9weBhiMW91fXFcD/eeBRoNLMpkdYZ3+uAH7h7t3u/gKwheD/xohy933uvjacbgU2AjMZY/tzgDr7E9X+dHdvC2fzwy8HLgJuC5cfuT979vNtwCozs4hq7E9kv0OQm81HM4FdveZ3M/B/9tHkwJ/MbI2ZXRMum+ru+8Lp/cDUaEo7Sn91jcX9+6HwNPxHvZrfIq8zbLpYTvCX45jdn0fUCWNsf5pZ3MzWAXXA3QRnKU3unuqjlmyd4fpmYMpo1+juPfvy8+G+/JqZFR5ZYx/1j7hcDIWx7AJ3XwFcCnzQzC7svdKDc8sxdw3xWK0r9F1gIXAmsA/4SrTlBMysFPgl8M/u3tJ73Vjan33UOeb2p7un3f1MYBbB2cmSiEs6ypE1mtlpwCcJaj0LqAI+EWGJWbkYCnuA2b3mZ4XLIufue8LvdcCvCf6DH+g5dQy/10VX4WH6q2tM7V93PxD+QmaA/+XFJo3I6jSzfIID7Y3u/qtw8Zjbn33VORb3Zw93bwLuA84laHLJ66OWbJ3h+grgUAQ1XhI20bm7dwM/Zozsy1wMhSeAxeHVCQUEnU13RlwTZjbJzMp6poGLgQ0EtV0dvuxq4I5oKjxKf3XdCbwzvILiHKC5V7PIqDuiLfb1BPsUgjqvCq9GmQ8sBh4fhXoM+CGw0d2/2mvVmNqf/dU5BvdnjZlVhtPFwN8Q9H/cB7wpfNmR+7NnP78J+HN4ZjbaNT7X648AI+jz6L0vo/sdGs1e7bHyRdC7/zxB2+Ono64nrGkBwdUbTwHP9NRF0N55L7AZuAeoiqC2mwiaCpIE7Zvv7q8ugismvhPu26eBlRHX+dOwjvUEv2zTe73+02Gdm4BLR6nGCwiahtYD68Kvy8ba/hygzrG2P5cBT4b1bAD+LVy+gCCUtgC3AoXh8qJwfku4fkGENf453JcbgJ/x4hVKkf0OubuGuRARkRflYvORiIj0Q6EgIiJZCgUREclSKIiISJZCQUREshQKMu6YWVv4fZ6ZvW2Yt/2pI+YfHqbt/iQcXbQwnK82s+3DtO1XmNlvh2NbIgoFGc/mAccVCr3ucu3PYaHg7ucdZ00DSQP/MIzbGxZmFo+6Bhk7FAoynn0BeFk4Fv1Hw0HHvmxmT4SDjL0Psn9J/8XM7gSeDZfdHg48+EzP4INm9gWgONzejeGynrMSC7e9wYJnXryl17bvN7PbzOw5M7txgFE3vw589MhgOvIvfTP7tpm9K5zebmb/Hda02sxWmNldZrbVzN7fazPlZvY7C55l8D0zi4Xvv9jMHjGztWZ2aziWUc92v2hma4G/PZF/BJlYjvVXk8hYdh3B2P6XA4QH92Z3PytspnnIzP4UvnYFcJoHwzoD/IO7N4TDDjxhZr909+vM7EMeDFx2pDcQDAJ3BlAdvufBcN1y4FRgL/AQcD7w1z62sTNc/g7gN8fxc+509zPN7GsEY/CfT3Bn7gaCZwVAMG7OUmAH8EfgDWZ2P/AZ4FXu3m5mnwA+BvxH+J5DHgzAKJKlUJCJ5GJgmZn1jHlTQTAGTwJ4vFcgAPyTmb0+nJ4dvm6ggdEuAG5y9zTB4HUPEIxu2RJuezeABcMjz6PvUAD4b4JxeH53HD9Xz9hcTxMMhdAKtJpZd8+YOmEN28Iabgrr7SIIiofCk5cC4JFe2735OGqQHKFQkInEgA+7+12HLTR7BdB+xPyrgHPdvSP8i7roBD63u9d0mgF+r9x9cxgcb+61OMXhTblH1tKz/cwRn5Xp9VlHjlfjBPvjbnd/az/ltPezXHKY+hRkPGsleFRkj7uAD1gw5DNmdpIFI84eqQJoDANhCcEjD3ske95/hL8Abwn7LWoIHv051FFAPw98vNf8DmBpOMJoJbBqCNs824KRf2PAWwjOVB4FzjezRZAdifekIdYsOUKhIOPZeiBtwQPRPwr8gKAjea2ZbQC+T99/tf8RyDOzjQSd1Y/2Wnc9sL6no7mXX4ef9xTB6JbXuvv+oRTt7s8Aa3vN7yJ4pvCG8PuTQ9jsE8C3CYaNfgH4tbvXA+8CbjKz9QRNR2PuATQytmiUVBERydKZgoiIZCkUREQkS6EgIiJZCgUREclSKIiISJZCQUREshQKIiKS9f8BEitq3gXXgrYAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "avg_train_losses = []\n",
        "for i in range(len(train_losses[0])):\n",
        "    avg_loss = 0\n",
        "    for j in range(num_workers):\n",
        "        avg_loss += train_losses[j][i]\n",
        "\n",
        "    avg_train_losses.append(avg_loss / num_workers)    \n"
      ],
      "metadata": {
        "id": "kEUwM_aVuh-y"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_losses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24b97111-23c4-4e1b-8653-a8bfbcd9fa6a",
        "id": "6RnEefbzuh-y"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "366"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(avg_train_losses, label='train')\n",
        "plt.plot(test_losses, label='test')\n",
        "plt.xlabel('Iteration Number')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Loss function');"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "eefe357f-4d10-4e15-a0a5-e73305759e88",
        "id": "odg7eVO6uh-y"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "UK8yhL-WUUOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nmTMlxH4UURX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Distributed Data Optimization"
      ],
      "metadata": {
        "id": "ERzyUjE_VunZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_users = 8\n",
        "\n",
        "s = len(mnist_trainset)\n",
        "client_data_sizes = []\n",
        "for i in range(num_users-1):\n",
        "    # l = random.randrange(1000, 4000)\n",
        "    l = len(mnist_trainset) // num_users\n",
        "    client_data_sizes.append(l)\n",
        "    s -= l\n",
        "\n",
        "client_data_sizes.append(len(mnist_trainset) - sum(client_data_sizes))\n",
        " \n",
        "distributed_train_data = random_split(mnist_trainset, client_data_sizes)"
      ],
      "metadata": {
        "id": "8YOGLgaNVu4m"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "train_loaders = []\n",
        "for i in range(num_users):\n",
        "    train_loaders.append(torch.utils.data.DataLoader(\n",
        "                 dataset=distributed_train_data[i],\n",
        "                 batch_size=batch_size,\n",
        "                 shuffle=True\n",
        "                 ))"
      ],
      "metadata": {
        "id": "vc3Nxt2dVvEc"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_messages(queue_i, weights_i, alpha_i, with_noise, alpha_update):\n",
        "    while len(queue_i) != 0:\n",
        "        weights_j, alpha_j = queue_i.pop()\n",
        "        for layer in weights_i.keys():\n",
        "            if  with_noise == True:\n",
        "                noise = torch.randn_like(weights_j[layer])\n",
        "            else:\n",
        "                noise = 0    \n",
        "            \n",
        "            weights_i[layer] = (alpha_j/(alpha_i+alpha_j)) * (weights_j[layer] + 0.1*noise) + (alpha_i/(alpha_i+alpha_j)) * weights_i[layer]\n",
        "            if alpha_update == True:\n",
        "                alpha_i += alpha_j\n",
        "\n",
        "    return weights_i, alpha_i        "
      ],
      "metadata": {
        "id": "cLx2rJ1pXnbT"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def push_message(queue_j, weights_i, alpha_i):\n",
        "    queue_j.append([weights_i, alpha_i/2])"
      ],
      "metadata": {
        "id": "Dkm820AoXnbU"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(iter_no, worker_num, with_noise=False, alpha_update=True):\n",
        "    net = nets_list[worker_num]\n",
        "    queue = queues_list[worker_num]\n",
        "    alpha = alpha_list[worker_num]\n",
        "\n",
        "    new_weights, new_alpha = process_messages(queue, net.state_dict(), alpha, with_noise, alpha_update)\n",
        "    net.load_state_dict(new_weights)\n",
        "    nets_list[worker_num] = net\n",
        "    alpha_list[worker_num] = new_alpha\n",
        "\n",
        "    net = nets_list[worker_num]\n",
        "    queue = queues_list[worker_num]\n",
        "    alpha = alpha_list[worker_num]\n",
        "\n",
        "    loss_batch = 0\n",
        "    for batch_data, batch_labels in train_loaders[worker_num]:\n",
        "        # Transfer to GPU\n",
        "        batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
        "        # zero the parameter gradients\n",
        "        optimizer = optimizers[worker_num]\n",
        "        optimizer.zero_grad()\n",
        "        # Model computations\n",
        "        batch_outputs = net(batch_data)        \n",
        "        loss = criterion(batch_outputs, batch_labels)\n",
        "        loss.backward()\n",
        "        loss_batch += loss.item()\n",
        "        # print statistics\n",
        "        \n",
        "        clipping_value = 1 # arbitrary value of your choosing\n",
        "        torch.nn.utils.clip_grad_norm(net.parameters(), clipping_value)\n",
        "        optimizer.step()\n",
        "\n",
        "    train_losses[worker_num].append(loss_batch / len(train_loaders[worker_num]))\n",
        "    print('Training: ', 'Iteration No: ', iter_no+1, 'Worker Num: ', worker_num, '\\n',\n",
        "        'Loss: ', train_losses[worker_num][-1])\n",
        "\n",
        "    S = np.random.binomial(1, p, 1)\n",
        "    if S == 1:\n",
        "        j = np.random.choice(comm_matrix[worker_num])\n",
        "        push_message(queues_list[j], net.state_dict(), alpha)\n",
        "\n",
        "    nets_list[worker_num] = net   \n",
        "    optimizers[worker_num] = optimizer"
      ],
      "metadata": {
        "id": "OVZhzsuaXnbV"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(iter_no, networks):\n",
        "    layers = networks[0].state_dict().keys()\n",
        "    test_weights = {}\n",
        "    for layer in layers:\n",
        "        test_weights[layer] = 0\n",
        "        for network in networks:\n",
        "            test_weights[layer] += network.state_dict()[layer]\n",
        "\n",
        "        test_weights[layer] /= num_workers\n",
        "\n",
        "    test_network.load_state_dict(test_weights)    \n",
        "\n",
        "    # Validation\n",
        "    with torch.set_grad_enabled(False):\n",
        "        batch_losses = []\n",
        "        counter = 0\n",
        "        total = 0\n",
        "        for batch_data, batch_labels in test_loader:\n",
        "            total += len(batch_data)\n",
        "            # Transfer to GPU\n",
        "            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
        "            # Model computations\n",
        "            batch_outputs = test_network(batch_data)            \n",
        "            loss = criterion(batch_outputs, batch_labels)\n",
        "            batch_losses.append(loss.item())\n",
        "            \n",
        "            # compute accuracy\n",
        "            for i in range(len(batch_data)):\n",
        "                true = int(batch_labels[i])\n",
        "                pred = int(torch.argmax(batch_outputs[i,:]))\n",
        "                if pred == true:\n",
        "                    counter += 1\n",
        "\n",
        "        test_losses.append(sum(batch_losses) / len(batch_losses))\n",
        "        print('Test: ', 'Iteration No: ', iter_no+1,'\\n',\n",
        "            'Loss: ', test_losses[-1])\n",
        "        acc = (counter*100) / total   \n",
        "        test_accuracy.append(acc)    \n",
        "        print('Test accuracy: ', acc) \n",
        "    if acc >= 99:\n",
        "        print('96 % accuracy reached!')\n",
        "        \n",
        "    return acc"
      ],
      "metadata": {
        "id": "diAuUV4IXnbV"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_workers = 8\n",
        "all_workers = [i for i in range (num_workers)]"
      ],
      "metadata": {
        "id": "aO_tTcxAY84W"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nets_list = [CNN().to(device)]\n",
        "optimizers = [optim.SGD(nets_list[0].parameters(), lr = 0.1)]\n",
        "# schedulers = [optim.lr_scheduler.MultiStepLR(optimizers[0], milestones=[210], gamma=0.1)]\n",
        "for i in range(1,num_workers):\n",
        "    new_net = CNN().to(device)\n",
        "    new_net.load_state_dict(nets_list[0].state_dict())\n",
        "    nets_list.append(new_net)\n",
        "    optimizer = optim.SGD(new_net.parameters(), lr = 0.1) \n",
        "    optimizers.append(optimizer)\n",
        "    # scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[210], gamma=0.1)\n",
        "    # schedulers.append(scheduler)"
      ],
      "metadata": {
        "id": "-q6CqQUCY84r"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_network = CNN().to(device)"
      ],
      "metadata": {
        "id": "QD6AUL9iY84r"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "queues_list = [[] for i in range (num_workers)]\n",
        "alpha_list = [1 / num_workers for i in range (num_workers)]"
      ],
      "metadata": {
        "id": "HxyDxRtSY84s"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_iters = 1000\n",
        "p = 0.5\n",
        "batch_size = 128"
      ],
      "metadata": {
        "id": "i7sLvdwTY84s"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader = torch.utils.data.DataLoader(\n",
        "                dataset=mnist_testset,\n",
        "                batch_size=batch_size,\n",
        "                shuffle=False\n",
        "                )"
      ],
      "metadata": {
        "id": "gRQe6ZTKY84s"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "train_losses = [[] for i in range (num_workers)]\n",
        "test_losses = []\n",
        "test_accuracy = []"
      ],
      "metadata": {
        "id": "wHovcaLiY84s"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comm_matrix = []\n",
        "for i in range (num_workers):\n",
        "    adj_nodes = list(range(num_workers))\n",
        "    del adj_nodes[i]\n",
        "    comm_matrix.append(adj_nodes)"
      ],
      "metadata": {
        "id": "9NogKqIqY84t"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for iter_no in range(max_iters):\n",
        "    for worker_num in range(num_workers):\n",
        "        train(iter_no, worker_num, alpha_update=False)\n",
        "    acc = test(iter_no, nets_list)\n",
        "    if acc >= 99:\n",
        "        break\n",
        "\n",
        "    # for scheduler in schedulers:\n",
        "    #     scheduler.step()    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gMVoetfXaPg",
        "outputId": "e0c46adb-731a-45ad-dcb5-504afaa249a6"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:30: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training:  Iteration No:  1 Worker Num:  0 \n",
            " Loss:  1.3886847546545125\n",
            "Training:  Iteration No:  1 Worker Num:  1 \n",
            " Loss:  1.3618089566796512\n",
            "Training:  Iteration No:  1 Worker Num:  2 \n",
            " Loss:  1.3524300506559468\n",
            "Training:  Iteration No:  1 Worker Num:  3 \n",
            " Loss:  1.3463359656980483\n",
            "Training:  Iteration No:  1 Worker Num:  4 \n",
            " Loss:  1.3765747213767747\n",
            "Training:  Iteration No:  1 Worker Num:  5 \n",
            " Loss:  0.8391237789291447\n",
            "Training:  Iteration No:  1 Worker Num:  6 \n",
            " Loss:  1.368616917375791\n",
            "Training:  Iteration No:  1 Worker Num:  7 \n",
            " Loss:  0.8576579881926715\n",
            "Test:  Iteration No:  1 \n",
            " Loss:  0.5208080661824986\n",
            "Test accuracy:  86.16\n",
            "Training:  Iteration No:  2 Worker Num:  0 \n",
            " Loss:  0.47590430055634453\n",
            "Training:  Iteration No:  2 Worker Num:  1 \n",
            " Loss:  0.5036695806656853\n",
            "Training:  Iteration No:  2 Worker Num:  2 \n",
            " Loss:  0.5019807648860802\n",
            "Training:  Iteration No:  2 Worker Num:  3 \n",
            " Loss:  0.5105557840759471\n",
            "Training:  Iteration No:  2 Worker Num:  4 \n",
            " Loss:  0.4982825022632793\n",
            "Training:  Iteration No:  2 Worker Num:  5 \n",
            " Loss:  0.41026642160900567\n",
            "Training:  Iteration No:  2 Worker Num:  6 \n",
            " Loss:  0.4180879514601271\n",
            "Training:  Iteration No:  2 Worker Num:  7 \n",
            " Loss:  0.4435491574517751\n",
            "Test:  Iteration No:  2 \n",
            " Loss:  0.3330786130567895\n",
            "Test accuracy:  90.41\n",
            "Training:  Iteration No:  3 Worker Num:  0 \n",
            " Loss:  0.35602148054009775\n",
            "Training:  Iteration No:  3 Worker Num:  1 \n",
            " Loss:  0.37108862576848367\n",
            "Training:  Iteration No:  3 Worker Num:  2 \n",
            " Loss:  0.3606229139081502\n",
            "Training:  Iteration No:  3 Worker Num:  3 \n",
            " Loss:  0.3711408566115266\n",
            "Training:  Iteration No:  3 Worker Num:  4 \n",
            " Loss:  0.3748173675799774\n",
            "Training:  Iteration No:  3 Worker Num:  5 \n",
            " Loss:  0.3184323770514989\n",
            "Training:  Iteration No:  3 Worker Num:  6 \n",
            " Loss:  0.31720176844273584\n",
            "Training:  Iteration No:  3 Worker Num:  7 \n",
            " Loss:  0.33347313560671726\n",
            "Test:  Iteration No:  3 \n",
            " Loss:  0.26422541107557995\n",
            "Test accuracy:  92.23\n",
            "Training:  Iteration No:  4 Worker Num:  0 \n",
            " Loss:  0.2903506778559442\n",
            "Training:  Iteration No:  4 Worker Num:  1 \n",
            " Loss:  0.2792315760911521\n",
            "Training:  Iteration No:  4 Worker Num:  2 \n",
            " Loss:  0.28889612701989836\n",
            "Training:  Iteration No:  4 Worker Num:  3 \n",
            " Loss:  0.28267862382581677\n",
            "Training:  Iteration No:  4 Worker Num:  4 \n",
            " Loss:  0.29592218692019834\n",
            "Training:  Iteration No:  4 Worker Num:  5 \n",
            " Loss:  0.2573189336364552\n",
            "Training:  Iteration No:  4 Worker Num:  6 \n",
            " Loss:  0.2568044611963175\n",
            "Training:  Iteration No:  4 Worker Num:  7 \n",
            " Loss:  0.2613097803067353\n",
            "Test:  Iteration No:  4 \n",
            " Loss:  0.2097617475598862\n",
            "Test accuracy:  93.81\n",
            "Training:  Iteration No:  5 Worker Num:  0 \n",
            " Loss:  0.23200259963839742\n",
            "Training:  Iteration No:  5 Worker Num:  1 \n",
            " Loss:  0.22543037354441012\n",
            "Training:  Iteration No:  5 Worker Num:  2 \n",
            " Loss:  0.23263894109907796\n",
            "Training:  Iteration No:  5 Worker Num:  3 \n",
            " Loss:  0.22460068408715522\n",
            "Training:  Iteration No:  5 Worker Num:  4 \n",
            " Loss:  0.22317834867764327\n",
            "Training:  Iteration No:  5 Worker Num:  5 \n",
            " Loss:  0.2063163227196467\n",
            "Training:  Iteration No:  5 Worker Num:  6 \n",
            " Loss:  0.20986560567960902\n",
            "Training:  Iteration No:  5 Worker Num:  7 \n",
            " Loss:  0.19684645205231036\n",
            "Test:  Iteration No:  5 \n",
            " Loss:  0.1691646399330112\n",
            "Test accuracy:  95.06\n",
            "Training:  Iteration No:  6 Worker Num:  0 \n",
            " Loss:  0.19850029101816274\n",
            "Training:  Iteration No:  6 Worker Num:  1 \n",
            " Loss:  0.18170687972994173\n",
            "Training:  Iteration No:  6 Worker Num:  2 \n",
            " Loss:  0.19382976298615084\n",
            "Training:  Iteration No:  6 Worker Num:  3 \n",
            " Loss:  0.1837566362346633\n",
            "Training:  Iteration No:  6 Worker Num:  4 \n",
            " Loss:  0.18012909944784844\n",
            "Training:  Iteration No:  6 Worker Num:  5 \n",
            " Loss:  0.1717195036047596\n",
            "Training:  Iteration No:  6 Worker Num:  6 \n",
            " Loss:  0.1733937354411109\n",
            "Training:  Iteration No:  6 Worker Num:  7 \n",
            " Loss:  0.16158661309440256\n",
            "Test:  Iteration No:  6 \n",
            " Loss:  0.13609525276041484\n",
            "Test accuracy:  95.92\n",
            "Training:  Iteration No:  7 Worker Num:  0 \n",
            " Loss:  0.1636181821004819\n",
            "Training:  Iteration No:  7 Worker Num:  1 \n",
            " Loss:  0.1457886264612109\n",
            "Training:  Iteration No:  7 Worker Num:  2 \n",
            " Loss:  0.15830229349055533\n",
            "Training:  Iteration No:  7 Worker Num:  3 \n",
            " Loss:  0.15181332296234065\n",
            "Training:  Iteration No:  7 Worker Num:  4 \n",
            " Loss:  0.14704193118012557\n",
            "Training:  Iteration No:  7 Worker Num:  5 \n",
            " Loss:  0.14530921152840226\n",
            "Training:  Iteration No:  7 Worker Num:  6 \n",
            " Loss:  0.14377800444677724\n",
            "Training:  Iteration No:  7 Worker Num:  7 \n",
            " Loss:  0.13062584558028287\n",
            "Test:  Iteration No:  7 \n",
            " Loss:  0.122437602009247\n",
            "Test accuracy:  96.35\n",
            "96 % accuracy reached!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(test_accuracy)\n",
        "plt.xlabel('Iteration Number')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Test accuracy');"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "2216bca9-173d-4ab7-936b-fe9732624b91",
        "id": "b7W8r6SWaOUl"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "avg_train_losses = []\n",
        "for i in range(len(train_losses[0])):\n",
        "    avg_loss = 0\n",
        "    for j in range(num_workers):\n",
        "        avg_loss += train_losses[j][i]\n",
        "\n",
        "    avg_train_losses.append(avg_loss / num_workers)    \n"
      ],
      "metadata": {
        "id": "5rHxTnF6aOU3"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_losses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d52fbde5-1049-4601-9330-eed21d641c6c",
        "id": "Uv7ZdcG_aOU3"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {},
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(avg_train_losses, label='train')\n",
        "plt.plot(test_losses, label='test')\n",
        "plt.xlabel('Iteration Number')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Loss function');"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "65d6750a-d5ac-4301-c73e-4689d7b06021",
        "id": "X7Lto2ypaOU3"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "shgHVC4AXaTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stochastic Matrix"
      ],
      "metadata": {
        "id": "kmZiWT50Sxty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_messages(i, queue_i, weights_i, with_noise, alpha_update):\n",
        "    while len(queue_i) != 0:\n",
        "        j, weights_j, alpha_j_i = queue_i.pop()\n",
        "        alpha_i_j = alpha_matrix[i,j]\n",
        "        for layer in weights_i.keys():\n",
        "            if  with_noise == True:\n",
        "                noise = torch.randn_like(weights_j[layer])\n",
        "            else:\n",
        "                noise = 0    \n",
        "            \n",
        "            weights_i[layer] = (alpha_j_i/(alpha_i_j+alpha_j_i)) * (weights_j[layer] + 0.1*noise) + (alpha_i_j/(alpha_i_j+alpha_j_i)) * weights_i[layer]\n",
        "            if alpha_update == True:\n",
        "                alpha_i_j += alpha_j_i\n",
        "\n",
        "    return weights_i     "
      ],
      "metadata": {
        "id": "uGm07lkmZ6v2"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def push_message(i, j, queue_j, weights_i):\n",
        "    queue_j.append([i, weights_i, alpha_matrix[i,j]/2])"
      ],
      "metadata": {
        "id": "przlIvePZ6wH"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(iter_no, worker_num, with_noise=False, alpha_update=True):\n",
        "    net = nets_list[worker_num]\n",
        "    queue = queues_list[worker_num]\n",
        "    # alpha = alpha_list[worker_num]\n",
        "\n",
        "    new_weights = process_messages(worker_num, queue, net.state_dict(), with_noise, alpha_update)\n",
        "    net.load_state_dict(new_weights)\n",
        "    nets_list[worker_num] = net\n",
        "    # alpha_list[worker_num] = new_alpha\n",
        "\n",
        "    net = nets_list[worker_num]\n",
        "    queue = queues_list[worker_num]\n",
        "    # alpha = alpha_list[worker_num]\n",
        "\n",
        "    \n",
        "    try :\n",
        "        batch_data, batch_labels = data_iters[worker_num].next()\n",
        "    except:\n",
        "        train_loader = torch.utils.data.DataLoader(\n",
        "                 dataset=mnist_trainset,\n",
        "                 batch_size=batch_size,\n",
        "                 shuffle=True\n",
        "                 )\n",
        "    \n",
        "        data_iters[worker_num] = iter(train_loader)\n",
        "\n",
        "        batch_data, batch_labels = data_iters[worker_num].next()\n",
        "\n",
        "\n",
        "    # Transfer to GPU\n",
        "    batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
        "    # zero the parameter gradients\n",
        "    optimizer = optimizers[worker_num]\n",
        "    optimizer.zero_grad()\n",
        "    # Model computations\n",
        "    batch_outputs = net(batch_data)        \n",
        "    loss = criterion(batch_outputs, batch_labels)\n",
        "    loss.backward()\n",
        "    # print statistics\n",
        "    print('Training: ', 'Iteration No: ', iter_no+1, 'Worker Num: ', worker_num, '\\n',\n",
        "    'Loss: ', loss.item())\n",
        "    clipping_value = 1 # arbitrary value of your choosing\n",
        "    torch.nn.utils.clip_grad_norm(net.parameters(), clipping_value)\n",
        "    optimizer.step()\n",
        "    train_losses[worker_num].append(loss.item())\n",
        "\n",
        "    S = np.random.binomial(1, p, 1)\n",
        "    if S == 1:\n",
        "        j = np.random.choice(comm_matrix[worker_num])\n",
        "        push_message(j, worker_num, queues_list[j], net.state_dict())\n",
        "\n",
        "    nets_list[worker_num] = net   \n",
        "    optimizers[worker_num] = optimizer"
      ],
      "metadata": {
        "id": "6_pvnpTFZ6wI"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(iter_no, networks):\n",
        "    layers = networks[0].state_dict().keys()\n",
        "    test_weights = {}\n",
        "    for layer in layers:\n",
        "        test_weights[layer] = 0\n",
        "        for network in networks:\n",
        "            test_weights[layer] += network.state_dict()[layer]\n",
        "\n",
        "        test_weights[layer] /= num_workers\n",
        "\n",
        "    test_network.load_state_dict(test_weights)    \n",
        "\n",
        "    # Validation\n",
        "    with torch.set_grad_enabled(False):\n",
        "        batch_losses = []\n",
        "        counter = 0\n",
        "        total = 0\n",
        "        for batch_data, batch_labels in test_loader:\n",
        "            total += len(batch_data)\n",
        "            # Transfer to GPU\n",
        "            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
        "            # Model computations\n",
        "            batch_outputs = test_network(batch_data)            \n",
        "            loss = criterion(batch_outputs, batch_labels)\n",
        "            batch_losses.append(loss.item())\n",
        "            \n",
        "            # compute accuracy\n",
        "            for i in range(len(batch_data)):\n",
        "                true = int(batch_labels[i])\n",
        "                pred = int(torch.argmax(batch_outputs[i,:]))\n",
        "                if pred == true:\n",
        "                    counter += 1\n",
        "\n",
        "        test_losses.append(sum(batch_losses) / len(batch_losses))\n",
        "        print('Test: ', 'Iteration No: ', iter_no+1,'\\n',\n",
        "            'Loss: ', test_losses[-1])\n",
        "        acc = (counter*100) / total   \n",
        "        test_accuracy.append(acc)    \n",
        "        print('Test accuracy: ', acc) \n",
        "    if acc >= 96:\n",
        "        print('96 % accuracy reached!')\n",
        "        \n",
        "    return acc"
      ],
      "metadata": {
        "id": "xLERdOHSVieC"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_workers = 8\n",
        "all_workers = [i for i in range (num_workers)]"
      ],
      "metadata": {
        "id": "1hUdsDuhR8fd"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alpha_matrix = F.normalize(torch.rand(num_workers, num_workers))"
      ],
      "metadata": {
        "id": "9jySgG9WNpgP"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nets_list = [CNN().to(device)]\n",
        "optimizers = [optim.SGD(nets_list[0].parameters(), lr = 0.1)]\n",
        "# schedulers = [optim.lr_scheduler.MultiStepLR(optimizers[0], milestones=[210], gamma=0.1)]\n",
        "for i in range(1,num_workers):\n",
        "    new_net = CNN().to(device)\n",
        "    new_net.load_state_dict(nets_list[0].state_dict())\n",
        "    nets_list.append(new_net)\n",
        "    optimizer = optim.SGD(new_net.parameters(), lr = 0.1) \n",
        "    optimizers.append(optimizer)\n",
        "    # scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[210], gamma=0.1)\n",
        "    # schedulers.append(scheduler)"
      ],
      "metadata": {
        "id": "9Qnb9xDGR8fe"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_network = CNN().to(device)"
      ],
      "metadata": {
        "id": "LC0Rf8oqR8ff"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "queues_list = [[] for i in range (num_workers)]\n",
        "# alpha_list = [1 / num_workers for i in range (num_workers)]"
      ],
      "metadata": {
        "id": "A2dP4MoXR8ff"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_iters = 1000\n",
        "p = 0.5\n",
        "batch_size = 128"
      ],
      "metadata": {
        "id": "yu3T8VIvR8ff"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_iters = []\n",
        "for i in range(num_workers):\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "                 dataset=mnist_trainset,\n",
        "                 batch_size=batch_size,\n",
        "                 shuffle=True\n",
        "                 )\n",
        "    \n",
        "    data_iters.append(iter(train_loader))"
      ],
      "metadata": {
        "id": "OJiYYC9MR8fg"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader = torch.utils.data.DataLoader(\n",
        "                dataset=mnist_testset,\n",
        "                batch_size=batch_size,\n",
        "                shuffle=False\n",
        "                )"
      ],
      "metadata": {
        "id": "uS9fn48hR8fg"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "train_losses = [[] for i in range (num_workers)]\n",
        "test_losses = []\n",
        "test_accuracy = []"
      ],
      "metadata": {
        "id": "C8DfIrgbR8fg"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comm_matrix = []\n",
        "for i in range (num_workers):\n",
        "    adj_nodes = list(range(num_workers))\n",
        "    del adj_nodes[i]\n",
        "    comm_matrix.append(adj_nodes)"
      ],
      "metadata": {
        "id": "3HJoO3VtR8fg"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for iter_no in range(max_iters):\n",
        "    for worker_num in range(num_workers):\n",
        "        train(iter_no, worker_num, alpha_update=False)\n",
        "    acc = test(iter_no, nets_list)\n",
        "    if acc >= 96:\n",
        "        break\n",
        "\n",
        "    # for scheduler in schedulers:\n",
        "    #     scheduler.step()    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "776877e8-080d-448e-a113-32ba1bda3c59",
        "id": "28ViSQtPR8fh"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training:  Iteration No:  1 Worker Num:  0 \n",
            " Loss:  2.3137171268463135\n",
            "Training:  Iteration No:  1 Worker Num:  1 \n",
            " Loss:  2.308102607727051\n",
            "Training:  Iteration No:  1 Worker Num:  2 \n",
            " Loss:  2.3135571479797363\n",
            "Training:  Iteration No:  1 Worker Num:  3 \n",
            " Loss:  2.3140082359313965\n",
            "Training:  Iteration No:  1 Worker Num:  4 \n",
            " Loss:  2.310858726501465\n",
            "Training:  Iteration No:  1 Worker Num:  5 \n",
            " Loss:  2.3074374198913574\n",
            "Training:  Iteration No:  1 Worker Num:  6 \n",
            " Loss:  2.313650369644165\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:43: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            " Loss:  0.44192833915541446\n",
            "Test accuracy:  87.66\n",
            "Training:  Iteration No:  76 Worker Num:  0 \n",
            " Loss:  0.5950531959533691\n",
            "Training:  Iteration No:  76 Worker Num:  1 \n",
            " Loss:  0.4464334547519684\n",
            "Training:  Iteration No:  76 Worker Num:  2 \n",
            " Loss:  0.4745464026927948\n",
            "Training:  Iteration No:  76 Worker Num:  3 \n",
            " Loss:  0.45453640818595886\n",
            "Training:  Iteration No:  76 Worker Num:  4 \n",
            " Loss:  0.5553104281425476\n",
            "Training:  Iteration No:  76 Worker Num:  5 \n",
            " Loss:  0.4745117723941803\n",
            "Training:  Iteration No:  76 Worker Num:  6 \n",
            " Loss:  0.4318024516105652\n",
            "Training:  Iteration No:  76 Worker Num:  7 \n",
            " Loss:  0.5118237137794495\n",
            "Test:  Iteration No:  76 \n",
            " Loss:  0.43326282142838346\n",
            "Test accuracy:  88.01\n",
            "Training:  Iteration No:  77 Worker Num:  0 \n",
            " Loss:  0.5170127153396606\n",
            "Training:  Iteration No:  77 Worker Num:  1 \n",
            " Loss:  0.5205230712890625\n",
            "Training:  Iteration No:  77 Worker Num:  2 \n",
            " Loss:  0.6030197143554688\n",
            "Training:  Iteration No:  77 Worker Num:  3 \n",
            " Loss:  0.5804605484008789\n",
            "Training:  Iteration No:  77 Worker Num:  4 \n",
            " Loss:  0.6530264616012573\n",
            "Training:  Iteration No:  77 Worker Num:  5 \n",
            " Loss:  0.5026530623435974\n",
            "Training:  Iteration No:  77 Worker Num:  6 \n",
            " Loss:  0.5678786039352417\n",
            "Training:  Iteration No:  77 Worker Num:  7 \n",
            " Loss:  0.4598005414009094\n",
            "Test:  Iteration No:  77 \n",
            " Loss:  0.440188827299619\n",
            "Test accuracy:  87.56\n",
            "Training:  Iteration No:  78 Worker Num:  0 \n",
            " Loss:  0.45361852645874023\n",
            "Training:  Iteration No:  78 Worker Num:  1 \n",
            " Loss:  0.683264970779419\n",
            "Training:  Iteration No:  78 Worker Num:  2 \n",
            " Loss:  0.46899473667144775\n",
            "Training:  Iteration No:  78 Worker Num:  3 \n",
            " Loss:  0.5953428745269775\n",
            "Training:  Iteration No:  78 Worker Num:  4 \n",
            " Loss:  0.6274205446243286\n",
            "Training:  Iteration No:  78 Worker Num:  5 \n",
            " Loss:  0.5487037301063538\n",
            "Training:  Iteration No:  78 Worker Num:  6 \n",
            " Loss:  0.35746678709983826\n",
            "Training:  Iteration No:  78 Worker Num:  7 \n",
            " Loss:  0.375113308429718\n",
            "Test:  Iteration No:  78 \n",
            " Loss:  0.43092879964203773\n",
            "Test accuracy:  87.75\n",
            "Training:  Iteration No:  79 Worker Num:  0 \n",
            " Loss:  0.4189442992210388\n",
            "Training:  Iteration No:  79 Worker Num:  1 \n",
            " Loss:  0.4816758632659912\n",
            "Training:  Iteration No:  79 Worker Num:  2 \n",
            " Loss:  0.5230045318603516\n",
            "Training:  Iteration No:  79 Worker Num:  3 \n",
            " Loss:  0.6238107681274414\n",
            "Training:  Iteration No:  79 Worker Num:  4 \n",
            " Loss:  0.54380202293396\n",
            "Training:  Iteration No:  79 Worker Num:  5 \n",
            " Loss:  0.5541769862174988\n",
            "Training:  Iteration No:  79 Worker Num:  6 \n",
            " Loss:  0.3383162319660187\n",
            "Training:  Iteration No:  79 Worker Num:  7 \n",
            " Loss:  0.4320460855960846\n",
            "Test:  Iteration No:  79 \n",
            " Loss:  0.42280259549240523\n",
            "Test accuracy:  88.53\n",
            "Training:  Iteration No:  80 Worker Num:  0 \n",
            " Loss:  0.48774874210357666\n",
            "Training:  Iteration No:  80 Worker Num:  1 \n",
            " Loss:  0.5634787678718567\n",
            "Training:  Iteration No:  80 Worker Num:  2 \n",
            " Loss:  0.5660325288772583\n",
            "Training:  Iteration No:  80 Worker Num:  3 \n",
            " Loss:  0.6363509297370911\n",
            "Training:  Iteration No:  80 Worker Num:  4 \n",
            " Loss:  0.6925424337387085\n",
            "Training:  Iteration No:  80 Worker Num:  5 \n",
            " Loss:  0.4033505916595459\n",
            "Training:  Iteration No:  80 Worker Num:  6 \n",
            " Loss:  0.796805202960968\n",
            "Training:  Iteration No:  80 Worker Num:  7 \n",
            " Loss:  0.579871416091919\n",
            "Test:  Iteration No:  80 \n",
            " Loss:  0.42277545613955847\n",
            "Test accuracy:  88.21\n",
            "Training:  Iteration No:  81 Worker Num:  0 \n",
            " Loss:  0.4555351436138153\n",
            "Training:  Iteration No:  81 Worker Num:  1 \n",
            " Loss:  0.4786725342273712\n",
            "Training:  Iteration No:  81 Worker Num:  2 \n",
            " Loss:  0.4767972230911255\n",
            "Training:  Iteration No:  81 Worker Num:  3 \n",
            " Loss:  0.3849041759967804\n",
            "Training:  Iteration No:  81 Worker Num:  4 \n",
            " Loss:  0.44162681698799133\n",
            "Training:  Iteration No:  81 Worker Num:  5 \n",
            " Loss:  0.5911939144134521\n",
            "Training:  Iteration No:  81 Worker Num:  6 \n",
            " Loss:  0.5844644904136658\n",
            "Training:  Iteration No:  81 Worker Num:  7 \n",
            " Loss:  0.31802424788475037\n",
            "Test:  Iteration No:  81 \n",
            " Loss:  0.41916329724879203\n",
            "Test accuracy:  88.65\n",
            "Training:  Iteration No:  82 Worker Num:  0 \n",
            " Loss:  0.7000541090965271\n",
            "Training:  Iteration No:  82 Worker Num:  1 \n",
            " Loss:  0.49300625920295715\n",
            "Training:  Iteration No:  82 Worker Num:  2 \n",
            " Loss:  0.41479751467704773\n",
            "Training:  Iteration No:  82 Worker Num:  3 \n",
            " Loss:  0.3940746784210205\n",
            "Training:  Iteration No:  82 Worker Num:  4 \n",
            " Loss:  0.518419623374939\n",
            "Training:  Iteration No:  82 Worker Num:  5 \n",
            " Loss:  0.4884915351867676\n",
            "Training:  Iteration No:  82 Worker Num:  6 \n",
            " Loss:  0.5677096247673035\n",
            "Training:  Iteration No:  82 Worker Num:  7 \n",
            " Loss:  0.5285874009132385\n",
            "Test:  Iteration No:  82 \n",
            " Loss:  0.4136067653381372\n",
            "Test accuracy:  88.74\n",
            "Training:  Iteration No:  83 Worker Num:  0 \n",
            " Loss:  0.6755009889602661\n",
            "Training:  Iteration No:  83 Worker Num:  1 \n",
            " Loss:  0.5044068098068237\n",
            "Training:  Iteration No:  83 Worker Num:  2 \n",
            " Loss:  0.4480374753475189\n",
            "Training:  Iteration No:  83 Worker Num:  3 \n",
            " Loss:  0.4532381296157837\n",
            "Training:  Iteration No:  83 Worker Num:  4 \n",
            " Loss:  0.47308292984962463\n",
            "Training:  Iteration No:  83 Worker Num:  5 \n",
            " Loss:  0.3481254279613495\n",
            "Training:  Iteration No:  83 Worker Num:  6 \n",
            " Loss:  0.4760390520095825\n",
            "Training:  Iteration No:  83 Worker Num:  7 \n",
            " Loss:  0.5575323104858398\n",
            "Test:  Iteration No:  83 \n",
            " Loss:  0.42084388180246834\n",
            "Test accuracy:  88.15\n",
            "Training:  Iteration No:  84 Worker Num:  0 \n",
            " Loss:  0.533884584903717\n",
            "Training:  Iteration No:  84 Worker Num:  1 \n",
            " Loss:  0.3828057646751404\n",
            "Training:  Iteration No:  84 Worker Num:  2 \n",
            " Loss:  0.5702781081199646\n",
            "Training:  Iteration No:  84 Worker Num:  3 \n",
            " Loss:  0.3935641646385193\n",
            "Training:  Iteration No:  84 Worker Num:  4 \n",
            " Loss:  0.6232814788818359\n",
            "Training:  Iteration No:  84 Worker Num:  5 \n",
            " Loss:  0.4061243236064911\n",
            "Training:  Iteration No:  84 Worker Num:  6 \n",
            " Loss:  0.3704685866832733\n",
            "Training:  Iteration No:  84 Worker Num:  7 \n",
            " Loss:  0.46135690808296204\n",
            "Test:  Iteration No:  84 \n",
            " Loss:  0.4090429497293279\n",
            "Test accuracy:  88.73\n",
            "Training:  Iteration No:  85 Worker Num:  0 \n",
            " Loss:  0.48741504549980164\n",
            "Training:  Iteration No:  85 Worker Num:  1 \n",
            " Loss:  0.5165658593177795\n",
            "Training:  Iteration No:  85 Worker Num:  2 \n",
            " Loss:  0.33436739444732666\n",
            "Training:  Iteration No:  85 Worker Num:  3 \n",
            " Loss:  0.4164222776889801\n",
            "Training:  Iteration No:  85 Worker Num:  4 \n",
            " Loss:  0.4104335308074951\n",
            "Training:  Iteration No:  85 Worker Num:  5 \n",
            " Loss:  0.5749564170837402\n",
            "Training:  Iteration No:  85 Worker Num:  6 \n",
            " Loss:  0.5435580015182495\n",
            "Training:  Iteration No:  85 Worker Num:  7 \n",
            " Loss:  0.5508083701133728\n",
            "Test:  Iteration No:  85 \n",
            " Loss:  0.408162631283078\n",
            "Test accuracy:  88.89\n",
            "Training:  Iteration No:  86 Worker Num:  0 \n",
            " Loss:  0.5441886186599731\n",
            "Training:  Iteration No:  86 Worker Num:  1 \n",
            " Loss:  0.5524531602859497\n",
            "Training:  Iteration No:  86 Worker Num:  2 \n",
            " Loss:  0.43222638964653015\n",
            "Training:  Iteration No:  86 Worker Num:  3 \n",
            " Loss:  0.6746390461921692\n",
            "Training:  Iteration No:  86 Worker Num:  4 \n",
            " Loss:  0.5260440111160278\n",
            "Training:  Iteration No:  86 Worker Num:  5 \n",
            " Loss:  0.427396297454834\n",
            "Training:  Iteration No:  86 Worker Num:  6 \n",
            " Loss:  0.3743099272251129\n",
            "Training:  Iteration No:  86 Worker Num:  7 \n",
            " Loss:  0.5493937730789185\n",
            "Test:  Iteration No:  86 \n",
            " Loss:  0.422465730788587\n",
            "Test accuracy:  87.16\n",
            "Training:  Iteration No:  87 Worker Num:  0 \n",
            " Loss:  0.4816049635410309\n",
            "Training:  Iteration No:  87 Worker Num:  1 \n",
            " Loss:  0.5455931425094604\n",
            "Training:  Iteration No:  87 Worker Num:  2 \n",
            " Loss:  0.47353270649909973\n",
            "Training:  Iteration No:  87 Worker Num:  3 \n",
            " Loss:  0.5194230079650879\n",
            "Training:  Iteration No:  87 Worker Num:  4 \n",
            " Loss:  0.40198981761932373\n",
            "Training:  Iteration No:  87 Worker Num:  5 \n",
            " Loss:  0.44627082347869873\n",
            "Training:  Iteration No:  87 Worker Num:  6 \n",
            " Loss:  0.5608890652656555\n",
            "Training:  Iteration No:  87 Worker Num:  7 \n",
            " Loss:  0.5575180053710938\n",
            "Test:  Iteration No:  87 \n",
            " Loss:  0.4075034059867074\n",
            "Test accuracy:  88.75\n",
            "Training:  Iteration No:  88 Worker Num:  0 \n",
            " Loss:  0.49808526039123535\n",
            "Training:  Iteration No:  88 Worker Num:  1 \n",
            " Loss:  0.4831182658672333\n",
            "Training:  Iteration No:  88 Worker Num:  2 \n",
            " Loss:  0.5559196472167969\n",
            "Training:  Iteration No:  88 Worker Num:  3 \n",
            " Loss:  0.5312039852142334\n",
            "Training:  Iteration No:  88 Worker Num:  4 \n",
            " Loss:  0.469226211309433\n",
            "Training:  Iteration No:  88 Worker Num:  5 \n",
            " Loss:  0.46968191862106323\n",
            "Training:  Iteration No:  88 Worker Num:  6 \n",
            " Loss:  0.5907533168792725\n",
            "Training:  Iteration No:  88 Worker Num:  7 \n",
            " Loss:  0.5375540256500244\n",
            "Test:  Iteration No:  88 \n",
            " Loss:  0.40571203959893576\n",
            "Test accuracy:  88.44\n",
            "Training:  Iteration No:  89 Worker Num:  0 \n",
            " Loss:  0.4989473521709442\n",
            "Training:  Iteration No:  89 Worker Num:  1 \n",
            " Loss:  0.48302385210990906\n",
            "Training:  Iteration No:  89 Worker Num:  2 \n",
            " Loss:  0.46547672152519226\n",
            "Training:  Iteration No:  89 Worker Num:  3 \n",
            " Loss:  0.4038873016834259\n",
            "Training:  Iteration No:  89 Worker Num:  4 \n",
            " Loss:  0.4819130301475525\n",
            "Training:  Iteration No:  89 Worker Num:  5 \n",
            " Loss:  0.4452255070209503\n",
            "Training:  Iteration No:  89 Worker Num:  6 \n",
            " Loss:  0.4802531599998474\n",
            "Training:  Iteration No:  89 Worker Num:  7 \n",
            " Loss:  0.4302353262901306\n",
            "Test:  Iteration No:  89 \n",
            " Loss:  0.39979628888489327\n",
            "Test accuracy:  89.12\n",
            "Training:  Iteration No:  90 Worker Num:  0 \n",
            " Loss:  0.45859137177467346\n",
            "Training:  Iteration No:  90 Worker Num:  1 \n",
            " Loss:  0.38412079215049744\n",
            "Training:  Iteration No:  90 Worker Num:  2 \n",
            " Loss:  0.4930706024169922\n",
            "Training:  Iteration No:  90 Worker Num:  3 \n",
            " Loss:  0.5696734189987183\n",
            "Training:  Iteration No:  90 Worker Num:  4 \n",
            " Loss:  0.41926315426826477\n",
            "Training:  Iteration No:  90 Worker Num:  5 \n",
            " Loss:  0.6248340606689453\n",
            "Training:  Iteration No:  90 Worker Num:  6 \n",
            " Loss:  0.3914790749549866\n",
            "Training:  Iteration No:  90 Worker Num:  7 \n",
            " Loss:  0.4701537787914276\n",
            "Test:  Iteration No:  90 \n",
            " Loss:  0.39738078773776186\n",
            "Test accuracy:  88.67\n",
            "Training:  Iteration No:  91 Worker Num:  0 \n",
            " Loss:  0.39614981412887573\n",
            "Training:  Iteration No:  91 Worker Num:  1 \n",
            " Loss:  0.3491709232330322\n",
            "Training:  Iteration No:  91 Worker Num:  2 \n",
            " Loss:  0.4266502559185028\n",
            "Training:  Iteration No:  91 Worker Num:  3 \n",
            " Loss:  0.4550793170928955\n",
            "Training:  Iteration No:  91 Worker Num:  4 \n",
            " Loss:  0.4386056959629059\n",
            "Training:  Iteration No:  91 Worker Num:  5 \n",
            " Loss:  0.39720186591148376\n",
            "Training:  Iteration No:  91 Worker Num:  6 \n",
            " Loss:  0.4801192283630371\n",
            "Training:  Iteration No:  91 Worker Num:  7 \n",
            " Loss:  0.43965649604797363\n",
            "Test:  Iteration No:  91 \n",
            " Loss:  0.40185717540451243\n",
            "Test accuracy:  88.48\n",
            "Training:  Iteration No:  92 Worker Num:  0 \n",
            " Loss:  0.4311826229095459\n",
            "Training:  Iteration No:  92 Worker Num:  1 \n",
            " Loss:  0.4879809319972992\n",
            "Training:  Iteration No:  92 Worker Num:  2 \n",
            " Loss:  0.5615849494934082\n",
            "Training:  Iteration No:  92 Worker Num:  3 \n",
            " Loss:  0.4469659626483917\n",
            "Training:  Iteration No:  92 Worker Num:  4 \n",
            " Loss:  0.42895397543907166\n",
            "Training:  Iteration No:  92 Worker Num:  5 \n",
            " Loss:  0.6673266291618347\n",
            "Training:  Iteration No:  92 Worker Num:  6 \n",
            " Loss:  0.5271929502487183\n",
            "Training:  Iteration No:  92 Worker Num:  7 \n",
            " Loss:  0.5739132165908813\n",
            "Test:  Iteration No:  92 \n",
            " Loss:  0.38920991217033774\n",
            "Test accuracy:  89.3\n",
            "Training:  Iteration No:  93 Worker Num:  0 \n",
            " Loss:  0.5013309717178345\n",
            "Training:  Iteration No:  93 Worker Num:  1 \n",
            " Loss:  0.47749748826026917\n",
            "Training:  Iteration No:  93 Worker Num:  2 \n",
            " Loss:  0.41927191615104675\n",
            "Training:  Iteration No:  93 Worker Num:  3 \n",
            " Loss:  0.31036069989204407\n",
            "Training:  Iteration No:  93 Worker Num:  4 \n",
            " Loss:  0.42222362756729126\n",
            "Training:  Iteration No:  93 Worker Num:  5 \n",
            " Loss:  0.5460870862007141\n",
            "Training:  Iteration No:  93 Worker Num:  6 \n",
            " Loss:  0.5171769261360168\n",
            "Training:  Iteration No:  93 Worker Num:  7 \n",
            " Loss:  0.4635467529296875\n",
            "Test:  Iteration No:  93 \n",
            " Loss:  0.38172273460445527\n",
            "Test accuracy:  89.42\n",
            "Training:  Iteration No:  94 Worker Num:  0 \n",
            " Loss:  0.6178462505340576\n",
            "Training:  Iteration No:  94 Worker Num:  1 \n",
            " Loss:  0.3177243173122406\n",
            "Training:  Iteration No:  94 Worker Num:  2 \n",
            " Loss:  0.4580647647380829\n",
            "Training:  Iteration No:  94 Worker Num:  3 \n",
            " Loss:  0.6397876739501953\n",
            "Training:  Iteration No:  94 Worker Num:  4 \n",
            " Loss:  0.32556644082069397\n",
            "Training:  Iteration No:  94 Worker Num:  5 \n",
            " Loss:  0.35513174533843994\n",
            "Training:  Iteration No:  94 Worker Num:  6 \n",
            " Loss:  0.5900497436523438\n",
            "Training:  Iteration No:  94 Worker Num:  7 \n",
            " Loss:  0.47953134775161743\n",
            "Test:  Iteration No:  94 \n",
            " Loss:  0.3856628253301488\n",
            "Test accuracy:  89.11\n",
            "Training:  Iteration No:  95 Worker Num:  0 \n",
            " Loss:  0.5211341977119446\n",
            "Training:  Iteration No:  95 Worker Num:  1 \n",
            " Loss:  0.3847392499446869\n",
            "Training:  Iteration No:  95 Worker Num:  2 \n",
            " Loss:  0.4316617548465729\n",
            "Training:  Iteration No:  95 Worker Num:  3 \n",
            " Loss:  0.3661622703075409\n",
            "Training:  Iteration No:  95 Worker Num:  4 \n",
            " Loss:  0.37519022822380066\n",
            "Training:  Iteration No:  95 Worker Num:  5 \n",
            " Loss:  0.5016210675239563\n",
            "Training:  Iteration No:  95 Worker Num:  6 \n",
            " Loss:  0.4612688422203064\n",
            "Training:  Iteration No:  95 Worker Num:  7 \n",
            " Loss:  0.34184789657592773\n",
            "Test:  Iteration No:  95 \n",
            " Loss:  0.3811300185662282\n",
            "Test accuracy:  89.47\n",
            "Training:  Iteration No:  96 Worker Num:  0 \n",
            " Loss:  0.5137641429901123\n",
            "Training:  Iteration No:  96 Worker Num:  1 \n",
            " Loss:  0.3785270154476166\n",
            "Training:  Iteration No:  96 Worker Num:  2 \n",
            " Loss:  0.4416469931602478\n",
            "Training:  Iteration No:  96 Worker Num:  3 \n",
            " Loss:  0.40314099192619324\n",
            "Training:  Iteration No:  96 Worker Num:  4 \n",
            " Loss:  0.4594200849533081\n",
            "Training:  Iteration No:  96 Worker Num:  5 \n",
            " Loss:  0.35441792011260986\n",
            "Training:  Iteration No:  96 Worker Num:  6 \n",
            " Loss:  0.40746092796325684\n",
            "Training:  Iteration No:  96 Worker Num:  7 \n",
            " Loss:  0.5754595994949341\n",
            "Test:  Iteration No:  96 \n",
            " Loss:  0.3807442834294295\n",
            "Test accuracy:  89.28\n",
            "Training:  Iteration No:  97 Worker Num:  0 \n",
            " Loss:  0.2848653793334961\n",
            "Training:  Iteration No:  97 Worker Num:  1 \n",
            " Loss:  0.36824074387550354\n",
            "Training:  Iteration No:  97 Worker Num:  2 \n",
            " Loss:  0.42998042702674866\n",
            "Training:  Iteration No:  97 Worker Num:  3 \n",
            " Loss:  0.4357060194015503\n",
            "Training:  Iteration No:  97 Worker Num:  4 \n",
            " Loss:  0.5121316313743591\n",
            "Training:  Iteration No:  97 Worker Num:  5 \n",
            " Loss:  0.37972280383110046\n",
            "Training:  Iteration No:  97 Worker Num:  6 \n",
            " Loss:  0.4290908873081207\n",
            "Training:  Iteration No:  97 Worker Num:  7 \n",
            " Loss:  0.38954946398735046\n",
            "Test:  Iteration No:  97 \n",
            " Loss:  0.37714375282012963\n",
            "Test accuracy:  89.37\n",
            "Training:  Iteration No:  98 Worker Num:  0 \n",
            " Loss:  0.3714118003845215\n",
            "Training:  Iteration No:  98 Worker Num:  1 \n",
            " Loss:  0.49679675698280334\n",
            "Training:  Iteration No:  98 Worker Num:  2 \n",
            " Loss:  0.38276776671409607\n",
            "Training:  Iteration No:  98 Worker Num:  3 \n",
            " Loss:  0.3996589183807373\n",
            "Training:  Iteration No:  98 Worker Num:  4 \n",
            " Loss:  0.6230672597885132\n",
            "Training:  Iteration No:  98 Worker Num:  5 \n",
            " Loss:  0.4857715368270874\n",
            "Training:  Iteration No:  98 Worker Num:  6 \n",
            " Loss:  0.4267159402370453\n",
            "Training:  Iteration No:  98 Worker Num:  7 \n",
            " Loss:  0.34987059235572815\n",
            "Test:  Iteration No:  98 \n",
            " Loss:  0.37031165425536\n",
            "Test accuracy:  89.65\n",
            "Training:  Iteration No:  99 Worker Num:  0 \n",
            " Loss:  0.45082277059555054\n",
            "Training:  Iteration No:  99 Worker Num:  1 \n",
            " Loss:  0.3687283396720886\n",
            "Training:  Iteration No:  99 Worker Num:  2 \n",
            " Loss:  0.3953719139099121\n",
            "Training:  Iteration No:  99 Worker Num:  3 \n",
            " Loss:  0.4677099585533142\n",
            "Training:  Iteration No:  99 Worker Num:  4 \n",
            " Loss:  0.5313384532928467\n",
            "Training:  Iteration No:  99 Worker Num:  5 \n",
            " Loss:  0.36610329151153564\n",
            "Training:  Iteration No:  99 Worker Num:  6 \n",
            " Loss:  0.34627649188041687\n",
            "Training:  Iteration No:  99 Worker Num:  7 \n",
            " Loss:  0.4472655951976776\n",
            "Test:  Iteration No:  99 \n",
            " Loss:  0.3776016874781138\n",
            "Test accuracy:  89.41\n",
            "Training:  Iteration No:  100 Worker Num:  0 \n",
            " Loss:  0.35748153924942017\n",
            "Training:  Iteration No:  100 Worker Num:  1 \n",
            " Loss:  0.5203692317008972\n",
            "Training:  Iteration No:  100 Worker Num:  2 \n",
            " Loss:  0.4716653823852539\n",
            "Training:  Iteration No:  100 Worker Num:  3 \n",
            " Loss:  0.3651232123374939\n",
            "Training:  Iteration No:  100 Worker Num:  4 \n",
            " Loss:  0.2594037353992462\n",
            "Training:  Iteration No:  100 Worker Num:  5 \n",
            " Loss:  0.4395911991596222\n",
            "Training:  Iteration No:  100 Worker Num:  6 \n",
            " Loss:  0.4028083384037018\n",
            "Training:  Iteration No:  100 Worker Num:  7 \n",
            " Loss:  0.526309609413147\n",
            "Test:  Iteration No:  100 \n",
            " Loss:  0.3694088272278822\n",
            "Test accuracy:  89.64\n",
            "Training:  Iteration No:  101 Worker Num:  0 \n",
            " Loss:  0.4278019964694977\n",
            "Training:  Iteration No:  101 Worker Num:  1 \n",
            " Loss:  0.3092440962791443\n",
            "Training:  Iteration No:  101 Worker Num:  2 \n",
            " Loss:  0.49611419439315796\n",
            "Training:  Iteration No:  101 Worker Num:  3 \n",
            " Loss:  0.35235530138015747\n",
            "Training:  Iteration No:  101 Worker Num:  4 \n",
            " Loss:  0.4732799828052521\n",
            "Training:  Iteration No:  101 Worker Num:  5 \n",
            " Loss:  0.49871373176574707\n",
            "Training:  Iteration No:  101 Worker Num:  6 \n",
            " Loss:  0.43823298811912537\n",
            "Training:  Iteration No:  101 Worker Num:  7 \n",
            " Loss:  0.5115233659744263\n",
            "Test:  Iteration No:  101 \n",
            " Loss:  0.3680669720037074\n",
            "Test accuracy:  89.65\n",
            "Training:  Iteration No:  102 Worker Num:  0 \n",
            " Loss:  0.34247303009033203\n",
            "Training:  Iteration No:  102 Worker Num:  1 \n",
            " Loss:  0.4708956778049469\n",
            "Training:  Iteration No:  102 Worker Num:  2 \n",
            " Loss:  0.34273895621299744\n",
            "Training:  Iteration No:  102 Worker Num:  3 \n",
            " Loss:  0.5342831611633301\n",
            "Training:  Iteration No:  102 Worker Num:  4 \n",
            " Loss:  0.5117074847221375\n",
            "Training:  Iteration No:  102 Worker Num:  5 \n",
            " Loss:  0.34909066557884216\n",
            "Training:  Iteration No:  102 Worker Num:  6 \n",
            " Loss:  0.3803788125514984\n",
            "Training:  Iteration No:  102 Worker Num:  7 \n",
            " Loss:  0.36743032932281494\n",
            "Test:  Iteration No:  102 \n",
            " Loss:  0.3668381308641615\n",
            "Test accuracy:  89.61\n",
            "Training:  Iteration No:  103 Worker Num:  0 \n",
            " Loss:  0.5611287355422974\n",
            "Training:  Iteration No:  103 Worker Num:  1 \n",
            " Loss:  0.47610780596733093\n",
            "Training:  Iteration No:  103 Worker Num:  2 \n",
            " Loss:  0.6020171046257019\n",
            "Training:  Iteration No:  103 Worker Num:  3 \n",
            " Loss:  0.3458302319049835\n",
            "Training:  Iteration No:  103 Worker Num:  4 \n",
            " Loss:  0.42613935470581055\n",
            "Training:  Iteration No:  103 Worker Num:  5 \n",
            " Loss:  0.30064693093299866\n",
            "Training:  Iteration No:  103 Worker Num:  6 \n",
            " Loss:  0.4471909701824188\n",
            "Training:  Iteration No:  103 Worker Num:  7 \n",
            " Loss:  0.3763265013694763\n",
            "Test:  Iteration No:  103 \n",
            " Loss:  0.36376105777070494\n",
            "Test accuracy:  89.72\n",
            "Training:  Iteration No:  104 Worker Num:  0 \n",
            " Loss:  0.4523850083351135\n",
            "Training:  Iteration No:  104 Worker Num:  1 \n",
            " Loss:  0.34325429797172546\n",
            "Training:  Iteration No:  104 Worker Num:  2 \n",
            " Loss:  0.3986637592315674\n",
            "Training:  Iteration No:  104 Worker Num:  3 \n",
            " Loss:  0.35811910033226013\n",
            "Training:  Iteration No:  104 Worker Num:  4 \n",
            " Loss:  0.4479196071624756\n",
            "Training:  Iteration No:  104 Worker Num:  5 \n",
            " Loss:  0.3947676420211792\n",
            "Training:  Iteration No:  104 Worker Num:  6 \n",
            " Loss:  0.4817146956920624\n",
            "Training:  Iteration No:  104 Worker Num:  7 \n",
            " Loss:  0.328033983707428\n",
            "Test:  Iteration No:  104 \n",
            " Loss:  0.3607572721718233\n",
            "Test accuracy:  90.06\n",
            "Training:  Iteration No:  105 Worker Num:  0 \n",
            " Loss:  0.40067580342292786\n",
            "Training:  Iteration No:  105 Worker Num:  1 \n",
            " Loss:  0.5282548069953918\n",
            "Training:  Iteration No:  105 Worker Num:  2 \n",
            " Loss:  0.32291707396507263\n",
            "Training:  Iteration No:  105 Worker Num:  3 \n",
            " Loss:  0.5108657479286194\n",
            "Training:  Iteration No:  105 Worker Num:  4 \n",
            " Loss:  0.5333049297332764\n",
            "Training:  Iteration No:  105 Worker Num:  5 \n",
            " Loss:  0.47913217544555664\n",
            "Training:  Iteration No:  105 Worker Num:  6 \n",
            " Loss:  0.4801497459411621\n",
            "Training:  Iteration No:  105 Worker Num:  7 \n",
            " Loss:  0.4672602713108063\n",
            "Test:  Iteration No:  105 \n",
            " Loss:  0.36691253038146826\n",
            "Test accuracy:  89.5\n",
            "Training:  Iteration No:  106 Worker Num:  0 \n",
            " Loss:  0.45195290446281433\n",
            "Training:  Iteration No:  106 Worker Num:  1 \n",
            " Loss:  0.4787702262401581\n",
            "Training:  Iteration No:  106 Worker Num:  2 \n",
            " Loss:  0.3830930292606354\n",
            "Training:  Iteration No:  106 Worker Num:  3 \n",
            " Loss:  0.40693581104278564\n",
            "Training:  Iteration No:  106 Worker Num:  4 \n",
            " Loss:  0.5050173401832581\n",
            "Training:  Iteration No:  106 Worker Num:  5 \n",
            " Loss:  0.519353985786438\n",
            "Training:  Iteration No:  106 Worker Num:  6 \n",
            " Loss:  0.524088978767395\n",
            "Training:  Iteration No:  106 Worker Num:  7 \n",
            " Loss:  0.4553503692150116\n",
            "Test:  Iteration No:  106 \n",
            " Loss:  0.3627504026399383\n",
            "Test accuracy:  89.89\n",
            "Training:  Iteration No:  107 Worker Num:  0 \n",
            " Loss:  0.4664823114871979\n",
            "Training:  Iteration No:  107 Worker Num:  1 \n",
            " Loss:  0.38300901651382446\n",
            "Training:  Iteration No:  107 Worker Num:  2 \n",
            " Loss:  0.2861279845237732\n",
            "Training:  Iteration No:  107 Worker Num:  3 \n",
            " Loss:  0.3077601194381714\n",
            "Training:  Iteration No:  107 Worker Num:  4 \n",
            " Loss:  0.3393727242946625\n",
            "Training:  Iteration No:  107 Worker Num:  5 \n",
            " Loss:  0.37924543023109436\n",
            "Training:  Iteration No:  107 Worker Num:  6 \n",
            " Loss:  0.3405904769897461\n",
            "Training:  Iteration No:  107 Worker Num:  7 \n",
            " Loss:  0.438120573759079\n",
            "Test:  Iteration No:  107 \n",
            " Loss:  0.3660928377810913\n",
            "Test accuracy:  89.28\n",
            "Training:  Iteration No:  108 Worker Num:  0 \n",
            " Loss:  0.31054607033729553\n",
            "Training:  Iteration No:  108 Worker Num:  1 \n",
            " Loss:  0.42553016543388367\n",
            "Training:  Iteration No:  108 Worker Num:  2 \n",
            " Loss:  0.379647433757782\n",
            "Training:  Iteration No:  108 Worker Num:  3 \n",
            " Loss:  0.4202457666397095\n",
            "Training:  Iteration No:  108 Worker Num:  4 \n",
            " Loss:  0.38532164692878723\n",
            "Training:  Iteration No:  108 Worker Num:  5 \n",
            " Loss:  0.39001789689064026\n",
            "Training:  Iteration No:  108 Worker Num:  6 \n",
            " Loss:  0.5018007755279541\n",
            "Training:  Iteration No:  108 Worker Num:  7 \n",
            " Loss:  0.3682221472263336\n",
            "Test:  Iteration No:  108 \n",
            " Loss:  0.3501021609276156\n",
            "Test accuracy:  90.04\n",
            "Training:  Iteration No:  109 Worker Num:  0 \n",
            " Loss:  0.442855566740036\n",
            "Training:  Iteration No:  109 Worker Num:  1 \n",
            " Loss:  0.4082169532775879\n",
            "Training:  Iteration No:  109 Worker Num:  2 \n",
            " Loss:  0.4784786105155945\n",
            "Training:  Iteration No:  109 Worker Num:  3 \n",
            " Loss:  0.3781447112560272\n",
            "Training:  Iteration No:  109 Worker Num:  4 \n",
            " Loss:  0.5116980671882629\n",
            "Training:  Iteration No:  109 Worker Num:  5 \n",
            " Loss:  0.5305866003036499\n",
            "Training:  Iteration No:  109 Worker Num:  6 \n",
            " Loss:  0.464139848947525\n",
            "Training:  Iteration No:  109 Worker Num:  7 \n",
            " Loss:  0.3318377137184143\n",
            "Test:  Iteration No:  109 \n",
            " Loss:  0.3520149812102318\n",
            "Test accuracy:  90.09\n",
            "Training:  Iteration No:  110 Worker Num:  0 \n",
            " Loss:  0.2712196409702301\n",
            "Training:  Iteration No:  110 Worker Num:  1 \n",
            " Loss:  0.41209274530410767\n",
            "Training:  Iteration No:  110 Worker Num:  2 \n",
            " Loss:  0.37341198325157166\n",
            "Training:  Iteration No:  110 Worker Num:  3 \n",
            " Loss:  0.36195963621139526\n",
            "Training:  Iteration No:  110 Worker Num:  4 \n",
            " Loss:  0.4445559084415436\n",
            "Training:  Iteration No:  110 Worker Num:  5 \n",
            " Loss:  0.3393179774284363\n",
            "Training:  Iteration No:  110 Worker Num:  6 \n",
            " Loss:  0.3417602479457855\n",
            "Training:  Iteration No:  110 Worker Num:  7 \n",
            " Loss:  0.38666006922721863\n",
            "Test:  Iteration No:  110 \n",
            " Loss:  0.34767950830768934\n",
            "Test accuracy:  90.2\n",
            "Training:  Iteration No:  111 Worker Num:  0 \n",
            " Loss:  0.5106622576713562\n",
            "Training:  Iteration No:  111 Worker Num:  1 \n",
            " Loss:  0.3474573493003845\n",
            "Training:  Iteration No:  111 Worker Num:  2 \n",
            " Loss:  0.306958943605423\n",
            "Training:  Iteration No:  111 Worker Num:  3 \n",
            " Loss:  0.40568116307258606\n",
            "Training:  Iteration No:  111 Worker Num:  4 \n",
            " Loss:  0.39901235699653625\n",
            "Training:  Iteration No:  111 Worker Num:  5 \n",
            " Loss:  0.4426639974117279\n",
            "Training:  Iteration No:  111 Worker Num:  6 \n",
            " Loss:  0.42664045095443726\n",
            "Training:  Iteration No:  111 Worker Num:  7 \n",
            " Loss:  0.41999438405036926\n",
            "Test:  Iteration No:  111 \n",
            " Loss:  0.35122898725580565\n",
            "Test accuracy:  90.3\n",
            "Training:  Iteration No:  112 Worker Num:  0 \n",
            " Loss:  0.3745185434818268\n",
            "Training:  Iteration No:  112 Worker Num:  1 \n",
            " Loss:  0.33209937810897827\n",
            "Training:  Iteration No:  112 Worker Num:  2 \n",
            " Loss:  0.4377695918083191\n",
            "Training:  Iteration No:  112 Worker Num:  3 \n",
            " Loss:  0.25463271141052246\n",
            "Training:  Iteration No:  112 Worker Num:  4 \n",
            " Loss:  0.424448698759079\n",
            "Training:  Iteration No:  112 Worker Num:  5 \n",
            " Loss:  0.4094657003879547\n",
            "Training:  Iteration No:  112 Worker Num:  6 \n",
            " Loss:  0.4471820592880249\n",
            "Training:  Iteration No:  112 Worker Num:  7 \n",
            " Loss:  0.32150641083717346\n",
            "Test:  Iteration No:  112 \n",
            " Loss:  0.3434446935978117\n",
            "Test accuracy:  89.84\n",
            "Training:  Iteration No:  113 Worker Num:  0 \n",
            " Loss:  0.3679109513759613\n",
            "Training:  Iteration No:  113 Worker Num:  1 \n",
            " Loss:  0.2934695780277252\n",
            "Training:  Iteration No:  113 Worker Num:  2 \n",
            " Loss:  0.4277614951133728\n",
            "Training:  Iteration No:  113 Worker Num:  3 \n",
            " Loss:  0.3171003758907318\n",
            "Training:  Iteration No:  113 Worker Num:  4 \n",
            " Loss:  0.37787291407585144\n",
            "Training:  Iteration No:  113 Worker Num:  5 \n",
            " Loss:  0.3537370562553406\n",
            "Training:  Iteration No:  113 Worker Num:  6 \n",
            " Loss:  0.45985549688339233\n",
            "Training:  Iteration No:  113 Worker Num:  7 \n",
            " Loss:  0.4464279115200043\n",
            "Test:  Iteration No:  113 \n",
            " Loss:  0.34280631120634986\n",
            "Test accuracy:  90.41\n",
            "Training:  Iteration No:  114 Worker Num:  0 \n",
            " Loss:  0.3621397316455841\n",
            "Training:  Iteration No:  114 Worker Num:  1 \n",
            " Loss:  0.4076607823371887\n",
            "Training:  Iteration No:  114 Worker Num:  2 \n",
            " Loss:  0.37258830666542053\n",
            "Training:  Iteration No:  114 Worker Num:  3 \n",
            " Loss:  0.44523105025291443\n",
            "Training:  Iteration No:  114 Worker Num:  4 \n",
            " Loss:  0.4977201819419861\n",
            "Training:  Iteration No:  114 Worker Num:  5 \n",
            " Loss:  0.4593888521194458\n",
            "Training:  Iteration No:  114 Worker Num:  6 \n",
            " Loss:  0.5090685486793518\n",
            "Training:  Iteration No:  114 Worker Num:  7 \n",
            " Loss:  0.32869309186935425\n",
            "Test:  Iteration No:  114 \n",
            " Loss:  0.34107063427756107\n",
            "Test accuracy:  90.16\n",
            "Training:  Iteration No:  115 Worker Num:  0 \n",
            " Loss:  0.35441386699676514\n",
            "Training:  Iteration No:  115 Worker Num:  1 \n",
            " Loss:  0.5179309844970703\n",
            "Training:  Iteration No:  115 Worker Num:  2 \n",
            " Loss:  0.4639092683792114\n",
            "Training:  Iteration No:  115 Worker Num:  3 \n",
            " Loss:  0.36980709433555603\n",
            "Training:  Iteration No:  115 Worker Num:  4 \n",
            " Loss:  0.30926865339279175\n",
            "Training:  Iteration No:  115 Worker Num:  5 \n",
            " Loss:  0.403253436088562\n",
            "Training:  Iteration No:  115 Worker Num:  6 \n",
            " Loss:  0.27408871054649353\n",
            "Training:  Iteration No:  115 Worker Num:  7 \n",
            " Loss:  0.4787139296531677\n",
            "Test:  Iteration No:  115 \n",
            " Loss:  0.34452090553845033\n",
            "Test accuracy:  90.34\n",
            "Training:  Iteration No:  116 Worker Num:  0 \n",
            " Loss:  0.444302499294281\n",
            "Training:  Iteration No:  116 Worker Num:  1 \n",
            " Loss:  0.31616324186325073\n",
            "Training:  Iteration No:  116 Worker Num:  2 \n",
            " Loss:  0.45084354281425476\n",
            "Training:  Iteration No:  116 Worker Num:  3 \n",
            " Loss:  0.2648765742778778\n",
            "Training:  Iteration No:  116 Worker Num:  4 \n",
            " Loss:  0.37071195244789124\n",
            "Training:  Iteration No:  116 Worker Num:  5 \n",
            " Loss:  0.5740295648574829\n",
            "Training:  Iteration No:  116 Worker Num:  6 \n",
            " Loss:  0.4798249304294586\n",
            "Training:  Iteration No:  116 Worker Num:  7 \n",
            " Loss:  0.4354912042617798\n",
            "Test:  Iteration No:  116 \n",
            " Loss:  0.3424919962505751\n",
            "Test accuracy:  90.16\n",
            "Training:  Iteration No:  117 Worker Num:  0 \n",
            " Loss:  0.37864646315574646\n",
            "Training:  Iteration No:  117 Worker Num:  1 \n",
            " Loss:  0.3787044584751129\n",
            "Training:  Iteration No:  117 Worker Num:  2 \n",
            " Loss:  0.2919444143772125\n",
            "Training:  Iteration No:  117 Worker Num:  3 \n",
            " Loss:  0.4329075217247009\n",
            "Training:  Iteration No:  117 Worker Num:  4 \n",
            " Loss:  0.5693720579147339\n",
            "Training:  Iteration No:  117 Worker Num:  5 \n",
            " Loss:  0.47925713658332825\n",
            "Training:  Iteration No:  117 Worker Num:  6 \n",
            " Loss:  0.3021409511566162\n",
            "Training:  Iteration No:  117 Worker Num:  7 \n",
            " Loss:  0.2984033524990082\n",
            "Test:  Iteration No:  117 \n",
            " Loss:  0.3389991363864156\n",
            "Test accuracy:  90.43\n",
            "Training:  Iteration No:  118 Worker Num:  0 \n",
            " Loss:  0.4711054563522339\n",
            "Training:  Iteration No:  118 Worker Num:  1 \n",
            " Loss:  0.23417554795742035\n",
            "Training:  Iteration No:  118 Worker Num:  2 \n",
            " Loss:  0.2632552683353424\n",
            "Training:  Iteration No:  118 Worker Num:  3 \n",
            " Loss:  0.4862709045410156\n",
            "Training:  Iteration No:  118 Worker Num:  4 \n",
            " Loss:  0.3358936011791229\n",
            "Training:  Iteration No:  118 Worker Num:  5 \n",
            " Loss:  0.4976353645324707\n",
            "Training:  Iteration No:  118 Worker Num:  6 \n",
            " Loss:  0.5188112258911133\n",
            "Training:  Iteration No:  118 Worker Num:  7 \n",
            " Loss:  0.33774101734161377\n",
            "Test:  Iteration No:  118 \n",
            " Loss:  0.3387944182451767\n",
            "Test accuracy:  89.96\n",
            "Training:  Iteration No:  119 Worker Num:  0 \n",
            " Loss:  0.5390647053718567\n",
            "Training:  Iteration No:  119 Worker Num:  1 \n",
            " Loss:  0.38505980372428894\n",
            "Training:  Iteration No:  119 Worker Num:  2 \n",
            " Loss:  0.33037135004997253\n",
            "Training:  Iteration No:  119 Worker Num:  3 \n",
            " Loss:  0.3134946823120117\n",
            "Training:  Iteration No:  119 Worker Num:  4 \n",
            " Loss:  0.4444350302219391\n",
            "Training:  Iteration No:  119 Worker Num:  5 \n",
            " Loss:  0.2825969457626343\n",
            "Training:  Iteration No:  119 Worker Num:  6 \n",
            " Loss:  0.45944660902023315\n",
            "Training:  Iteration No:  119 Worker Num:  7 \n",
            " Loss:  0.5020760893821716\n",
            "Test:  Iteration No:  119 \n",
            " Loss:  0.34380868652576135\n",
            "Test accuracy:  90.21\n",
            "Training:  Iteration No:  120 Worker Num:  0 \n",
            " Loss:  0.39755335450172424\n",
            "Training:  Iteration No:  120 Worker Num:  1 \n",
            " Loss:  0.3990360200405121\n",
            "Training:  Iteration No:  120 Worker Num:  2 \n",
            " Loss:  0.3256252408027649\n",
            "Training:  Iteration No:  120 Worker Num:  3 \n",
            " Loss:  0.5013106465339661\n",
            "Training:  Iteration No:  120 Worker Num:  4 \n",
            " Loss:  0.3135845959186554\n",
            "Training:  Iteration No:  120 Worker Num:  5 \n",
            " Loss:  0.35970523953437805\n",
            "Training:  Iteration No:  120 Worker Num:  6 \n",
            " Loss:  0.4362524151802063\n",
            "Training:  Iteration No:  120 Worker Num:  7 \n",
            " Loss:  0.4508577883243561\n",
            "Test:  Iteration No:  120 \n",
            " Loss:  0.33887009681025637\n",
            "Test accuracy:  90.09\n",
            "Training:  Iteration No:  121 Worker Num:  0 \n",
            " Loss:  0.3093695640563965\n",
            "Training:  Iteration No:  121 Worker Num:  1 \n",
            " Loss:  0.411236435174942\n",
            "Training:  Iteration No:  121 Worker Num:  2 \n",
            " Loss:  0.36919665336608887\n",
            "Training:  Iteration No:  121 Worker Num:  3 \n",
            " Loss:  0.36641767621040344\n",
            "Training:  Iteration No:  121 Worker Num:  4 \n",
            " Loss:  0.29848870635032654\n",
            "Training:  Iteration No:  121 Worker Num:  5 \n",
            " Loss:  0.35457828640937805\n",
            "Training:  Iteration No:  121 Worker Num:  6 \n",
            " Loss:  0.3804984986782074\n",
            "Training:  Iteration No:  121 Worker Num:  7 \n",
            " Loss:  0.3583759069442749\n",
            "Test:  Iteration No:  121 \n",
            " Loss:  0.3436842460326756\n",
            "Test accuracy:  89.88\n",
            "Training:  Iteration No:  122 Worker Num:  0 \n",
            " Loss:  0.35785385966300964\n",
            "Training:  Iteration No:  122 Worker Num:  1 \n",
            " Loss:  0.47321072220802307\n",
            "Training:  Iteration No:  122 Worker Num:  2 \n",
            " Loss:  0.4246692955493927\n",
            "Training:  Iteration No:  122 Worker Num:  3 \n",
            " Loss:  0.29952272772789\n",
            "Training:  Iteration No:  122 Worker Num:  4 \n",
            " Loss:  0.44574421644210815\n",
            "Training:  Iteration No:  122 Worker Num:  5 \n",
            " Loss:  0.5056824088096619\n",
            "Training:  Iteration No:  122 Worker Num:  6 \n",
            " Loss:  0.38795745372772217\n",
            "Training:  Iteration No:  122 Worker Num:  7 \n",
            " Loss:  0.3643247187137604\n",
            "Test:  Iteration No:  122 \n",
            " Loss:  0.338281005668112\n",
            "Test accuracy:  89.92\n",
            "Training:  Iteration No:  123 Worker Num:  0 \n",
            " Loss:  0.36616331338882446\n",
            "Training:  Iteration No:  123 Worker Num:  1 \n",
            " Loss:  0.6914490461349487\n",
            "Training:  Iteration No:  123 Worker Num:  2 \n",
            " Loss:  0.4054000675678253\n",
            "Training:  Iteration No:  123 Worker Num:  3 \n",
            " Loss:  0.36834630370140076\n",
            "Training:  Iteration No:  123 Worker Num:  4 \n",
            " Loss:  0.4669911563396454\n",
            "Training:  Iteration No:  123 Worker Num:  5 \n",
            " Loss:  0.34588152170181274\n",
            "Training:  Iteration No:  123 Worker Num:  6 \n",
            " Loss:  0.29870346188545227\n",
            "Training:  Iteration No:  123 Worker Num:  7 \n",
            " Loss:  0.5016621351242065\n",
            "Test:  Iteration No:  123 \n",
            " Loss:  0.35305727593883685\n",
            "Test accuracy:  89.39\n",
            "Training:  Iteration No:  124 Worker Num:  0 \n",
            " Loss:  0.28462687134742737\n",
            "Training:  Iteration No:  124 Worker Num:  1 \n",
            " Loss:  0.3496757745742798\n",
            "Training:  Iteration No:  124 Worker Num:  2 \n",
            " Loss:  0.4295910596847534\n",
            "Training:  Iteration No:  124 Worker Num:  3 \n",
            " Loss:  0.4637371301651001\n",
            "Training:  Iteration No:  124 Worker Num:  4 \n",
            " Loss:  0.3759089410305023\n",
            "Training:  Iteration No:  124 Worker Num:  5 \n",
            " Loss:  0.305648535490036\n",
            "Training:  Iteration No:  124 Worker Num:  6 \n",
            " Loss:  0.3670748472213745\n",
            "Training:  Iteration No:  124 Worker Num:  7 \n",
            " Loss:  0.5600214600563049\n",
            "Test:  Iteration No:  124 \n",
            " Loss:  0.3225631895416145\n",
            "Test accuracy:  90.7\n",
            "Training:  Iteration No:  125 Worker Num:  0 \n",
            " Loss:  0.41246476769447327\n",
            "Training:  Iteration No:  125 Worker Num:  1 \n",
            " Loss:  0.3902485966682434\n",
            "Training:  Iteration No:  125 Worker Num:  2 \n",
            " Loss:  0.3241029381752014\n",
            "Training:  Iteration No:  125 Worker Num:  3 \n",
            " Loss:  0.466092586517334\n",
            "Training:  Iteration No:  125 Worker Num:  4 \n",
            " Loss:  0.3870496451854706\n",
            "Training:  Iteration No:  125 Worker Num:  5 \n",
            " Loss:  0.5526129603385925\n",
            "Training:  Iteration No:  125 Worker Num:  6 \n",
            " Loss:  0.25472569465637207\n",
            "Training:  Iteration No:  125 Worker Num:  7 \n",
            " Loss:  0.36102816462516785\n",
            "Test:  Iteration No:  125 \n",
            " Loss:  0.3264101784251913\n",
            "Test accuracy:  90.82\n",
            "Training:  Iteration No:  126 Worker Num:  0 \n",
            " Loss:  0.2923092246055603\n",
            "Training:  Iteration No:  126 Worker Num:  1 \n",
            " Loss:  0.394233375787735\n",
            "Training:  Iteration No:  126 Worker Num:  2 \n",
            " Loss:  0.28234946727752686\n",
            "Training:  Iteration No:  126 Worker Num:  3 \n",
            " Loss:  0.3398326635360718\n",
            "Training:  Iteration No:  126 Worker Num:  4 \n",
            " Loss:  0.4198143482208252\n",
            "Training:  Iteration No:  126 Worker Num:  5 \n",
            " Loss:  0.3479524850845337\n",
            "Training:  Iteration No:  126 Worker Num:  6 \n",
            " Loss:  0.3740861713886261\n",
            "Training:  Iteration No:  126 Worker Num:  7 \n",
            " Loss:  0.34527379274368286\n",
            "Test:  Iteration No:  126 \n",
            " Loss:  0.3201115916518471\n",
            "Test accuracy:  90.64\n",
            "Training:  Iteration No:  127 Worker Num:  0 \n",
            " Loss:  0.5869937539100647\n",
            "Training:  Iteration No:  127 Worker Num:  1 \n",
            " Loss:  0.3191756010055542\n",
            "Training:  Iteration No:  127 Worker Num:  2 \n",
            " Loss:  0.3674982190132141\n",
            "Training:  Iteration No:  127 Worker Num:  3 \n",
            " Loss:  0.32352787256240845\n",
            "Training:  Iteration No:  127 Worker Num:  4 \n",
            " Loss:  0.4901719093322754\n",
            "Training:  Iteration No:  127 Worker Num:  5 \n",
            " Loss:  0.32922884821891785\n",
            "Training:  Iteration No:  127 Worker Num:  6 \n",
            " Loss:  0.5502864122390747\n",
            "Training:  Iteration No:  127 Worker Num:  7 \n",
            " Loss:  0.463238924741745\n",
            "Test:  Iteration No:  127 \n",
            " Loss:  0.3200841001411782\n",
            "Test accuracy:  90.72\n",
            "Training:  Iteration No:  128 Worker Num:  0 \n",
            " Loss:  0.4239436388015747\n",
            "Training:  Iteration No:  128 Worker Num:  1 \n",
            " Loss:  0.3424589931964874\n",
            "Training:  Iteration No:  128 Worker Num:  2 \n",
            " Loss:  0.29355430603027344\n",
            "Training:  Iteration No:  128 Worker Num:  3 \n",
            " Loss:  0.34337079524993896\n",
            "Training:  Iteration No:  128 Worker Num:  4 \n",
            " Loss:  0.462710976600647\n",
            "Training:  Iteration No:  128 Worker Num:  5 \n",
            " Loss:  0.422789603471756\n",
            "Training:  Iteration No:  128 Worker Num:  6 \n",
            " Loss:  0.44613635540008545\n",
            "Training:  Iteration No:  128 Worker Num:  7 \n",
            " Loss:  0.4131808876991272\n",
            "Test:  Iteration No:  128 \n",
            " Loss:  0.3193963997631888\n",
            "Test accuracy:  90.58\n",
            "Training:  Iteration No:  129 Worker Num:  0 \n",
            " Loss:  0.29951101541519165\n",
            "Training:  Iteration No:  129 Worker Num:  1 \n",
            " Loss:  0.3977513909339905\n",
            "Training:  Iteration No:  129 Worker Num:  2 \n",
            " Loss:  0.35163795948028564\n",
            "Training:  Iteration No:  129 Worker Num:  3 \n",
            " Loss:  0.49431711435317993\n",
            "Training:  Iteration No:  129 Worker Num:  4 \n",
            " Loss:  0.40377774834632874\n",
            "Training:  Iteration No:  129 Worker Num:  5 \n",
            " Loss:  0.34562644362449646\n",
            "Training:  Iteration No:  129 Worker Num:  6 \n",
            " Loss:  0.28758829832077026\n",
            "Training:  Iteration No:  129 Worker Num:  7 \n",
            " Loss:  0.2775108814239502\n",
            "Test:  Iteration No:  129 \n",
            " Loss:  0.3220489584381067\n",
            "Test accuracy:  90.67\n",
            "Training:  Iteration No:  130 Worker Num:  0 \n",
            " Loss:  0.5947839617729187\n",
            "Training:  Iteration No:  130 Worker Num:  1 \n",
            " Loss:  0.38281798362731934\n",
            "Training:  Iteration No:  130 Worker Num:  2 \n",
            " Loss:  0.521144449710846\n",
            "Training:  Iteration No:  130 Worker Num:  3 \n",
            " Loss:  0.41334375739097595\n",
            "Training:  Iteration No:  130 Worker Num:  4 \n",
            " Loss:  0.32344287633895874\n",
            "Training:  Iteration No:  130 Worker Num:  5 \n",
            " Loss:  0.4071573317050934\n",
            "Training:  Iteration No:  130 Worker Num:  6 \n",
            " Loss:  0.3516501784324646\n",
            "Training:  Iteration No:  130 Worker Num:  7 \n",
            " Loss:  0.2994652986526489\n",
            "Test:  Iteration No:  130 \n",
            " Loss:  0.3137560747563839\n",
            "Test accuracy:  91.0\n",
            "Training:  Iteration No:  131 Worker Num:  0 \n",
            " Loss:  0.3097894787788391\n",
            "Training:  Iteration No:  131 Worker Num:  1 \n",
            " Loss:  0.4452221691608429\n",
            "Training:  Iteration No:  131 Worker Num:  2 \n",
            " Loss:  0.28361374139785767\n",
            "Training:  Iteration No:  131 Worker Num:  3 \n",
            " Loss:  0.2906724512577057\n",
            "Training:  Iteration No:  131 Worker Num:  4 \n",
            " Loss:  0.34658920764923096\n",
            "Training:  Iteration No:  131 Worker Num:  5 \n",
            " Loss:  0.4729834198951721\n",
            "Training:  Iteration No:  131 Worker Num:  6 \n",
            " Loss:  0.3895752727985382\n",
            "Training:  Iteration No:  131 Worker Num:  7 \n",
            " Loss:  0.42956310510635376\n",
            "Test:  Iteration No:  131 \n",
            " Loss:  0.31027541616106336\n",
            "Test accuracy:  91.02\n",
            "Training:  Iteration No:  132 Worker Num:  0 \n",
            " Loss:  0.4666624367237091\n",
            "Training:  Iteration No:  132 Worker Num:  1 \n",
            " Loss:  0.34959277510643005\n",
            "Training:  Iteration No:  132 Worker Num:  2 \n",
            " Loss:  0.38864681124687195\n",
            "Training:  Iteration No:  132 Worker Num:  3 \n",
            " Loss:  0.38504576683044434\n",
            "Training:  Iteration No:  132 Worker Num:  4 \n",
            " Loss:  0.4909754693508148\n",
            "Training:  Iteration No:  132 Worker Num:  5 \n",
            " Loss:  0.36651065945625305\n",
            "Training:  Iteration No:  132 Worker Num:  6 \n",
            " Loss:  0.3999040424823761\n",
            "Training:  Iteration No:  132 Worker Num:  7 \n",
            " Loss:  0.4822901487350464\n",
            "Test:  Iteration No:  132 \n",
            " Loss:  0.3127333030010326\n",
            "Test accuracy:  91.22\n",
            "Training:  Iteration No:  133 Worker Num:  0 \n",
            " Loss:  0.4729277789592743\n",
            "Training:  Iteration No:  133 Worker Num:  1 \n",
            " Loss:  0.4874350130558014\n",
            "Training:  Iteration No:  133 Worker Num:  2 \n",
            " Loss:  0.3521670997142792\n",
            "Training:  Iteration No:  133 Worker Num:  3 \n",
            " Loss:  0.4011484980583191\n",
            "Training:  Iteration No:  133 Worker Num:  4 \n",
            " Loss:  0.3508761525154114\n",
            "Training:  Iteration No:  133 Worker Num:  5 \n",
            " Loss:  0.3515666425228119\n",
            "Training:  Iteration No:  133 Worker Num:  6 \n",
            " Loss:  0.2847769856452942\n",
            "Training:  Iteration No:  133 Worker Num:  7 \n",
            " Loss:  0.3789219260215759\n",
            "Test:  Iteration No:  133 \n",
            " Loss:  0.3150553008706509\n",
            "Test accuracy:  90.63\n",
            "Training:  Iteration No:  134 Worker Num:  0 \n",
            " Loss:  0.4299931228160858\n",
            "Training:  Iteration No:  134 Worker Num:  1 \n",
            " Loss:  0.2973189651966095\n",
            "Training:  Iteration No:  134 Worker Num:  2 \n",
            " Loss:  0.410453736782074\n",
            "Training:  Iteration No:  134 Worker Num:  3 \n",
            " Loss:  0.481808602809906\n",
            "Training:  Iteration No:  134 Worker Num:  4 \n",
            " Loss:  0.40987059473991394\n",
            "Training:  Iteration No:  134 Worker Num:  5 \n",
            " Loss:  0.30181288719177246\n",
            "Training:  Iteration No:  134 Worker Num:  6 \n",
            " Loss:  0.4516926407814026\n",
            "Training:  Iteration No:  134 Worker Num:  7 \n",
            " Loss:  0.463858425617218\n",
            "Test:  Iteration No:  134 \n",
            " Loss:  0.3063529985897903\n",
            "Test accuracy:  91.0\n",
            "Training:  Iteration No:  135 Worker Num:  0 \n",
            " Loss:  0.4192182719707489\n",
            "Training:  Iteration No:  135 Worker Num:  1 \n",
            " Loss:  0.41122210025787354\n",
            "Training:  Iteration No:  135 Worker Num:  2 \n",
            " Loss:  0.40864771604537964\n",
            "Training:  Iteration No:  135 Worker Num:  3 \n",
            " Loss:  0.2407168596982956\n",
            "Training:  Iteration No:  135 Worker Num:  4 \n",
            " Loss:  0.3354234993457794\n",
            "Training:  Iteration No:  135 Worker Num:  5 \n",
            " Loss:  0.4391755759716034\n",
            "Training:  Iteration No:  135 Worker Num:  6 \n",
            " Loss:  0.32516220211982727\n",
            "Training:  Iteration No:  135 Worker Num:  7 \n",
            " Loss:  0.4208275377750397\n",
            "Test:  Iteration No:  135 \n",
            " Loss:  0.305009909492882\n",
            "Test accuracy:  91.12\n",
            "Training:  Iteration No:  136 Worker Num:  0 \n",
            " Loss:  0.32806396484375\n",
            "Training:  Iteration No:  136 Worker Num:  1 \n",
            " Loss:  0.2646314799785614\n",
            "Training:  Iteration No:  136 Worker Num:  2 \n",
            " Loss:  0.3894672095775604\n",
            "Training:  Iteration No:  136 Worker Num:  3 \n",
            " Loss:  0.3043747544288635\n",
            "Training:  Iteration No:  136 Worker Num:  4 \n",
            " Loss:  0.4111420214176178\n",
            "Training:  Iteration No:  136 Worker Num:  5 \n",
            " Loss:  0.22690480947494507\n",
            "Training:  Iteration No:  136 Worker Num:  6 \n",
            " Loss:  0.3887091875076294\n",
            "Training:  Iteration No:  136 Worker Num:  7 \n",
            " Loss:  0.44099777936935425\n",
            "Test:  Iteration No:  136 \n",
            " Loss:  0.30897861841735963\n",
            "Test accuracy:  91.08\n",
            "Training:  Iteration No:  137 Worker Num:  0 \n",
            " Loss:  0.46678730845451355\n",
            "Training:  Iteration No:  137 Worker Num:  1 \n",
            " Loss:  0.4975300431251526\n",
            "Training:  Iteration No:  137 Worker Num:  2 \n",
            " Loss:  0.4716454744338989\n",
            "Training:  Iteration No:  137 Worker Num:  3 \n",
            " Loss:  0.5417287945747375\n",
            "Training:  Iteration No:  137 Worker Num:  4 \n",
            " Loss:  0.589283287525177\n",
            "Training:  Iteration No:  137 Worker Num:  5 \n",
            " Loss:  0.45323723554611206\n",
            "Training:  Iteration No:  137 Worker Num:  6 \n",
            " Loss:  0.3058878183364868\n",
            "Training:  Iteration No:  137 Worker Num:  7 \n",
            " Loss:  0.3080673813819885\n",
            "Test:  Iteration No:  137 \n",
            " Loss:  0.3039085881053647\n",
            "Test accuracy:  91.06\n",
            "Training:  Iteration No:  138 Worker Num:  0 \n",
            " Loss:  0.3811798691749573\n",
            "Training:  Iteration No:  138 Worker Num:  1 \n",
            " Loss:  0.3754732310771942\n",
            "Training:  Iteration No:  138 Worker Num:  2 \n",
            " Loss:  0.4290424585342407\n",
            "Training:  Iteration No:  138 Worker Num:  3 \n",
            " Loss:  0.3894128203392029\n",
            "Training:  Iteration No:  138 Worker Num:  4 \n",
            " Loss:  0.30750977993011475\n",
            "Training:  Iteration No:  138 Worker Num:  5 \n",
            " Loss:  0.3917466998100281\n",
            "Training:  Iteration No:  138 Worker Num:  6 \n",
            " Loss:  0.27098968625068665\n",
            "Training:  Iteration No:  138 Worker Num:  7 \n",
            " Loss:  0.3586181402206421\n",
            "Test:  Iteration No:  138 \n",
            " Loss:  0.3104372767236414\n",
            "Test accuracy:  90.97\n",
            "Training:  Iteration No:  139 Worker Num:  0 \n",
            " Loss:  0.31808769702911377\n",
            "Training:  Iteration No:  139 Worker Num:  1 \n",
            " Loss:  0.36544039845466614\n",
            "Training:  Iteration No:  139 Worker Num:  2 \n",
            " Loss:  0.31641173362731934\n",
            "Training:  Iteration No:  139 Worker Num:  3 \n",
            " Loss:  0.346540629863739\n",
            "Training:  Iteration No:  139 Worker Num:  4 \n",
            " Loss:  0.3666039705276489\n",
            "Training:  Iteration No:  139 Worker Num:  5 \n",
            " Loss:  0.3911983370780945\n",
            "Training:  Iteration No:  139 Worker Num:  6 \n",
            " Loss:  0.22671960294246674\n",
            "Training:  Iteration No:  139 Worker Num:  7 \n",
            " Loss:  0.5762977004051208\n",
            "Test:  Iteration No:  139 \n",
            " Loss:  0.30676552610872665\n",
            "Test accuracy:  90.74\n",
            "Training:  Iteration No:  140 Worker Num:  0 \n",
            " Loss:  0.31923070549964905\n",
            "Training:  Iteration No:  140 Worker Num:  1 \n",
            " Loss:  0.3886908292770386\n",
            "Training:  Iteration No:  140 Worker Num:  2 \n",
            " Loss:  0.5251714587211609\n",
            "Training:  Iteration No:  140 Worker Num:  3 \n",
            " Loss:  0.313533753156662\n",
            "Training:  Iteration No:  140 Worker Num:  4 \n",
            " Loss:  0.44776761531829834\n",
            "Training:  Iteration No:  140 Worker Num:  5 \n",
            " Loss:  0.322231650352478\n",
            "Training:  Iteration No:  140 Worker Num:  6 \n",
            " Loss:  0.41669583320617676\n",
            "Training:  Iteration No:  140 Worker Num:  7 \n",
            " Loss:  0.31032246351242065\n",
            "Test:  Iteration No:  140 \n",
            " Loss:  0.2974044393417956\n",
            "Test accuracy:  91.4\n",
            "Training:  Iteration No:  141 Worker Num:  0 \n",
            " Loss:  0.5330374836921692\n",
            "Training:  Iteration No:  141 Worker Num:  1 \n",
            " Loss:  0.32529693841934204\n",
            "Training:  Iteration No:  141 Worker Num:  2 \n",
            " Loss:  0.29245758056640625\n",
            "Training:  Iteration No:  141 Worker Num:  3 \n",
            " Loss:  0.40182551741600037\n",
            "Training:  Iteration No:  141 Worker Num:  4 \n",
            " Loss:  0.34995007514953613\n",
            "Training:  Iteration No:  141 Worker Num:  5 \n",
            " Loss:  0.28791260719299316\n",
            "Training:  Iteration No:  141 Worker Num:  6 \n",
            " Loss:  0.27587392926216125\n",
            "Training:  Iteration No:  141 Worker Num:  7 \n",
            " Loss:  0.3063777983188629\n",
            "Test:  Iteration No:  141 \n",
            " Loss:  0.2974016013307662\n",
            "Test accuracy:  91.26\n",
            "Training:  Iteration No:  142 Worker Num:  0 \n",
            " Loss:  0.2830839157104492\n",
            "Training:  Iteration No:  142 Worker Num:  1 \n",
            " Loss:  0.26808252930641174\n",
            "Training:  Iteration No:  142 Worker Num:  2 \n",
            " Loss:  0.393329918384552\n",
            "Training:  Iteration No:  142 Worker Num:  3 \n",
            " Loss:  0.37961748242378235\n",
            "Training:  Iteration No:  142 Worker Num:  4 \n",
            " Loss:  0.35059303045272827\n",
            "Training:  Iteration No:  142 Worker Num:  5 \n",
            " Loss:  0.3650890290737152\n",
            "Training:  Iteration No:  142 Worker Num:  6 \n",
            " Loss:  0.3114047348499298\n",
            "Training:  Iteration No:  142 Worker Num:  7 \n",
            " Loss:  0.3402731716632843\n",
            "Test:  Iteration No:  142 \n",
            " Loss:  0.2979732612737372\n",
            "Test accuracy:  91.27\n",
            "Training:  Iteration No:  143 Worker Num:  0 \n",
            " Loss:  0.41108614206314087\n",
            "Training:  Iteration No:  143 Worker Num:  1 \n",
            " Loss:  0.29835447669029236\n",
            "Training:  Iteration No:  143 Worker Num:  2 \n",
            " Loss:  0.36771103739738464\n",
            "Training:  Iteration No:  143 Worker Num:  3 \n",
            " Loss:  0.38700711727142334\n",
            "Training:  Iteration No:  143 Worker Num:  4 \n",
            " Loss:  0.408847451210022\n",
            "Training:  Iteration No:  143 Worker Num:  5 \n",
            " Loss:  0.2366882711648941\n",
            "Training:  Iteration No:  143 Worker Num:  6 \n",
            " Loss:  0.33456289768218994\n",
            "Training:  Iteration No:  143 Worker Num:  7 \n",
            " Loss:  0.3390631675720215\n",
            "Test:  Iteration No:  143 \n",
            " Loss:  0.30185765516060065\n",
            "Test accuracy:  91.27\n",
            "Training:  Iteration No:  144 Worker Num:  0 \n",
            " Loss:  0.42698511481285095\n",
            "Training:  Iteration No:  144 Worker Num:  1 \n",
            " Loss:  0.3124573826789856\n",
            "Training:  Iteration No:  144 Worker Num:  2 \n",
            " Loss:  0.38209185004234314\n",
            "Training:  Iteration No:  144 Worker Num:  3 \n",
            " Loss:  0.32749855518341064\n",
            "Training:  Iteration No:  144 Worker Num:  4 \n",
            " Loss:  0.3738501965999603\n",
            "Training:  Iteration No:  144 Worker Num:  5 \n",
            " Loss:  0.4620801508426666\n",
            "Training:  Iteration No:  144 Worker Num:  6 \n",
            " Loss:  0.331682413816452\n",
            "Training:  Iteration No:  144 Worker Num:  7 \n",
            " Loss:  0.500619649887085\n",
            "Test:  Iteration No:  144 \n",
            " Loss:  0.30034898061163817\n",
            "Test accuracy:  91.3\n",
            "Training:  Iteration No:  145 Worker Num:  0 \n",
            " Loss:  0.4101637005805969\n",
            "Training:  Iteration No:  145 Worker Num:  1 \n",
            " Loss:  0.41266000270843506\n",
            "Training:  Iteration No:  145 Worker Num:  2 \n",
            " Loss:  0.3277749717235565\n",
            "Training:  Iteration No:  145 Worker Num:  3 \n",
            " Loss:  0.4754507541656494\n",
            "Training:  Iteration No:  145 Worker Num:  4 \n",
            " Loss:  0.35828205943107605\n",
            "Training:  Iteration No:  145 Worker Num:  5 \n",
            " Loss:  0.36719322204589844\n",
            "Training:  Iteration No:  145 Worker Num:  6 \n",
            " Loss:  0.4623218774795532\n",
            "Training:  Iteration No:  145 Worker Num:  7 \n",
            " Loss:  0.29246246814727783\n",
            "Test:  Iteration No:  145 \n",
            " Loss:  0.2940155310391248\n",
            "Test accuracy:  91.51\n",
            "Training:  Iteration No:  146 Worker Num:  0 \n",
            " Loss:  0.40636196732521057\n",
            "Training:  Iteration No:  146 Worker Num:  1 \n",
            " Loss:  0.2840966284275055\n",
            "Training:  Iteration No:  146 Worker Num:  2 \n",
            " Loss:  0.4363900125026703\n",
            "Training:  Iteration No:  146 Worker Num:  3 \n",
            " Loss:  0.34345313906669617\n",
            "Training:  Iteration No:  146 Worker Num:  4 \n",
            " Loss:  0.25262007117271423\n",
            "Training:  Iteration No:  146 Worker Num:  5 \n",
            " Loss:  0.3367091715335846\n",
            "Training:  Iteration No:  146 Worker Num:  6 \n",
            " Loss:  0.32213303446769714\n",
            "Training:  Iteration No:  146 Worker Num:  7 \n",
            " Loss:  0.4490506947040558\n",
            "Test:  Iteration No:  146 \n",
            " Loss:  0.29351798598241957\n",
            "Test accuracy:  91.3\n",
            "Training:  Iteration No:  147 Worker Num:  0 \n",
            " Loss:  0.38579708337783813\n",
            "Training:  Iteration No:  147 Worker Num:  1 \n",
            " Loss:  0.39494675397872925\n",
            "Training:  Iteration No:  147 Worker Num:  2 \n",
            " Loss:  0.43472370505332947\n",
            "Training:  Iteration No:  147 Worker Num:  3 \n",
            " Loss:  0.2927341163158417\n",
            "Training:  Iteration No:  147 Worker Num:  4 \n",
            " Loss:  0.45547419786453247\n",
            "Training:  Iteration No:  147 Worker Num:  5 \n",
            " Loss:  0.3613568842411041\n",
            "Training:  Iteration No:  147 Worker Num:  6 \n",
            " Loss:  0.3651261329650879\n",
            "Training:  Iteration No:  147 Worker Num:  7 \n",
            " Loss:  0.22722041606903076\n",
            "Test:  Iteration No:  147 \n",
            " Loss:  0.296440343903024\n",
            "Test accuracy:  91.5\n",
            "Training:  Iteration No:  148 Worker Num:  0 \n",
            " Loss:  0.3446590304374695\n",
            "Training:  Iteration No:  148 Worker Num:  1 \n",
            " Loss:  0.265337198972702\n",
            "Training:  Iteration No:  148 Worker Num:  2 \n",
            " Loss:  0.1823900192975998\n",
            "Training:  Iteration No:  148 Worker Num:  3 \n",
            " Loss:  0.480879008769989\n",
            "Training:  Iteration No:  148 Worker Num:  4 \n",
            " Loss:  0.2999386489391327\n",
            "Training:  Iteration No:  148 Worker Num:  5 \n",
            " Loss:  0.3265167474746704\n",
            "Training:  Iteration No:  148 Worker Num:  6 \n",
            " Loss:  0.33010968565940857\n",
            "Training:  Iteration No:  148 Worker Num:  7 \n",
            " Loss:  0.27254098653793335\n",
            "Test:  Iteration No:  148 \n",
            " Loss:  0.29015924413747424\n",
            "Test accuracy:  91.41\n",
            "Training:  Iteration No:  149 Worker Num:  0 \n",
            " Loss:  0.32054460048675537\n",
            "Training:  Iteration No:  149 Worker Num:  1 \n",
            " Loss:  0.3603873550891876\n",
            "Training:  Iteration No:  149 Worker Num:  2 \n",
            " Loss:  0.3657735288143158\n",
            "Training:  Iteration No:  149 Worker Num:  3 \n",
            " Loss:  0.2693783640861511\n",
            "Training:  Iteration No:  149 Worker Num:  4 \n",
            " Loss:  0.2584579586982727\n",
            "Training:  Iteration No:  149 Worker Num:  5 \n",
            " Loss:  0.29750049114227295\n",
            "Training:  Iteration No:  149 Worker Num:  6 \n",
            " Loss:  0.44006094336509705\n",
            "Training:  Iteration No:  149 Worker Num:  7 \n",
            " Loss:  0.41242483258247375\n",
            "Test:  Iteration No:  149 \n",
            " Loss:  0.28660951752828645\n",
            "Test accuracy:  91.49\n",
            "Training:  Iteration No:  150 Worker Num:  0 \n",
            " Loss:  0.23601292073726654\n",
            "Training:  Iteration No:  150 Worker Num:  1 \n",
            " Loss:  0.47639378905296326\n",
            "Training:  Iteration No:  150 Worker Num:  2 \n",
            " Loss:  0.28264257311820984\n",
            "Training:  Iteration No:  150 Worker Num:  3 \n",
            " Loss:  0.3800658881664276\n",
            "Training:  Iteration No:  150 Worker Num:  4 \n",
            " Loss:  0.3573589622974396\n",
            "Training:  Iteration No:  150 Worker Num:  5 \n",
            " Loss:  0.40748804807662964\n",
            "Training:  Iteration No:  150 Worker Num:  6 \n",
            " Loss:  0.251867413520813\n",
            "Training:  Iteration No:  150 Worker Num:  7 \n",
            " Loss:  0.2873208522796631\n",
            "Test:  Iteration No:  150 \n",
            " Loss:  0.29370012308789206\n",
            "Test accuracy:  91.48\n",
            "Training:  Iteration No:  151 Worker Num:  0 \n",
            " Loss:  0.30358290672302246\n",
            "Training:  Iteration No:  151 Worker Num:  1 \n",
            " Loss:  0.33326005935668945\n",
            "Training:  Iteration No:  151 Worker Num:  2 \n",
            " Loss:  0.3618571162223816\n",
            "Training:  Iteration No:  151 Worker Num:  3 \n",
            " Loss:  0.40060070157051086\n",
            "Training:  Iteration No:  151 Worker Num:  4 \n",
            " Loss:  0.2791047692298889\n",
            "Training:  Iteration No:  151 Worker Num:  5 \n",
            " Loss:  0.24639999866485596\n",
            "Training:  Iteration No:  151 Worker Num:  6 \n",
            " Loss:  0.39979982376098633\n",
            "Training:  Iteration No:  151 Worker Num:  7 \n",
            " Loss:  0.34489819407463074\n",
            "Test:  Iteration No:  151 \n",
            " Loss:  0.28801403628497185\n",
            "Test accuracy:  91.55\n",
            "Training:  Iteration No:  152 Worker Num:  0 \n",
            " Loss:  0.45240986347198486\n",
            "Training:  Iteration No:  152 Worker Num:  1 \n",
            " Loss:  0.2914137542247772\n",
            "Training:  Iteration No:  152 Worker Num:  2 \n",
            " Loss:  0.5285428762435913\n",
            "Training:  Iteration No:  152 Worker Num:  3 \n",
            " Loss:  0.4526432156562805\n",
            "Training:  Iteration No:  152 Worker Num:  4 \n",
            " Loss:  0.41827625036239624\n",
            "Training:  Iteration No:  152 Worker Num:  5 \n",
            " Loss:  0.1889439821243286\n",
            "Training:  Iteration No:  152 Worker Num:  6 \n",
            " Loss:  0.33905932307243347\n",
            "Training:  Iteration No:  152 Worker Num:  7 \n",
            " Loss:  0.30618980526924133\n",
            "Test:  Iteration No:  152 \n",
            " Loss:  0.2843826833782317\n",
            "Test accuracy:  91.66\n",
            "Training:  Iteration No:  153 Worker Num:  0 \n",
            " Loss:  0.32854193449020386\n",
            "Training:  Iteration No:  153 Worker Num:  1 \n",
            " Loss:  0.3085905909538269\n",
            "Training:  Iteration No:  153 Worker Num:  2 \n",
            " Loss:  0.27888044714927673\n",
            "Training:  Iteration No:  153 Worker Num:  3 \n",
            " Loss:  0.31744125485420227\n",
            "Training:  Iteration No:  153 Worker Num:  4 \n",
            " Loss:  0.3305802643299103\n",
            "Training:  Iteration No:  153 Worker Num:  5 \n",
            " Loss:  0.3536108732223511\n",
            "Training:  Iteration No:  153 Worker Num:  6 \n",
            " Loss:  0.3713085949420929\n",
            "Training:  Iteration No:  153 Worker Num:  7 \n",
            " Loss:  0.3666456341743469\n",
            "Test:  Iteration No:  153 \n",
            " Loss:  0.2865667843460282\n",
            "Test accuracy:  91.85\n",
            "Training:  Iteration No:  154 Worker Num:  0 \n",
            " Loss:  0.26246169209480286\n",
            "Training:  Iteration No:  154 Worker Num:  1 \n",
            " Loss:  0.25085264444351196\n",
            "Training:  Iteration No:  154 Worker Num:  2 \n",
            " Loss:  0.3759170174598694\n",
            "Training:  Iteration No:  154 Worker Num:  3 \n",
            " Loss:  0.2738226056098938\n",
            "Training:  Iteration No:  154 Worker Num:  4 \n",
            " Loss:  0.41311967372894287\n",
            "Training:  Iteration No:  154 Worker Num:  5 \n",
            " Loss:  0.4212498068809509\n",
            "Training:  Iteration No:  154 Worker Num:  6 \n",
            " Loss:  0.33597201108932495\n",
            "Training:  Iteration No:  154 Worker Num:  7 \n",
            " Loss:  0.31572288274765015\n",
            "Test:  Iteration No:  154 \n",
            " Loss:  0.281418103182429\n",
            "Test accuracy:  91.72\n",
            "Training:  Iteration No:  155 Worker Num:  0 \n",
            " Loss:  0.44938576221466064\n",
            "Training:  Iteration No:  155 Worker Num:  1 \n",
            " Loss:  0.3930138647556305\n",
            "Training:  Iteration No:  155 Worker Num:  2 \n",
            " Loss:  0.2773720324039459\n",
            "Training:  Iteration No:  155 Worker Num:  3 \n",
            " Loss:  0.4239743947982788\n",
            "Training:  Iteration No:  155 Worker Num:  4 \n",
            " Loss:  0.3852781057357788\n",
            "Training:  Iteration No:  155 Worker Num:  5 \n",
            " Loss:  0.27894288301467896\n",
            "Training:  Iteration No:  155 Worker Num:  6 \n",
            " Loss:  0.35499805212020874\n",
            "Training:  Iteration No:  155 Worker Num:  7 \n",
            " Loss:  0.20796695351600647\n",
            "Test:  Iteration No:  155 \n",
            " Loss:  0.2792674334177488\n",
            "Test accuracy:  91.68\n",
            "Training:  Iteration No:  156 Worker Num:  0 \n",
            " Loss:  0.37120598554611206\n",
            "Training:  Iteration No:  156 Worker Num:  1 \n",
            " Loss:  0.30659958720207214\n",
            "Training:  Iteration No:  156 Worker Num:  2 \n",
            " Loss:  0.3024408519268036\n",
            "Training:  Iteration No:  156 Worker Num:  3 \n",
            " Loss:  0.4003522992134094\n",
            "Training:  Iteration No:  156 Worker Num:  4 \n",
            " Loss:  0.2661301791667938\n",
            "Training:  Iteration No:  156 Worker Num:  5 \n",
            " Loss:  0.4031686782836914\n",
            "Training:  Iteration No:  156 Worker Num:  6 \n",
            " Loss:  0.21403171122074127\n",
            "Training:  Iteration No:  156 Worker Num:  7 \n",
            " Loss:  0.29096025228500366\n",
            "Test:  Iteration No:  156 \n",
            " Loss:  0.28285472854217397\n",
            "Test accuracy:  91.51\n",
            "Training:  Iteration No:  157 Worker Num:  0 \n",
            " Loss:  0.30833372473716736\n",
            "Training:  Iteration No:  157 Worker Num:  1 \n",
            " Loss:  0.23435446619987488\n",
            "Training:  Iteration No:  157 Worker Num:  2 \n",
            " Loss:  0.39427101612091064\n",
            "Training:  Iteration No:  157 Worker Num:  3 \n",
            " Loss:  0.36967194080352783\n",
            "Training:  Iteration No:  157 Worker Num:  4 \n",
            " Loss:  0.28289487957954407\n",
            "Training:  Iteration No:  157 Worker Num:  5 \n",
            " Loss:  0.23433004319667816\n",
            "Training:  Iteration No:  157 Worker Num:  6 \n",
            " Loss:  0.40326210856437683\n",
            "Training:  Iteration No:  157 Worker Num:  7 \n",
            " Loss:  0.4369521737098694\n",
            "Test:  Iteration No:  157 \n",
            " Loss:  0.27777875118146217\n",
            "Test accuracy:  91.77\n",
            "Training:  Iteration No:  158 Worker Num:  0 \n",
            " Loss:  0.3511289656162262\n",
            "Training:  Iteration No:  158 Worker Num:  1 \n",
            " Loss:  0.5734778046607971\n",
            "Training:  Iteration No:  158 Worker Num:  2 \n",
            " Loss:  0.3003292977809906\n",
            "Training:  Iteration No:  158 Worker Num:  3 \n",
            " Loss:  0.2780386805534363\n",
            "Training:  Iteration No:  158 Worker Num:  4 \n",
            " Loss:  0.33297085762023926\n",
            "Training:  Iteration No:  158 Worker Num:  5 \n",
            " Loss:  0.33984023332595825\n",
            "Training:  Iteration No:  158 Worker Num:  6 \n",
            " Loss:  0.3059535324573517\n",
            "Training:  Iteration No:  158 Worker Num:  7 \n",
            " Loss:  0.5133066773414612\n",
            "Test:  Iteration No:  158 \n",
            " Loss:  0.2778361801388143\n",
            "Test accuracy:  91.73\n",
            "Training:  Iteration No:  159 Worker Num:  0 \n",
            " Loss:  0.43617919087409973\n",
            "Training:  Iteration No:  159 Worker Num:  1 \n",
            " Loss:  0.36349502205848694\n",
            "Training:  Iteration No:  159 Worker Num:  2 \n",
            " Loss:  0.5103211998939514\n",
            "Training:  Iteration No:  159 Worker Num:  3 \n",
            " Loss:  0.41768622398376465\n",
            "Training:  Iteration No:  159 Worker Num:  4 \n",
            " Loss:  0.3193182647228241\n",
            "Training:  Iteration No:  159 Worker Num:  5 \n",
            " Loss:  0.3184792697429657\n",
            "Training:  Iteration No:  159 Worker Num:  6 \n",
            " Loss:  0.2775353491306305\n",
            "Training:  Iteration No:  159 Worker Num:  7 \n",
            " Loss:  0.3215622901916504\n",
            "Test:  Iteration No:  159 \n",
            " Loss:  0.2745277760668269\n",
            "Test accuracy:  91.89\n",
            "Training:  Iteration No:  160 Worker Num:  0 \n",
            " Loss:  0.3299969434738159\n",
            "Training:  Iteration No:  160 Worker Num:  1 \n",
            " Loss:  0.29217061400413513\n",
            "Training:  Iteration No:  160 Worker Num:  2 \n",
            " Loss:  0.4492054581642151\n",
            "Training:  Iteration No:  160 Worker Num:  3 \n",
            " Loss:  0.2877212166786194\n",
            "Training:  Iteration No:  160 Worker Num:  4 \n",
            " Loss:  0.17247889935970306\n",
            "Training:  Iteration No:  160 Worker Num:  5 \n",
            " Loss:  0.3712542951107025\n",
            "Training:  Iteration No:  160 Worker Num:  6 \n",
            " Loss:  0.26790088415145874\n",
            "Training:  Iteration No:  160 Worker Num:  7 \n",
            " Loss:  0.34412506222724915\n",
            "Test:  Iteration No:  160 \n",
            " Loss:  0.2783061585896\n",
            "Test accuracy:  91.66\n",
            "Training:  Iteration No:  161 Worker Num:  0 \n",
            " Loss:  0.48247480392456055\n",
            "Training:  Iteration No:  161 Worker Num:  1 \n",
            " Loss:  0.31994691491127014\n",
            "Training:  Iteration No:  161 Worker Num:  2 \n",
            " Loss:  0.3115193545818329\n",
            "Training:  Iteration No:  161 Worker Num:  3 \n",
            " Loss:  0.36721619963645935\n",
            "Training:  Iteration No:  161 Worker Num:  4 \n",
            " Loss:  0.4010397791862488\n",
            "Training:  Iteration No:  161 Worker Num:  5 \n",
            " Loss:  0.41883736848831177\n",
            "Training:  Iteration No:  161 Worker Num:  6 \n",
            " Loss:  0.34737780690193176\n",
            "Training:  Iteration No:  161 Worker Num:  7 \n",
            " Loss:  0.39894241094589233\n",
            "Test:  Iteration No:  161 \n",
            " Loss:  0.27785506349387046\n",
            "Test accuracy:  91.84\n",
            "Training:  Iteration No:  162 Worker Num:  0 \n",
            " Loss:  0.22559286653995514\n",
            "Training:  Iteration No:  162 Worker Num:  1 \n",
            " Loss:  0.37863752245903015\n",
            "Training:  Iteration No:  162 Worker Num:  2 \n",
            " Loss:  0.3394867479801178\n",
            "Training:  Iteration No:  162 Worker Num:  3 \n",
            " Loss:  0.4169413447380066\n",
            "Training:  Iteration No:  162 Worker Num:  4 \n",
            " Loss:  0.3448762893676758\n",
            "Training:  Iteration No:  162 Worker Num:  5 \n",
            " Loss:  0.2014191895723343\n",
            "Training:  Iteration No:  162 Worker Num:  6 \n",
            " Loss:  0.19452640414237976\n",
            "Training:  Iteration No:  162 Worker Num:  7 \n",
            " Loss:  0.31573522090911865\n",
            "Test:  Iteration No:  162 \n",
            " Loss:  0.27457242614672156\n",
            "Test accuracy:  91.76\n",
            "Training:  Iteration No:  163 Worker Num:  0 \n",
            " Loss:  0.30811619758605957\n",
            "Training:  Iteration No:  163 Worker Num:  1 \n",
            " Loss:  0.35303977131843567\n",
            "Training:  Iteration No:  163 Worker Num:  2 \n",
            " Loss:  0.33510154485702515\n",
            "Training:  Iteration No:  163 Worker Num:  3 \n",
            " Loss:  0.3054755628108978\n",
            "Training:  Iteration No:  163 Worker Num:  4 \n",
            " Loss:  0.4536076784133911\n",
            "Training:  Iteration No:  163 Worker Num:  5 \n",
            " Loss:  0.321908175945282\n",
            "Training:  Iteration No:  163 Worker Num:  6 \n",
            " Loss:  0.27667903900146484\n",
            "Training:  Iteration No:  163 Worker Num:  7 \n",
            " Loss:  0.3986518681049347\n",
            "Test:  Iteration No:  163 \n",
            " Loss:  0.27347053118238723\n",
            "Test accuracy:  91.77\n",
            "Training:  Iteration No:  164 Worker Num:  0 \n",
            " Loss:  0.26324623823165894\n",
            "Training:  Iteration No:  164 Worker Num:  1 \n",
            " Loss:  0.3787572383880615\n",
            "Training:  Iteration No:  164 Worker Num:  2 \n",
            " Loss:  0.3167690932750702\n",
            "Training:  Iteration No:  164 Worker Num:  3 \n",
            " Loss:  0.3109872043132782\n",
            "Training:  Iteration No:  164 Worker Num:  4 \n",
            " Loss:  0.32874569296836853\n",
            "Training:  Iteration No:  164 Worker Num:  5 \n",
            " Loss:  0.28605055809020996\n",
            "Training:  Iteration No:  164 Worker Num:  6 \n",
            " Loss:  0.3389512002468109\n",
            "Training:  Iteration No:  164 Worker Num:  7 \n",
            " Loss:  0.26185300946235657\n",
            "Test:  Iteration No:  164 \n",
            " Loss:  0.2816538256651993\n",
            "Test accuracy:  91.66\n",
            "Training:  Iteration No:  165 Worker Num:  0 \n",
            " Loss:  0.304255872964859\n",
            "Training:  Iteration No:  165 Worker Num:  1 \n",
            " Loss:  0.28338080644607544\n",
            "Training:  Iteration No:  165 Worker Num:  2 \n",
            " Loss:  0.33865755796432495\n",
            "Training:  Iteration No:  165 Worker Num:  3 \n",
            " Loss:  0.4152297079563141\n",
            "Training:  Iteration No:  165 Worker Num:  4 \n",
            " Loss:  0.4035005271434784\n",
            "Training:  Iteration No:  165 Worker Num:  5 \n",
            " Loss:  0.33877360820770264\n",
            "Training:  Iteration No:  165 Worker Num:  6 \n",
            " Loss:  0.29632750153541565\n",
            "Training:  Iteration No:  165 Worker Num:  7 \n",
            " Loss:  0.2869321405887604\n",
            "Test:  Iteration No:  165 \n",
            " Loss:  0.2676105657474527\n",
            "Test accuracy:  92.16\n",
            "Training:  Iteration No:  166 Worker Num:  0 \n",
            " Loss:  0.3839954435825348\n",
            "Training:  Iteration No:  166 Worker Num:  1 \n",
            " Loss:  0.37364932894706726\n",
            "Training:  Iteration No:  166 Worker Num:  2 \n",
            " Loss:  0.4514218270778656\n",
            "Training:  Iteration No:  166 Worker Num:  3 \n",
            " Loss:  0.30142274498939514\n",
            "Training:  Iteration No:  166 Worker Num:  4 \n",
            " Loss:  0.32110974192619324\n",
            "Training:  Iteration No:  166 Worker Num:  5 \n",
            " Loss:  0.37304091453552246\n",
            "Training:  Iteration No:  166 Worker Num:  6 \n",
            " Loss:  0.3427176773548126\n",
            "Training:  Iteration No:  166 Worker Num:  7 \n",
            " Loss:  0.35665351152420044\n",
            "Test:  Iteration No:  166 \n",
            " Loss:  0.27110611795907535\n",
            "Test accuracy:  91.91\n",
            "Training:  Iteration No:  167 Worker Num:  0 \n",
            " Loss:  0.3968280851840973\n",
            "Training:  Iteration No:  167 Worker Num:  1 \n",
            " Loss:  0.4081391394138336\n",
            "Training:  Iteration No:  167 Worker Num:  2 \n",
            " Loss:  0.32523781061172485\n",
            "Training:  Iteration No:  167 Worker Num:  3 \n",
            " Loss:  0.3624971807003021\n",
            "Training:  Iteration No:  167 Worker Num:  4 \n",
            " Loss:  0.3002607822418213\n",
            "Training:  Iteration No:  167 Worker Num:  5 \n",
            " Loss:  0.4718362092971802\n",
            "Training:  Iteration No:  167 Worker Num:  6 \n",
            " Loss:  0.3368508517742157\n",
            "Training:  Iteration No:  167 Worker Num:  7 \n",
            " Loss:  0.40310725569725037\n",
            "Test:  Iteration No:  167 \n",
            " Loss:  0.2735693205996782\n",
            "Test accuracy:  92.1\n",
            "Training:  Iteration No:  168 Worker Num:  0 \n",
            " Loss:  0.20366674661636353\n",
            "Training:  Iteration No:  168 Worker Num:  1 \n",
            " Loss:  0.3752729594707489\n",
            "Training:  Iteration No:  168 Worker Num:  2 \n",
            " Loss:  0.3473845422267914\n",
            "Training:  Iteration No:  168 Worker Num:  3 \n",
            " Loss:  0.3331027925014496\n",
            "Training:  Iteration No:  168 Worker Num:  4 \n",
            " Loss:  0.2925010919570923\n",
            "Training:  Iteration No:  168 Worker Num:  5 \n",
            " Loss:  0.3298880159854889\n",
            "Training:  Iteration No:  168 Worker Num:  6 \n",
            " Loss:  0.3268214166164398\n",
            "Training:  Iteration No:  168 Worker Num:  7 \n",
            " Loss:  0.43913495540618896\n",
            "Test:  Iteration No:  168 \n",
            " Loss:  0.2665213876981524\n",
            "Test accuracy:  92.04\n",
            "Training:  Iteration No:  169 Worker Num:  0 \n",
            " Loss:  0.239862859249115\n",
            "Training:  Iteration No:  169 Worker Num:  1 \n",
            " Loss:  0.2570364475250244\n",
            "Training:  Iteration No:  169 Worker Num:  2 \n",
            " Loss:  0.41460278630256653\n",
            "Training:  Iteration No:  169 Worker Num:  3 \n",
            " Loss:  0.2893124222755432\n",
            "Training:  Iteration No:  169 Worker Num:  4 \n",
            " Loss:  0.31035077571868896\n",
            "Training:  Iteration No:  169 Worker Num:  5 \n",
            " Loss:  0.350938081741333\n",
            "Training:  Iteration No:  169 Worker Num:  6 \n",
            " Loss:  0.26686421036720276\n",
            "Training:  Iteration No:  169 Worker Num:  7 \n",
            " Loss:  0.30378901958465576\n",
            "Test:  Iteration No:  169 \n",
            " Loss:  0.26788304599968693\n",
            "Test accuracy:  92.26\n",
            "Training:  Iteration No:  170 Worker Num:  0 \n",
            " Loss:  0.3668503165245056\n",
            "Training:  Iteration No:  170 Worker Num:  1 \n",
            " Loss:  0.2820854187011719\n",
            "Training:  Iteration No:  170 Worker Num:  2 \n",
            " Loss:  0.28924334049224854\n",
            "Training:  Iteration No:  170 Worker Num:  3 \n",
            " Loss:  0.2910712957382202\n",
            "Training:  Iteration No:  170 Worker Num:  4 \n",
            " Loss:  0.2684842050075531\n",
            "Training:  Iteration No:  170 Worker Num:  5 \n",
            " Loss:  0.40522903203964233\n",
            "Training:  Iteration No:  170 Worker Num:  6 \n",
            " Loss:  0.4184274673461914\n",
            "Training:  Iteration No:  170 Worker Num:  7 \n",
            " Loss:  0.23345355689525604\n",
            "Test:  Iteration No:  170 \n",
            " Loss:  0.2669318452288833\n",
            "Test accuracy:  92.35\n",
            "Training:  Iteration No:  171 Worker Num:  0 \n",
            " Loss:  0.2806164622306824\n",
            "Training:  Iteration No:  171 Worker Num:  1 \n",
            " Loss:  0.33321189880371094\n",
            "Training:  Iteration No:  171 Worker Num:  2 \n",
            " Loss:  0.34550565481185913\n",
            "Training:  Iteration No:  171 Worker Num:  3 \n",
            " Loss:  0.2725493609905243\n",
            "Training:  Iteration No:  171 Worker Num:  4 \n",
            " Loss:  0.1826164424419403\n",
            "Training:  Iteration No:  171 Worker Num:  5 \n",
            " Loss:  0.2908167243003845\n",
            "Training:  Iteration No:  171 Worker Num:  6 \n",
            " Loss:  0.3248044550418854\n",
            "Training:  Iteration No:  171 Worker Num:  7 \n",
            " Loss:  0.31738078594207764\n",
            "Test:  Iteration No:  171 \n",
            " Loss:  0.2641390570545498\n",
            "Test accuracy:  92.19\n",
            "Training:  Iteration No:  172 Worker Num:  0 \n",
            " Loss:  0.37777280807495117\n",
            "Training:  Iteration No:  172 Worker Num:  1 \n",
            " Loss:  0.36174193024635315\n",
            "Training:  Iteration No:  172 Worker Num:  2 \n",
            " Loss:  0.3165663480758667\n",
            "Training:  Iteration No:  172 Worker Num:  3 \n",
            " Loss:  0.3070196509361267\n",
            "Training:  Iteration No:  172 Worker Num:  4 \n",
            " Loss:  0.30339840054512024\n",
            "Training:  Iteration No:  172 Worker Num:  5 \n",
            " Loss:  0.25288718938827515\n",
            "Training:  Iteration No:  172 Worker Num:  6 \n",
            " Loss:  0.20304061472415924\n",
            "Training:  Iteration No:  172 Worker Num:  7 \n",
            " Loss:  0.3591887950897217\n",
            "Test:  Iteration No:  172 \n",
            " Loss:  0.26148995969303046\n",
            "Test accuracy:  92.34\n",
            "Training:  Iteration No:  173 Worker Num:  0 \n",
            " Loss:  0.28676438331604004\n",
            "Training:  Iteration No:  173 Worker Num:  1 \n",
            " Loss:  0.24033556878566742\n",
            "Training:  Iteration No:  173 Worker Num:  2 \n",
            " Loss:  0.3294086456298828\n",
            "Training:  Iteration No:  173 Worker Num:  3 \n",
            " Loss:  0.34556952118873596\n",
            "Training:  Iteration No:  173 Worker Num:  4 \n",
            " Loss:  0.3810576796531677\n",
            "Training:  Iteration No:  173 Worker Num:  5 \n",
            " Loss:  0.35790902376174927\n",
            "Training:  Iteration No:  173 Worker Num:  6 \n",
            " Loss:  0.2966219186782837\n",
            "Training:  Iteration No:  173 Worker Num:  7 \n",
            " Loss:  0.3273552358150482\n",
            "Test:  Iteration No:  173 \n",
            " Loss:  0.2600576739192386\n",
            "Test accuracy:  92.54\n",
            "Training:  Iteration No:  174 Worker Num:  0 \n",
            " Loss:  0.508897602558136\n",
            "Training:  Iteration No:  174 Worker Num:  1 \n",
            " Loss:  0.31491678953170776\n",
            "Training:  Iteration No:  174 Worker Num:  2 \n",
            " Loss:  0.30032625794410706\n",
            "Training:  Iteration No:  174 Worker Num:  3 \n",
            " Loss:  0.34939226508140564\n",
            "Training:  Iteration No:  174 Worker Num:  4 \n",
            " Loss:  0.28987616300582886\n",
            "Training:  Iteration No:  174 Worker Num:  5 \n",
            " Loss:  0.30309414863586426\n",
            "Training:  Iteration No:  174 Worker Num:  6 \n",
            " Loss:  0.3920925557613373\n",
            "Training:  Iteration No:  174 Worker Num:  7 \n",
            " Loss:  0.32380127906799316\n",
            "Test:  Iteration No:  174 \n",
            " Loss:  0.2583668029195146\n",
            "Test accuracy:  92.22\n",
            "Training:  Iteration No:  175 Worker Num:  0 \n",
            " Loss:  0.24811290204524994\n",
            "Training:  Iteration No:  175 Worker Num:  1 \n",
            " Loss:  0.2054286003112793\n",
            "Training:  Iteration No:  175 Worker Num:  2 \n",
            " Loss:  0.25086545944213867\n",
            "Training:  Iteration No:  175 Worker Num:  3 \n",
            " Loss:  0.22674348950386047\n",
            "Training:  Iteration No:  175 Worker Num:  4 \n",
            " Loss:  0.2070872038602829\n",
            "Training:  Iteration No:  175 Worker Num:  5 \n",
            " Loss:  0.3200857937335968\n",
            "Training:  Iteration No:  175 Worker Num:  6 \n",
            " Loss:  0.2833152711391449\n",
            "Training:  Iteration No:  175 Worker Num:  7 \n",
            " Loss:  0.2946714460849762\n",
            "Test:  Iteration No:  175 \n",
            " Loss:  0.2676574866747177\n",
            "Test accuracy:  92.09\n",
            "Training:  Iteration No:  176 Worker Num:  0 \n",
            " Loss:  0.3282175660133362\n",
            "Training:  Iteration No:  176 Worker Num:  1 \n",
            " Loss:  0.29341840744018555\n",
            "Training:  Iteration No:  176 Worker Num:  2 \n",
            " Loss:  0.2723288834095001\n",
            "Training:  Iteration No:  176 Worker Num:  3 \n",
            " Loss:  0.2251451015472412\n",
            "Training:  Iteration No:  176 Worker Num:  4 \n",
            " Loss:  0.26478350162506104\n",
            "Training:  Iteration No:  176 Worker Num:  5 \n",
            " Loss:  0.37574583292007446\n",
            "Training:  Iteration No:  176 Worker Num:  6 \n",
            " Loss:  0.2104068398475647\n",
            "Training:  Iteration No:  176 Worker Num:  7 \n",
            " Loss:  0.2643339931964874\n",
            "Test:  Iteration No:  176 \n",
            " Loss:  0.2583595413856114\n",
            "Test accuracy:  92.38\n",
            "Training:  Iteration No:  177 Worker Num:  0 \n",
            " Loss:  0.21623659133911133\n",
            "Training:  Iteration No:  177 Worker Num:  1 \n",
            " Loss:  0.33245518803596497\n",
            "Training:  Iteration No:  177 Worker Num:  2 \n",
            " Loss:  0.25252529978752136\n",
            "Training:  Iteration No:  177 Worker Num:  3 \n",
            " Loss:  0.29493460059165955\n",
            "Training:  Iteration No:  177 Worker Num:  4 \n",
            " Loss:  0.4271712005138397\n",
            "Training:  Iteration No:  177 Worker Num:  5 \n",
            " Loss:  0.22021640837192535\n",
            "Training:  Iteration No:  177 Worker Num:  6 \n",
            " Loss:  0.3311712443828583\n",
            "Training:  Iteration No:  177 Worker Num:  7 \n",
            " Loss:  0.3537472188472748\n",
            "Test:  Iteration No:  177 \n",
            " Loss:  0.2666660303462155\n",
            "Test accuracy:  92.12\n",
            "Training:  Iteration No:  178 Worker Num:  0 \n",
            " Loss:  0.265737384557724\n",
            "Training:  Iteration No:  178 Worker Num:  1 \n",
            " Loss:  0.32437312602996826\n",
            "Training:  Iteration No:  178 Worker Num:  2 \n",
            " Loss:  0.2656179368495941\n",
            "Training:  Iteration No:  178 Worker Num:  3 \n",
            " Loss:  0.33207985758781433\n",
            "Training:  Iteration No:  178 Worker Num:  4 \n",
            " Loss:  0.286077618598938\n",
            "Training:  Iteration No:  178 Worker Num:  5 \n",
            " Loss:  0.21721145510673523\n",
            "Training:  Iteration No:  178 Worker Num:  6 \n",
            " Loss:  0.2864283621311188\n",
            "Training:  Iteration No:  178 Worker Num:  7 \n",
            " Loss:  0.33101826906204224\n",
            "Test:  Iteration No:  178 \n",
            " Loss:  0.25534091526760333\n",
            "Test accuracy:  92.56\n",
            "Training:  Iteration No:  179 Worker Num:  0 \n",
            " Loss:  0.26296859979629517\n",
            "Training:  Iteration No:  179 Worker Num:  1 \n",
            " Loss:  0.2678019106388092\n",
            "Training:  Iteration No:  179 Worker Num:  2 \n",
            " Loss:  0.29927539825439453\n",
            "Training:  Iteration No:  179 Worker Num:  3 \n",
            " Loss:  0.2626934349536896\n",
            "Training:  Iteration No:  179 Worker Num:  4 \n",
            " Loss:  0.3706730306148529\n",
            "Training:  Iteration No:  179 Worker Num:  5 \n",
            " Loss:  0.29495373368263245\n",
            "Training:  Iteration No:  179 Worker Num:  6 \n",
            " Loss:  0.3165925443172455\n",
            "Training:  Iteration No:  179 Worker Num:  7 \n",
            " Loss:  0.2362026870250702\n",
            "Test:  Iteration No:  179 \n",
            " Loss:  0.25540256856268717\n",
            "Test accuracy:  92.42\n",
            "Training:  Iteration No:  180 Worker Num:  0 \n",
            " Loss:  0.32748329639434814\n",
            "Training:  Iteration No:  180 Worker Num:  1 \n",
            " Loss:  0.23305092751979828\n",
            "Training:  Iteration No:  180 Worker Num:  2 \n",
            " Loss:  0.28295257687568665\n",
            "Training:  Iteration No:  180 Worker Num:  3 \n",
            " Loss:  0.32205817103385925\n",
            "Training:  Iteration No:  180 Worker Num:  4 \n",
            " Loss:  0.24259068071842194\n",
            "Training:  Iteration No:  180 Worker Num:  5 \n",
            " Loss:  0.35871145129203796\n",
            "Training:  Iteration No:  180 Worker Num:  6 \n",
            " Loss:  0.37726274132728577\n",
            "Training:  Iteration No:  180 Worker Num:  7 \n",
            " Loss:  0.24925918877124786\n",
            "Test:  Iteration No:  180 \n",
            " Loss:  0.25182301404921315\n",
            "Test accuracy:  92.74\n",
            "Training:  Iteration No:  181 Worker Num:  0 \n",
            " Loss:  0.30146607756614685\n",
            "Training:  Iteration No:  181 Worker Num:  1 \n",
            " Loss:  0.2010948807001114\n",
            "Training:  Iteration No:  181 Worker Num:  2 \n",
            " Loss:  0.2616434693336487\n",
            "Training:  Iteration No:  181 Worker Num:  3 \n",
            " Loss:  0.3497965335845947\n",
            "Training:  Iteration No:  181 Worker Num:  4 \n",
            " Loss:  0.3735501766204834\n",
            "Training:  Iteration No:  181 Worker Num:  5 \n",
            " Loss:  0.17547231912612915\n",
            "Training:  Iteration No:  181 Worker Num:  6 \n",
            " Loss:  0.31040480732917786\n",
            "Training:  Iteration No:  181 Worker Num:  7 \n",
            " Loss:  0.27299362421035767\n",
            "Test:  Iteration No:  181 \n",
            " Loss:  0.255657065706917\n",
            "Test accuracy:  92.44\n",
            "Training:  Iteration No:  182 Worker Num:  0 \n",
            " Loss:  0.41855230927467346\n",
            "Training:  Iteration No:  182 Worker Num:  1 \n",
            " Loss:  0.2651408612728119\n",
            "Training:  Iteration No:  182 Worker Num:  2 \n",
            " Loss:  0.28860828280448914\n",
            "Training:  Iteration No:  182 Worker Num:  3 \n",
            " Loss:  0.2534487843513489\n",
            "Training:  Iteration No:  182 Worker Num:  4 \n",
            " Loss:  0.466609925031662\n",
            "Training:  Iteration No:  182 Worker Num:  5 \n",
            " Loss:  0.32543855905532837\n",
            "Training:  Iteration No:  182 Worker Num:  6 \n",
            " Loss:  0.298357754945755\n",
            "Training:  Iteration No:  182 Worker Num:  7 \n",
            " Loss:  0.21865607798099518\n",
            "Test:  Iteration No:  182 \n",
            " Loss:  0.2529323519785193\n",
            "Test accuracy:  92.81\n",
            "Training:  Iteration No:  183 Worker Num:  0 \n",
            " Loss:  0.3654833137989044\n",
            "Training:  Iteration No:  183 Worker Num:  1 \n",
            " Loss:  0.2925056517124176\n",
            "Training:  Iteration No:  183 Worker Num:  2 \n",
            " Loss:  0.26263755559921265\n",
            "Training:  Iteration No:  183 Worker Num:  3 \n",
            " Loss:  0.3904518783092499\n",
            "Training:  Iteration No:  183 Worker Num:  4 \n",
            " Loss:  0.23940907418727875\n",
            "Training:  Iteration No:  183 Worker Num:  5 \n",
            " Loss:  0.26669812202453613\n",
            "Training:  Iteration No:  183 Worker Num:  6 \n",
            " Loss:  0.23771268129348755\n",
            "Training:  Iteration No:  183 Worker Num:  7 \n",
            " Loss:  0.27614647150039673\n",
            "Test:  Iteration No:  183 \n",
            " Loss:  0.25551887163067166\n",
            "Test accuracy:  92.58\n",
            "Training:  Iteration No:  184 Worker Num:  0 \n",
            " Loss:  0.325984388589859\n",
            "Training:  Iteration No:  184 Worker Num:  1 \n",
            " Loss:  0.3380598723888397\n",
            "Training:  Iteration No:  184 Worker Num:  2 \n",
            " Loss:  0.34753310680389404\n",
            "Training:  Iteration No:  184 Worker Num:  3 \n",
            " Loss:  0.2931237518787384\n",
            "Training:  Iteration No:  184 Worker Num:  4 \n",
            " Loss:  0.3311641216278076\n",
            "Training:  Iteration No:  184 Worker Num:  5 \n",
            " Loss:  0.2606903910636902\n",
            "Training:  Iteration No:  184 Worker Num:  6 \n",
            " Loss:  0.31811365485191345\n",
            "Training:  Iteration No:  184 Worker Num:  7 \n",
            " Loss:  0.2661406993865967\n",
            "Test:  Iteration No:  184 \n",
            " Loss:  0.25364327506174017\n",
            "Test accuracy:  92.62\n",
            "Training:  Iteration No:  185 Worker Num:  0 \n",
            " Loss:  0.25243768095970154\n",
            "Training:  Iteration No:  185 Worker Num:  1 \n",
            " Loss:  0.43475058674812317\n",
            "Training:  Iteration No:  185 Worker Num:  2 \n",
            " Loss:  0.32931262254714966\n",
            "Training:  Iteration No:  185 Worker Num:  3 \n",
            " Loss:  0.21754670143127441\n",
            "Training:  Iteration No:  185 Worker Num:  4 \n",
            " Loss:  0.19236725568771362\n",
            "Training:  Iteration No:  185 Worker Num:  5 \n",
            " Loss:  0.31742119789123535\n",
            "Training:  Iteration No:  185 Worker Num:  6 \n",
            " Loss:  0.2650390565395355\n",
            "Training:  Iteration No:  185 Worker Num:  7 \n",
            " Loss:  0.4653070271015167\n",
            "Test:  Iteration No:  185 \n",
            " Loss:  0.2507054772510936\n",
            "Test accuracy:  92.69\n",
            "Training:  Iteration No:  186 Worker Num:  0 \n",
            " Loss:  0.3150155246257782\n",
            "Training:  Iteration No:  186 Worker Num:  1 \n",
            " Loss:  0.4177842140197754\n",
            "Training:  Iteration No:  186 Worker Num:  2 \n",
            " Loss:  0.26995497941970825\n",
            "Training:  Iteration No:  186 Worker Num:  3 \n",
            " Loss:  0.36516469717025757\n",
            "Training:  Iteration No:  186 Worker Num:  4 \n",
            " Loss:  0.3722631633281708\n",
            "Training:  Iteration No:  186 Worker Num:  5 \n",
            " Loss:  0.30926793813705444\n",
            "Training:  Iteration No:  186 Worker Num:  6 \n",
            " Loss:  0.3581582009792328\n",
            "Training:  Iteration No:  186 Worker Num:  7 \n",
            " Loss:  0.14396020770072937\n",
            "Test:  Iteration No:  186 \n",
            " Loss:  0.24856245588464074\n",
            "Test accuracy:  92.76\n",
            "Training:  Iteration No:  187 Worker Num:  0 \n",
            " Loss:  0.2118862122297287\n",
            "Training:  Iteration No:  187 Worker Num:  1 \n",
            " Loss:  0.23230992257595062\n",
            "Training:  Iteration No:  187 Worker Num:  2 \n",
            " Loss:  0.2946527898311615\n",
            "Training:  Iteration No:  187 Worker Num:  3 \n",
            " Loss:  0.36091041564941406\n",
            "Training:  Iteration No:  187 Worker Num:  4 \n",
            " Loss:  0.24750399589538574\n",
            "Training:  Iteration No:  187 Worker Num:  5 \n",
            " Loss:  0.3907817304134369\n",
            "Training:  Iteration No:  187 Worker Num:  6 \n",
            " Loss:  0.19259333610534668\n",
            "Training:  Iteration No:  187 Worker Num:  7 \n",
            " Loss:  0.33916568756103516\n",
            "Test:  Iteration No:  187 \n",
            " Loss:  0.2476215555886679\n",
            "Test accuracy:  92.8\n",
            "Training:  Iteration No:  188 Worker Num:  0 \n",
            " Loss:  0.19291000068187714\n",
            "Training:  Iteration No:  188 Worker Num:  1 \n",
            " Loss:  0.24026310443878174\n",
            "Training:  Iteration No:  188 Worker Num:  2 \n",
            " Loss:  0.3354513347148895\n",
            "Training:  Iteration No:  188 Worker Num:  3 \n",
            " Loss:  0.2997637987136841\n",
            "Training:  Iteration No:  188 Worker Num:  4 \n",
            " Loss:  0.26388442516326904\n",
            "Training:  Iteration No:  188 Worker Num:  5 \n",
            " Loss:  0.31483879685401917\n",
            "Training:  Iteration No:  188 Worker Num:  6 \n",
            " Loss:  0.31572043895721436\n",
            "Training:  Iteration No:  188 Worker Num:  7 \n",
            " Loss:  0.26793602108955383\n",
            "Test:  Iteration No:  188 \n",
            " Loss:  0.246165141321813\n",
            "Test accuracy:  92.91\n",
            "Training:  Iteration No:  189 Worker Num:  0 \n",
            " Loss:  0.22236478328704834\n",
            "Training:  Iteration No:  189 Worker Num:  1 \n",
            " Loss:  0.26225167512893677\n",
            "Training:  Iteration No:  189 Worker Num:  2 \n",
            " Loss:  0.3461335003376007\n",
            "Training:  Iteration No:  189 Worker Num:  3 \n",
            " Loss:  0.3278595805168152\n",
            "Training:  Iteration No:  189 Worker Num:  4 \n",
            " Loss:  0.2219519168138504\n",
            "Training:  Iteration No:  189 Worker Num:  5 \n",
            " Loss:  0.22926409542560577\n",
            "Training:  Iteration No:  189 Worker Num:  6 \n",
            " Loss:  0.3683970868587494\n",
            "Training:  Iteration No:  189 Worker Num:  7 \n",
            " Loss:  0.29428282380104065\n",
            "Test:  Iteration No:  189 \n",
            " Loss:  0.25152863743656045\n",
            "Test accuracy:  92.64\n",
            "Training:  Iteration No:  190 Worker Num:  0 \n",
            " Loss:  0.37843504548072815\n",
            "Training:  Iteration No:  190 Worker Num:  1 \n",
            " Loss:  0.28924375772476196\n",
            "Training:  Iteration No:  190 Worker Num:  2 \n",
            " Loss:  0.23066279292106628\n",
            "Training:  Iteration No:  190 Worker Num:  3 \n",
            " Loss:  0.2688494324684143\n",
            "Training:  Iteration No:  190 Worker Num:  4 \n",
            " Loss:  0.31468629837036133\n",
            "Training:  Iteration No:  190 Worker Num:  5 \n",
            " Loss:  0.239424467086792\n",
            "Training:  Iteration No:  190 Worker Num:  6 \n",
            " Loss:  0.2211143970489502\n",
            "Training:  Iteration No:  190 Worker Num:  7 \n",
            " Loss:  0.27854108810424805\n",
            "Test:  Iteration No:  190 \n",
            " Loss:  0.24375390030349356\n",
            "Test accuracy:  92.87\n",
            "Training:  Iteration No:  191 Worker Num:  0 \n",
            " Loss:  0.2118600755929947\n",
            "Training:  Iteration No:  191 Worker Num:  1 \n",
            " Loss:  0.30696988105773926\n",
            "Training:  Iteration No:  191 Worker Num:  2 \n",
            " Loss:  0.3191203474998474\n",
            "Training:  Iteration No:  191 Worker Num:  3 \n",
            " Loss:  0.4112793803215027\n",
            "Training:  Iteration No:  191 Worker Num:  4 \n",
            " Loss:  0.276135116815567\n",
            "Training:  Iteration No:  191 Worker Num:  5 \n",
            " Loss:  0.31684163212776184\n",
            "Training:  Iteration No:  191 Worker Num:  6 \n",
            " Loss:  0.22352705895900726\n",
            "Training:  Iteration No:  191 Worker Num:  7 \n",
            " Loss:  0.34900784492492676\n",
            "Test:  Iteration No:  191 \n",
            " Loss:  0.2459110584864511\n",
            "Test accuracy:  92.79\n",
            "Training:  Iteration No:  192 Worker Num:  0 \n",
            " Loss:  0.24538595974445343\n",
            "Training:  Iteration No:  192 Worker Num:  1 \n",
            " Loss:  0.3588689863681793\n",
            "Training:  Iteration No:  192 Worker Num:  2 \n",
            " Loss:  0.3283171057701111\n",
            "Training:  Iteration No:  192 Worker Num:  3 \n",
            " Loss:  0.34644925594329834\n",
            "Training:  Iteration No:  192 Worker Num:  4 \n",
            " Loss:  0.28501299023628235\n",
            "Training:  Iteration No:  192 Worker Num:  5 \n",
            " Loss:  0.34416380524635315\n",
            "Training:  Iteration No:  192 Worker Num:  6 \n",
            " Loss:  0.20217345654964447\n",
            "Training:  Iteration No:  192 Worker Num:  7 \n",
            " Loss:  0.15787723660469055\n",
            "Test:  Iteration No:  192 \n",
            " Loss:  0.2426119965701541\n",
            "Test accuracy:  93.01\n",
            "Training:  Iteration No:  193 Worker Num:  0 \n",
            " Loss:  0.18180564045906067\n",
            "Training:  Iteration No:  193 Worker Num:  1 \n",
            " Loss:  0.299836128950119\n",
            "Training:  Iteration No:  193 Worker Num:  2 \n",
            " Loss:  0.29024428129196167\n",
            "Training:  Iteration No:  193 Worker Num:  3 \n",
            " Loss:  0.23039540648460388\n",
            "Training:  Iteration No:  193 Worker Num:  4 \n",
            " Loss:  0.2077682465314865\n",
            "Training:  Iteration No:  193 Worker Num:  5 \n",
            " Loss:  0.34056705236434937\n",
            "Training:  Iteration No:  193 Worker Num:  6 \n",
            " Loss:  0.3875497281551361\n",
            "Training:  Iteration No:  193 Worker Num:  7 \n",
            " Loss:  0.29018354415893555\n",
            "Test:  Iteration No:  193 \n",
            " Loss:  0.24164398181947727\n",
            "Test accuracy:  92.94\n",
            "Training:  Iteration No:  194 Worker Num:  0 \n",
            " Loss:  0.5357664823532104\n",
            "Training:  Iteration No:  194 Worker Num:  1 \n",
            " Loss:  0.3553015887737274\n",
            "Training:  Iteration No:  194 Worker Num:  2 \n",
            " Loss:  0.25457051396369934\n",
            "Training:  Iteration No:  194 Worker Num:  3 \n",
            " Loss:  0.2420843243598938\n",
            "Training:  Iteration No:  194 Worker Num:  4 \n",
            " Loss:  0.23839519917964935\n",
            "Training:  Iteration No:  194 Worker Num:  5 \n",
            " Loss:  0.32951754331588745\n",
            "Training:  Iteration No:  194 Worker Num:  6 \n",
            " Loss:  0.29253143072128296\n",
            "Training:  Iteration No:  194 Worker Num:  7 \n",
            " Loss:  0.32851430773735046\n",
            "Test:  Iteration No:  194 \n",
            " Loss:  0.24187947559771658\n",
            "Test accuracy:  92.78\n",
            "Training:  Iteration No:  195 Worker Num:  0 \n",
            " Loss:  0.15947139263153076\n",
            "Training:  Iteration No:  195 Worker Num:  1 \n",
            " Loss:  0.2491728812456131\n",
            "Training:  Iteration No:  195 Worker Num:  2 \n",
            " Loss:  0.4139944911003113\n",
            "Training:  Iteration No:  195 Worker Num:  3 \n",
            " Loss:  0.4285425543785095\n",
            "Training:  Iteration No:  195 Worker Num:  4 \n",
            " Loss:  0.3814179003238678\n",
            "Training:  Iteration No:  195 Worker Num:  5 \n",
            " Loss:  0.35029321908950806\n",
            "Training:  Iteration No:  195 Worker Num:  6 \n",
            " Loss:  0.3088889420032501\n",
            "Training:  Iteration No:  195 Worker Num:  7 \n",
            " Loss:  0.2622207999229431\n",
            "Test:  Iteration No:  195 \n",
            " Loss:  0.23925501071623986\n",
            "Test accuracy:  93.03\n",
            "Training:  Iteration No:  196 Worker Num:  0 \n",
            " Loss:  0.32036828994750977\n",
            "Training:  Iteration No:  196 Worker Num:  1 \n",
            " Loss:  0.28808534145355225\n",
            "Training:  Iteration No:  196 Worker Num:  2 \n",
            " Loss:  0.40329688787460327\n",
            "Training:  Iteration No:  196 Worker Num:  3 \n",
            " Loss:  0.23150353133678436\n",
            "Training:  Iteration No:  196 Worker Num:  4 \n",
            " Loss:  0.40865418314933777\n",
            "Training:  Iteration No:  196 Worker Num:  5 \n",
            " Loss:  0.23474648594856262\n",
            "Training:  Iteration No:  196 Worker Num:  6 \n",
            " Loss:  0.22543716430664062\n",
            "Training:  Iteration No:  196 Worker Num:  7 \n",
            " Loss:  0.3243848383426666\n",
            "Test:  Iteration No:  196 \n",
            " Loss:  0.237774093552858\n",
            "Test accuracy:  92.96\n",
            "Training:  Iteration No:  197 Worker Num:  0 \n",
            " Loss:  0.20414721965789795\n",
            "Training:  Iteration No:  197 Worker Num:  1 \n",
            " Loss:  0.304245263338089\n",
            "Training:  Iteration No:  197 Worker Num:  2 \n",
            " Loss:  0.3163997530937195\n",
            "Training:  Iteration No:  197 Worker Num:  3 \n",
            " Loss:  0.24161356687545776\n",
            "Training:  Iteration No:  197 Worker Num:  4 \n",
            " Loss:  0.24328577518463135\n",
            "Training:  Iteration No:  197 Worker Num:  5 \n",
            " Loss:  0.2328880876302719\n",
            "Training:  Iteration No:  197 Worker Num:  6 \n",
            " Loss:  0.27834823727607727\n",
            "Training:  Iteration No:  197 Worker Num:  7 \n",
            " Loss:  0.3047802150249481\n",
            "Test:  Iteration No:  197 \n",
            " Loss:  0.23597448330985593\n",
            "Test accuracy:  93.01\n",
            "Training:  Iteration No:  198 Worker Num:  0 \n",
            " Loss:  0.28612375259399414\n",
            "Training:  Iteration No:  198 Worker Num:  1 \n",
            " Loss:  0.4038633108139038\n",
            "Training:  Iteration No:  198 Worker Num:  2 \n",
            " Loss:  0.31701451539993286\n",
            "Training:  Iteration No:  198 Worker Num:  3 \n",
            " Loss:  0.3536507487297058\n",
            "Training:  Iteration No:  198 Worker Num:  4 \n",
            " Loss:  0.2572207748889923\n",
            "Training:  Iteration No:  198 Worker Num:  5 \n",
            " Loss:  0.3762394189834595\n",
            "Training:  Iteration No:  198 Worker Num:  6 \n",
            " Loss:  0.27850815653800964\n",
            "Training:  Iteration No:  198 Worker Num:  7 \n",
            " Loss:  0.3790661096572876\n",
            "Test:  Iteration No:  198 \n",
            " Loss:  0.2366394852111234\n",
            "Test accuracy:  93.02\n",
            "Training:  Iteration No:  199 Worker Num:  0 \n",
            " Loss:  0.38209789991378784\n",
            "Training:  Iteration No:  199 Worker Num:  1 \n",
            " Loss:  0.25120818614959717\n",
            "Training:  Iteration No:  199 Worker Num:  2 \n",
            " Loss:  0.2766495645046234\n",
            "Training:  Iteration No:  199 Worker Num:  3 \n",
            " Loss:  0.2244654893875122\n",
            "Training:  Iteration No:  199 Worker Num:  4 \n",
            " Loss:  0.45101669430732727\n",
            "Training:  Iteration No:  199 Worker Num:  5 \n",
            " Loss:  0.3407059609889984\n",
            "Training:  Iteration No:  199 Worker Num:  6 \n",
            " Loss:  0.30951061844825745\n",
            "Training:  Iteration No:  199 Worker Num:  7 \n",
            " Loss:  0.24084517359733582\n",
            "Test:  Iteration No:  199 \n",
            " Loss:  0.23314532378240477\n",
            "Test accuracy:  93.12\n",
            "Training:  Iteration No:  200 Worker Num:  0 \n",
            " Loss:  0.2606106698513031\n",
            "Training:  Iteration No:  200 Worker Num:  1 \n",
            " Loss:  0.2638276517391205\n",
            "Training:  Iteration No:  200 Worker Num:  2 \n",
            " Loss:  0.24452349543571472\n",
            "Training:  Iteration No:  200 Worker Num:  3 \n",
            " Loss:  0.2781224250793457\n",
            "Training:  Iteration No:  200 Worker Num:  4 \n",
            " Loss:  0.26005446910858154\n",
            "Training:  Iteration No:  200 Worker Num:  5 \n",
            " Loss:  0.2711339592933655\n",
            "Training:  Iteration No:  200 Worker Num:  6 \n",
            " Loss:  0.19459442794322968\n",
            "Training:  Iteration No:  200 Worker Num:  7 \n",
            " Loss:  0.4106864333152771\n",
            "Test:  Iteration No:  200 \n",
            " Loss:  0.2364717985656631\n",
            "Test accuracy:  93.25\n",
            "Training:  Iteration No:  201 Worker Num:  0 \n",
            " Loss:  0.3711134195327759\n",
            "Training:  Iteration No:  201 Worker Num:  1 \n",
            " Loss:  0.21788163483142853\n",
            "Training:  Iteration No:  201 Worker Num:  2 \n",
            " Loss:  0.20602941513061523\n",
            "Training:  Iteration No:  201 Worker Num:  3 \n",
            " Loss:  0.30603620409965515\n",
            "Training:  Iteration No:  201 Worker Num:  4 \n",
            " Loss:  0.23494796454906464\n",
            "Training:  Iteration No:  201 Worker Num:  5 \n",
            " Loss:  0.38195574283599854\n",
            "Training:  Iteration No:  201 Worker Num:  6 \n",
            " Loss:  0.2013361155986786\n",
            "Training:  Iteration No:  201 Worker Num:  7 \n",
            " Loss:  0.23586644232273102\n",
            "Test:  Iteration No:  201 \n",
            " Loss:  0.23288705750498212\n",
            "Test accuracy:  93.23\n",
            "Training:  Iteration No:  202 Worker Num:  0 \n",
            " Loss:  0.15405425429344177\n",
            "Training:  Iteration No:  202 Worker Num:  1 \n",
            " Loss:  0.2974170744419098\n",
            "Training:  Iteration No:  202 Worker Num:  2 \n",
            " Loss:  0.16158156096935272\n",
            "Training:  Iteration No:  202 Worker Num:  3 \n",
            " Loss:  0.32110297679901123\n",
            "Training:  Iteration No:  202 Worker Num:  4 \n",
            " Loss:  0.2572326064109802\n",
            "Training:  Iteration No:  202 Worker Num:  5 \n",
            " Loss:  0.35653141140937805\n",
            "Training:  Iteration No:  202 Worker Num:  6 \n",
            " Loss:  0.3670156002044678\n",
            "Training:  Iteration No:  202 Worker Num:  7 \n",
            " Loss:  0.27769967913627625\n",
            "Test:  Iteration No:  202 \n",
            " Loss:  0.22986661902145494\n",
            "Test accuracy:  93.15\n",
            "Training:  Iteration No:  203 Worker Num:  0 \n",
            " Loss:  0.3205564320087433\n",
            "Training:  Iteration No:  203 Worker Num:  1 \n",
            " Loss:  0.26851263642311096\n",
            "Training:  Iteration No:  203 Worker Num:  2 \n",
            " Loss:  0.2951415777206421\n",
            "Training:  Iteration No:  203 Worker Num:  3 \n",
            " Loss:  0.25881150364875793\n",
            "Training:  Iteration No:  203 Worker Num:  4 \n",
            " Loss:  0.2614349126815796\n",
            "Training:  Iteration No:  203 Worker Num:  5 \n",
            " Loss:  0.3407707214355469\n",
            "Training:  Iteration No:  203 Worker Num:  6 \n",
            " Loss:  0.3079952001571655\n",
            "Training:  Iteration No:  203 Worker Num:  7 \n",
            " Loss:  0.3750729262828827\n",
            "Test:  Iteration No:  203 \n",
            " Loss:  0.22847210648882238\n",
            "Test accuracy:  93.48\n",
            "Training:  Iteration No:  204 Worker Num:  0 \n",
            " Loss:  0.3731122612953186\n",
            "Training:  Iteration No:  204 Worker Num:  1 \n",
            " Loss:  0.369094580411911\n",
            "Training:  Iteration No:  204 Worker Num:  2 \n",
            " Loss:  0.23578666150569916\n",
            "Training:  Iteration No:  204 Worker Num:  3 \n",
            " Loss:  0.43047788739204407\n",
            "Training:  Iteration No:  204 Worker Num:  4 \n",
            " Loss:  0.23608897626399994\n",
            "Training:  Iteration No:  204 Worker Num:  5 \n",
            " Loss:  0.25805380940437317\n",
            "Training:  Iteration No:  204 Worker Num:  6 \n",
            " Loss:  0.22291116416454315\n",
            "Training:  Iteration No:  204 Worker Num:  7 \n",
            " Loss:  0.19412599503993988\n",
            "Test:  Iteration No:  204 \n",
            " Loss:  0.22909520170356654\n",
            "Test accuracy:  93.16\n",
            "Training:  Iteration No:  205 Worker Num:  0 \n",
            " Loss:  0.2344311773777008\n",
            "Training:  Iteration No:  205 Worker Num:  1 \n",
            " Loss:  0.20261521637439728\n",
            "Training:  Iteration No:  205 Worker Num:  2 \n",
            " Loss:  0.20720870792865753\n",
            "Training:  Iteration No:  205 Worker Num:  3 \n",
            " Loss:  0.23284679651260376\n",
            "Training:  Iteration No:  205 Worker Num:  4 \n",
            " Loss:  0.3466201722621918\n",
            "Training:  Iteration No:  205 Worker Num:  5 \n",
            " Loss:  0.31335070729255676\n",
            "Training:  Iteration No:  205 Worker Num:  6 \n",
            " Loss:  0.2608640491962433\n",
            "Training:  Iteration No:  205 Worker Num:  7 \n",
            " Loss:  0.2842930555343628\n",
            "Test:  Iteration No:  205 \n",
            " Loss:  0.22581623225838324\n",
            "Test accuracy:  93.39\n",
            "Training:  Iteration No:  206 Worker Num:  0 \n",
            " Loss:  0.27116912603378296\n",
            "Training:  Iteration No:  206 Worker Num:  1 \n",
            " Loss:  0.3801117539405823\n",
            "Training:  Iteration No:  206 Worker Num:  2 \n",
            " Loss:  0.2710508108139038\n",
            "Training:  Iteration No:  206 Worker Num:  3 \n",
            " Loss:  0.1709665060043335\n",
            "Training:  Iteration No:  206 Worker Num:  4 \n",
            " Loss:  0.2766070067882538\n",
            "Training:  Iteration No:  206 Worker Num:  5 \n",
            " Loss:  0.2559773325920105\n",
            "Training:  Iteration No:  206 Worker Num:  6 \n",
            " Loss:  0.32709193229675293\n",
            "Training:  Iteration No:  206 Worker Num:  7 \n",
            " Loss:  0.27480804920196533\n",
            "Test:  Iteration No:  206 \n",
            " Loss:  0.22740764160275082\n",
            "Test accuracy:  93.31\n",
            "Training:  Iteration No:  207 Worker Num:  0 \n",
            " Loss:  0.3450200855731964\n",
            "Training:  Iteration No:  207 Worker Num:  1 \n",
            " Loss:  0.24354103207588196\n",
            "Training:  Iteration No:  207 Worker Num:  2 \n",
            " Loss:  0.3160598576068878\n",
            "Training:  Iteration No:  207 Worker Num:  3 \n",
            " Loss:  0.1802925169467926\n",
            "Training:  Iteration No:  207 Worker Num:  4 \n",
            " Loss:  0.24991342425346375\n",
            "Training:  Iteration No:  207 Worker Num:  5 \n",
            " Loss:  0.20427487790584564\n",
            "Training:  Iteration No:  207 Worker Num:  6 \n",
            " Loss:  0.25760960578918457\n",
            "Training:  Iteration No:  207 Worker Num:  7 \n",
            " Loss:  0.31971579790115356\n",
            "Test:  Iteration No:  207 \n",
            " Loss:  0.22542267252655723\n",
            "Test accuracy:  93.43\n",
            "Training:  Iteration No:  208 Worker Num:  0 \n",
            " Loss:  0.31748083233833313\n",
            "Training:  Iteration No:  208 Worker Num:  1 \n",
            " Loss:  0.39780130982398987\n",
            "Training:  Iteration No:  208 Worker Num:  2 \n",
            " Loss:  0.2818252146244049\n",
            "Training:  Iteration No:  208 Worker Num:  3 \n",
            " Loss:  0.21579895913600922\n",
            "Training:  Iteration No:  208 Worker Num:  4 \n",
            " Loss:  0.2668178677558899\n",
            "Training:  Iteration No:  208 Worker Num:  5 \n",
            " Loss:  0.18864494562149048\n",
            "Training:  Iteration No:  208 Worker Num:  6 \n",
            " Loss:  0.36082252860069275\n",
            "Training:  Iteration No:  208 Worker Num:  7 \n",
            " Loss:  0.19722317159175873\n",
            "Test:  Iteration No:  208 \n",
            " Loss:  0.22516331666066677\n",
            "Test accuracy:  93.4\n",
            "Training:  Iteration No:  209 Worker Num:  0 \n",
            " Loss:  0.24723337590694427\n",
            "Training:  Iteration No:  209 Worker Num:  1 \n",
            " Loss:  0.35926640033721924\n",
            "Training:  Iteration No:  209 Worker Num:  2 \n",
            " Loss:  0.44002705812454224\n",
            "Training:  Iteration No:  209 Worker Num:  3 \n",
            " Loss:  0.3034802973270416\n",
            "Training:  Iteration No:  209 Worker Num:  4 \n",
            " Loss:  0.2842257022857666\n",
            "Training:  Iteration No:  209 Worker Num:  5 \n",
            " Loss:  0.38210296630859375\n",
            "Training:  Iteration No:  209 Worker Num:  6 \n",
            " Loss:  0.3432461619377136\n",
            "Training:  Iteration No:  209 Worker Num:  7 \n",
            " Loss:  0.2714956998825073\n",
            "Test:  Iteration No:  209 \n",
            " Loss:  0.22617545578911713\n",
            "Test accuracy:  93.34\n",
            "Training:  Iteration No:  210 Worker Num:  0 \n",
            " Loss:  0.2851185202598572\n",
            "Training:  Iteration No:  210 Worker Num:  1 \n",
            " Loss:  0.21988895535469055\n",
            "Training:  Iteration No:  210 Worker Num:  2 \n",
            " Loss:  0.3188316226005554\n",
            "Training:  Iteration No:  210 Worker Num:  3 \n",
            " Loss:  0.18014778196811676\n",
            "Training:  Iteration No:  210 Worker Num:  4 \n",
            " Loss:  0.2710418403148651\n",
            "Training:  Iteration No:  210 Worker Num:  5 \n",
            " Loss:  0.2738771140575409\n",
            "Training:  Iteration No:  210 Worker Num:  6 \n",
            " Loss:  0.4161355793476105\n",
            "Training:  Iteration No:  210 Worker Num:  7 \n",
            " Loss:  0.34374091029167175\n",
            "Test:  Iteration No:  210 \n",
            " Loss:  0.2227540728105586\n",
            "Test accuracy:  93.52\n",
            "Training:  Iteration No:  211 Worker Num:  0 \n",
            " Loss:  0.2562011778354645\n",
            "Training:  Iteration No:  211 Worker Num:  1 \n",
            " Loss:  0.23660938441753387\n",
            "Training:  Iteration No:  211 Worker Num:  2 \n",
            " Loss:  0.23797360062599182\n",
            "Training:  Iteration No:  211 Worker Num:  3 \n",
            " Loss:  0.26760807633399963\n",
            "Training:  Iteration No:  211 Worker Num:  4 \n",
            " Loss:  0.2961738407611847\n",
            "Training:  Iteration No:  211 Worker Num:  5 \n",
            " Loss:  0.304197758436203\n",
            "Training:  Iteration No:  211 Worker Num:  6 \n",
            " Loss:  0.26101112365722656\n",
            "Training:  Iteration No:  211 Worker Num:  7 \n",
            " Loss:  0.2330004870891571\n",
            "Test:  Iteration No:  211 \n",
            " Loss:  0.22108695215156562\n",
            "Test accuracy:  93.57\n",
            "Training:  Iteration No:  212 Worker Num:  0 \n",
            " Loss:  0.2399839162826538\n",
            "Training:  Iteration No:  212 Worker Num:  1 \n",
            " Loss:  0.25048381090164185\n",
            "Training:  Iteration No:  212 Worker Num:  2 \n",
            " Loss:  0.3229406476020813\n",
            "Training:  Iteration No:  212 Worker Num:  3 \n",
            " Loss:  0.31684648990631104\n",
            "Training:  Iteration No:  212 Worker Num:  4 \n",
            " Loss:  0.28523460030555725\n",
            "Training:  Iteration No:  212 Worker Num:  5 \n",
            " Loss:  0.3127688765525818\n",
            "Training:  Iteration No:  212 Worker Num:  6 \n",
            " Loss:  0.2786630392074585\n",
            "Training:  Iteration No:  212 Worker Num:  7 \n",
            " Loss:  0.29490742087364197\n",
            "Test:  Iteration No:  212 \n",
            " Loss:  0.2210986561059386\n",
            "Test accuracy:  93.62\n",
            "Training:  Iteration No:  213 Worker Num:  0 \n",
            " Loss:  0.3062090277671814\n",
            "Training:  Iteration No:  213 Worker Num:  1 \n",
            " Loss:  0.19824208319187164\n",
            "Training:  Iteration No:  213 Worker Num:  2 \n",
            " Loss:  0.2299650013446808\n",
            "Training:  Iteration No:  213 Worker Num:  3 \n",
            " Loss:  0.21900714933872223\n",
            "Training:  Iteration No:  213 Worker Num:  4 \n",
            " Loss:  0.30258435010910034\n",
            "Training:  Iteration No:  213 Worker Num:  5 \n",
            " Loss:  0.3797124922275543\n",
            "Training:  Iteration No:  213 Worker Num:  6 \n",
            " Loss:  0.23636198043823242\n",
            "Training:  Iteration No:  213 Worker Num:  7 \n",
            " Loss:  0.2921163737773895\n",
            "Test:  Iteration No:  213 \n",
            " Loss:  0.21852531332428318\n",
            "Test accuracy:  93.46\n",
            "Training:  Iteration No:  214 Worker Num:  0 \n",
            " Loss:  0.33896487951278687\n",
            "Training:  Iteration No:  214 Worker Num:  1 \n",
            " Loss:  0.2996441125869751\n",
            "Training:  Iteration No:  214 Worker Num:  2 \n",
            " Loss:  0.24007122218608856\n",
            "Training:  Iteration No:  214 Worker Num:  3 \n",
            " Loss:  0.32154351472854614\n",
            "Training:  Iteration No:  214 Worker Num:  4 \n",
            " Loss:  0.28043490648269653\n",
            "Training:  Iteration No:  214 Worker Num:  5 \n",
            " Loss:  0.16966547071933746\n",
            "Training:  Iteration No:  214 Worker Num:  6 \n",
            " Loss:  0.26499250531196594\n",
            "Training:  Iteration No:  214 Worker Num:  7 \n",
            " Loss:  0.31931206583976746\n",
            "Test:  Iteration No:  214 \n",
            " Loss:  0.22084318575319611\n",
            "Test accuracy:  93.57\n",
            "Training:  Iteration No:  215 Worker Num:  0 \n",
            " Loss:  0.2369825690984726\n",
            "Training:  Iteration No:  215 Worker Num:  1 \n",
            " Loss:  0.29724156856536865\n",
            "Training:  Iteration No:  215 Worker Num:  2 \n",
            " Loss:  0.18487559258937836\n",
            "Training:  Iteration No:  215 Worker Num:  3 \n",
            " Loss:  0.22775733470916748\n",
            "Training:  Iteration No:  215 Worker Num:  4 \n",
            " Loss:  0.2952687442302704\n",
            "Training:  Iteration No:  215 Worker Num:  5 \n",
            " Loss:  0.22156229615211487\n",
            "Training:  Iteration No:  215 Worker Num:  6 \n",
            " Loss:  0.358039528131485\n",
            "Training:  Iteration No:  215 Worker Num:  7 \n",
            " Loss:  0.2777031660079956\n",
            "Test:  Iteration No:  215 \n",
            " Loss:  0.21625224304020027\n",
            "Test accuracy:  93.62\n",
            "Training:  Iteration No:  216 Worker Num:  0 \n",
            " Loss:  0.1942766308784485\n",
            "Training:  Iteration No:  216 Worker Num:  1 \n",
            " Loss:  0.39998406171798706\n",
            "Training:  Iteration No:  216 Worker Num:  2 \n",
            " Loss:  0.3452945649623871\n",
            "Training:  Iteration No:  216 Worker Num:  3 \n",
            " Loss:  0.24301035702228546\n",
            "Training:  Iteration No:  216 Worker Num:  4 \n",
            " Loss:  0.2662804126739502\n",
            "Training:  Iteration No:  216 Worker Num:  5 \n",
            " Loss:  0.2230527698993683\n",
            "Training:  Iteration No:  216 Worker Num:  6 \n",
            " Loss:  0.21915817260742188\n",
            "Training:  Iteration No:  216 Worker Num:  7 \n",
            " Loss:  0.25008660554885864\n",
            "Test:  Iteration No:  216 \n",
            " Loss:  0.216526894087467\n",
            "Test accuracy:  93.51\n",
            "Training:  Iteration No:  217 Worker Num:  0 \n",
            " Loss:  0.2993128001689911\n",
            "Training:  Iteration No:  217 Worker Num:  1 \n",
            " Loss:  0.26161062717437744\n",
            "Training:  Iteration No:  217 Worker Num:  2 \n",
            " Loss:  0.28046467900276184\n",
            "Training:  Iteration No:  217 Worker Num:  3 \n",
            " Loss:  0.20829802751541138\n",
            "Training:  Iteration No:  217 Worker Num:  4 \n",
            " Loss:  0.25686392188072205\n",
            "Training:  Iteration No:  217 Worker Num:  5 \n",
            " Loss:  0.29804420471191406\n",
            "Training:  Iteration No:  217 Worker Num:  6 \n",
            " Loss:  0.23353661596775055\n",
            "Training:  Iteration No:  217 Worker Num:  7 \n",
            " Loss:  0.24781960248947144\n",
            "Test:  Iteration No:  217 \n",
            " Loss:  0.2198585922700129\n",
            "Test accuracy:  93.4\n",
            "Training:  Iteration No:  218 Worker Num:  0 \n",
            " Loss:  0.19125865399837494\n",
            "Training:  Iteration No:  218 Worker Num:  1 \n",
            " Loss:  0.2426379770040512\n",
            "Training:  Iteration No:  218 Worker Num:  2 \n",
            " Loss:  0.40050625801086426\n",
            "Training:  Iteration No:  218 Worker Num:  3 \n",
            " Loss:  0.22453010082244873\n",
            "Training:  Iteration No:  218 Worker Num:  4 \n",
            " Loss:  0.19892965257167816\n",
            "Training:  Iteration No:  218 Worker Num:  5 \n",
            " Loss:  0.19424019753932953\n",
            "Training:  Iteration No:  218 Worker Num:  6 \n",
            " Loss:  0.23174148797988892\n",
            "Training:  Iteration No:  218 Worker Num:  7 \n",
            " Loss:  0.26584675908088684\n",
            "Test:  Iteration No:  218 \n",
            " Loss:  0.22398972422874805\n",
            "Test accuracy:  93.57\n",
            "Training:  Iteration No:  219 Worker Num:  0 \n",
            " Loss:  0.2659667730331421\n",
            "Training:  Iteration No:  219 Worker Num:  1 \n",
            " Loss:  0.3558836579322815\n",
            "Training:  Iteration No:  219 Worker Num:  2 \n",
            " Loss:  0.3148588538169861\n",
            "Training:  Iteration No:  219 Worker Num:  3 \n",
            " Loss:  0.3407825529575348\n",
            "Training:  Iteration No:  219 Worker Num:  4 \n",
            " Loss:  0.1996450126171112\n",
            "Training:  Iteration No:  219 Worker Num:  5 \n",
            " Loss:  0.42637547850608826\n",
            "Training:  Iteration No:  219 Worker Num:  6 \n",
            " Loss:  0.41311976313591003\n",
            "Training:  Iteration No:  219 Worker Num:  7 \n",
            " Loss:  0.24072647094726562\n",
            "Test:  Iteration No:  219 \n",
            " Loss:  0.21785796747271774\n",
            "Test accuracy:  93.61\n",
            "Training:  Iteration No:  220 Worker Num:  0 \n",
            " Loss:  0.34426647424697876\n",
            "Training:  Iteration No:  220 Worker Num:  1 \n",
            " Loss:  0.33629778027534485\n",
            "Training:  Iteration No:  220 Worker Num:  2 \n",
            " Loss:  0.38448935747146606\n",
            "Training:  Iteration No:  220 Worker Num:  3 \n",
            " Loss:  0.28576070070266724\n",
            "Training:  Iteration No:  220 Worker Num:  4 \n",
            " Loss:  0.24839141964912415\n",
            "Training:  Iteration No:  220 Worker Num:  5 \n",
            " Loss:  0.21616967022418976\n",
            "Training:  Iteration No:  220 Worker Num:  6 \n",
            " Loss:  0.13766959309577942\n",
            "Training:  Iteration No:  220 Worker Num:  7 \n",
            " Loss:  0.24351345002651215\n",
            "Test:  Iteration No:  220 \n",
            " Loss:  0.21321594403891625\n",
            "Test accuracy:  93.62\n",
            "Training:  Iteration No:  221 Worker Num:  0 \n",
            " Loss:  0.22379182279109955\n",
            "Training:  Iteration No:  221 Worker Num:  1 \n",
            " Loss:  0.22708244621753693\n",
            "Training:  Iteration No:  221 Worker Num:  2 \n",
            " Loss:  0.21093837916851044\n",
            "Training:  Iteration No:  221 Worker Num:  3 \n",
            " Loss:  0.27555495500564575\n",
            "Training:  Iteration No:  221 Worker Num:  4 \n",
            " Loss:  0.28891369700431824\n",
            "Training:  Iteration No:  221 Worker Num:  5 \n",
            " Loss:  0.3652641177177429\n",
            "Training:  Iteration No:  221 Worker Num:  6 \n",
            " Loss:  0.23039455711841583\n",
            "Training:  Iteration No:  221 Worker Num:  7 \n",
            " Loss:  0.22917868196964264\n",
            "Test:  Iteration No:  221 \n",
            " Loss:  0.21440084818514843\n",
            "Test accuracy:  93.72\n",
            "Training:  Iteration No:  222 Worker Num:  0 \n",
            " Loss:  0.2131938636302948\n",
            "Training:  Iteration No:  222 Worker Num:  1 \n",
            " Loss:  0.15874819457530975\n",
            "Training:  Iteration No:  222 Worker Num:  2 \n",
            " Loss:  0.23553816974163055\n",
            "Training:  Iteration No:  222 Worker Num:  3 \n",
            " Loss:  0.2767643332481384\n",
            "Training:  Iteration No:  222 Worker Num:  4 \n",
            " Loss:  0.3483012318611145\n",
            "Training:  Iteration No:  222 Worker Num:  5 \n",
            " Loss:  0.23192098736763\n",
            "Training:  Iteration No:  222 Worker Num:  6 \n",
            " Loss:  0.12847471237182617\n",
            "Training:  Iteration No:  222 Worker Num:  7 \n",
            " Loss:  0.1574402153491974\n",
            "Test:  Iteration No:  222 \n",
            " Loss:  0.21917943384121114\n",
            "Test accuracy:  93.15\n",
            "Training:  Iteration No:  223 Worker Num:  0 \n",
            " Loss:  0.25526028871536255\n",
            "Training:  Iteration No:  223 Worker Num:  1 \n",
            " Loss:  0.24802440404891968\n",
            "Training:  Iteration No:  223 Worker Num:  2 \n",
            " Loss:  0.35161295533180237\n",
            "Training:  Iteration No:  223 Worker Num:  3 \n",
            " Loss:  0.22059163451194763\n",
            "Training:  Iteration No:  223 Worker Num:  4 \n",
            " Loss:  0.20869044959545135\n",
            "Training:  Iteration No:  223 Worker Num:  5 \n",
            " Loss:  0.23127543926239014\n",
            "Training:  Iteration No:  223 Worker Num:  6 \n",
            " Loss:  0.35348090529441833\n",
            "Training:  Iteration No:  223 Worker Num:  7 \n",
            " Loss:  0.20971091091632843\n",
            "Test:  Iteration No:  223 \n",
            " Loss:  0.21316059804934112\n",
            "Test accuracy:  93.8\n",
            "Training:  Iteration No:  224 Worker Num:  0 \n",
            " Loss:  0.21669664978981018\n",
            "Training:  Iteration No:  224 Worker Num:  1 \n",
            " Loss:  0.29933643341064453\n",
            "Training:  Iteration No:  224 Worker Num:  2 \n",
            " Loss:  0.34410691261291504\n",
            "Training:  Iteration No:  224 Worker Num:  3 \n",
            " Loss:  0.1873398721218109\n",
            "Training:  Iteration No:  224 Worker Num:  4 \n",
            " Loss:  0.21334712207317352\n",
            "Training:  Iteration No:  224 Worker Num:  5 \n",
            " Loss:  0.37066733837127686\n",
            "Training:  Iteration No:  224 Worker Num:  6 \n",
            " Loss:  0.21181149780750275\n",
            "Training:  Iteration No:  224 Worker Num:  7 \n",
            " Loss:  0.19610044360160828\n",
            "Test:  Iteration No:  224 \n",
            " Loss:  0.2102635461082564\n",
            "Test accuracy:  93.96\n",
            "Training:  Iteration No:  225 Worker Num:  0 \n",
            " Loss:  0.31833261251449585\n",
            "Training:  Iteration No:  225 Worker Num:  1 \n",
            " Loss:  0.3171803951263428\n",
            "Training:  Iteration No:  225 Worker Num:  2 \n",
            " Loss:  0.22528208792209625\n",
            "Training:  Iteration No:  225 Worker Num:  3 \n",
            " Loss:  0.2512442469596863\n",
            "Training:  Iteration No:  225 Worker Num:  4 \n",
            " Loss:  0.34910082817077637\n",
            "Training:  Iteration No:  225 Worker Num:  5 \n",
            " Loss:  0.3509441614151001\n",
            "Training:  Iteration No:  225 Worker Num:  6 \n",
            " Loss:  0.3208799958229065\n",
            "Training:  Iteration No:  225 Worker Num:  7 \n",
            " Loss:  0.19097071886062622\n",
            "Test:  Iteration No:  225 \n",
            " Loss:  0.20964891402215896\n",
            "Test accuracy:  93.84\n",
            "Training:  Iteration No:  226 Worker Num:  0 \n",
            " Loss:  0.0818059891462326\n",
            "Training:  Iteration No:  226 Worker Num:  1 \n",
            " Loss:  0.22312287986278534\n",
            "Training:  Iteration No:  226 Worker Num:  2 \n",
            " Loss:  0.3140294551849365\n",
            "Training:  Iteration No:  226 Worker Num:  3 \n",
            " Loss:  0.19823071360588074\n",
            "Training:  Iteration No:  226 Worker Num:  4 \n",
            " Loss:  0.2952058017253876\n",
            "Training:  Iteration No:  226 Worker Num:  5 \n",
            " Loss:  0.2419046014547348\n",
            "Training:  Iteration No:  226 Worker Num:  6 \n",
            " Loss:  0.14872130751609802\n",
            "Training:  Iteration No:  226 Worker Num:  7 \n",
            " Loss:  0.27950307726860046\n",
            "Test:  Iteration No:  226 \n",
            " Loss:  0.2130126297308863\n",
            "Test accuracy:  93.64\n",
            "Training:  Iteration No:  227 Worker Num:  0 \n",
            " Loss:  0.24560129642486572\n",
            "Training:  Iteration No:  227 Worker Num:  1 \n",
            " Loss:  0.2716193199157715\n",
            "Training:  Iteration No:  227 Worker Num:  2 \n",
            " Loss:  0.1782202571630478\n",
            "Training:  Iteration No:  227 Worker Num:  3 \n",
            " Loss:  0.23260852694511414\n",
            "Training:  Iteration No:  227 Worker Num:  4 \n",
            " Loss:  0.2126847505569458\n",
            "Training:  Iteration No:  227 Worker Num:  5 \n",
            " Loss:  0.1751413494348526\n",
            "Training:  Iteration No:  227 Worker Num:  6 \n",
            " Loss:  0.1823570430278778\n",
            "Training:  Iteration No:  227 Worker Num:  7 \n",
            " Loss:  0.2181607037782669\n",
            "Test:  Iteration No:  227 \n",
            " Loss:  0.20915776949753112\n",
            "Test accuracy:  93.8\n",
            "Training:  Iteration No:  228 Worker Num:  0 \n",
            " Loss:  0.1889754831790924\n",
            "Training:  Iteration No:  228 Worker Num:  1 \n",
            " Loss:  0.23525308072566986\n",
            "Training:  Iteration No:  228 Worker Num:  2 \n",
            " Loss:  0.3940124809741974\n",
            "Training:  Iteration No:  228 Worker Num:  3 \n",
            " Loss:  0.21294346451759338\n",
            "Training:  Iteration No:  228 Worker Num:  4 \n",
            " Loss:  0.2927241921424866\n",
            "Training:  Iteration No:  228 Worker Num:  5 \n",
            " Loss:  0.3355039060115814\n",
            "Training:  Iteration No:  228 Worker Num:  6 \n",
            " Loss:  0.33070656657218933\n",
            "Training:  Iteration No:  228 Worker Num:  7 \n",
            " Loss:  0.3113904297351837\n",
            "Test:  Iteration No:  228 \n",
            " Loss:  0.20999628471658577\n",
            "Test accuracy:  93.87\n",
            "Training:  Iteration No:  229 Worker Num:  0 \n",
            " Loss:  0.1558358371257782\n",
            "Training:  Iteration No:  229 Worker Num:  1 \n",
            " Loss:  0.31205546855926514\n",
            "Training:  Iteration No:  229 Worker Num:  2 \n",
            " Loss:  0.38146159052848816\n",
            "Training:  Iteration No:  229 Worker Num:  3 \n",
            " Loss:  0.2699544429779053\n",
            "Training:  Iteration No:  229 Worker Num:  4 \n",
            " Loss:  0.30515238642692566\n",
            "Training:  Iteration No:  229 Worker Num:  5 \n",
            " Loss:  0.19690142571926117\n",
            "Training:  Iteration No:  229 Worker Num:  6 \n",
            " Loss:  0.3846738338470459\n",
            "Training:  Iteration No:  229 Worker Num:  7 \n",
            " Loss:  0.14017567038536072\n",
            "Test:  Iteration No:  229 \n",
            " Loss:  0.20882047201164916\n",
            "Test accuracy:  93.9\n",
            "Training:  Iteration No:  230 Worker Num:  0 \n",
            " Loss:  0.13309237360954285\n",
            "Training:  Iteration No:  230 Worker Num:  1 \n",
            " Loss:  0.1971011608839035\n",
            "Training:  Iteration No:  230 Worker Num:  2 \n",
            " Loss:  0.31764528155326843\n",
            "Training:  Iteration No:  230 Worker Num:  3 \n",
            " Loss:  0.34277892112731934\n",
            "Training:  Iteration No:  230 Worker Num:  4 \n",
            " Loss:  0.21970820426940918\n",
            "Training:  Iteration No:  230 Worker Num:  5 \n",
            " Loss:  0.22726799547672272\n",
            "Training:  Iteration No:  230 Worker Num:  6 \n",
            " Loss:  0.29590505361557007\n",
            "Training:  Iteration No:  230 Worker Num:  7 \n",
            " Loss:  0.2662297785282135\n",
            "Test:  Iteration No:  230 \n",
            " Loss:  0.20901705772627757\n",
            "Test accuracy:  93.95\n",
            "Training:  Iteration No:  231 Worker Num:  0 \n",
            " Loss:  0.268259197473526\n",
            "Training:  Iteration No:  231 Worker Num:  1 \n",
            " Loss:  0.2200392186641693\n",
            "Training:  Iteration No:  231 Worker Num:  2 \n",
            " Loss:  0.2163023054599762\n",
            "Training:  Iteration No:  231 Worker Num:  3 \n",
            " Loss:  0.24301709234714508\n",
            "Training:  Iteration No:  231 Worker Num:  4 \n",
            " Loss:  0.44627225399017334\n",
            "Training:  Iteration No:  231 Worker Num:  5 \n",
            " Loss:  0.2565615177154541\n",
            "Training:  Iteration No:  231 Worker Num:  6 \n",
            " Loss:  0.2893161475658417\n",
            "Training:  Iteration No:  231 Worker Num:  7 \n",
            " Loss:  0.23456799983978271\n",
            "Test:  Iteration No:  231 \n",
            " Loss:  0.20617351660952915\n",
            "Test accuracy:  93.94\n",
            "Training:  Iteration No:  232 Worker Num:  0 \n",
            " Loss:  0.33199402689933777\n",
            "Training:  Iteration No:  232 Worker Num:  1 \n",
            " Loss:  0.32105550169944763\n",
            "Training:  Iteration No:  232 Worker Num:  2 \n",
            " Loss:  0.41698887944221497\n",
            "Training:  Iteration No:  232 Worker Num:  3 \n",
            " Loss:  0.2186187505722046\n",
            "Training:  Iteration No:  232 Worker Num:  4 \n",
            " Loss:  0.25457093119621277\n",
            "Training:  Iteration No:  232 Worker Num:  5 \n",
            " Loss:  0.22957366704940796\n",
            "Training:  Iteration No:  232 Worker Num:  6 \n",
            " Loss:  0.25613975524902344\n",
            "Training:  Iteration No:  232 Worker Num:  7 \n",
            " Loss:  0.22915413975715637\n",
            "Test:  Iteration No:  232 \n",
            " Loss:  0.20364507484615227\n",
            "Test accuracy:  93.99\n",
            "Training:  Iteration No:  233 Worker Num:  0 \n",
            " Loss:  0.2527943253517151\n",
            "Training:  Iteration No:  233 Worker Num:  1 \n",
            " Loss:  0.39480891823768616\n",
            "Training:  Iteration No:  233 Worker Num:  2 \n",
            " Loss:  0.22958077490329742\n",
            "Training:  Iteration No:  233 Worker Num:  3 \n",
            " Loss:  0.27428293228149414\n",
            "Training:  Iteration No:  233 Worker Num:  4 \n",
            " Loss:  0.3186395466327667\n",
            "Training:  Iteration No:  233 Worker Num:  5 \n",
            " Loss:  0.2209271937608719\n",
            "Training:  Iteration No:  233 Worker Num:  6 \n",
            " Loss:  0.3203066289424896\n",
            "Training:  Iteration No:  233 Worker Num:  7 \n",
            " Loss:  0.23360958695411682\n",
            "Test:  Iteration No:  233 \n",
            " Loss:  0.20383356930098578\n",
            "Test accuracy:  94.28\n",
            "Training:  Iteration No:  234 Worker Num:  0 \n",
            " Loss:  0.2932593822479248\n",
            "Training:  Iteration No:  234 Worker Num:  1 \n",
            " Loss:  0.16447535157203674\n",
            "Training:  Iteration No:  234 Worker Num:  2 \n",
            " Loss:  0.2242933064699173\n",
            "Training:  Iteration No:  234 Worker Num:  3 \n",
            " Loss:  0.26022469997406006\n",
            "Training:  Iteration No:  234 Worker Num:  4 \n",
            " Loss:  0.28569158911705017\n",
            "Training:  Iteration No:  234 Worker Num:  5 \n",
            " Loss:  0.30503326654434204\n",
            "Training:  Iteration No:  234 Worker Num:  6 \n",
            " Loss:  0.21080610156059265\n",
            "Training:  Iteration No:  234 Worker Num:  7 \n",
            " Loss:  0.1392894983291626\n",
            "Test:  Iteration No:  234 \n",
            " Loss:  0.20315012133008317\n",
            "Test accuracy:  94.12\n",
            "Training:  Iteration No:  235 Worker Num:  0 \n",
            " Loss:  0.21112823486328125\n",
            "Training:  Iteration No:  235 Worker Num:  1 \n",
            " Loss:  0.12038703262805939\n",
            "Training:  Iteration No:  235 Worker Num:  2 \n",
            " Loss:  0.27205026149749756\n",
            "Training:  Iteration No:  235 Worker Num:  3 \n",
            " Loss:  0.24724791944026947\n",
            "Training:  Iteration No:  235 Worker Num:  4 \n",
            " Loss:  0.14262041449546814\n",
            "Training:  Iteration No:  235 Worker Num:  5 \n",
            " Loss:  0.16469706594944\n",
            "Training:  Iteration No:  235 Worker Num:  6 \n",
            " Loss:  0.23936517536640167\n",
            "Training:  Iteration No:  235 Worker Num:  7 \n",
            " Loss:  0.28531092405319214\n",
            "Test:  Iteration No:  235 \n",
            " Loss:  0.20805196117468272\n",
            "Test accuracy:  93.8\n",
            "Training:  Iteration No:  236 Worker Num:  0 \n",
            " Loss:  0.18860329687595367\n",
            "Training:  Iteration No:  236 Worker Num:  1 \n",
            " Loss:  0.236243337392807\n",
            "Training:  Iteration No:  236 Worker Num:  2 \n",
            " Loss:  0.19656983017921448\n",
            "Training:  Iteration No:  236 Worker Num:  3 \n",
            " Loss:  0.2292531281709671\n",
            "Training:  Iteration No:  236 Worker Num:  4 \n",
            " Loss:  0.3362472653388977\n",
            "Training:  Iteration No:  236 Worker Num:  5 \n",
            " Loss:  0.2189968377351761\n",
            "Training:  Iteration No:  236 Worker Num:  6 \n",
            " Loss:  0.23466899991035461\n",
            "Training:  Iteration No:  236 Worker Num:  7 \n",
            " Loss:  0.2554531395435333\n",
            "Test:  Iteration No:  236 \n",
            " Loss:  0.20288881974271203\n",
            "Test accuracy:  94.25\n",
            "Training:  Iteration No:  237 Worker Num:  0 \n",
            " Loss:  0.25093236565589905\n",
            "Training:  Iteration No:  237 Worker Num:  1 \n",
            " Loss:  0.3024601340293884\n",
            "Training:  Iteration No:  237 Worker Num:  2 \n",
            " Loss:  0.22644340991973877\n",
            "Training:  Iteration No:  237 Worker Num:  3 \n",
            " Loss:  0.1695828139781952\n",
            "Training:  Iteration No:  237 Worker Num:  4 \n",
            " Loss:  0.3498164415359497\n",
            "Training:  Iteration No:  237 Worker Num:  5 \n",
            " Loss:  0.2171507030725479\n",
            "Training:  Iteration No:  237 Worker Num:  6 \n",
            " Loss:  0.18902283906936646\n",
            "Training:  Iteration No:  237 Worker Num:  7 \n",
            " Loss:  0.28548264503479004\n",
            "Test:  Iteration No:  237 \n",
            " Loss:  0.19864414481422568\n",
            "Test accuracy:  94.21\n",
            "Training:  Iteration No:  238 Worker Num:  0 \n",
            " Loss:  0.2665090560913086\n",
            "Training:  Iteration No:  238 Worker Num:  1 \n",
            " Loss:  0.30090847611427307\n",
            "Training:  Iteration No:  238 Worker Num:  2 \n",
            " Loss:  0.33132225275039673\n",
            "Training:  Iteration No:  238 Worker Num:  3 \n",
            " Loss:  0.2710150182247162\n",
            "Training:  Iteration No:  238 Worker Num:  4 \n",
            " Loss:  0.14342190325260162\n",
            "Training:  Iteration No:  238 Worker Num:  5 \n",
            " Loss:  0.2608891725540161\n",
            "Training:  Iteration No:  238 Worker Num:  6 \n",
            " Loss:  0.3269541561603546\n",
            "Training:  Iteration No:  238 Worker Num:  7 \n",
            " Loss:  0.1605689972639084\n",
            "Test:  Iteration No:  238 \n",
            " Loss:  0.20208198756356782\n",
            "Test accuracy:  94.0\n",
            "Training:  Iteration No:  239 Worker Num:  0 \n",
            " Loss:  0.2516399025917053\n",
            "Training:  Iteration No:  239 Worker Num:  1 \n",
            " Loss:  0.2308010458946228\n",
            "Training:  Iteration No:  239 Worker Num:  2 \n",
            " Loss:  0.20647329092025757\n",
            "Training:  Iteration No:  239 Worker Num:  3 \n",
            " Loss:  0.4119589328765869\n",
            "Training:  Iteration No:  239 Worker Num:  4 \n",
            " Loss:  0.24760857224464417\n",
            "Training:  Iteration No:  239 Worker Num:  5 \n",
            " Loss:  0.243272602558136\n",
            "Training:  Iteration No:  239 Worker Num:  6 \n",
            " Loss:  0.2560011148452759\n",
            "Training:  Iteration No:  239 Worker Num:  7 \n",
            " Loss:  0.1698186993598938\n",
            "Test:  Iteration No:  239 \n",
            " Loss:  0.19724320391593855\n",
            "Test accuracy:  94.35\n",
            "Training:  Iteration No:  240 Worker Num:  0 \n",
            " Loss:  0.2540471851825714\n",
            "Training:  Iteration No:  240 Worker Num:  1 \n",
            " Loss:  0.24549637734889984\n",
            "Training:  Iteration No:  240 Worker Num:  2 \n",
            " Loss:  0.17235301434993744\n",
            "Training:  Iteration No:  240 Worker Num:  3 \n",
            " Loss:  0.19460800290107727\n",
            "Training:  Iteration No:  240 Worker Num:  4 \n",
            " Loss:  0.13940659165382385\n",
            "Training:  Iteration No:  240 Worker Num:  5 \n",
            " Loss:  0.20261049270629883\n",
            "Training:  Iteration No:  240 Worker Num:  6 \n",
            " Loss:  0.2082490473985672\n",
            "Training:  Iteration No:  240 Worker Num:  7 \n",
            " Loss:  0.29324352741241455\n",
            "Test:  Iteration No:  240 \n",
            " Loss:  0.19898204984072643\n",
            "Test accuracy:  94.37\n",
            "Training:  Iteration No:  241 Worker Num:  0 \n",
            " Loss:  0.3205924928188324\n",
            "Training:  Iteration No:  241 Worker Num:  1 \n",
            " Loss:  0.112797811627388\n",
            "Training:  Iteration No:  241 Worker Num:  2 \n",
            " Loss:  0.18174214661121368\n",
            "Training:  Iteration No:  241 Worker Num:  3 \n",
            " Loss:  0.3582822382450104\n",
            "Training:  Iteration No:  241 Worker Num:  4 \n",
            " Loss:  0.20528005063533783\n",
            "Training:  Iteration No:  241 Worker Num:  5 \n",
            " Loss:  0.25632864236831665\n",
            "Training:  Iteration No:  241 Worker Num:  6 \n",
            " Loss:  0.3109981417655945\n",
            "Training:  Iteration No:  241 Worker Num:  7 \n",
            " Loss:  0.16912788152694702\n",
            "Test:  Iteration No:  241 \n",
            " Loss:  0.2012480072960069\n",
            "Test accuracy:  94.15\n",
            "Training:  Iteration No:  242 Worker Num:  0 \n",
            " Loss:  0.3309481739997864\n",
            "Training:  Iteration No:  242 Worker Num:  1 \n",
            " Loss:  0.15710194408893585\n",
            "Training:  Iteration No:  242 Worker Num:  2 \n",
            " Loss:  0.2818494737148285\n",
            "Training:  Iteration No:  242 Worker Num:  3 \n",
            " Loss:  0.23086094856262207\n",
            "Training:  Iteration No:  242 Worker Num:  4 \n",
            " Loss:  0.2403605431318283\n",
            "Training:  Iteration No:  242 Worker Num:  5 \n",
            " Loss:  0.23583045601844788\n",
            "Training:  Iteration No:  242 Worker Num:  6 \n",
            " Loss:  0.27649205923080444\n",
            "Training:  Iteration No:  242 Worker Num:  7 \n",
            " Loss:  0.240631103515625\n",
            "Test:  Iteration No:  242 \n",
            " Loss:  0.2048102923539243\n",
            "Test accuracy:  94.09\n",
            "Training:  Iteration No:  243 Worker Num:  0 \n",
            " Loss:  0.20228587090969086\n",
            "Training:  Iteration No:  243 Worker Num:  1 \n",
            " Loss:  0.18537813425064087\n",
            "Training:  Iteration No:  243 Worker Num:  2 \n",
            " Loss:  0.19456765055656433\n",
            "Training:  Iteration No:  243 Worker Num:  3 \n",
            " Loss:  0.2515934109687805\n",
            "Training:  Iteration No:  243 Worker Num:  4 \n",
            " Loss:  0.1944393515586853\n",
            "Training:  Iteration No:  243 Worker Num:  5 \n",
            " Loss:  0.29110586643218994\n",
            "Training:  Iteration No:  243 Worker Num:  6 \n",
            " Loss:  0.20611582696437836\n",
            "Training:  Iteration No:  243 Worker Num:  7 \n",
            " Loss:  0.19301529228687286\n",
            "Test:  Iteration No:  243 \n",
            " Loss:  0.196162445553213\n",
            "Test accuracy:  94.48\n",
            "Training:  Iteration No:  244 Worker Num:  0 \n",
            " Loss:  0.19092638790607452\n",
            "Training:  Iteration No:  244 Worker Num:  1 \n",
            " Loss:  0.23079530894756317\n",
            "Training:  Iteration No:  244 Worker Num:  2 \n",
            " Loss:  0.15974625945091248\n",
            "Training:  Iteration No:  244 Worker Num:  3 \n",
            " Loss:  0.28166261315345764\n",
            "Training:  Iteration No:  244 Worker Num:  4 \n",
            " Loss:  0.24157282710075378\n",
            "Training:  Iteration No:  244 Worker Num:  5 \n",
            " Loss:  0.17041775584220886\n",
            "Training:  Iteration No:  244 Worker Num:  6 \n",
            " Loss:  0.21270491182804108\n",
            "Training:  Iteration No:  244 Worker Num:  7 \n",
            " Loss:  0.20393289625644684\n",
            "Test:  Iteration No:  244 \n",
            " Loss:  0.19594071564983717\n",
            "Test accuracy:  94.38\n",
            "Training:  Iteration No:  245 Worker Num:  0 \n",
            " Loss:  0.23034773766994476\n",
            "Training:  Iteration No:  245 Worker Num:  1 \n",
            " Loss:  0.21229667961597443\n",
            "Training:  Iteration No:  245 Worker Num:  2 \n",
            " Loss:  0.3356238901615143\n",
            "Training:  Iteration No:  245 Worker Num:  3 \n",
            " Loss:  0.23497986793518066\n",
            "Training:  Iteration No:  245 Worker Num:  4 \n",
            " Loss:  0.12132450193166733\n",
            "Training:  Iteration No:  245 Worker Num:  5 \n",
            " Loss:  0.2691248059272766\n",
            "Training:  Iteration No:  245 Worker Num:  6 \n",
            " Loss:  0.19333772361278534\n",
            "Training:  Iteration No:  245 Worker Num:  7 \n",
            " Loss:  0.22370605170726776\n",
            "Test:  Iteration No:  245 \n",
            " Loss:  0.19801119908313208\n",
            "Test accuracy:  94.21\n",
            "Training:  Iteration No:  246 Worker Num:  0 \n",
            " Loss:  0.2087048590183258\n",
            "Training:  Iteration No:  246 Worker Num:  1 \n",
            " Loss:  0.20696566998958588\n",
            "Training:  Iteration No:  246 Worker Num:  2 \n",
            " Loss:  0.2782456576824188\n",
            "Training:  Iteration No:  246 Worker Num:  3 \n",
            " Loss:  0.19064050912857056\n",
            "Training:  Iteration No:  246 Worker Num:  4 \n",
            " Loss:  0.2719704210758209\n",
            "Training:  Iteration No:  246 Worker Num:  5 \n",
            " Loss:  0.15362440049648285\n",
            "Training:  Iteration No:  246 Worker Num:  6 \n",
            " Loss:  0.2215663343667984\n",
            "Training:  Iteration No:  246 Worker Num:  7 \n",
            " Loss:  0.2818366289138794\n",
            "Test:  Iteration No:  246 \n",
            " Loss:  0.192851614466385\n",
            "Test accuracy:  94.32\n",
            "Training:  Iteration No:  247 Worker Num:  0 \n",
            " Loss:  0.2513616681098938\n",
            "Training:  Iteration No:  247 Worker Num:  1 \n",
            " Loss:  0.18140697479248047\n",
            "Training:  Iteration No:  247 Worker Num:  2 \n",
            " Loss:  0.1713632345199585\n",
            "Training:  Iteration No:  247 Worker Num:  3 \n",
            " Loss:  0.3905133306980133\n",
            "Training:  Iteration No:  247 Worker Num:  4 \n",
            " Loss:  0.28588229417800903\n",
            "Training:  Iteration No:  247 Worker Num:  5 \n",
            " Loss:  0.25338664650917053\n",
            "Training:  Iteration No:  247 Worker Num:  6 \n",
            " Loss:  0.15388938784599304\n",
            "Training:  Iteration No:  247 Worker Num:  7 \n",
            " Loss:  0.2403159737586975\n",
            "Test:  Iteration No:  247 \n",
            " Loss:  0.1944370329191413\n",
            "Test accuracy:  94.39\n",
            "Training:  Iteration No:  248 Worker Num:  0 \n",
            " Loss:  0.27843260765075684\n",
            "Training:  Iteration No:  248 Worker Num:  1 \n",
            " Loss:  0.1853436976671219\n",
            "Training:  Iteration No:  248 Worker Num:  2 \n",
            " Loss:  0.187388077378273\n",
            "Training:  Iteration No:  248 Worker Num:  3 \n",
            " Loss:  0.26988840103149414\n",
            "Training:  Iteration No:  248 Worker Num:  4 \n",
            " Loss:  0.1399107724428177\n",
            "Training:  Iteration No:  248 Worker Num:  5 \n",
            " Loss:  0.28708988428115845\n",
            "Training:  Iteration No:  248 Worker Num:  6 \n",
            " Loss:  0.246359720826149\n",
            "Training:  Iteration No:  248 Worker Num:  7 \n",
            " Loss:  0.16152316331863403\n",
            "Test:  Iteration No:  248 \n",
            " Loss:  0.1940416572851282\n",
            "Test accuracy:  94.23\n",
            "Training:  Iteration No:  249 Worker Num:  0 \n",
            " Loss:  0.23530007898807526\n",
            "Training:  Iteration No:  249 Worker Num:  1 \n",
            " Loss:  0.2005598098039627\n",
            "Training:  Iteration No:  249 Worker Num:  2 \n",
            " Loss:  0.222004234790802\n",
            "Training:  Iteration No:  249 Worker Num:  3 \n",
            " Loss:  0.20803262293338776\n",
            "Training:  Iteration No:  249 Worker Num:  4 \n",
            " Loss:  0.14332611858844757\n",
            "Training:  Iteration No:  249 Worker Num:  5 \n",
            " Loss:  0.15673016011714935\n",
            "Training:  Iteration No:  249 Worker Num:  6 \n",
            " Loss:  0.24267469346523285\n",
            "Training:  Iteration No:  249 Worker Num:  7 \n",
            " Loss:  0.2880758047103882\n",
            "Test:  Iteration No:  249 \n",
            " Loss:  0.18970138691601496\n",
            "Test accuracy:  94.47\n",
            "Training:  Iteration No:  250 Worker Num:  0 \n",
            " Loss:  0.17725564539432526\n",
            "Training:  Iteration No:  250 Worker Num:  1 \n",
            " Loss:  0.18047945201396942\n",
            "Training:  Iteration No:  250 Worker Num:  2 \n",
            " Loss:  0.20707719027996063\n",
            "Training:  Iteration No:  250 Worker Num:  3 \n",
            " Loss:  0.16928918659687042\n",
            "Training:  Iteration No:  250 Worker Num:  4 \n",
            " Loss:  0.19156020879745483\n",
            "Training:  Iteration No:  250 Worker Num:  5 \n",
            " Loss:  0.17548136413097382\n",
            "Training:  Iteration No:  250 Worker Num:  6 \n",
            " Loss:  0.2058723419904709\n",
            "Training:  Iteration No:  250 Worker Num:  7 \n",
            " Loss:  0.3217168152332306\n",
            "Test:  Iteration No:  250 \n",
            " Loss:  0.19200595701702788\n",
            "Test accuracy:  94.44\n",
            "Training:  Iteration No:  251 Worker Num:  0 \n",
            " Loss:  0.3260306119918823\n",
            "Training:  Iteration No:  251 Worker Num:  1 \n",
            " Loss:  0.24641700088977814\n",
            "Training:  Iteration No:  251 Worker Num:  2 \n",
            " Loss:  0.191038116812706\n",
            "Training:  Iteration No:  251 Worker Num:  3 \n",
            " Loss:  0.13039590418338776\n",
            "Training:  Iteration No:  251 Worker Num:  4 \n",
            " Loss:  0.28923141956329346\n",
            "Training:  Iteration No:  251 Worker Num:  5 \n",
            " Loss:  0.2800656855106354\n",
            "Training:  Iteration No:  251 Worker Num:  6 \n",
            " Loss:  0.20350635051727295\n",
            "Training:  Iteration No:  251 Worker Num:  7 \n",
            " Loss:  0.1655452698469162\n",
            "Test:  Iteration No:  251 \n",
            " Loss:  0.18810491648184346\n",
            "Test accuracy:  94.5\n",
            "Training:  Iteration No:  252 Worker Num:  0 \n",
            " Loss:  0.33812254667282104\n",
            "Training:  Iteration No:  252 Worker Num:  1 \n",
            " Loss:  0.21850460767745972\n",
            "Training:  Iteration No:  252 Worker Num:  2 \n",
            " Loss:  0.24686366319656372\n",
            "Training:  Iteration No:  252 Worker Num:  3 \n",
            " Loss:  0.326817125082016\n",
            "Training:  Iteration No:  252 Worker Num:  4 \n",
            " Loss:  0.26314809918403625\n",
            "Training:  Iteration No:  252 Worker Num:  5 \n",
            " Loss:  0.19115974009037018\n",
            "Training:  Iteration No:  252 Worker Num:  6 \n",
            " Loss:  0.1762866973876953\n",
            "Training:  Iteration No:  252 Worker Num:  7 \n",
            " Loss:  0.16417504847049713\n",
            "Test:  Iteration No:  252 \n",
            " Loss:  0.18681152575189555\n",
            "Test accuracy:  94.56\n",
            "Training:  Iteration No:  253 Worker Num:  0 \n",
            " Loss:  0.27460628747940063\n",
            "Training:  Iteration No:  253 Worker Num:  1 \n",
            " Loss:  0.16958534717559814\n",
            "Training:  Iteration No:  253 Worker Num:  2 \n",
            " Loss:  0.2463492453098297\n",
            "Training:  Iteration No:  253 Worker Num:  3 \n",
            " Loss:  0.2053348422050476\n",
            "Training:  Iteration No:  253 Worker Num:  4 \n",
            " Loss:  0.3606918752193451\n",
            "Training:  Iteration No:  253 Worker Num:  5 \n",
            " Loss:  0.2884882092475891\n",
            "Training:  Iteration No:  253 Worker Num:  6 \n",
            " Loss:  0.2248150110244751\n",
            "Training:  Iteration No:  253 Worker Num:  7 \n",
            " Loss:  0.365669846534729\n",
            "Test:  Iteration No:  253 \n",
            " Loss:  0.18792713009103945\n",
            "Test accuracy:  94.36\n",
            "Training:  Iteration No:  254 Worker Num:  0 \n",
            " Loss:  0.2932317554950714\n",
            "Training:  Iteration No:  254 Worker Num:  1 \n",
            " Loss:  0.28753426671028137\n",
            "Training:  Iteration No:  254 Worker Num:  2 \n",
            " Loss:  0.18627135455608368\n",
            "Training:  Iteration No:  254 Worker Num:  3 \n",
            " Loss:  0.2846737802028656\n",
            "Training:  Iteration No:  254 Worker Num:  4 \n",
            " Loss:  0.17237588763237\n",
            "Training:  Iteration No:  254 Worker Num:  5 \n",
            " Loss:  0.2693249583244324\n",
            "Training:  Iteration No:  254 Worker Num:  6 \n",
            " Loss:  0.24156329035758972\n",
            "Training:  Iteration No:  254 Worker Num:  7 \n",
            " Loss:  0.161025270819664\n",
            "Test:  Iteration No:  254 \n",
            " Loss:  0.18533646337735124\n",
            "Test accuracy:  94.54\n",
            "Training:  Iteration No:  255 Worker Num:  0 \n",
            " Loss:  0.23402585089206696\n",
            "Training:  Iteration No:  255 Worker Num:  1 \n",
            " Loss:  0.28845739364624023\n",
            "Training:  Iteration No:  255 Worker Num:  2 \n",
            " Loss:  0.18199580907821655\n",
            "Training:  Iteration No:  255 Worker Num:  3 \n",
            " Loss:  0.15713711082935333\n",
            "Training:  Iteration No:  255 Worker Num:  4 \n",
            " Loss:  0.1759023219347\n",
            "Training:  Iteration No:  255 Worker Num:  5 \n",
            " Loss:  0.4100726544857025\n",
            "Training:  Iteration No:  255 Worker Num:  6 \n",
            " Loss:  0.15879298746585846\n",
            "Training:  Iteration No:  255 Worker Num:  7 \n",
            " Loss:  0.22208352386951447\n",
            "Test:  Iteration No:  255 \n",
            " Loss:  0.1842460036159882\n",
            "Test accuracy:  94.62\n",
            "Training:  Iteration No:  256 Worker Num:  0 \n",
            " Loss:  0.095786914229393\n",
            "Training:  Iteration No:  256 Worker Num:  1 \n",
            " Loss:  0.1961061954498291\n",
            "Training:  Iteration No:  256 Worker Num:  2 \n",
            " Loss:  0.1264246553182602\n",
            "Training:  Iteration No:  256 Worker Num:  3 \n",
            " Loss:  0.2651742398738861\n",
            "Training:  Iteration No:  256 Worker Num:  4 \n",
            " Loss:  0.21984067559242249\n",
            "Training:  Iteration No:  256 Worker Num:  5 \n",
            " Loss:  0.2894461154937744\n",
            "Training:  Iteration No:  256 Worker Num:  6 \n",
            " Loss:  0.2613998055458069\n",
            "Training:  Iteration No:  256 Worker Num:  7 \n",
            " Loss:  0.20334623754024506\n",
            "Test:  Iteration No:  256 \n",
            " Loss:  0.1882683431753252\n",
            "Test accuracy:  94.58\n",
            "Training:  Iteration No:  257 Worker Num:  0 \n",
            " Loss:  0.37815696001052856\n",
            "Training:  Iteration No:  257 Worker Num:  1 \n",
            " Loss:  0.1399080753326416\n",
            "Training:  Iteration No:  257 Worker Num:  2 \n",
            " Loss:  0.21757729351520538\n",
            "Training:  Iteration No:  257 Worker Num:  3 \n",
            " Loss:  0.150943323969841\n",
            "Training:  Iteration No:  257 Worker Num:  4 \n",
            " Loss:  0.21028122305870056\n",
            "Training:  Iteration No:  257 Worker Num:  5 \n",
            " Loss:  0.3109388053417206\n",
            "Training:  Iteration No:  257 Worker Num:  6 \n",
            " Loss:  0.174673929810524\n",
            "Training:  Iteration No:  257 Worker Num:  7 \n",
            " Loss:  0.2229747772216797\n",
            "Test:  Iteration No:  257 \n",
            " Loss:  0.18427073380237893\n",
            "Test accuracy:  94.66\n",
            "Training:  Iteration No:  258 Worker Num:  0 \n",
            " Loss:  0.2394813597202301\n",
            "Training:  Iteration No:  258 Worker Num:  1 \n",
            " Loss:  0.1433538794517517\n",
            "Training:  Iteration No:  258 Worker Num:  2 \n",
            " Loss:  0.20863299071788788\n",
            "Training:  Iteration No:  258 Worker Num:  3 \n",
            " Loss:  0.19254088401794434\n",
            "Training:  Iteration No:  258 Worker Num:  4 \n",
            " Loss:  0.2132430076599121\n",
            "Training:  Iteration No:  258 Worker Num:  5 \n",
            " Loss:  0.2265414446592331\n",
            "Training:  Iteration No:  258 Worker Num:  6 \n",
            " Loss:  0.44688743352890015\n",
            "Training:  Iteration No:  258 Worker Num:  7 \n",
            " Loss:  0.18066191673278809\n",
            "Test:  Iteration No:  258 \n",
            " Loss:  0.19063537583179488\n",
            "Test accuracy:  94.3\n",
            "Training:  Iteration No:  259 Worker Num:  0 \n",
            " Loss:  0.12874861061573029\n",
            "Training:  Iteration No:  259 Worker Num:  1 \n",
            " Loss:  0.17210668325424194\n",
            "Training:  Iteration No:  259 Worker Num:  2 \n",
            " Loss:  0.1400187611579895\n",
            "Training:  Iteration No:  259 Worker Num:  3 \n",
            " Loss:  0.18609429895877838\n",
            "Training:  Iteration No:  259 Worker Num:  4 \n",
            " Loss:  0.19629821181297302\n",
            "Training:  Iteration No:  259 Worker Num:  5 \n",
            " Loss:  0.2059667557477951\n",
            "Training:  Iteration No:  259 Worker Num:  6 \n",
            " Loss:  0.19328823685646057\n",
            "Training:  Iteration No:  259 Worker Num:  7 \n",
            " Loss:  0.37274622917175293\n",
            "Test:  Iteration No:  259 \n",
            " Loss:  0.18674913237366494\n",
            "Test accuracy:  94.5\n",
            "Training:  Iteration No:  260 Worker Num:  0 \n",
            " Loss:  0.1257428675889969\n",
            "Training:  Iteration No:  260 Worker Num:  1 \n",
            " Loss:  0.27168163657188416\n",
            "Training:  Iteration No:  260 Worker Num:  2 \n",
            " Loss:  0.15612323582172394\n",
            "Training:  Iteration No:  260 Worker Num:  3 \n",
            " Loss:  0.23544275760650635\n",
            "Training:  Iteration No:  260 Worker Num:  4 \n",
            " Loss:  0.2444988489151001\n",
            "Training:  Iteration No:  260 Worker Num:  5 \n",
            " Loss:  0.2182735800743103\n",
            "Training:  Iteration No:  260 Worker Num:  6 \n",
            " Loss:  0.26919370889663696\n",
            "Training:  Iteration No:  260 Worker Num:  7 \n",
            " Loss:  0.27258187532424927\n",
            "Test:  Iteration No:  260 \n",
            " Loss:  0.1819474388833476\n",
            "Test accuracy:  94.56\n",
            "Training:  Iteration No:  261 Worker Num:  0 \n",
            " Loss:  0.10660679638385773\n",
            "Training:  Iteration No:  261 Worker Num:  1 \n",
            " Loss:  0.21956169605255127\n",
            "Training:  Iteration No:  261 Worker Num:  2 \n",
            " Loss:  0.24262374639511108\n",
            "Training:  Iteration No:  261 Worker Num:  3 \n",
            " Loss:  0.13550081849098206\n",
            "Training:  Iteration No:  261 Worker Num:  4 \n",
            " Loss:  0.26162615418434143\n",
            "Training:  Iteration No:  261 Worker Num:  5 \n",
            " Loss:  0.2848873436450958\n",
            "Training:  Iteration No:  261 Worker Num:  6 \n",
            " Loss:  0.18457096815109253\n",
            "Training:  Iteration No:  261 Worker Num:  7 \n",
            " Loss:  0.15646839141845703\n",
            "Test:  Iteration No:  261 \n",
            " Loss:  0.17941343647581112\n",
            "Test accuracy:  94.78\n",
            "Training:  Iteration No:  262 Worker Num:  0 \n",
            " Loss:  0.13387754559516907\n",
            "Training:  Iteration No:  262 Worker Num:  1 \n",
            " Loss:  0.18213297426700592\n",
            "Training:  Iteration No:  262 Worker Num:  2 \n",
            " Loss:  0.17680057883262634\n",
            "Training:  Iteration No:  262 Worker Num:  3 \n",
            " Loss:  0.20623457431793213\n",
            "Training:  Iteration No:  262 Worker Num:  4 \n",
            " Loss:  0.16186067461967468\n",
            "Training:  Iteration No:  262 Worker Num:  5 \n",
            " Loss:  0.20980538427829742\n",
            "Training:  Iteration No:  262 Worker Num:  6 \n",
            " Loss:  0.3211747407913208\n",
            "Training:  Iteration No:  262 Worker Num:  7 \n",
            " Loss:  0.18234014511108398\n",
            "Test:  Iteration No:  262 \n",
            " Loss:  0.18410911705769314\n",
            "Test accuracy:  94.68\n",
            "Training:  Iteration No:  263 Worker Num:  0 \n",
            " Loss:  0.17404595017433167\n",
            "Training:  Iteration No:  263 Worker Num:  1 \n",
            " Loss:  0.2916717827320099\n",
            "Training:  Iteration No:  263 Worker Num:  2 \n",
            " Loss:  0.19955308735370636\n",
            "Training:  Iteration No:  263 Worker Num:  3 \n",
            " Loss:  0.23348386585712433\n",
            "Training:  Iteration No:  263 Worker Num:  4 \n",
            " Loss:  0.2719007730484009\n",
            "Training:  Iteration No:  263 Worker Num:  5 \n",
            " Loss:  0.23845042288303375\n",
            "Training:  Iteration No:  263 Worker Num:  6 \n",
            " Loss:  0.19982251524925232\n",
            "Training:  Iteration No:  263 Worker Num:  7 \n",
            " Loss:  0.3411496877670288\n",
            "Test:  Iteration No:  263 \n",
            " Loss:  0.18010396907648332\n",
            "Test accuracy:  94.78\n",
            "Training:  Iteration No:  264 Worker Num:  0 \n",
            " Loss:  0.11845074594020844\n",
            "Training:  Iteration No:  264 Worker Num:  1 \n",
            " Loss:  0.12542001903057098\n",
            "Training:  Iteration No:  264 Worker Num:  2 \n",
            " Loss:  0.15652605891227722\n",
            "Training:  Iteration No:  264 Worker Num:  3 \n",
            " Loss:  0.13233885169029236\n",
            "Training:  Iteration No:  264 Worker Num:  4 \n",
            " Loss:  0.19844649732112885\n",
            "Training:  Iteration No:  264 Worker Num:  5 \n",
            " Loss:  0.2128528207540512\n",
            "Training:  Iteration No:  264 Worker Num:  6 \n",
            " Loss:  0.1973959058523178\n",
            "Training:  Iteration No:  264 Worker Num:  7 \n",
            " Loss:  0.16639992594718933\n",
            "Test:  Iteration No:  264 \n",
            " Loss:  0.18000166673401866\n",
            "Test accuracy:  94.71\n",
            "Training:  Iteration No:  265 Worker Num:  0 \n",
            " Loss:  0.2945985794067383\n",
            "Training:  Iteration No:  265 Worker Num:  1 \n",
            " Loss:  0.14145037531852722\n",
            "Training:  Iteration No:  265 Worker Num:  2 \n",
            " Loss:  0.24912673234939575\n",
            "Training:  Iteration No:  265 Worker Num:  3 \n",
            " Loss:  0.26664504408836365\n",
            "Training:  Iteration No:  265 Worker Num:  4 \n",
            " Loss:  0.31440457701683044\n",
            "Training:  Iteration No:  265 Worker Num:  5 \n",
            " Loss:  0.18330927193164825\n",
            "Training:  Iteration No:  265 Worker Num:  6 \n",
            " Loss:  0.19928494095802307\n",
            "Training:  Iteration No:  265 Worker Num:  7 \n",
            " Loss:  0.2650313973426819\n",
            "Test:  Iteration No:  265 \n",
            " Loss:  0.17977137551230343\n",
            "Test accuracy:  94.88\n",
            "Training:  Iteration No:  266 Worker Num:  0 \n",
            " Loss:  0.16592149436473846\n",
            "Training:  Iteration No:  266 Worker Num:  1 \n",
            " Loss:  0.2997521460056305\n",
            "Training:  Iteration No:  266 Worker Num:  2 \n",
            " Loss:  0.19135454297065735\n",
            "Training:  Iteration No:  266 Worker Num:  3 \n",
            " Loss:  0.17223532497882843\n",
            "Training:  Iteration No:  266 Worker Num:  4 \n",
            " Loss:  0.18767908215522766\n",
            "Training:  Iteration No:  266 Worker Num:  5 \n",
            " Loss:  0.206750750541687\n",
            "Training:  Iteration No:  266 Worker Num:  6 \n",
            " Loss:  0.1456098109483719\n",
            "Training:  Iteration No:  266 Worker Num:  7 \n",
            " Loss:  0.2166195809841156\n",
            "Test:  Iteration No:  266 \n",
            " Loss:  0.18522855427847068\n",
            "Test accuracy:  94.43\n",
            "Training:  Iteration No:  267 Worker Num:  0 \n",
            " Loss:  0.2469319850206375\n",
            "Training:  Iteration No:  267 Worker Num:  1 \n",
            " Loss:  0.161423459649086\n",
            "Training:  Iteration No:  267 Worker Num:  2 \n",
            " Loss:  0.19423113763332367\n",
            "Training:  Iteration No:  267 Worker Num:  3 \n",
            " Loss:  0.23798172175884247\n",
            "Training:  Iteration No:  267 Worker Num:  4 \n",
            " Loss:  0.23788172006607056\n",
            "Training:  Iteration No:  267 Worker Num:  5 \n",
            " Loss:  0.2536734342575073\n",
            "Training:  Iteration No:  267 Worker Num:  6 \n",
            " Loss:  0.12914612889289856\n",
            "Training:  Iteration No:  267 Worker Num:  7 \n",
            " Loss:  0.19340722262859344\n",
            "Test:  Iteration No:  267 \n",
            " Loss:  0.1808775570715153\n",
            "Test accuracy:  94.86\n",
            "Training:  Iteration No:  268 Worker Num:  0 \n",
            " Loss:  0.16687574982643127\n",
            "Training:  Iteration No:  268 Worker Num:  1 \n",
            " Loss:  0.23022779822349548\n",
            "Training:  Iteration No:  268 Worker Num:  2 \n",
            " Loss:  0.34103432297706604\n",
            "Training:  Iteration No:  268 Worker Num:  3 \n",
            " Loss:  0.20366694033145905\n",
            "Training:  Iteration No:  268 Worker Num:  4 \n",
            " Loss:  0.30669572949409485\n",
            "Training:  Iteration No:  268 Worker Num:  5 \n",
            " Loss:  0.2689518630504608\n",
            "Training:  Iteration No:  268 Worker Num:  6 \n",
            " Loss:  0.19628429412841797\n",
            "Training:  Iteration No:  268 Worker Num:  7 \n",
            " Loss:  0.2108180969953537\n",
            "Test:  Iteration No:  268 \n",
            " Loss:  0.1785465400265176\n",
            "Test accuracy:  94.92\n",
            "Training:  Iteration No:  269 Worker Num:  0 \n",
            " Loss:  0.2421194463968277\n",
            "Training:  Iteration No:  269 Worker Num:  1 \n",
            " Loss:  0.28637269139289856\n",
            "Training:  Iteration No:  269 Worker Num:  2 \n",
            " Loss:  0.3125249445438385\n",
            "Training:  Iteration No:  269 Worker Num:  3 \n",
            " Loss:  0.1635322868824005\n",
            "Training:  Iteration No:  269 Worker Num:  4 \n",
            " Loss:  0.21672646701335907\n",
            "Training:  Iteration No:  269 Worker Num:  5 \n",
            " Loss:  0.1412602663040161\n",
            "Training:  Iteration No:  269 Worker Num:  6 \n",
            " Loss:  0.23808753490447998\n",
            "Training:  Iteration No:  269 Worker Num:  7 \n",
            " Loss:  0.25448790192604065\n",
            "Test:  Iteration No:  269 \n",
            " Loss:  0.1763491273114953\n",
            "Test accuracy:  94.95\n",
            "Training:  Iteration No:  270 Worker Num:  0 \n",
            " Loss:  0.11619619280099869\n",
            "Training:  Iteration No:  270 Worker Num:  1 \n",
            " Loss:  0.24462896585464478\n",
            "Training:  Iteration No:  270 Worker Num:  2 \n",
            " Loss:  0.1782924383878708\n",
            "Training:  Iteration No:  270 Worker Num:  3 \n",
            " Loss:  0.19927865266799927\n",
            "Training:  Iteration No:  270 Worker Num:  4 \n",
            " Loss:  0.18818631768226624\n",
            "Training:  Iteration No:  270 Worker Num:  5 \n",
            " Loss:  0.2263605147600174\n",
            "Training:  Iteration No:  270 Worker Num:  6 \n",
            " Loss:  0.36899450421333313\n",
            "Training:  Iteration No:  270 Worker Num:  7 \n",
            " Loss:  0.26062408089637756\n",
            "Test:  Iteration No:  270 \n",
            " Loss:  0.17597074099356616\n",
            "Test accuracy:  94.82\n",
            "Training:  Iteration No:  271 Worker Num:  0 \n",
            " Loss:  0.2319248467683792\n",
            "Training:  Iteration No:  271 Worker Num:  1 \n",
            " Loss:  0.3686417043209076\n",
            "Training:  Iteration No:  271 Worker Num:  2 \n",
            " Loss:  0.25835880637168884\n",
            "Training:  Iteration No:  271 Worker Num:  3 \n",
            " Loss:  0.15289239585399628\n",
            "Training:  Iteration No:  271 Worker Num:  4 \n",
            " Loss:  0.2196349948644638\n",
            "Training:  Iteration No:  271 Worker Num:  5 \n",
            " Loss:  0.2143605500459671\n",
            "Training:  Iteration No:  271 Worker Num:  6 \n",
            " Loss:  0.12042061984539032\n",
            "Training:  Iteration No:  271 Worker Num:  7 \n",
            " Loss:  0.2108595371246338\n",
            "Test:  Iteration No:  271 \n",
            " Loss:  0.17561470765526158\n",
            "Test accuracy:  95.01\n",
            "Training:  Iteration No:  272 Worker Num:  0 \n",
            " Loss:  0.118567556142807\n",
            "Training:  Iteration No:  272 Worker Num:  1 \n",
            " Loss:  0.2517528533935547\n",
            "Training:  Iteration No:  272 Worker Num:  2 \n",
            " Loss:  0.20362229645252228\n",
            "Training:  Iteration No:  272 Worker Num:  3 \n",
            " Loss:  0.32784971594810486\n",
            "Training:  Iteration No:  272 Worker Num:  4 \n",
            " Loss:  0.20067749917507172\n",
            "Training:  Iteration No:  272 Worker Num:  5 \n",
            " Loss:  0.18392488360404968\n",
            "Training:  Iteration No:  272 Worker Num:  6 \n",
            " Loss:  0.2445487082004547\n",
            "Training:  Iteration No:  272 Worker Num:  7 \n",
            " Loss:  0.1825837939977646\n",
            "Test:  Iteration No:  272 \n",
            " Loss:  0.17378462935927547\n",
            "Test accuracy:  94.9\n",
            "Training:  Iteration No:  273 Worker Num:  0 \n",
            " Loss:  0.2272815704345703\n",
            "Training:  Iteration No:  273 Worker Num:  1 \n",
            " Loss:  0.2015681117773056\n",
            "Training:  Iteration No:  273 Worker Num:  2 \n",
            " Loss:  0.31580373644828796\n",
            "Training:  Iteration No:  273 Worker Num:  3 \n",
            " Loss:  0.16455568373203278\n",
            "Training:  Iteration No:  273 Worker Num:  4 \n",
            " Loss:  0.15850989520549774\n",
            "Training:  Iteration No:  273 Worker Num:  5 \n",
            " Loss:  0.24735873937606812\n",
            "Training:  Iteration No:  273 Worker Num:  6 \n",
            " Loss:  0.12459443509578705\n",
            "Training:  Iteration No:  273 Worker Num:  7 \n",
            " Loss:  0.19482801854610443\n",
            "Test:  Iteration No:  273 \n",
            " Loss:  0.1797233107322945\n",
            "Test accuracy:  94.77\n",
            "Training:  Iteration No:  274 Worker Num:  0 \n",
            " Loss:  0.17840801179409027\n",
            "Training:  Iteration No:  274 Worker Num:  1 \n",
            " Loss:  0.1950036883354187\n",
            "Training:  Iteration No:  274 Worker Num:  2 \n",
            " Loss:  0.14004839956760406\n",
            "Training:  Iteration No:  274 Worker Num:  3 \n",
            " Loss:  0.10526677966117859\n",
            "Training:  Iteration No:  274 Worker Num:  4 \n",
            " Loss:  0.230990931391716\n",
            "Training:  Iteration No:  274 Worker Num:  5 \n",
            " Loss:  0.3108978569507599\n",
            "Training:  Iteration No:  274 Worker Num:  6 \n",
            " Loss:  0.2944822311401367\n",
            "Training:  Iteration No:  274 Worker Num:  7 \n",
            " Loss:  0.29349973797798157\n",
            "Test:  Iteration No:  274 \n",
            " Loss:  0.1739104242606348\n",
            "Test accuracy:  95.05\n",
            "Training:  Iteration No:  275 Worker Num:  0 \n",
            " Loss:  0.14308366179466248\n",
            "Training:  Iteration No:  275 Worker Num:  1 \n",
            " Loss:  0.17220918834209442\n",
            "Training:  Iteration No:  275 Worker Num:  2 \n",
            " Loss:  0.24439087510108948\n",
            "Training:  Iteration No:  275 Worker Num:  3 \n",
            " Loss:  0.24545587599277496\n",
            "Training:  Iteration No:  275 Worker Num:  4 \n",
            " Loss:  0.2425171136856079\n",
            "Training:  Iteration No:  275 Worker Num:  5 \n",
            " Loss:  0.15373529493808746\n",
            "Training:  Iteration No:  275 Worker Num:  6 \n",
            " Loss:  0.21631592512130737\n",
            "Training:  Iteration No:  275 Worker Num:  7 \n",
            " Loss:  0.4131951630115509\n",
            "Test:  Iteration No:  275 \n",
            " Loss:  0.17533793580965906\n",
            "Test accuracy:  94.95\n",
            "Training:  Iteration No:  276 Worker Num:  0 \n",
            " Loss:  0.3377953767776489\n",
            "Training:  Iteration No:  276 Worker Num:  1 \n",
            " Loss:  0.18262946605682373\n",
            "Training:  Iteration No:  276 Worker Num:  2 \n",
            " Loss:  0.23983220756053925\n",
            "Training:  Iteration No:  276 Worker Num:  3 \n",
            " Loss:  0.1587163209915161\n",
            "Training:  Iteration No:  276 Worker Num:  4 \n",
            " Loss:  0.2131289690732956\n",
            "Training:  Iteration No:  276 Worker Num:  5 \n",
            " Loss:  0.20341520011425018\n",
            "Training:  Iteration No:  276 Worker Num:  6 \n",
            " Loss:  0.2182028889656067\n",
            "Training:  Iteration No:  276 Worker Num:  7 \n",
            " Loss:  0.14229059219360352\n",
            "Test:  Iteration No:  276 \n",
            " Loss:  0.1702355741982973\n",
            "Test accuracy:  95.04\n",
            "Training:  Iteration No:  277 Worker Num:  0 \n",
            " Loss:  0.1421816051006317\n",
            "Training:  Iteration No:  277 Worker Num:  1 \n",
            " Loss:  0.21209752559661865\n",
            "Training:  Iteration No:  277 Worker Num:  2 \n",
            " Loss:  0.1652344912290573\n",
            "Training:  Iteration No:  277 Worker Num:  3 \n",
            " Loss:  0.22621150314807892\n",
            "Training:  Iteration No:  277 Worker Num:  4 \n",
            " Loss:  0.20731554925441742\n",
            "Training:  Iteration No:  277 Worker Num:  5 \n",
            " Loss:  0.33607760071754456\n",
            "Training:  Iteration No:  277 Worker Num:  6 \n",
            " Loss:  0.22441624104976654\n",
            "Training:  Iteration No:  277 Worker Num:  7 \n",
            " Loss:  0.348613977432251\n",
            "Test:  Iteration No:  277 \n",
            " Loss:  0.17353433491426368\n",
            "Test accuracy:  95.13\n",
            "Training:  Iteration No:  278 Worker Num:  0 \n",
            " Loss:  0.1974683701992035\n",
            "Training:  Iteration No:  278 Worker Num:  1 \n",
            " Loss:  0.2793920338153839\n",
            "Training:  Iteration No:  278 Worker Num:  2 \n",
            " Loss:  0.12908855080604553\n",
            "Training:  Iteration No:  278 Worker Num:  3 \n",
            " Loss:  0.27554917335510254\n",
            "Training:  Iteration No:  278 Worker Num:  4 \n",
            " Loss:  0.17804747819900513\n",
            "Training:  Iteration No:  278 Worker Num:  5 \n",
            " Loss:  0.285986065864563\n",
            "Training:  Iteration No:  278 Worker Num:  6 \n",
            " Loss:  0.14666447043418884\n",
            "Training:  Iteration No:  278 Worker Num:  7 \n",
            " Loss:  0.1333901286125183\n",
            "Test:  Iteration No:  278 \n",
            " Loss:  0.17157692623713725\n",
            "Test accuracy:  94.88\n",
            "Training:  Iteration No:  279 Worker Num:  0 \n",
            " Loss:  0.2705429792404175\n",
            "Training:  Iteration No:  279 Worker Num:  1 \n",
            " Loss:  0.18842993676662445\n",
            "Training:  Iteration No:  279 Worker Num:  2 \n",
            " Loss:  0.17738910019397736\n",
            "Training:  Iteration No:  279 Worker Num:  3 \n",
            " Loss:  0.23180820047855377\n",
            "Training:  Iteration No:  279 Worker Num:  4 \n",
            " Loss:  0.23507100343704224\n",
            "Training:  Iteration No:  279 Worker Num:  5 \n",
            " Loss:  0.1910371035337448\n",
            "Training:  Iteration No:  279 Worker Num:  6 \n",
            " Loss:  0.22294023633003235\n",
            "Training:  Iteration No:  279 Worker Num:  7 \n",
            " Loss:  0.15226520597934723\n",
            "Test:  Iteration No:  279 \n",
            " Loss:  0.17495292875491367\n",
            "Test accuracy:  94.83\n",
            "Training:  Iteration No:  280 Worker Num:  0 \n",
            " Loss:  0.22829239070415497\n",
            "Training:  Iteration No:  280 Worker Num:  1 \n",
            " Loss:  0.22090983390808105\n",
            "Training:  Iteration No:  280 Worker Num:  2 \n",
            " Loss:  0.20349113643169403\n",
            "Training:  Iteration No:  280 Worker Num:  3 \n",
            " Loss:  0.12987008690834045\n",
            "Training:  Iteration No:  280 Worker Num:  4 \n",
            " Loss:  0.2176358997821808\n",
            "Training:  Iteration No:  280 Worker Num:  5 \n",
            " Loss:  0.14779488742351532\n",
            "Training:  Iteration No:  280 Worker Num:  6 \n",
            " Loss:  0.21781271696090698\n",
            "Training:  Iteration No:  280 Worker Num:  7 \n",
            " Loss:  0.21930553019046783\n",
            "Test:  Iteration No:  280 \n",
            " Loss:  0.17432437099139148\n",
            "Test accuracy:  94.76\n",
            "Training:  Iteration No:  281 Worker Num:  0 \n",
            " Loss:  0.4041453003883362\n",
            "Training:  Iteration No:  281 Worker Num:  1 \n",
            " Loss:  0.1210893765091896\n",
            "Training:  Iteration No:  281 Worker Num:  2 \n",
            " Loss:  0.320462167263031\n",
            "Training:  Iteration No:  281 Worker Num:  3 \n",
            " Loss:  0.24160346388816833\n",
            "Training:  Iteration No:  281 Worker Num:  4 \n",
            " Loss:  0.1674404889345169\n",
            "Training:  Iteration No:  281 Worker Num:  5 \n",
            " Loss:  0.152098149061203\n",
            "Training:  Iteration No:  281 Worker Num:  6 \n",
            " Loss:  0.2577804625034332\n",
            "Training:  Iteration No:  281 Worker Num:  7 \n",
            " Loss:  0.3732169568538666\n",
            "Test:  Iteration No:  281 \n",
            " Loss:  0.16906598806852782\n",
            "Test accuracy:  95.04\n",
            "Training:  Iteration No:  282 Worker Num:  0 \n",
            " Loss:  0.19657745957374573\n",
            "Training:  Iteration No:  282 Worker Num:  1 \n",
            " Loss:  0.23261786997318268\n",
            "Training:  Iteration No:  282 Worker Num:  2 \n",
            " Loss:  0.1363929659128189\n",
            "Training:  Iteration No:  282 Worker Num:  3 \n",
            " Loss:  0.08963597565889359\n",
            "Training:  Iteration No:  282 Worker Num:  4 \n",
            " Loss:  0.18717800080776215\n",
            "Training:  Iteration No:  282 Worker Num:  5 \n",
            " Loss:  0.17474237084388733\n",
            "Training:  Iteration No:  282 Worker Num:  6 \n",
            " Loss:  0.17405933141708374\n",
            "Training:  Iteration No:  282 Worker Num:  7 \n",
            " Loss:  0.15757067501544952\n",
            "Test:  Iteration No:  282 \n",
            " Loss:  0.17250422005176166\n",
            "Test accuracy:  94.96\n",
            "Training:  Iteration No:  283 Worker Num:  0 \n",
            " Loss:  0.23341481387615204\n",
            "Training:  Iteration No:  283 Worker Num:  1 \n",
            " Loss:  0.1582365185022354\n",
            "Training:  Iteration No:  283 Worker Num:  2 \n",
            " Loss:  0.3140820860862732\n",
            "Training:  Iteration No:  283 Worker Num:  3 \n",
            " Loss:  0.16167967021465302\n",
            "Training:  Iteration No:  283 Worker Num:  4 \n",
            " Loss:  0.20171192288398743\n",
            "Training:  Iteration No:  283 Worker Num:  5 \n",
            " Loss:  0.11877821385860443\n",
            "Training:  Iteration No:  283 Worker Num:  6 \n",
            " Loss:  0.19612394273281097\n",
            "Training:  Iteration No:  283 Worker Num:  7 \n",
            " Loss:  0.30828362703323364\n",
            "Test:  Iteration No:  283 \n",
            " Loss:  0.1709675920797111\n",
            "Test accuracy:  94.95\n",
            "Training:  Iteration No:  284 Worker Num:  0 \n",
            " Loss:  0.15633048117160797\n",
            "Training:  Iteration No:  284 Worker Num:  1 \n",
            " Loss:  0.2682654559612274\n",
            "Training:  Iteration No:  284 Worker Num:  2 \n",
            " Loss:  0.2203826606273651\n",
            "Training:  Iteration No:  284 Worker Num:  3 \n",
            " Loss:  0.27456867694854736\n",
            "Training:  Iteration No:  284 Worker Num:  4 \n",
            " Loss:  0.20194269716739655\n",
            "Training:  Iteration No:  284 Worker Num:  5 \n",
            " Loss:  0.15576452016830444\n",
            "Training:  Iteration No:  284 Worker Num:  6 \n",
            " Loss:  0.16612349450588226\n",
            "Training:  Iteration No:  284 Worker Num:  7 \n",
            " Loss:  0.11603420972824097\n",
            "Test:  Iteration No:  284 \n",
            " Loss:  0.16980365993855875\n",
            "Test accuracy:  94.92\n",
            "Training:  Iteration No:  285 Worker Num:  0 \n",
            " Loss:  0.2707749009132385\n",
            "Training:  Iteration No:  285 Worker Num:  1 \n",
            " Loss:  0.21793889999389648\n",
            "Training:  Iteration No:  285 Worker Num:  2 \n",
            " Loss:  0.17305715382099152\n",
            "Training:  Iteration No:  285 Worker Num:  3 \n",
            " Loss:  0.23735548555850983\n",
            "Training:  Iteration No:  285 Worker Num:  4 \n",
            " Loss:  0.29772892594337463\n",
            "Training:  Iteration No:  285 Worker Num:  5 \n",
            " Loss:  0.18652482330799103\n",
            "Training:  Iteration No:  285 Worker Num:  6 \n",
            " Loss:  0.272737979888916\n",
            "Training:  Iteration No:  285 Worker Num:  7 \n",
            " Loss:  0.302542120218277\n",
            "Test:  Iteration No:  285 \n",
            " Loss:  0.1687869161508883\n",
            "Test accuracy:  95.08\n",
            "Training:  Iteration No:  286 Worker Num:  0 \n",
            " Loss:  0.28297778964042664\n",
            "Training:  Iteration No:  286 Worker Num:  1 \n",
            " Loss:  0.24662671983242035\n",
            "Training:  Iteration No:  286 Worker Num:  2 \n",
            " Loss:  0.242000550031662\n",
            "Training:  Iteration No:  286 Worker Num:  3 \n",
            " Loss:  0.14234548807144165\n",
            "Training:  Iteration No:  286 Worker Num:  4 \n",
            " Loss:  0.19420678913593292\n",
            "Training:  Iteration No:  286 Worker Num:  5 \n",
            " Loss:  0.19500891864299774\n",
            "Training:  Iteration No:  286 Worker Num:  6 \n",
            " Loss:  0.34140485525131226\n",
            "Training:  Iteration No:  286 Worker Num:  7 \n",
            " Loss:  0.12145735323429108\n",
            "Test:  Iteration No:  286 \n",
            " Loss:  0.16796832941354642\n",
            "Test accuracy:  94.96\n",
            "Training:  Iteration No:  287 Worker Num:  0 \n",
            " Loss:  0.24253928661346436\n",
            "Training:  Iteration No:  287 Worker Num:  1 \n",
            " Loss:  0.20260968804359436\n",
            "Training:  Iteration No:  287 Worker Num:  2 \n",
            " Loss:  0.17116688191890717\n",
            "Training:  Iteration No:  287 Worker Num:  3 \n",
            " Loss:  0.18481490015983582\n",
            "Training:  Iteration No:  287 Worker Num:  4 \n",
            " Loss:  0.1746000051498413\n",
            "Training:  Iteration No:  287 Worker Num:  5 \n",
            " Loss:  0.1484774798154831\n",
            "Training:  Iteration No:  287 Worker Num:  6 \n",
            " Loss:  0.16474637389183044\n",
            "Training:  Iteration No:  287 Worker Num:  7 \n",
            " Loss:  0.1781340390443802\n",
            "Test:  Iteration No:  287 \n",
            " Loss:  0.17037073160887142\n",
            "Test accuracy:  94.89\n",
            "Training:  Iteration No:  288 Worker Num:  0 \n",
            " Loss:  0.22518716752529144\n",
            "Training:  Iteration No:  288 Worker Num:  1 \n",
            " Loss:  0.21215395629405975\n",
            "Training:  Iteration No:  288 Worker Num:  2 \n",
            " Loss:  0.28062015771865845\n",
            "Training:  Iteration No:  288 Worker Num:  3 \n",
            " Loss:  0.2644428610801697\n",
            "Training:  Iteration No:  288 Worker Num:  4 \n",
            " Loss:  0.1286313682794571\n",
            "Training:  Iteration No:  288 Worker Num:  5 \n",
            " Loss:  0.2786984145641327\n",
            "Training:  Iteration No:  288 Worker Num:  6 \n",
            " Loss:  0.1452694833278656\n",
            "Training:  Iteration No:  288 Worker Num:  7 \n",
            " Loss:  0.17728334665298462\n",
            "Test:  Iteration No:  288 \n",
            " Loss:  0.1669054381384314\n",
            "Test accuracy:  95.09\n",
            "Training:  Iteration No:  289 Worker Num:  0 \n",
            " Loss:  0.13446557521820068\n",
            "Training:  Iteration No:  289 Worker Num:  1 \n",
            " Loss:  0.26958316564559937\n",
            "Training:  Iteration No:  289 Worker Num:  2 \n",
            " Loss:  0.09467143565416336\n",
            "Training:  Iteration No:  289 Worker Num:  3 \n",
            " Loss:  0.16554449498653412\n",
            "Training:  Iteration No:  289 Worker Num:  4 \n",
            " Loss:  0.15437614917755127\n",
            "Training:  Iteration No:  289 Worker Num:  5 \n",
            " Loss:  0.1527641862630844\n",
            "Training:  Iteration No:  289 Worker Num:  6 \n",
            " Loss:  0.23058092594146729\n",
            "Training:  Iteration No:  289 Worker Num:  7 \n",
            " Loss:  0.24035871028900146\n",
            "Test:  Iteration No:  289 \n",
            " Loss:  0.1636979144856428\n",
            "Test accuracy:  95.17\n",
            "Training:  Iteration No:  290 Worker Num:  0 \n",
            " Loss:  0.11085228621959686\n",
            "Training:  Iteration No:  290 Worker Num:  1 \n",
            " Loss:  0.25592485070228577\n",
            "Training:  Iteration No:  290 Worker Num:  2 \n",
            " Loss:  0.19011886417865753\n",
            "Training:  Iteration No:  290 Worker Num:  3 \n",
            " Loss:  0.16688649356365204\n",
            "Training:  Iteration No:  290 Worker Num:  4 \n",
            " Loss:  0.185027614235878\n",
            "Training:  Iteration No:  290 Worker Num:  5 \n",
            " Loss:  0.21046000719070435\n",
            "Training:  Iteration No:  290 Worker Num:  6 \n",
            " Loss:  0.2732052206993103\n",
            "Training:  Iteration No:  290 Worker Num:  7 \n",
            " Loss:  0.20728300511837006\n",
            "Test:  Iteration No:  290 \n",
            " Loss:  0.1656955291835379\n",
            "Test accuracy:  95.04\n",
            "Training:  Iteration No:  291 Worker Num:  0 \n",
            " Loss:  0.22582043707370758\n",
            "Training:  Iteration No:  291 Worker Num:  1 \n",
            " Loss:  0.24663183093070984\n",
            "Training:  Iteration No:  291 Worker Num:  2 \n",
            " Loss:  0.2874729633331299\n",
            "Training:  Iteration No:  291 Worker Num:  3 \n",
            " Loss:  0.15156765282154083\n",
            "Training:  Iteration No:  291 Worker Num:  4 \n",
            " Loss:  0.22704768180847168\n",
            "Training:  Iteration No:  291 Worker Num:  5 \n",
            " Loss:  0.13443678617477417\n",
            "Training:  Iteration No:  291 Worker Num:  6 \n",
            " Loss:  0.22056028246879578\n",
            "Training:  Iteration No:  291 Worker Num:  7 \n",
            " Loss:  0.22736051678657532\n",
            "Test:  Iteration No:  291 \n",
            " Loss:  0.16519839199896477\n",
            "Test accuracy:  95.21\n",
            "Training:  Iteration No:  292 Worker Num:  0 \n",
            " Loss:  0.10928915441036224\n",
            "Training:  Iteration No:  292 Worker Num:  1 \n",
            " Loss:  0.27445775270462036\n",
            "Training:  Iteration No:  292 Worker Num:  2 \n",
            " Loss:  0.1317014843225479\n",
            "Training:  Iteration No:  292 Worker Num:  3 \n",
            " Loss:  0.20431877672672272\n",
            "Training:  Iteration No:  292 Worker Num:  4 \n",
            " Loss:  0.14988359808921814\n",
            "Training:  Iteration No:  292 Worker Num:  5 \n",
            " Loss:  0.18255342543125153\n",
            "Training:  Iteration No:  292 Worker Num:  6 \n",
            " Loss:  0.1559615433216095\n",
            "Training:  Iteration No:  292 Worker Num:  7 \n",
            " Loss:  0.26054689288139343\n",
            "Test:  Iteration No:  292 \n",
            " Loss:  0.16226447215657444\n",
            "Test accuracy:  95.17\n",
            "Training:  Iteration No:  293 Worker Num:  0 \n",
            " Loss:  0.1302587240934372\n",
            "Training:  Iteration No:  293 Worker Num:  1 \n",
            " Loss:  0.15210235118865967\n",
            "Training:  Iteration No:  293 Worker Num:  2 \n",
            " Loss:  0.12050124257802963\n",
            "Training:  Iteration No:  293 Worker Num:  3 \n",
            " Loss:  0.2285536527633667\n",
            "Training:  Iteration No:  293 Worker Num:  4 \n",
            " Loss:  0.16125790774822235\n",
            "Training:  Iteration No:  293 Worker Num:  5 \n",
            " Loss:  0.14406079053878784\n",
            "Training:  Iteration No:  293 Worker Num:  6 \n",
            " Loss:  0.2512304186820984\n",
            "Training:  Iteration No:  293 Worker Num:  7 \n",
            " Loss:  0.13126444816589355\n",
            "Test:  Iteration No:  293 \n",
            " Loss:  0.16210244931987947\n",
            "Test accuracy:  95.22\n",
            "Training:  Iteration No:  294 Worker Num:  0 \n",
            " Loss:  0.16982433199882507\n",
            "Training:  Iteration No:  294 Worker Num:  1 \n",
            " Loss:  0.1752326786518097\n",
            "Training:  Iteration No:  294 Worker Num:  2 \n",
            " Loss:  0.18880866467952728\n",
            "Training:  Iteration No:  294 Worker Num:  3 \n",
            " Loss:  0.18980315327644348\n",
            "Training:  Iteration No:  294 Worker Num:  4 \n",
            " Loss:  0.2205403596162796\n",
            "Training:  Iteration No:  294 Worker Num:  5 \n",
            " Loss:  0.17958348989486694\n",
            "Training:  Iteration No:  294 Worker Num:  6 \n",
            " Loss:  0.09406473487615585\n",
            "Training:  Iteration No:  294 Worker Num:  7 \n",
            " Loss:  0.2069416642189026\n",
            "Test:  Iteration No:  294 \n",
            " Loss:  0.15955846716614464\n",
            "Test accuracy:  95.38\n",
            "Training:  Iteration No:  295 Worker Num:  0 \n",
            " Loss:  0.1660425215959549\n",
            "Training:  Iteration No:  295 Worker Num:  1 \n",
            " Loss:  0.2134520262479782\n",
            "Training:  Iteration No:  295 Worker Num:  2 \n",
            " Loss:  0.23941656947135925\n",
            "Training:  Iteration No:  295 Worker Num:  3 \n",
            " Loss:  0.19999738037586212\n",
            "Training:  Iteration No:  295 Worker Num:  4 \n",
            " Loss:  0.29408422112464905\n",
            "Training:  Iteration No:  295 Worker Num:  5 \n",
            " Loss:  0.2791423797607422\n",
            "Training:  Iteration No:  295 Worker Num:  6 \n",
            " Loss:  0.1455942839384079\n",
            "Training:  Iteration No:  295 Worker Num:  7 \n",
            " Loss:  0.2250015139579773\n",
            "Test:  Iteration No:  295 \n",
            " Loss:  0.16159194329447008\n",
            "Test accuracy:  95.13\n",
            "Training:  Iteration No:  296 Worker Num:  0 \n",
            " Loss:  0.25400906801223755\n",
            "Training:  Iteration No:  296 Worker Num:  1 \n",
            " Loss:  0.139869824051857\n",
            "Training:  Iteration No:  296 Worker Num:  2 \n",
            " Loss:  0.13164760172367096\n",
            "Training:  Iteration No:  296 Worker Num:  3 \n",
            " Loss:  0.1137702539563179\n",
            "Training:  Iteration No:  296 Worker Num:  4 \n",
            " Loss:  0.2333402782678604\n",
            "Training:  Iteration No:  296 Worker Num:  5 \n",
            " Loss:  0.14366908371448517\n",
            "Training:  Iteration No:  296 Worker Num:  6 \n",
            " Loss:  0.22063936293125153\n",
            "Training:  Iteration No:  296 Worker Num:  7 \n",
            " Loss:  0.3048698604106903\n",
            "Test:  Iteration No:  296 \n",
            " Loss:  0.16440295492287113\n",
            "Test accuracy:  95.28\n",
            "Training:  Iteration No:  297 Worker Num:  0 \n",
            " Loss:  0.2638375759124756\n",
            "Training:  Iteration No:  297 Worker Num:  1 \n",
            " Loss:  0.2748841941356659\n",
            "Training:  Iteration No:  297 Worker Num:  2 \n",
            " Loss:  0.11312661319971085\n",
            "Training:  Iteration No:  297 Worker Num:  3 \n",
            " Loss:  0.2535099983215332\n",
            "Training:  Iteration No:  297 Worker Num:  4 \n",
            " Loss:  0.1812354028224945\n",
            "Training:  Iteration No:  297 Worker Num:  5 \n",
            " Loss:  0.2512066960334778\n",
            "Training:  Iteration No:  297 Worker Num:  6 \n",
            " Loss:  0.16487054526805878\n",
            "Training:  Iteration No:  297 Worker Num:  7 \n",
            " Loss:  0.17174504697322845\n",
            "Test:  Iteration No:  297 \n",
            " Loss:  0.16911825298367045\n",
            "Test accuracy:  95.02\n",
            "Training:  Iteration No:  298 Worker Num:  0 \n",
            " Loss:  0.2189825177192688\n",
            "Training:  Iteration No:  298 Worker Num:  1 \n",
            " Loss:  0.23929962515830994\n",
            "Training:  Iteration No:  298 Worker Num:  2 \n",
            " Loss:  0.3081488609313965\n",
            "Training:  Iteration No:  298 Worker Num:  3 \n",
            " Loss:  0.2730487287044525\n",
            "Training:  Iteration No:  298 Worker Num:  4 \n",
            " Loss:  0.13532373309135437\n",
            "Training:  Iteration No:  298 Worker Num:  5 \n",
            " Loss:  0.2731735408306122\n",
            "Training:  Iteration No:  298 Worker Num:  6 \n",
            " Loss:  0.12968289852142334\n",
            "Training:  Iteration No:  298 Worker Num:  7 \n",
            " Loss:  0.20937508344650269\n",
            "Test:  Iteration No:  298 \n",
            " Loss:  0.1609162997536927\n",
            "Test accuracy:  95.24\n",
            "Training:  Iteration No:  299 Worker Num:  0 \n",
            " Loss:  0.2233455330133438\n",
            "Training:  Iteration No:  299 Worker Num:  1 \n",
            " Loss:  0.21133196353912354\n",
            "Training:  Iteration No:  299 Worker Num:  2 \n",
            " Loss:  0.23035036027431488\n",
            "Training:  Iteration No:  299 Worker Num:  3 \n",
            " Loss:  0.16536135971546173\n",
            "Training:  Iteration No:  299 Worker Num:  4 \n",
            " Loss:  0.28049537539482117\n",
            "Training:  Iteration No:  299 Worker Num:  5 \n",
            " Loss:  0.18407222628593445\n",
            "Training:  Iteration No:  299 Worker Num:  6 \n",
            " Loss:  0.11932309716939926\n",
            "Training:  Iteration No:  299 Worker Num:  7 \n",
            " Loss:  0.16878804564476013\n",
            "Test:  Iteration No:  299 \n",
            " Loss:  0.15854615063253272\n",
            "Test accuracy:  95.35\n",
            "Training:  Iteration No:  300 Worker Num:  0 \n",
            " Loss:  0.16240417957305908\n",
            "Training:  Iteration No:  300 Worker Num:  1 \n",
            " Loss:  0.1946214884519577\n",
            "Training:  Iteration No:  300 Worker Num:  2 \n",
            " Loss:  0.20491616427898407\n",
            "Training:  Iteration No:  300 Worker Num:  3 \n",
            " Loss:  0.19316671788692474\n",
            "Training:  Iteration No:  300 Worker Num:  4 \n",
            " Loss:  0.22385434806346893\n",
            "Training:  Iteration No:  300 Worker Num:  5 \n",
            " Loss:  0.13870008289813995\n",
            "Training:  Iteration No:  300 Worker Num:  6 \n",
            " Loss:  0.20434391498565674\n",
            "Training:  Iteration No:  300 Worker Num:  7 \n",
            " Loss:  0.14990051090717316\n",
            "Test:  Iteration No:  300 \n",
            " Loss:  0.1616989041713974\n",
            "Test accuracy:  95.19\n",
            "Training:  Iteration No:  301 Worker Num:  0 \n",
            " Loss:  0.1614198088645935\n",
            "Training:  Iteration No:  301 Worker Num:  1 \n",
            " Loss:  0.2279525101184845\n",
            "Training:  Iteration No:  301 Worker Num:  2 \n",
            " Loss:  0.17012111842632294\n",
            "Training:  Iteration No:  301 Worker Num:  3 \n",
            " Loss:  0.2708432376384735\n",
            "Training:  Iteration No:  301 Worker Num:  4 \n",
            " Loss:  0.18209563195705414\n",
            "Training:  Iteration No:  301 Worker Num:  5 \n",
            " Loss:  0.14803966879844666\n",
            "Training:  Iteration No:  301 Worker Num:  6 \n",
            " Loss:  0.1394084244966507\n",
            "Training:  Iteration No:  301 Worker Num:  7 \n",
            " Loss:  0.13107895851135254\n",
            "Test:  Iteration No:  301 \n",
            " Loss:  0.1589723885578068\n",
            "Test accuracy:  95.29\n",
            "Training:  Iteration No:  302 Worker Num:  0 \n",
            " Loss:  0.12067805230617523\n",
            "Training:  Iteration No:  302 Worker Num:  1 \n",
            " Loss:  0.16965119540691376\n",
            "Training:  Iteration No:  302 Worker Num:  2 \n",
            " Loss:  0.16746771335601807\n",
            "Training:  Iteration No:  302 Worker Num:  3 \n",
            " Loss:  0.21065054833889008\n",
            "Training:  Iteration No:  302 Worker Num:  4 \n",
            " Loss:  0.25925207138061523\n",
            "Training:  Iteration No:  302 Worker Num:  5 \n",
            " Loss:  0.3030741810798645\n",
            "Training:  Iteration No:  302 Worker Num:  6 \n",
            " Loss:  0.2067616730928421\n",
            "Training:  Iteration No:  302 Worker Num:  7 \n",
            " Loss:  0.13066460192203522\n",
            "Test:  Iteration No:  302 \n",
            " Loss:  0.15951167048202664\n",
            "Test accuracy:  95.19\n",
            "Training:  Iteration No:  303 Worker Num:  0 \n",
            " Loss:  0.2562115788459778\n",
            "Training:  Iteration No:  303 Worker Num:  1 \n",
            " Loss:  0.19031213223934174\n",
            "Training:  Iteration No:  303 Worker Num:  2 \n",
            " Loss:  0.11768662184476852\n",
            "Training:  Iteration No:  303 Worker Num:  3 \n",
            " Loss:  0.1747143417596817\n",
            "Training:  Iteration No:  303 Worker Num:  4 \n",
            " Loss:  0.20431575179100037\n",
            "Training:  Iteration No:  303 Worker Num:  5 \n",
            " Loss:  0.15998125076293945\n",
            "Training:  Iteration No:  303 Worker Num:  6 \n",
            " Loss:  0.12275861203670502\n",
            "Training:  Iteration No:  303 Worker Num:  7 \n",
            " Loss:  0.17048026621341705\n",
            "Test:  Iteration No:  303 \n",
            " Loss:  0.15568286600770265\n",
            "Test accuracy:  95.45\n",
            "Training:  Iteration No:  304 Worker Num:  0 \n",
            " Loss:  0.1258542835712433\n",
            "Training:  Iteration No:  304 Worker Num:  1 \n",
            " Loss:  0.12819857895374298\n",
            "Training:  Iteration No:  304 Worker Num:  2 \n",
            " Loss:  0.15746405720710754\n",
            "Training:  Iteration No:  304 Worker Num:  3 \n",
            " Loss:  0.14492054283618927\n",
            "Training:  Iteration No:  304 Worker Num:  4 \n",
            " Loss:  0.34385785460472107\n",
            "Training:  Iteration No:  304 Worker Num:  5 \n",
            " Loss:  0.3174547851085663\n",
            "Training:  Iteration No:  304 Worker Num:  6 \n",
            " Loss:  0.17288102209568024\n",
            "Training:  Iteration No:  304 Worker Num:  7 \n",
            " Loss:  0.1883896440267563\n",
            "Test:  Iteration No:  304 \n",
            " Loss:  0.15863116099676\n",
            "Test accuracy:  95.15\n",
            "Training:  Iteration No:  305 Worker Num:  0 \n",
            " Loss:  0.1857975721359253\n",
            "Training:  Iteration No:  305 Worker Num:  1 \n",
            " Loss:  0.15677541494369507\n",
            "Training:  Iteration No:  305 Worker Num:  2 \n",
            " Loss:  0.2672578990459442\n",
            "Training:  Iteration No:  305 Worker Num:  3 \n",
            " Loss:  0.2021777331829071\n",
            "Training:  Iteration No:  305 Worker Num:  4 \n",
            " Loss:  0.21398697793483734\n",
            "Training:  Iteration No:  305 Worker Num:  5 \n",
            " Loss:  0.13012219965457916\n",
            "Training:  Iteration No:  305 Worker Num:  6 \n",
            " Loss:  0.153406023979187\n",
            "Training:  Iteration No:  305 Worker Num:  7 \n",
            " Loss:  0.10260271281003952\n",
            "Test:  Iteration No:  305 \n",
            " Loss:  0.15593781268601364\n",
            "Test accuracy:  95.32\n",
            "Training:  Iteration No:  306 Worker Num:  0 \n",
            " Loss:  0.2298174500465393\n",
            "Training:  Iteration No:  306 Worker Num:  1 \n",
            " Loss:  0.254778116941452\n",
            "Training:  Iteration No:  306 Worker Num:  2 \n",
            " Loss:  0.2171735316514969\n",
            "Training:  Iteration No:  306 Worker Num:  3 \n",
            " Loss:  0.1693379282951355\n",
            "Training:  Iteration No:  306 Worker Num:  4 \n",
            " Loss:  0.13387200236320496\n",
            "Training:  Iteration No:  306 Worker Num:  5 \n",
            " Loss:  0.14719876646995544\n",
            "Training:  Iteration No:  306 Worker Num:  6 \n",
            " Loss:  0.3020821511745453\n",
            "Training:  Iteration No:  306 Worker Num:  7 \n",
            " Loss:  0.17393428087234497\n",
            "Test:  Iteration No:  306 \n",
            " Loss:  0.15688795360583294\n",
            "Test accuracy:  95.36\n",
            "Training:  Iteration No:  307 Worker Num:  0 \n",
            " Loss:  0.13705475628376007\n",
            "Training:  Iteration No:  307 Worker Num:  1 \n",
            " Loss:  0.1951722353696823\n",
            "Training:  Iteration No:  307 Worker Num:  2 \n",
            " Loss:  0.17797504365444183\n",
            "Training:  Iteration No:  307 Worker Num:  3 \n",
            " Loss:  0.2584913671016693\n",
            "Training:  Iteration No:  307 Worker Num:  4 \n",
            " Loss:  0.23660248517990112\n",
            "Training:  Iteration No:  307 Worker Num:  5 \n",
            " Loss:  0.12987548112869263\n",
            "Training:  Iteration No:  307 Worker Num:  6 \n",
            " Loss:  0.1828845739364624\n",
            "Training:  Iteration No:  307 Worker Num:  7 \n",
            " Loss:  0.2097638100385666\n",
            "Test:  Iteration No:  307 \n",
            " Loss:  0.15543978556377602\n",
            "Test accuracy:  95.38\n",
            "Training:  Iteration No:  308 Worker Num:  0 \n",
            " Loss:  0.22856029868125916\n",
            "Training:  Iteration No:  308 Worker Num:  1 \n",
            " Loss:  0.23811538517475128\n",
            "Training:  Iteration No:  308 Worker Num:  2 \n",
            " Loss:  0.12907366454601288\n",
            "Training:  Iteration No:  308 Worker Num:  3 \n",
            " Loss:  0.2544907033443451\n",
            "Training:  Iteration No:  308 Worker Num:  4 \n",
            " Loss:  0.1781029999256134\n",
            "Training:  Iteration No:  308 Worker Num:  5 \n",
            " Loss:  0.1453397423028946\n",
            "Training:  Iteration No:  308 Worker Num:  6 \n",
            " Loss:  0.23601104319095612\n",
            "Training:  Iteration No:  308 Worker Num:  7 \n",
            " Loss:  0.08561914414167404\n",
            "Test:  Iteration No:  308 \n",
            " Loss:  0.153745040268036\n",
            "Test accuracy:  95.56\n",
            "Training:  Iteration No:  309 Worker Num:  0 \n",
            " Loss:  0.1673976480960846\n",
            "Training:  Iteration No:  309 Worker Num:  1 \n",
            " Loss:  0.19867460429668427\n",
            "Training:  Iteration No:  309 Worker Num:  2 \n",
            " Loss:  0.16753077507019043\n",
            "Training:  Iteration No:  309 Worker Num:  3 \n",
            " Loss:  0.12394067645072937\n",
            "Training:  Iteration No:  309 Worker Num:  4 \n",
            " Loss:  0.1389407068490982\n",
            "Training:  Iteration No:  309 Worker Num:  5 \n",
            " Loss:  0.22230273485183716\n",
            "Training:  Iteration No:  309 Worker Num:  6 \n",
            " Loss:  0.2506940960884094\n",
            "Training:  Iteration No:  309 Worker Num:  7 \n",
            " Loss:  0.1941804736852646\n",
            "Test:  Iteration No:  309 \n",
            " Loss:  0.15317862783263944\n",
            "Test accuracy:  95.61\n",
            "Training:  Iteration No:  310 Worker Num:  0 \n",
            " Loss:  0.17858131229877472\n",
            "Training:  Iteration No:  310 Worker Num:  1 \n",
            " Loss:  0.13915427029132843\n",
            "Training:  Iteration No:  310 Worker Num:  2 \n",
            " Loss:  0.20208264887332916\n",
            "Training:  Iteration No:  310 Worker Num:  3 \n",
            " Loss:  0.15359579026699066\n",
            "Training:  Iteration No:  310 Worker Num:  4 \n",
            " Loss:  0.11417117714881897\n",
            "Training:  Iteration No:  310 Worker Num:  5 \n",
            " Loss:  0.1477857381105423\n",
            "Training:  Iteration No:  310 Worker Num:  6 \n",
            " Loss:  0.18913650512695312\n",
            "Training:  Iteration No:  310 Worker Num:  7 \n",
            " Loss:  0.13213060796260834\n",
            "Test:  Iteration No:  310 \n",
            " Loss:  0.15438900169433106\n",
            "Test accuracy:  95.4\n",
            "Training:  Iteration No:  311 Worker Num:  0 \n",
            " Loss:  0.20222267508506775\n",
            "Training:  Iteration No:  311 Worker Num:  1 \n",
            " Loss:  0.17808058857917786\n",
            "Training:  Iteration No:  311 Worker Num:  2 \n",
            " Loss:  0.1495516002178192\n",
            "Training:  Iteration No:  311 Worker Num:  3 \n",
            " Loss:  0.23951473832130432\n",
            "Training:  Iteration No:  311 Worker Num:  4 \n",
            " Loss:  0.11159661412239075\n",
            "Training:  Iteration No:  311 Worker Num:  5 \n",
            " Loss:  0.33156272768974304\n",
            "Training:  Iteration No:  311 Worker Num:  6 \n",
            " Loss:  0.10127250105142593\n",
            "Training:  Iteration No:  311 Worker Num:  7 \n",
            " Loss:  0.11815310269594193\n",
            "Test:  Iteration No:  311 \n",
            " Loss:  0.15270825360512622\n",
            "Test accuracy:  95.46\n",
            "Training:  Iteration No:  312 Worker Num:  0 \n",
            " Loss:  0.3356415927410126\n",
            "Training:  Iteration No:  312 Worker Num:  1 \n",
            " Loss:  0.22235789895057678\n",
            "Training:  Iteration No:  312 Worker Num:  2 \n",
            " Loss:  0.11942432075738907\n",
            "Training:  Iteration No:  312 Worker Num:  3 \n",
            " Loss:  0.12408744543790817\n",
            "Training:  Iteration No:  312 Worker Num:  4 \n",
            " Loss:  0.13435378670692444\n",
            "Training:  Iteration No:  312 Worker Num:  5 \n",
            " Loss:  0.09497194737195969\n",
            "Training:  Iteration No:  312 Worker Num:  6 \n",
            " Loss:  0.23107194900512695\n",
            "Training:  Iteration No:  312 Worker Num:  7 \n",
            " Loss:  0.10305694490671158\n",
            "Test:  Iteration No:  312 \n",
            " Loss:  0.15374497679073976\n",
            "Test accuracy:  95.44\n",
            "Training:  Iteration No:  313 Worker Num:  0 \n",
            " Loss:  0.18866899609565735\n",
            "Training:  Iteration No:  313 Worker Num:  1 \n",
            " Loss:  0.1141120195388794\n",
            "Training:  Iteration No:  313 Worker Num:  2 \n",
            " Loss:  0.26267653703689575\n",
            "Training:  Iteration No:  313 Worker Num:  3 \n",
            " Loss:  0.13889294862747192\n",
            "Training:  Iteration No:  313 Worker Num:  4 \n",
            " Loss:  0.21617315709590912\n",
            "Training:  Iteration No:  313 Worker Num:  5 \n",
            " Loss:  0.23499906063079834\n",
            "Training:  Iteration No:  313 Worker Num:  6 \n",
            " Loss:  0.2359452247619629\n",
            "Training:  Iteration No:  313 Worker Num:  7 \n",
            " Loss:  0.1443728506565094\n",
            "Test:  Iteration No:  313 \n",
            " Loss:  0.15382038698755676\n",
            "Test accuracy:  95.42\n",
            "Training:  Iteration No:  314 Worker Num:  0 \n",
            " Loss:  0.2828839421272278\n",
            "Training:  Iteration No:  314 Worker Num:  1 \n",
            " Loss:  0.1332809031009674\n",
            "Training:  Iteration No:  314 Worker Num:  2 \n",
            " Loss:  0.11506465077400208\n",
            "Training:  Iteration No:  314 Worker Num:  3 \n",
            " Loss:  0.09735455363988876\n",
            "Training:  Iteration No:  314 Worker Num:  4 \n",
            " Loss:  0.2026577889919281\n",
            "Training:  Iteration No:  314 Worker Num:  5 \n",
            " Loss:  0.24914565682411194\n",
            "Training:  Iteration No:  314 Worker Num:  6 \n",
            " Loss:  0.14154809713363647\n",
            "Training:  Iteration No:  314 Worker Num:  7 \n",
            " Loss:  0.2114264965057373\n",
            "Test:  Iteration No:  314 \n",
            " Loss:  0.15120677261530788\n",
            "Test accuracy:  95.54\n",
            "Training:  Iteration No:  315 Worker Num:  0 \n",
            " Loss:  0.18617843091487885\n",
            "Training:  Iteration No:  315 Worker Num:  1 \n",
            " Loss:  0.18896864354610443\n",
            "Training:  Iteration No:  315 Worker Num:  2 \n",
            " Loss:  0.12075139582157135\n",
            "Training:  Iteration No:  315 Worker Num:  3 \n",
            " Loss:  0.11485064774751663\n",
            "Training:  Iteration No:  315 Worker Num:  4 \n",
            " Loss:  0.16256320476531982\n",
            "Training:  Iteration No:  315 Worker Num:  5 \n",
            " Loss:  0.21633905172348022\n",
            "Training:  Iteration No:  315 Worker Num:  6 \n",
            " Loss:  0.10843629390001297\n",
            "Training:  Iteration No:  315 Worker Num:  7 \n",
            " Loss:  0.08504749089479446\n",
            "Test:  Iteration No:  315 \n",
            " Loss:  0.15009288464472453\n",
            "Test accuracy:  95.59\n",
            "Training:  Iteration No:  316 Worker Num:  0 \n",
            " Loss:  0.13429120182991028\n",
            "Training:  Iteration No:  316 Worker Num:  1 \n",
            " Loss:  0.15442778170108795\n",
            "Training:  Iteration No:  316 Worker Num:  2 \n",
            " Loss:  0.15458950400352478\n",
            "Training:  Iteration No:  316 Worker Num:  3 \n",
            " Loss:  0.1368970423936844\n",
            "Training:  Iteration No:  316 Worker Num:  4 \n",
            " Loss:  0.14503315091133118\n",
            "Training:  Iteration No:  316 Worker Num:  5 \n",
            " Loss:  0.09552276134490967\n",
            "Training:  Iteration No:  316 Worker Num:  6 \n",
            " Loss:  0.17113527655601501\n",
            "Training:  Iteration No:  316 Worker Num:  7 \n",
            " Loss:  0.1631879061460495\n",
            "Test:  Iteration No:  316 \n",
            " Loss:  0.149835350999868\n",
            "Test accuracy:  95.69\n",
            "Training:  Iteration No:  317 Worker Num:  0 \n",
            " Loss:  0.12452474236488342\n",
            "Training:  Iteration No:  317 Worker Num:  1 \n",
            " Loss:  0.18446867167949677\n",
            "Training:  Iteration No:  317 Worker Num:  2 \n",
            " Loss:  0.15674258768558502\n",
            "Training:  Iteration No:  317 Worker Num:  3 \n",
            " Loss:  0.17621833086013794\n",
            "Training:  Iteration No:  317 Worker Num:  4 \n",
            " Loss:  0.274990439414978\n",
            "Training:  Iteration No:  317 Worker Num:  5 \n",
            " Loss:  0.2279161959886551\n",
            "Training:  Iteration No:  317 Worker Num:  6 \n",
            " Loss:  0.12412941455841064\n",
            "Training:  Iteration No:  317 Worker Num:  7 \n",
            " Loss:  0.18688713014125824\n",
            "Test:  Iteration No:  317 \n",
            " Loss:  0.14955430643786263\n",
            "Test accuracy:  95.63\n",
            "Training:  Iteration No:  318 Worker Num:  0 \n",
            " Loss:  0.20732557773590088\n",
            "Training:  Iteration No:  318 Worker Num:  1 \n",
            " Loss:  0.12959283590316772\n",
            "Training:  Iteration No:  318 Worker Num:  2 \n",
            " Loss:  0.17346417903900146\n",
            "Training:  Iteration No:  318 Worker Num:  3 \n",
            " Loss:  0.1276126205921173\n",
            "Training:  Iteration No:  318 Worker Num:  4 \n",
            " Loss:  0.19925296306610107\n",
            "Training:  Iteration No:  318 Worker Num:  5 \n",
            " Loss:  0.1804027110338211\n",
            "Training:  Iteration No:  318 Worker Num:  6 \n",
            " Loss:  0.18474580347537994\n",
            "Training:  Iteration No:  318 Worker Num:  7 \n",
            " Loss:  0.1168326660990715\n",
            "Test:  Iteration No:  318 \n",
            " Loss:  0.15030919995654043\n",
            "Test accuracy:  95.64\n",
            "Training:  Iteration No:  319 Worker Num:  0 \n",
            " Loss:  0.1202349066734314\n",
            "Training:  Iteration No:  319 Worker Num:  1 \n",
            " Loss:  0.322630375623703\n",
            "Training:  Iteration No:  319 Worker Num:  2 \n",
            " Loss:  0.07693726569414139\n",
            "Training:  Iteration No:  319 Worker Num:  3 \n",
            " Loss:  0.19469435513019562\n",
            "Training:  Iteration No:  319 Worker Num:  4 \n",
            " Loss:  0.22074350714683533\n",
            "Training:  Iteration No:  319 Worker Num:  5 \n",
            " Loss:  0.2651975452899933\n",
            "Training:  Iteration No:  319 Worker Num:  6 \n",
            " Loss:  0.21847042441368103\n",
            "Training:  Iteration No:  319 Worker Num:  7 \n",
            " Loss:  0.17090560495853424\n",
            "Test:  Iteration No:  319 \n",
            " Loss:  0.14779613553133758\n",
            "Test accuracy:  95.67\n",
            "Training:  Iteration No:  320 Worker Num:  0 \n",
            " Loss:  0.2378953993320465\n",
            "Training:  Iteration No:  320 Worker Num:  1 \n",
            " Loss:  0.24329613149166107\n",
            "Training:  Iteration No:  320 Worker Num:  2 \n",
            " Loss:  0.18596883118152618\n",
            "Training:  Iteration No:  320 Worker Num:  3 \n",
            " Loss:  0.1844080537557602\n",
            "Training:  Iteration No:  320 Worker Num:  4 \n",
            " Loss:  0.1157781183719635\n",
            "Training:  Iteration No:  320 Worker Num:  5 \n",
            " Loss:  0.0909402146935463\n",
            "Training:  Iteration No:  320 Worker Num:  6 \n",
            " Loss:  0.17239992320537567\n",
            "Training:  Iteration No:  320 Worker Num:  7 \n",
            " Loss:  0.1394091546535492\n",
            "Test:  Iteration No:  320 \n",
            " Loss:  0.14837708827579701\n",
            "Test accuracy:  95.67\n",
            "Training:  Iteration No:  321 Worker Num:  0 \n",
            " Loss:  0.1593448519706726\n",
            "Training:  Iteration No:  321 Worker Num:  1 \n",
            " Loss:  0.21695099771022797\n",
            "Training:  Iteration No:  321 Worker Num:  2 \n",
            " Loss:  0.20272959768772125\n",
            "Training:  Iteration No:  321 Worker Num:  3 \n",
            " Loss:  0.13079231977462769\n",
            "Training:  Iteration No:  321 Worker Num:  4 \n",
            " Loss:  0.2675899267196655\n",
            "Training:  Iteration No:  321 Worker Num:  5 \n",
            " Loss:  0.13667771220207214\n",
            "Training:  Iteration No:  321 Worker Num:  6 \n",
            " Loss:  0.1428542137145996\n",
            "Training:  Iteration No:  321 Worker Num:  7 \n",
            " Loss:  0.14941054582595825\n",
            "Test:  Iteration No:  321 \n",
            " Loss:  0.14550357109784515\n",
            "Test accuracy:  95.59\n",
            "Training:  Iteration No:  322 Worker Num:  0 \n",
            " Loss:  0.14559920132160187\n",
            "Training:  Iteration No:  322 Worker Num:  1 \n",
            " Loss:  0.20233245193958282\n",
            "Training:  Iteration No:  322 Worker Num:  2 \n",
            " Loss:  0.12397392094135284\n",
            "Training:  Iteration No:  322 Worker Num:  3 \n",
            " Loss:  0.20266421139240265\n",
            "Training:  Iteration No:  322 Worker Num:  4 \n",
            " Loss:  0.14697763323783875\n",
            "Training:  Iteration No:  322 Worker Num:  5 \n",
            " Loss:  0.07333197444677353\n",
            "Training:  Iteration No:  322 Worker Num:  6 \n",
            " Loss:  0.16541467607021332\n",
            "Training:  Iteration No:  322 Worker Num:  7 \n",
            " Loss:  0.10513029992580414\n",
            "Test:  Iteration No:  322 \n",
            " Loss:  0.14535652899147966\n",
            "Test accuracy:  95.7\n",
            "Training:  Iteration No:  323 Worker Num:  0 \n",
            " Loss:  0.22396588325500488\n",
            "Training:  Iteration No:  323 Worker Num:  1 \n",
            " Loss:  0.24178749322891235\n",
            "Training:  Iteration No:  323 Worker Num:  2 \n",
            " Loss:  0.10021062195301056\n",
            "Training:  Iteration No:  323 Worker Num:  3 \n",
            " Loss:  0.20673106610774994\n",
            "Training:  Iteration No:  323 Worker Num:  4 \n",
            " Loss:  0.21521413326263428\n",
            "Training:  Iteration No:  323 Worker Num:  5 \n",
            " Loss:  0.16411317884922028\n",
            "Training:  Iteration No:  323 Worker Num:  6 \n",
            " Loss:  0.2043277472257614\n",
            "Training:  Iteration No:  323 Worker Num:  7 \n",
            " Loss:  0.1796899139881134\n",
            "Test:  Iteration No:  323 \n",
            " Loss:  0.1456178691108487\n",
            "Test accuracy:  95.72\n",
            "Training:  Iteration No:  324 Worker Num:  0 \n",
            " Loss:  0.09850037097930908\n",
            "Training:  Iteration No:  324 Worker Num:  1 \n",
            " Loss:  0.17749035358428955\n",
            "Training:  Iteration No:  324 Worker Num:  2 \n",
            " Loss:  0.15566807985305786\n",
            "Training:  Iteration No:  324 Worker Num:  3 \n",
            " Loss:  0.2423085868358612\n",
            "Training:  Iteration No:  324 Worker Num:  4 \n",
            " Loss:  0.21277599036693573\n",
            "Training:  Iteration No:  324 Worker Num:  5 \n",
            " Loss:  0.3176652193069458\n",
            "Training:  Iteration No:  324 Worker Num:  6 \n",
            " Loss:  0.2882883548736572\n",
            "Training:  Iteration No:  324 Worker Num:  7 \n",
            " Loss:  0.12942755222320557\n",
            "Test:  Iteration No:  324 \n",
            " Loss:  0.14769414970887046\n",
            "Test accuracy:  95.66\n",
            "Training:  Iteration No:  325 Worker Num:  0 \n",
            " Loss:  0.11883655935525894\n",
            "Training:  Iteration No:  325 Worker Num:  1 \n",
            " Loss:  0.22773152589797974\n",
            "Training:  Iteration No:  325 Worker Num:  2 \n",
            " Loss:  0.2347640097141266\n",
            "Training:  Iteration No:  325 Worker Num:  3 \n",
            " Loss:  0.1562919169664383\n",
            "Training:  Iteration No:  325 Worker Num:  4 \n",
            " Loss:  0.1901089996099472\n",
            "Training:  Iteration No:  325 Worker Num:  5 \n",
            " Loss:  0.18770594894886017\n",
            "Training:  Iteration No:  325 Worker Num:  6 \n",
            " Loss:  0.25373709201812744\n",
            "Training:  Iteration No:  325 Worker Num:  7 \n",
            " Loss:  0.14580360054969788\n",
            "Test:  Iteration No:  325 \n",
            " Loss:  0.1457611987414428\n",
            "Test accuracy:  95.65\n",
            "Training:  Iteration No:  326 Worker Num:  0 \n",
            " Loss:  0.2418278157711029\n",
            "Training:  Iteration No:  326 Worker Num:  1 \n",
            " Loss:  0.20302850008010864\n",
            "Training:  Iteration No:  326 Worker Num:  2 \n",
            " Loss:  0.1838165819644928\n",
            "Training:  Iteration No:  326 Worker Num:  3 \n",
            " Loss:  0.21916674077510834\n",
            "Training:  Iteration No:  326 Worker Num:  4 \n",
            " Loss:  0.1729038655757904\n",
            "Training:  Iteration No:  326 Worker Num:  5 \n",
            " Loss:  0.16121599078178406\n",
            "Training:  Iteration No:  326 Worker Num:  6 \n",
            " Loss:  0.2353021204471588\n",
            "Training:  Iteration No:  326 Worker Num:  7 \n",
            " Loss:  0.18319103121757507\n",
            "Test:  Iteration No:  326 \n",
            " Loss:  0.14378469634235283\n",
            "Test accuracy:  95.75\n",
            "Training:  Iteration No:  327 Worker Num:  0 \n",
            " Loss:  0.21663574874401093\n",
            "Training:  Iteration No:  327 Worker Num:  1 \n",
            " Loss:  0.09221631288528442\n",
            "Training:  Iteration No:  327 Worker Num:  2 \n",
            " Loss:  0.22471648454666138\n",
            "Training:  Iteration No:  327 Worker Num:  3 \n",
            " Loss:  0.2491273283958435\n",
            "Training:  Iteration No:  327 Worker Num:  4 \n",
            " Loss:  0.1353725790977478\n",
            "Training:  Iteration No:  327 Worker Num:  5 \n",
            " Loss:  0.12876957654953003\n",
            "Training:  Iteration No:  327 Worker Num:  6 \n",
            " Loss:  0.19807420670986176\n",
            "Training:  Iteration No:  327 Worker Num:  7 \n",
            " Loss:  0.2563979923725128\n",
            "Test:  Iteration No:  327 \n",
            " Loss:  0.14423401272985376\n",
            "Test accuracy:  95.72\n",
            "Training:  Iteration No:  328 Worker Num:  0 \n",
            " Loss:  0.14626608788967133\n",
            "Training:  Iteration No:  328 Worker Num:  1 \n",
            " Loss:  0.09607911109924316\n",
            "Training:  Iteration No:  328 Worker Num:  2 \n",
            " Loss:  0.22096067667007446\n",
            "Training:  Iteration No:  328 Worker Num:  3 \n",
            " Loss:  0.21356509625911713\n",
            "Training:  Iteration No:  328 Worker Num:  4 \n",
            " Loss:  0.27008262276649475\n",
            "Training:  Iteration No:  328 Worker Num:  5 \n",
            " Loss:  0.17181046307086945\n",
            "Training:  Iteration No:  328 Worker Num:  6 \n",
            " Loss:  0.12792377173900604\n",
            "Training:  Iteration No:  328 Worker Num:  7 \n",
            " Loss:  0.19569042325019836\n",
            "Test:  Iteration No:  328 \n",
            " Loss:  0.1448324693166474\n",
            "Test accuracy:  95.8\n",
            "Training:  Iteration No:  329 Worker Num:  0 \n",
            " Loss:  0.2119479775428772\n",
            "Training:  Iteration No:  329 Worker Num:  1 \n",
            " Loss:  0.16484104096889496\n",
            "Training:  Iteration No:  329 Worker Num:  2 \n",
            " Loss:  0.24017111957073212\n",
            "Training:  Iteration No:  329 Worker Num:  3 \n",
            " Loss:  0.10573634505271912\n",
            "Training:  Iteration No:  329 Worker Num:  4 \n",
            " Loss:  0.1756237894296646\n",
            "Training:  Iteration No:  329 Worker Num:  5 \n",
            " Loss:  0.23493973910808563\n",
            "Training:  Iteration No:  329 Worker Num:  6 \n",
            " Loss:  0.1750170886516571\n",
            "Training:  Iteration No:  329 Worker Num:  7 \n",
            " Loss:  0.13926661014556885\n",
            "Test:  Iteration No:  329 \n",
            " Loss:  0.14382649868538108\n",
            "Test accuracy:  95.82\n",
            "Training:  Iteration No:  330 Worker Num:  0 \n",
            " Loss:  0.26497504115104675\n",
            "Training:  Iteration No:  330 Worker Num:  1 \n",
            " Loss:  0.09560976922512054\n",
            "Training:  Iteration No:  330 Worker Num:  2 \n",
            " Loss:  0.249376118183136\n",
            "Training:  Iteration No:  330 Worker Num:  3 \n",
            " Loss:  0.1854453831911087\n",
            "Training:  Iteration No:  330 Worker Num:  4 \n",
            " Loss:  0.22317631542682648\n",
            "Training:  Iteration No:  330 Worker Num:  5 \n",
            " Loss:  0.11424516886472702\n",
            "Training:  Iteration No:  330 Worker Num:  6 \n",
            " Loss:  0.18489159643650055\n",
            "Training:  Iteration No:  330 Worker Num:  7 \n",
            " Loss:  0.15922114253044128\n",
            "Test:  Iteration No:  330 \n",
            " Loss:  0.14471398846871114\n",
            "Test accuracy:  95.76\n",
            "Training:  Iteration No:  331 Worker Num:  0 \n",
            " Loss:  0.16828949749469757\n",
            "Training:  Iteration No:  331 Worker Num:  1 \n",
            " Loss:  0.13985827565193176\n",
            "Training:  Iteration No:  331 Worker Num:  2 \n",
            " Loss:  0.2475464642047882\n",
            "Training:  Iteration No:  331 Worker Num:  3 \n",
            " Loss:  0.07900120317935944\n",
            "Training:  Iteration No:  331 Worker Num:  4 \n",
            " Loss:  0.09883014112710953\n",
            "Training:  Iteration No:  331 Worker Num:  5 \n",
            " Loss:  0.11183779686689377\n",
            "Training:  Iteration No:  331 Worker Num:  6 \n",
            " Loss:  0.16701489686965942\n",
            "Training:  Iteration No:  331 Worker Num:  7 \n",
            " Loss:  0.2068803310394287\n",
            "Test:  Iteration No:  331 \n",
            " Loss:  0.14229639645524417\n",
            "Test accuracy:  95.76\n",
            "Training:  Iteration No:  332 Worker Num:  0 \n",
            " Loss:  0.18865059316158295\n",
            "Training:  Iteration No:  332 Worker Num:  1 \n",
            " Loss:  0.13264918327331543\n",
            "Training:  Iteration No:  332 Worker Num:  2 \n",
            " Loss:  0.24362237751483917\n",
            "Training:  Iteration No:  332 Worker Num:  3 \n",
            " Loss:  0.16496574878692627\n",
            "Training:  Iteration No:  332 Worker Num:  4 \n",
            " Loss:  0.10484680533409119\n",
            "Training:  Iteration No:  332 Worker Num:  5 \n",
            " Loss:  0.3464256227016449\n",
            "Training:  Iteration No:  332 Worker Num:  6 \n",
            " Loss:  0.07348813861608505\n",
            "Training:  Iteration No:  332 Worker Num:  7 \n",
            " Loss:  0.15399153530597687\n",
            "Test:  Iteration No:  332 \n",
            " Loss:  0.1407426331970322\n",
            "Test accuracy:  95.91\n",
            "Training:  Iteration No:  333 Worker Num:  0 \n",
            " Loss:  0.0977230966091156\n",
            "Training:  Iteration No:  333 Worker Num:  1 \n",
            " Loss:  0.15465441346168518\n",
            "Training:  Iteration No:  333 Worker Num:  2 \n",
            " Loss:  0.1826854795217514\n",
            "Training:  Iteration No:  333 Worker Num:  3 \n",
            " Loss:  0.1563011109828949\n",
            "Training:  Iteration No:  333 Worker Num:  4 \n",
            " Loss:  0.13508805632591248\n",
            "Training:  Iteration No:  333 Worker Num:  5 \n",
            " Loss:  0.17490273714065552\n",
            "Training:  Iteration No:  333 Worker Num:  6 \n",
            " Loss:  0.15732216835021973\n",
            "Training:  Iteration No:  333 Worker Num:  7 \n",
            " Loss:  0.1220148429274559\n",
            "Test:  Iteration No:  333 \n",
            " Loss:  0.14166051266373053\n",
            "Test accuracy:  95.84\n",
            "Training:  Iteration No:  334 Worker Num:  0 \n",
            " Loss:  0.1612914353609085\n",
            "Training:  Iteration No:  334 Worker Num:  1 \n",
            " Loss:  0.17488139867782593\n",
            "Training:  Iteration No:  334 Worker Num:  2 \n",
            " Loss:  0.20370648801326752\n",
            "Training:  Iteration No:  334 Worker Num:  3 \n",
            " Loss:  0.17347966134548187\n",
            "Training:  Iteration No:  334 Worker Num:  4 \n",
            " Loss:  0.21479293704032898\n",
            "Training:  Iteration No:  334 Worker Num:  5 \n",
            " Loss:  0.12284845858812332\n",
            "Training:  Iteration No:  334 Worker Num:  6 \n",
            " Loss:  0.1766400784254074\n",
            "Training:  Iteration No:  334 Worker Num:  7 \n",
            " Loss:  0.23278400301933289\n",
            "Test:  Iteration No:  334 \n",
            " Loss:  0.14403077290994645\n",
            "Test accuracy:  95.74\n",
            "Training:  Iteration No:  335 Worker Num:  0 \n",
            " Loss:  0.14312447607517242\n",
            "Training:  Iteration No:  335 Worker Num:  1 \n",
            " Loss:  0.09703060239553452\n",
            "Training:  Iteration No:  335 Worker Num:  2 \n",
            " Loss:  0.1656382828950882\n",
            "Training:  Iteration No:  335 Worker Num:  3 \n",
            " Loss:  0.2201952338218689\n",
            "Training:  Iteration No:  335 Worker Num:  4 \n",
            " Loss:  0.12855148315429688\n",
            "Training:  Iteration No:  335 Worker Num:  5 \n",
            " Loss:  0.10267503559589386\n",
            "Training:  Iteration No:  335 Worker Num:  6 \n",
            " Loss:  0.2321462482213974\n",
            "Training:  Iteration No:  335 Worker Num:  7 \n",
            " Loss:  0.2855331003665924\n",
            "Test:  Iteration No:  335 \n",
            " Loss:  0.13845164790348727\n",
            "Test accuracy:  95.87\n",
            "Training:  Iteration No:  336 Worker Num:  0 \n",
            " Loss:  0.12247548252344131\n",
            "Training:  Iteration No:  336 Worker Num:  1 \n",
            " Loss:  0.20963405072689056\n",
            "Training:  Iteration No:  336 Worker Num:  2 \n",
            " Loss:  0.1624542474746704\n",
            "Training:  Iteration No:  336 Worker Num:  3 \n",
            " Loss:  0.22527027130126953\n",
            "Training:  Iteration No:  336 Worker Num:  4 \n",
            " Loss:  0.1454215943813324\n",
            "Training:  Iteration No:  336 Worker Num:  5 \n",
            " Loss:  0.07243305444717407\n",
            "Training:  Iteration No:  336 Worker Num:  6 \n",
            " Loss:  0.2349213808774948\n",
            "Training:  Iteration No:  336 Worker Num:  7 \n",
            " Loss:  0.22874051332473755\n",
            "Test:  Iteration No:  336 \n",
            " Loss:  0.1430615402369088\n",
            "Test accuracy:  95.88\n",
            "Training:  Iteration No:  337 Worker Num:  0 \n",
            " Loss:  0.23160280287265778\n",
            "Training:  Iteration No:  337 Worker Num:  1 \n",
            " Loss:  0.20845504105091095\n",
            "Training:  Iteration No:  337 Worker Num:  2 \n",
            " Loss:  0.16906602680683136\n",
            "Training:  Iteration No:  337 Worker Num:  3 \n",
            " Loss:  0.21968406438827515\n",
            "Training:  Iteration No:  337 Worker Num:  4 \n",
            " Loss:  0.1981128305196762\n",
            "Training:  Iteration No:  337 Worker Num:  5 \n",
            " Loss:  0.2158447504043579\n",
            "Training:  Iteration No:  337 Worker Num:  6 \n",
            " Loss:  0.11967438459396362\n",
            "Training:  Iteration No:  337 Worker Num:  7 \n",
            " Loss:  0.1847149282693863\n",
            "Test:  Iteration No:  337 \n",
            " Loss:  0.1383709432144614\n",
            "Test accuracy:  95.84\n",
            "Training:  Iteration No:  338 Worker Num:  0 \n",
            " Loss:  0.1264071762561798\n",
            "Training:  Iteration No:  338 Worker Num:  1 \n",
            " Loss:  0.1662791222333908\n",
            "Training:  Iteration No:  338 Worker Num:  2 \n",
            " Loss:  0.10132719576358795\n",
            "Training:  Iteration No:  338 Worker Num:  3 \n",
            " Loss:  0.06234931945800781\n",
            "Training:  Iteration No:  338 Worker Num:  4 \n",
            " Loss:  0.1834128201007843\n",
            "Training:  Iteration No:  338 Worker Num:  5 \n",
            " Loss:  0.230077862739563\n",
            "Training:  Iteration No:  338 Worker Num:  6 \n",
            " Loss:  0.13451185822486877\n",
            "Training:  Iteration No:  338 Worker Num:  7 \n",
            " Loss:  0.29962724447250366\n",
            "Test:  Iteration No:  338 \n",
            " Loss:  0.13774706700026895\n",
            "Test accuracy:  96.04\n",
            "96 % accuracy reached!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(test_accuracy)\n",
        "plt.xlabel('Iteration Number')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Test accuracy');"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "01691044-fa55-43bd-c4f0-def934dc2ce1",
        "id": "41DwHj4rZNqI"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxddZ3/8dcn682+t033IoXSspayCa5lEBAHdFDBUXFGB51Rx3WU2XXm54zLzLjhqIyioAwgoIIbyDqCyNKWpYVSWrq3aZs0a5Ob3O3z++OchJAmvTdtk3uT+34+Hn3knHPvPfeTQ7jv+/1+z/kec3dEREQOpSDbBYiISO5TWIiISFoKCxERSUthISIiaSksREQkLYWFiIikpbAQEZG0FBYy5ZjZgWH/UmYWHbb+p4exv4fM7AMTUavIdFGU7QJExsvdKweXzWwr8AF3vy97FU0sMyty90S265D8ppaFTBtmVmBm15jZS2a238x+Ymb14WMRM/txuL3TzJ40s5lm9gXgNcC1Ycvk2jH2fZuZ7TGzLjP7nZktG/ZYmZn9p5ltCx9/xMzKwsfOM7NHw/fcYWbvC7e/ojVjZu8zs0eGrbuZfdjMNgIbw21fD/fRbWarzew1w55faGZ/F/7uPeHj88zsW2b2nyN+l7vM7BNHfsQlnygsZDr5KHAZ8DpgNtABfCt87CqgBpgHNAAfAqLu/vfAw8BH3L3S3T8yxr5/AywGZgBrgJuGPfYfwOnAq4F64DNAyswWhK/7JtAEnAo8PY7f5zLgLGBpuP5kuI964H+B28wsEj72SeBK4GKgGvhzoA+4AbjSzAoAzKwROD98vUjG1A0l08mHCD70dwKY2eeA7Wb2HiBOEBLHuvuzwOrx7Njdrx9cDvfbYWY1QA/BB/PZ7r4rfMqj4fPeBdzn7jeH2/eH/zL17+7ePqyGHw977D/N7B+A44FngA8An3H3DeHjzwy+p5l1ASuBe4ErgIfcfe846hBRy0KmlQXAz8Iun05gPZAEZgI/Au4BbjGz3Wb2ZTMrzmSnYRfPF8Munm5ga/hQY/gvArw0ykvnjbE9UztG1PFpM1sfdnV1ErSUGjN4rxuAd4fL7yY4FiLjorCQ6WQHcJG71w77F3H3Xe4ed/fPu/tSgu6iS4D3hq9LN/Xyu4BLCbpvaoCF4XYD2oB+4FVj1DPadoBeoHzY+qxRnjNUVzg+8RngHUCdu9cCXWEN6d7rx8ClZnYKcALw8zGeJzImhYVMJ98BvhCOFWBmTWZ2abj8BjM7ycwKgW6CbqlU+Lq9wDGH2G8VMEDQhVQO/NvgA+6eAq4H/svMZoetkHPMrJRgXON8M3uHmRWZWYOZnRq+9GngbWZWbmbHAu9P87tVAQmgFSgys38iGJsY9D3gX81ssQVONrOGsMadBOMdPwLucPdomvcSOYjCQqaTrwN3Ab81sx7gMYIBYgi+ud9OEBTrgf/j5e6YrwOXm1mHmX1jlP3eCGwDdgHPh/sd7tPAWoIP5HbgS0CBu28nGHD+VLj9aeCU8DVfBWIEQXUDrxwwH809wN3Ai2Et/byym+q/gJ8Avw1/x+8DZcMevwE4CXVByWEy3fxIZPozs9cSdEctcP1PL4dBLQuRaS4cyP8Y8D0FhRwuhYXINGZmJwCdQDPwtSyXI1PYhIWFmV1vZvvMbN2wbfVmdq+ZbQx/1oXbzcy+YWabzOxZM1s+UXWJ5BN3X+/uFe7+anfvznY9MnVNZMvih8CFI7ZdA9zv7ouB+8N1gIsIro5dDFwNfHsC6xIRkXGa0AFuM1sI/NLdTwzXNwCvd/cWM2smuJL0eDP7brh888jnHWr/jY2NvnDhwgmrX0RkOlq9enWbuzeN5zWTPd3HzGEBsIfgylqAObzyNMCd4baDwsLMriZofTB//nxWrVo1cdWKiExDZrZtvK/J2gB3eFbGuJs17n6du69w9xVNTeMKRhEROUyTHRZ7w+4nwp/7wu27COa2GTQ33CYiIjlgssPiLoKpogl/3jls+3vDs6LOBrrSjVeIiMjkmbAxCzO7GXg90GhmO4F/Br4I/MTM3k8wZcE7wqf/mmBahE0Ec/D/2UTVJSIi4zdhYeHuV47x0MpRnuvAhyeqFhEROTK6gltERNJSWIiISFoKCxGRHDN4sXRfLMFLrQdIppy+WIJt+3vZ0d7HV+55ga1tvZNak+7BLSIyTtFYkqQ7laXBR2hXNM6v17bQ3hvjj5bOxB3KSwrZ0d7Hc7u7Wburi5KiAooKjIFEcM+titJCNu49QHFhAU1VpWxp66U/nmRXZ5TegQQzqiJ098fpiyWpKCkkkfKh15rBrJoyFjZWTNrvrLAQkSnL3YklU0RjSWrLS2jpipJIOm0HBniptZeVS2ZQV1FCNJZkV2cfc+vKKSks4KkdHVSUFjG/vpx7n9/Lzo4o1WXFbNjTjTs8vLGNWTUR6stLWDa7mqd3dPLCnh5OaK4e+rafSDqLGit4qfUAndE4gzMnfeWeDQfVObO6NKwXSosLSCSdjr4YJ86uIRqPs3V/L801ERoqylixsI7qSDH7egaoKCnkhOZqnm/ppqiggKWzq4nGEpx1TAPHzayazEOtsBCR7OnojVEWfmte39KNAT0DCc47tpFYIsWdT+9mc+sBAOLJFM/u6uJ1xzXx4IZWNrceYCCRorK0iPbeGMtmV7NhTw+J1MsTQ1RHijhlXi2bW3vZ1RncTbbAYPApw5cBKkuLKCwwTppTwwt7unkx5dz93B6aqko5aU4NG/Z2Ux0pZl5dOfFkiq5onDef3Ex9RSl/dMJMasuL+d3GVmrKionGktSVl3D6gjpqy4sxM4Zz94O25bIpfae8FStWuOaGEpl8qfBDtKykkFe/qoG71+0hUlxIaVEBtz65g6aqUi47bQ5f+s0LdEXjXLBsFr0DQZ97VaSYhze2DX3Y1pYX0x9P0h9PDe1/Xn0ZO9qDD/eiAqOwwDCDSHEhnX1xmqpKefNJzSRTTntvjMUzK3lkYxsLGio4dV4NjZWlNFSWcssT29m47wBm8M4z5tHZF6d3IMHimZUkU/BCSzdvXDKDJc3V7Onq54TmqqEP8MHPxtXbOlg6u5rykunz3drMVrv7inG9RmEhIgAHBhLs6ogyo6qUNds7uPmJHcytK+PNJzfz/O5utrf3saO9j57+BG0HBti4L/jG31RVSmvPwNB+ZlSV0tkXJ5ZMUVZcyElza3hiSzulRUHffEtXPxedOIuasmIWNlTw9M5OqkqLeNOyWQBsbuvl+ke28JZTZvOaxY2csbAegOJCoy+W5BsPbOTtp8/l2BmT2w0znSgsRPJQT3+c9t4YFaVF/PeDL7FiYR37e2MUFxhb9vfSVFnK5rZentjSzu7OKAsaKuiPJ9ndGWXZ7Gpm15bx6Ev7ae+NATCntoyWrigzqiK098WIhYOqZcWFzK0ro7a8mLKSIi5cNotEKsUXfrWev3z9qzhxdg1PbmvnI284lu7+BI+9tJ+T59aweGYVrT0D1JQVU1xoxJIpSosKs3nI8p7CQmQK6uqLU1FaSFFhcCZ7a88A61u6KS0qYE5dGXu7B3hkYxspd06cU8ODG/ZRUVJIS1c/O9r7WN/SQyyZGnXfhQVGMuWUFBXwuuOamFNbxtpdXdSVlzCvvow12zvZ2tbL+SfM5NgZlRQVGF/49XoaK0u5/1OvI55M8eAL+zhzUT3z68tH7WOPJVKUFOks/KnkcMJi+nTCieSQaCxJUaGxp6ufXzy7m0tOms3cujLaegfY0trLY5vbeWDDPgoNnm/pZnZtGY0VQQug7cDLXTqDA7BmYATLkeICUimYVRNhQUM5V716AcfPqmZnRx+nL6hjfUs3Zy1qoKK0iEWNFfT0xyksMKoixRnVPru2jDl1ZdSUBc9/+4p5h3y+giI/qGUhMg6jncGyta2X7z2ymRUL6tm2v4812zt4eGMrVZFievrjpJygf76xgmd3dlJcWEAskaKqtIikO/Pry4kUF1JSWMCChnKWNFdzQnMVT23vZF93P+e8qoFjZ1TSVBnhD5v3c9aieuoqSrJ0BGQ6UDeUyDgkkik6+uJURYroisYpLiygsy/GlrZeHt7Yxm/WtXDczCqWzKqirKSIxzbvZ3NrL5WlhRQUGIVmVEWK2Ns9MHRaJgTB8K6z5rOnq59ZNRFWLpnBfz/0Eutbulm+oI49Xf3821tPYlZNhFTKqSgt0rdzmVTqhpK8lUim2N3Zz93PtfAny+cSTzob9vYwqzrC3LoyigsLePf3H8fdmVEdYSCeYtO+Hrbu76MqUkRPf+IV+ysqMF57XBO7O6M8vqWdWCLFiXOqOX1BLXu6+iktLqQ6bDkkU8633rWcqkgRpy+oo7iw4KAP/+vfVz+Zh0PkqFNYyJSSSKb43ye2s6sjyqnzaokUF2IG19yxlj3d/QDc9Ph2trf3MVqjuaKkkP29MUqLCmmuKeOCZbPY3RnsK+VOY2Ups2vLOGVuLWUlwRk7Hb0x9vUMcPwsnaop+UthIVnj7ry49wBP7+hg2ewabnp8G2cuqqezL05XNA7AlWfOp8CMpqpguoQfP7aNz/3i+aGzfAbNry/nc29ZStuBGNc+uIk3n9zMu89aQNuBAbbt7+XmJ3awfEEd37zytHHXWVdRojECyXsas5AJ5+788NGt/OrZFj66cjGJZIqHN7Zxx5qdQ90/I6ddGDQ4lrxyyUxOaK7it8/txXFuufoctu3vpSsapz+e5LzFTVSWFuHuPLe7m6XN1RQU2CtqCPY3daZXEJkoGrOQSbd2Zxfb2nuZVR3hqe2dHD+riq/cs4GqSBE7O6K09gxQVGhDoXDV9U9gFkyodv4JM7hg6SxS7nzuF8/xD29eysKGCpqqShlIJPnpml0MJJI0VJRy4x+2ct/6vQB86HWvor6ihPpRvu2bGSfOqRl1u4gcPoWFpNXVF2cgmWRGVYRkyvn8L57jNYubMOADNx7csmusLGFnR3BK6JuWzaQvlmTFwjriCeeXa1vYsKeblMM3r1w+NC7wtuVzDxoUPnlu7dDypy44jluf3ME1P13LJSc3T+jvKyIHUzeUvMKjL7VRHSnm2gc2sXV/Lxed2MxX73uRWdURLj6pmdXb2nlmZxfFhUY86SxsKOfDbziW329q40/PXkBPf5wzFtYf8gKw3Z3BNNLzG8rHXV/vQIKKUn3HETkSus5CDktXNE4q5ezsiPKWax8Bghu3FBbYQaeUApy+oI6iAuPxLe38x9tP4fLT5052ySJyBDRmIRl5fnc3P/j9Fi4+uZmZVRGu+sETQHBGEcAZC+v42MrjODCQ4JsPbOSzFy7hvdcHz/m7i5dw5ZnzqYoU09IVpbmmLGu/h4hMHoXFNNYXS9A7kKSpqpRUyrn2wU1E40lufHQrffEkt63eSV15MaVFhfTGEjyzo5N/e+tJvOus+UP7uPDEWaRSTm15MZ19cS4/fd5QF5OCQiR/KCymGXdnb/cA963fy61P7uD5lm7m15ezsKGcBze0AnDOMQ18+fKT+ejNT/Hszk5u+9AKZlZHKCywUQOgoMA479hGdnRERz0DSUSmP41ZTAM9/XGuvnE1zbUR/m9DK01VpbywpweAy0+fy/b9fTyxtZ1jGiu49YPn0FhZgpnR3R9n+/6+UU81HakvliCRcqoznLlURHKXxizyiLvzxbtf4I7VO2k7EBvaXlRg7O+NccnJzVxxxnzOW9xIMuV884GNvGZx49CV0ADVkeKMggKYVreUFJHx0yfAFJBKOWbQ0tXP7NoydnVG+cKvnufXa/dw5sJ62g60c/Yx9fzNm5ZQUVrIbat28qkLjhv6gC8sMD5+/nFZ/i1EZCpTWOSwTft6eGhDK9+4fyMnza3h95v2c917Tud/Ht7M2l1dfGzlYj5+/mIe2dTG0uZqGiqDVsM/XrI0y5WLyHSjMYsctWZ7B1d89zFiyRSVpUUcGAiud6gpK6YrGudfL13Ge85ZmN0iRWRK0pjFFDX87mu3r97JU9s7uPf5vcysKeW7717B7NoI1z+yhaWza/jqvS9y/Mwq3nHGoW91KSJyNCkssux3L7ZyzR3P8qdnL+DOp3excd8B3IMrqG98/6tZMqsagE9ecDwQXPcgIjLZFBZZ8MKebqojxZSXFPKBG1cRS6T4yj0bgOBU1yvPnE9pUcFQUIiIZJvCYpL19Me58GsPs6ixgr9eeSyxRIrTF9SxelsH/++yE3n32QuyXaKIyEEUFpPsfx/fDsCWtl4eeKGVxspSvv3u5dz46DZNyCciOasg/VPkSD27s5PP3P4MG/f2cMuTO4Dg2of/27CPNxzfxIyqCJ9+0/FEiguzXKmIyOjUsphg7s7nf/E8q7d18JNVO4FgVtcnt3bQ3Z/gDUtmZLlCEZH01LKYYE9u7WD1tg6uOuflsYi/fP2rgGBqjvMWN2arNBGRjGWlZWFmnwA+ADiwFvgzoBm4BWgAVgPvcffYmDuZIm59cgdVpUVcc9EJNFWVsr29j7MWNQBwxsJ6TcwnIlPCpIeFmc0B/hpY6u5RM/sJcAVwMfBVd7/FzL4DvB/49mTXdzT1xRLcva6FN5/cTFlJIR954+Khx/783EW8Rq0KEZkistUNVQSUmVkRUA60AG8Ebg8fvwG4LEu1HTW/fW4vvbEkb1t+8FlO//SWpRqvEJEpY9LDwt13Af8BbCcIiS6CbqdOdx+84fNOYM5orzezq81slZmtam1tnYySx+Uff76OHz22DYCfPrWLObVlnLmwPstViYgcmUkPCzOrAy4FFgGzgQrgwkxf7+7XufsKd1/R1NQ0QVUevh89to1//Pk62g4M8MjGVi47bTYFBZbtskREjkg2uqHOB7a4e6u7x4GfAucCtWG3FMBcYFcWajsiqdTLM/g+uaWdlMPKE2ZmsSIRkaMjG2GxHTjbzMotmGp1JfA88CBweficq4A7s1DbEenpTwwt3/XMbooKjKXNmt9JRKa+bIxZPE4wkL2G4LTZAuA64LPAJ81sE8Hps9+f7NqOVGf05TN9f7NuD8fPqtJV2SIyLWTlOgt3/2fgn0ds3gycmYVyjlgimSKRcjr74q/Yvmy2WhUiMj3oCu6j4GO3PM2Sf7ybzmgQFn/zpuM5prGCS06eneXKRESODs0NdYTcnV+tbQFgc+sBAN60bBYffsOx2SxLROSoUsviCG3d3ze0/PjmdgBqyzWFh4hMLwqLI/TIxpcvDHxsy34AasoUFiIyvagb6ghtb+8jUlxARUkR+3tjVJYWUVyoDBaR6UWfakdof2+MhopSljRXAWpViMj0pLA4Qu29MeorSrj4pGYAdnVGs1yRiMjRp7A4QoNh8SfhzLKaBkpEpiONWRyh/QdiHNtUSaS4kDs/fC6lxcpfEZl+9Mk2Tr9e28LuYV1Ngy0LgFPm1bJklq7aFpHpRy2LcWjtGeCvbloDwAv/eiHuEI0nqa8syXJlIiITS2GRoa/d9yKPbd4/tH7Hmp3s6eoHoKFCYSEi05vCIkNfu2/jQeutPQMA1FeUZqMkEZFJozGLw1BXXkzbgYGh9Xq1LERkmlNYZCCZciw8JfYbV55GU1Up/vJN8ajTXFAiMs0pLDLQ0x/HHf7pkqX88SmzaaoKup0KC4yPn7+YhQ0VWa5QRGRiacwiA4M3NRqcTbapMgiLU+bW8PHzj8taXSIik0UtiwwM3tRoKCzClsXM6kjWahIRmUwKiwx0hWFRUxYMZCssRCTfKCwy0NkXAw5uWQz+FBGZ7hQWGRhsWdSWDY5ZBC0KtSxEJF8oLDIwOMA9eK+KpbOrOWVuDSsW1GWzLBGRSaOzoTLQ2RenqrSIovAOePUVJdz5kfOyXJWIyORRyyIDndEYNbrwTkTymMIiA519cd0uVUTymsIiA3u6+pmhM59EJI8pLDLQ0hWlubYs22WIiGSNwiKNaCxJR1+c2TU6TVZE8pfCIo2WruAWqs01almISP5SWKTREt4Nr7lWLQsRyV8KizR2dwYti9lqWYhIHlNYpDHYspilMQsRyWMKizRauvppqCghUlyY7VJERLJGYZFGZ19M99gWkbynsEijuz9Ota7eFpE8p7BIozuaoDqi+RZFJL+lDQsze4uZHdVQMbNaM7vdzF4ws/Vmdo6Z1ZvZvWa2MfyZE/N/96hlISKSUcvincBGM/uymS05Su/7deBud18CnAKsB64B7nf3xcD94XrWdfcnqI4oLEQkv6UNC3d/N3Aa8BLwQzP7g5ldbWZVh/OGZlYDvBb4frj/mLt3ApcCN4RPuwG47HD2fzS5O93RONVl6oYSkfyWUfeSu3cDtwO3AM3AW4E1ZvbRw3jPRUAr8AMze8rMvmdmFcBMd28Jn7MHmDnai8OgWmVmq1pbWw/j7TMXjSdJpFwtCxHJe5mMWfyxmf0MeAgoBs5094sIuo8+dRjvWQQsB77t7qcBvYzocnJ3B3y0F7v7de6+wt1XNDU1HcbbZ647mgDQmIWI5L1M+lf+BPiqu/9u+EZ37zOz9x/Ge+4Edrr74+H67QRhsdfMmt29xcyagX2Hse+jqrs/uPe2WhYiku8y6Yb6HPDE4IqZlZnZQgB3v3+8b+jue4AdZnZ8uGkl8DxwF3BVuO0q4M7x7vto646GYaExCxHJc5l8Ct4GvHrYejLcdsYRvO9HgZvMrATYDPwZQXD9JGytbAPecQT7PyrUshARCWQSFkXuHhtccfdY+CF/2Nz9aWDFKA+tPJL9Hm0asxARCWTSDdVqZn88uGJmlwJtE1dS7ni5ZaFuKBHJb5l8Cn6IoMvoWsCAHcB7J7SqHDE4ZlGlbigRyXNpw8LdXwLONrPKcP3AhFeVI3r6E5QWFVBSpCm0RCS/ZdS/YmZvBpYBETMDwN3/ZQLrygk9AwkqS9UFJSKSyUV53yGYH+qjBN1QbwcWTHBdOaF3IEGFwkJEJKMB7le7+3uBDnf/PHAOcNzElpUbFBYiIoFMwqI//NlnZrOBOMH8UNPegYEElaW6naqISCZfm39hZrXAV4A1BHM2/c+EVpUjegeSNFbqlqoiIocMi/CmR/eHU4jfYWa/BCLu3jUp1WVZ70CCBQ3l2S5DRCTrDtkN5e4p4FvD1gfyJShgsBtKYxYiIpmMWdxvZn9ig+fM5hENcIuIBDIJiw8STBw4YGbdZtZjZt0TXFfWpVJObyypsBARIbMruA/r9qlTXV88CaCzoUREyCAszOy1o20feTOk6aZ3IJhxVi0LEZHMTp39m2HLEeBMYDXwxgmpKEccCMNCA9wiIpl1Q71l+LqZzQO+NmEV5YA12zt4238/CkBFicJCRORwplPdCZxwtAvJJXc9vXtoWd1QIiKZjVl8k+CqbQjC5VSCK7mnrdm1kaFldUOJiGQ2ZrFq2HICuNndfz9B9eSE3oHk0HKFzoYSEckoLG4H+t09CWBmhWZW7u59E1ta9vT0J4aWyzVmISKS2RXcQNmw9TLgvokpJzcM3nv7g687hpnVpVmuRkQk+zIJi8jwW6mGy9N6dr2e/jjHz6ziby86gTyc5URE5CCZhEWvmS0fXDGz04HoxJWUfd3RBFURdT+JiAzK5BPx48BtZrab4Laqswhuszpt9QzEmVEVSf9EEZE8kclFeU+a2RLg+HDTBnePT2xZ2dUdTfCqJrUsREQGpe2GMrMPAxXuvs7d1wGVZvZXE19a9vT0x6mOFGe7DBGRnJHJmMVfhHfKA8DdO4C/mLiSssvd6enXmIWIyHCZhEXh8BsfmVkhMG1vTB2NJ0mknOoytSxERAZl8vX5buBWM/tuuP5B4DcTV1J2DV6Qp5aFiMjLMvlE/CxwNfChcP1ZgjOipqXuaDB2rzELEZGXpe2GcvcU8DiwleBeFm8E1k9sWdnTrZaFiMhBxvxENLPjgCvDf23ArQDu/obJKS07+mK6Q56IyEiH+kR8AXgYuMTdNwGY2Scmpaos6osFM86WFWu2WRGRQYfqhnob0AI8aGb/Y2YrCa7gntaiYViUlygsREQGjRkW7v5zd78CWAI8SDDtxwwz+7aZXTBZBU62vqGwUDeUiMigTAa4e939f8N7cc8FniI4Q2paisbDbii1LEREhozrHtzu3uHu17n7yokqKNui4QC3xixERF42rrA4msI77j1lZr8M1xeZ2eNmtsnMbjWzrFwl3hdLUlRglBRl7dCIiOScbH4ifoxXXq/xJeCr7n4s0AG8PxtF9cWS6oISERkhK2FhZnOBNwPfC9eN4GK/28On3ABclo3aorGkzoQSERkhWy2LrwGfAVLhegPQ6e6JcH0nMGe0F5rZ1Wa2ysxWtba2HvXC+uJJnQklIjLCpIeFmV0C7HP31Yfz+nCAfYW7r2hqajrK1QUtCw1ui4i8Uja+Qp8L/LGZXQxEgGrg60CtmRWFrYu5wK4s1EY0ntCYhYjICJPesnD3v3X3ue6+ELgCeMDd/5Tgwr/Lw6ddBdw52bVBMMCtMQsRkVfKpfNDPwt80sw2EYxhfD8bRagbSkTkYFkdyXX3h4CHwuXNBFOgZ5VaFiIiB8ullkVOCK6z0NlQIiLDKSxGiMYS6oYSERlBYTGMuxONqxtKRGQkhcUwA4kUKdeMsyIiIykshtGNj0RERqewGKY3nJ5cYSEi8koKi2F+s3YPAMfNrMpyJSIiuUVhEYrGknz3dy9x7rENnDa/LtvliIjkFF1QELrp8W20HYjx3yuPy3YpIiI5Ry2L0A9+v5Wzj6nnzEX12S5FRCTnKCyAZMrZ3RXlrEUN2S5FRCQnKSyA7mgcd6gtL852KSIiOUlhAXT0xQCFhYjIWBQWQGc0DkBteUmWKxERyU0KC6BzsGVRppaFiMhoFBZAZ1/QsqhTy0JEZFQKC6Cjb7AbSi0LEZHRKCyArr4YZlAVUViIiIxGYUHQsqgpK6awwLJdiohITlJYEJwNpcFtEZGxKSwIzobSabMiImNTWBCcDaXBbRGRsSksgK5onGoNbouIjElhAfTFElSUarZ2EZGxKCyA3oEklaW6laqIyFjyPiySKScaT1JeopaFiMhY8j4s+mIJACrUshARGZPCIpYE0JiFiMgh5H1Y9A6ELQt1Q4mIjCnvw2KwZVFeom4oEZGx5H1YHAhbFu2jIlQAAAsDSURBVJXqhhIRGVPeh8XgAHe5wkJEZEx5Hxa9A+EAt7qhRETGlPdhoZaFiEh6eR8WB8KWRaXOhhIRGVPeh0VfOMBdpm4oEZExTXpYmNk8M3vQzJ43s+fM7GPh9nozu9fMNoY/6yajnt5YkpLCAkqK8j43RUTGlI1PyATwKXdfCpwNfNjMlgLXAPe7+2Lg/nB9wvUOJDTVh4hIGpMeFu7e4u5rwuUeYD0wB7gUuCF82g3AZZNRT28soUkERUTSyGrfi5ktBE4DHgdmuntL+NAeYOYYr7nazFaZ2arW1tYjrqFvIKmWhYhIGlkLCzOrBO4APu7u3cMfc3cHfLTXuft17r7C3Vc0NTUdcR29uvGRiEhaWQkLMysmCIqb3P2n4ea9ZtYcPt4M7JuMWjr6YtSU6ZaqIiKHko2zoQz4PrDe3f9r2EN3AVeFy1cBd05GPXu6+mmuiUzGW4mITFnZ6H85F3gPsNbMng63/R3wReAnZvZ+YBvwjokuJJZI0XYgxqzqsol+KxGRKW3Sw8LdHwFsjIdXTmYte7v7AZhVUzqZbysiMuXk9ZVoL4eFWhYiIoeS12HR0hWEhcYsREQOLa/DYk8YFjOrFRYiIoeS32HR3U95SSHVEV1nISJyKHkdFi1dUWZVRwjO5hURkbHkdVhsb+9jXn15tssQEcl5+R0W+/tY0KCwEBFJJ2/DorMvRnd/gvlqWYiIpJW3YbG9vQ9A3VAiIhnI+7BQN5SISHp5Gxbb9octizqFhYhIOnkbFjs7otRXlOheFiIiGcjbsNjdGWVOreaEEhHJRF6HxexaTfMhIpKJvAwLdw/DQi0LEZFM5GVYdEcT9MaS6oYSEclQXobFrs4ogFoWIiIZysuwaOlSWIiIjEdehsXuoZaFBrhFRDKRl2ExszrCHy2dSWOF7r0tIpKJvLwi7YJls7hg2axslyEiMmXkZctCRETGR2EhIiJpKSxERCQthYWIiKSlsBARkbQUFiIikpbCQkRE0lJYiIhIWubu2a7hsJlZK7DtMF/eCLQdxXImw1SsGaZm3ap5cqjmyTGy5gXu3jSeHUzpsDgSZrbK3Vdku47xmIo1w9SsWzVPDtU8OY5GzeqGEhGRtBQWIiKSVj6HxXXZLuAwTMWaYWrWrZonh2qeHEdcc96OWYiISObyuWUhIiIZUliIiEhaeRkWZnahmW0ws01mdk226xmLmW01s7Vm9rSZrQq31ZvZvWa2MfxZl+UarzezfWa2bti2UWu0wDfC4/6smS3PoZo/Z2a7wmP9tJldPOyxvw1r3mBmb8pSzfPM7EEze97MnjOzj4Xbc/ZYH6LmXD/WETN7wsyeCev+fLh9kZk9HtZ3q5mVhNtLw/VN4eMLc6jmH5rZlmHH+tRw+/j/Ptw9r/4BhcBLwDFACfAMsDTbdY1R61agccS2LwPXhMvXAF/Kco2vBZYD69LVCFwM/AYw4Gzg8Ryq+XPAp0d57tLwb6QUWBT+7RRmoeZmYHm4XAW8GNaWs8f6EDXn+rE2oDJcLgYeD4/hT4Arwu3fAf4yXP4r4Dvh8hXArTlU8w+By0d5/rj/PvKxZXEmsMndN7t7DLgFuDTLNY3HpcAN4fINwGVZrAV3/x3QPmLzWDVeCtzogceAWjNrnpxKXzZGzWO5FLjF3QfcfQuwieBvaFK5e4u7rwmXe4D1wBxy+Fgfouax5Mqxdnc/EK4Wh/8ceCNwe7h95LEe/G9wO7DSzGySygUOWfNYxv33kY9hMQfYMWx9J4f+A84mB35rZqvN7Opw20x3bwmX9wAzs1PaIY1VY64f+4+ETfLrh3Xv5VzNYTfHaQTfHqfEsR5RM+T4sTazQjN7GtgH3EvQyul098QotQ3VHT7eBTRMbsUH1+zug8f6C+Gx/qqZlY6sOZT2WOdjWEwl57n7cuAi4MNm9trhD3rQnszpc5+nQo2hbwOvAk4FWoD/zG45ozOzSuAO4OPu3j38sVw91qPUnPPH2t2T7n4qMJegdbMkyyWlNbJmMzsR+FuC2s8A6oHPHu7+8zEsdgHzhq3PDbflHHffFf7cB/yM4I9272BzMfy5L3sVjmmsGnP22Lv73vB/thTwP7zc/ZEzNZtZMcGH7k3u/tNwc04f69FqngrHepC7dwIPAucQdNUUhQ8Nr22o7vDxGmD/JJc6ZFjNF4Zdge7uA8APOIJjnY9h8SSwODyzoYRgQOquLNd0EDOrMLOqwWXgAmAdQa1XhU+7CrgzOxUe0lg13gW8NzwT42yga1gXSlaN6K99K8GxhqDmK8IzXhYBi4EnslCfAd8H1rv7fw17KGeP9Vg1T4Fj3WRmteFyGfBHBOMtDwKXh08beawH/xtcDjwQtvImzRg1vzDsi4QRjLEMP9bj+/uY7FH7XPhHcCbAiwT9kH+f7XrGqPEYgjNDngGeG6yToC/0fmAjcB9Qn+U6byboSogT9Hu+f6waCc68+FZ43NcCK3Ko5h+FNT0b/o/UPOz5fx/WvAG4KEs1n0fQxfQs8HT47+JcPtaHqDnXj/XJwFNhfeuAfwq3H0MQXpuA24DScHskXN8UPn5MDtX8QHis1wE/5uUzpsb996HpPkREJK187IYSEZFxUliIiEhaCgsREUlLYSEiImkpLEREJC2FhUw5ZnYg/LnQzN51lPf9dyPWHz1K+/1hONNqabjeaGZbj9K+X29mvzwa+xIZi8JCprKFwLjCYtgVuGN5RVi4+6vHWdOhJIE/P4r7OyrMrDDbNUjuU1jIVPZF4DXhPP2fCCdS+4qZPRlOnPZBGPrm/bCZ3QU8H277eThB43ODkzSa2ReBsnB/N4XbBlsxFu57nQX3GHnnsH0/ZGa3m9kLZnbTIWYc/RrwiZGBNbJlYGbXmtn7wuWtZvbvYU2rzGy5md1jZi+Z2YeG7abazH5lwX0gvmNmBeHrLzCzP5jZGjO7LZynaXC/XzKzNcDbj+Q/guSHdN+yRHLZNQT3RbgEIPzQ73L3M8Lunt+b2W/D5y4HTvRg6muAP3f39nBqhCfN7A53v8bMPuLBZGwjvY1g4rtTgMbwNb8LHzsNWAbsBn4PnAs8Mso+tofb3wP8Yhy/53Z3P9XMvkpwf4JzCa4aXkdwXwUI5vxZCmwD7gbeZmYPAf8AnO/uvWb2WeCTwL+Er9nvwUSVImkpLGQ6uQA42cwG5++pIZhfKAY8MSwoAP7azN4aLs8Ln3eoyd/OA2529yTB5H3/RzCTZ3e4750AFkwRvZDRwwLg3wnmFPrVOH6vwbnL1hJM19AD9JjZwOB8QGENm8Mabg7r7ScIkN+HjZ0S4A/D9nvrOGqQPKewkOnEgI+6+z2v2Gj2eqB3xPr5wDnu3hd+A48cwfsODFtOcoj/r9x9Yxgo7xi2OcEru4RH1jK4/9SI90oNe6+R8/Y4wfG4192vHKOc3jG2ixxEYxYylfUQ3K5z0D3AX1owLTZmdpwFM/aOVAN0hEGxhOC2koPig68f4WHgneG4SBPBrVkPd0bULwCfHra+DVgazrZaC6w8jH2eacFMygXAOwlaNo8B55rZsTA0k/Fxh1mz5DmFhUxlzwJJC25S/wngewQD2GvMbB3wXUb/ln83UGRm6wkGyR8b9th1wLODA9zD/Cx8v2cIZvL8jLvvOZyi3f05YM2w9R0E93deF/586jB2+yRwLcFU2luAn7l7K/A+4GYze5agCyrnb+IjuUmzzoqISFpqWYiISFoKCxERSUthISIiaSksREQkLYWFiIikpbAQEZG0FBYiIpLW/wc2EwpHEcPzTQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "avg_train_losses = []\n",
        "for i in range(len(train_losses[0])):\n",
        "    avg_loss = 0\n",
        "    for j in range(num_workers):\n",
        "        avg_loss += train_losses[j][i]\n",
        "\n",
        "    avg_train_losses.append(avg_loss / num_workers)    \n"
      ],
      "metadata": {
        "id": "zbQp119rZNqU"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_losses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9104785b-627f-4004-9ff6-7e242bb9f898",
        "id": "O4lsZW-PZNqV"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "338"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(avg_train_losses, label='train')\n",
        "plt.plot(test_losses[0:-20], label='test')\n",
        "plt.xlabel('Iteration Number')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Loss function');"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "b0db46c0-6586-49db-858c-b922e0df6985",
        "id": "Y_9it68bZNqV"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}